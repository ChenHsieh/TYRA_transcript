Let me share the video.
While Ziqi is preparing for the video, let me introduce Ziqi.
Hello everyone, I am your host today, Zhang Yanping.
Our speaker today is Yan Ziqi.
Yan Ziqi is a neuroscientist based in Colorado.
You will hear more about her in her presentation later.
She specializes in community detection.
You will hear more about her in her presentation later.
Let's talk about Ziqi's background.
When Ziqi was in college, she studied biology, chemistry, and biochemistry.
This reminds me of some of my online friends.
Online friends seem to do a lot of other things at the same time.
She was a network scientist who was delayed by chemistry,
and then she still had to be a network scientist.
After graduating from college,
Ziqi did a lot of work as a research assistant or a software engineer.
So she slowly turned to information technology.
From the perspective of information technology, she became a network scientist.
All the information is very helpful to the host.
She has a very complete personal website.
If you are interested, you can find it on the website.
Including her recent activities.
She has done a lot of public service or network science promotion activities.
There will be a schedule for her recent activities.
You can follow her schedule.
You can also read her articles.
Without further ado,
let me give a round of applause to welcome our speaker today, Ziqi.
Okay, I'll leave it to you.
Thank you all for listening to my speech.
I prepared about 50 slides today.
The goal is to finish in an hour.
The topic of this speech is
to do community analysis on a network called bipartite network.
A bipartite network is the one you see on the screen now.
It consists of blue and red.
A bipartite network is the one you see on the screen now.
A bipartite network is the one you see on the screen now.
Daniel Laramore and I
developed a statistical model
called Stochastic Block Model
to fit the network
and find the community in the network.
The most important thing is
I don't know the structure of the network yet.
On the left side of the picture,
you only know it's bipartite.
We want to know
whether each type of bipartite network
can be subdivided into smaller networks.
If it's possible,
we want to know how big the statistical model is.
Then I'll know how to divide it.
You can see the PDF slides and related tweets.
My goal today is to talk about
why we need to look at the large-scale network structure
and how to find these structures
and the related references.
In 1718,
when I was still an engineer,
Mr. Li Zhengde from the Department of Statistics
asked me to share with the students
in the Department of Statistics
what they did
in software engineering.
I designed a course
to let them use Python
to analyze their own network.
I had six hours to talk about it.
I designed a questionnaire.
I asked them to fill in their name,
school number,
and their friends in the class.
They could also
include their own attributes,
such as gender, age,
age,
and other things.
Sorry,
which page are we at now?
We're looking at the editor's screen.
It's the first page.
Or are you...
I'm already on the second page.
Sorry.
You might want to switch...
Are you playing it?
I'm playing it on my Keynote.
But it doesn't look like it.
Let me see.
Can you play it first
and then share?
And then share.
Play it first and then share.
Yes, play it first and then share.
You have to press...
Yes.
Sorry, let me see.
But I have a question.
After I pressed play,
I entered the full screen
and I couldn't see
BlueJeans anymore.
Yes, so the host has to check
if anyone is talking to you.
Don't worry about BlueJeans.
Okay, then I...
Let me try it this way.
I'll share my full screen.
This should be all of it.
Yes, this is all of it.
Okay, I'll press play now
and see if anyone jumps in.
This is my first page
and I just got to the second page.
Did anyone jump in?
Okay, this is what I just said.
Everyone still has it, right?
Okay, if you have any questions,
you can tell me
and you can interrupt me.
Okay, I just said
that I'll send out a questionnaire
and let them...
Can I?
Let the students
fill out the questionnaire
and list their names.
The point is
who they think is their friend
in the class.
Including some of their own information.
And then...
Because I'm going to teach them
to do some network analysis with Python.
So,
after I let them fill out the questionnaire,
there's that Google Form when they come in
and then I'll sort it out
into a CSV form like this.
CSV form.
And then...
In fact,
for example, ID is the person
who fills out the questionnaire.
And then ID of Acquaintances
is who is their friend.
So, for example,
in the first line,
you can connect a line
from node 1 to node 58.
And so on.
And then there's some
metadata about that node.
And then I can turn it into
a network on that class.
And the funny thing is
this class is a statistics class,
so most of it is statistics.
And then there are some students
from other classes.
And then the students from other classes
seem to make friends
with the students from other classes.
And then even the students from the statistics class
have a lot of statistics students.
And then the funny thing is
if you say a person is a friend,
he doesn't necessarily say you're a friend.
So this direction is pretty cruel.
And then...
What we see here
is a simple example of a network.
It's a simple one.
It's a connection between two points.
It's just a line.
And it's a directional network.
And this network also has metadata.
That is, nodes or edges
can have some other data
added to it.
Or we can go to the US Congress.
And then we can see
that every Congress
has a speech.
And then there's the speaker
and the text of the speech.
And then this is...
He can see
that as this member takes office
and then as the years go by,
and then everyone
in this member
on the word
Is there any change?
Or is it that
everyone is slowly forming a consensus?
So this is an example.
It's a bipartite example.
That is, there are members
and then there's his speech and text.
It's just speech, text, and members.
And it's dynamic
because this network will change.
And then we can also
see examples from, for example, brain science.
And then, for example,
this is a study of human connectome.
And then he...
He stimulated the brain in different places.
Stimulate the brain.
And then he can see
the reaction of the brain in different places.
And then, for example,
in the upper left corner of this picture,
he first marked
every place in the brain.
After that,
let's see that
every place and every place
in some...
some tasks
will have a common reaction.
And then if there is, it will be brighter.
If not, it will be darker.
And then after that,
he can put those
It looks like a
kind of structural thing.
And then he can
do community detection for him.
And then we can see if
we can know
which different brain areas
have similar reactions.
That's it.
In this work,
what we see is a...
Because it's that...
The reaction of two brain areas.
So it's probably
0 to 1.
It's a weighted network.
And every brain area
has a spatial distribution.
So it's also a spatial network.
And he...
Or you could say
it's a correlation matrix.
But in this,
we want to understand its structure.
At this time,
if...
The tool we use at this time.
Is there any statistical value?
It's very important.
Because we don't really know
what's behind it.
What's the structure of his ground truth?
Okay.
So I'm going to define
what a network is.
And then we're going to ask
some questions about it.
So the network is
a lot of nodes
entity.
And then
what about these nodes?
I have to choose
two nodes to connect a line.
And then call it edge.
And then such a collection.
I call it a network.
So we're going to ask some questions up there.
For example, there are a lot of
computer science questions.
For example, how to find
uh...
The smallest...
What is it called?
The smallest node collection
Uh...
Every node that's not that collection
There's at least one side
connected to the one you chose.
And so on.
A lot of these questions are
NP-hard.
It's because of the network.
This combination
There are many kinds of relationships.
So it becomes very difficult.
Or you can also be up there.
For example, you want to make a statistical
A new distribution.
And then this distribution may be related to some
Uh...
The nature of some of the systems you've observed.
So you can say, uh...
This distribution can be
It's a good way to...
Um...
Generate these properties.
That's also a statistical job.
And then, uh...
There may be some on this network.
For example,
For example, we now have
Now there's a infectious disease.
So we're going to have an infectious disease between people.
Spread on the Internet.
But the contact between people
There may also be changes.
So...
So I say it's dynamics.
On or off networks.
And then there are a lot of interesting questions on this.
Or, uh...
We can look at its structure.
For example, we...
Uh...
What we're looking at now is...
Uh...
For example, we can predict...
Um...
Uh...
For example, if this network changes.
Then we might be able to predict that.
Uh...
Two nodes are in some time.
It's easy to have...
Uh...
A new generation.
That's what it's called.
Uh...
Link prediction.
That's it.
It's related to the work of that machine learning.
And then, um...
What I'm doing right now is talking about.
Uh...
Um...
Large scale...
Uh...
Community structure.
Um...
My large scale is to say...
For example, my network is very large.
For example, there are 100 points.
And then my large scale is at least...
It's definitely bigger than that log 100.
That's it.
Maybe it's his, uh...
Uh...
10%.
This...
This is called a large scale network.
That, um...
And I just said...
To connect...
When I connect.
I can only connect two or two points.
But if you have some...
The structure of the system is...
Need...
Three or more things.
There's an interaction.
Then I can connect a line.
It's connected to three points.
They're called Hyper Edge.
Yes, I can have a higher order structure in it.
That's a lot of, uh...
Related work.
And then I recommend...
These...
These references, like this.
And then, um...
What about these studies?
Um...
A long time ago, in...
Mathematical Graph Theory.
Um...
There's a lot of work, but...
Um...
For the past 20 years, because...
Everyone uses the Internet, and then there's more data.
And then there's a lot of...
From the data, the way to ask questions.
And then to develop new methods.
And then maybe...
Um...
In this large scale structure, the earliest...
Uh, a pretty early article might be this.
Um...
This work is 2002.
About 20 years ago.
There are a lot of citations now.
Okay, and then...
What is a large scale structure?
It's, uh...
I said, for example, if you have 100 points.
If it's a network, then this network might be...
You might want to divide it into many groups.
You might want to divide it into 5 groups, 6 groups, 10 groups.
And then...
For example, if you divide it into 5 groups, it's 100 divided by 5, which is 20.
So it's a number that's comparable to 20 and 100.
This is a large scale.
And then there are a lot of structures in there.
For example, the one in the upper left corner is called Assortative.
It's more like the circle of friends we're familiar with.
It's, uh...
Friends live together.
And then a group of friends.
So the connection between that group is more dense.
It's denser than the group and the group.
But it can also be, for example.
It's actually not that dense when you define it as a group.
But when you define it as a group, it's very dense with other groups.
It's like the Bipartite Network we're talking about.
It's an Assortative Structure.
Or something like that.
So, um...
How do we quantify these things?
It's a central theme of today's talk.
Okay.
So, um...
In the past, it was in 2002.
Uh...
After it started.
In the past, you had to look at the structure of a static network.
Um...
We use a, uh...
It's called Quality Function.
Or if it's physics, it's called Observable.
It's called Modularity.
Um...
Let's look at it.
That is to say, uh...
This Modularity is the Q of B and A in the upper right corner.
This B is, uh...
B is, uh...
Which community does each point belong to?
For example, I have this network now.
It divides it into a red and a blue community.
That is, 0 and 1.
And then let's say this network has a point of N.
Then I am...
Then my B is a...
A column at the end of N.
And then the value of each column.
Either 0 or 1.
Like this.
So this is an expression.
Uh...
Now this system.
Which group does each node belong to?
It's called B.
And then that A is called...
It's called...
Correlation.
It means, uh...
Um...
If you draw...
If you have a point of N.
Then this A is a correction of N times 1.
And then...
Every point has a label.
Uh...
Every point has its vertex ID.
And then two points.
If there is...
Um...
If there is a connection.
You write it as 1.
If there is no connection, it is 0.
So this A is a 0 and 1 correction.
And then, um...
If my system is confirmed.
Then this A is a static thing.
So, uh...
This Modularity.
It's the A and B I just said.
Uh...
The function of B.
That's it.
And then, uh...
If A is static.
My job is to...
I'm going to try to change.
Every node's 0 and 1.
And then...
Uh...
After the change.
I want to see if my Q has gotten bigger.
That's it.
And then I hope I can change it many times.
And then...
Find the biggest Q.
And then...
After finding it.
Uh...
I'll say I found a community.
That's it.
Because Q has actually been said.
That is...
I...
Q...
Uh...
If a lot of...
Uh...
People in the same group are grouped together.
And then they have a lot of...
Connections between groups.
Then that Q will be big.
And then...
Connections between groups.
If...
A little earlier.
Then that Q will be...
Uh...
Uh...
That Q will be big.
Why is it so big?
It's because there's a lot in the group.
It's dense.
Or it's very scarce between groups.
And it's big.
So you can find one like this.
A good result of a community.
Okay, and then...
Uh...
But this...
Uh...
Later, some problems were found.
That is...
If you...
Uh...
Give this method to a random network.
That is, you know you are randomly generated.
That...
You can actually find one.
Pretty good modularity.
But that structure is fake.
So what am I going to do at this time?
Look at my structure.
And it's a very important question in terms of statistics.
In addition...
Uh...
There are a lot of...
Probably...
Five to ten...
Uh...
Five to fifteen years ago.
During this period.
There are a lot of...
Uh...
Related heuristic methods.
There are common problems.
For example...
Uh...
It will have a thing called Resolution Limit.
That is, if you have a very large network.
That...
After you fit it.
You may not be able to find some...
The small structure in this network.
It will be...
Washed away.
Then a more interesting question is...
Uh...
If...
Uh...
I give a network.
Then I just...
Use the modularity just now.
To fit it.
Then I want to find...
The good partition in this network.
Good...
Good partition, like this.
That...
I will find that...
There are a lot of good partitions.
This is an interesting job.
He...
Said...
Uh...
There are actually a lot of good partitions.
And it's a bit degenerate.
Every...
Good, good, almost like this.
Then...
So every time you run...
This...
For this system...
Run that...
Modularity.
In fact, you don't know what you see.
What is the structure?
So...
Usually everyone just runs a lot of times.
Then say...
Uh...
See if there is consensus.
Then say...
Uh...
After running so many times.
Maybe most of them are...
Find what kind of community.
Then I said...
I'll take this as my community.
That...
This is also a problem.
And what I just said is...
The same method you use on this network.
If you have different methods.
If you use the same network.
You may see the community in different ways.
Not quite the same.
And...
Uh...
If this system has some...
Uh...
For example, I know that...
Uh...
The class you just said.
Every class has...
He has...
He has...
Separate...
Two different classes.
Then I want to know...
Uh...
The structure of this network.
Is it...
There is a correlation with the class.
Then I will fix it.
Then I'll see...
How much correlation does that result have with that class?
And then...
Uh...
In many...
Uh...
There may be this kind of...
There really is this kind of correlation system.
Actually...
Get some oil.
Also found that...
He has no way to follow his...
Uh...
Those annotations of nodes.
Uh...
Just...
There is a correlation.
So...
So we don't know what the true model is.
So...
In this work...
Uh...
I would say that...
To use statistics...
To do the right thing.
And...
There may be no...
True model.
Good.
Then...
Our...
Our work...
The content will be concentrated in a...
Uh...
Uh...
Uh...
Probability generation model.
Called...
Random block model.
And then...
Uh...
Uh...
Because of this random block model...
Parameters...
Uh...
Uh...
There is a probability nature.
So I can do some inside...
Uh...
Statistical statements.
So I can...
Uh...
Have...
Uh...
The result can have statistical significance.
And then...
Uh...
The most important thing is that I want to use it as a...
No model.
To see if my system has a structure.
So...
Repeat it again.
Is...
My job...
Is to hope...
There is a...
Bipartite network on the left.
I hope to know...
How many groups should it be?
Good.
And then...
Uh...
The kind of...
We have now introduced SBM.
And then...
Uh...
In the kind of network structure that I just said...
In fact...
Uh...
In SBM...
Can have...
Can have a corresponding...
For example...
Uh...
Uh...
For example...
Uh...
In that SBM...
There is a...
There is a...
Parameter called...
Is...
How do you connect...
Group to group...
Or group to group...
Own...
The...
The probability.
And then...
This is a...
Uh...
And then...
This is a...
If you divide it into...
For example...
The picture on the far left...
If you divide it into five groups...
It is a...
5x5 matrix.
And then...
Uh...
5x5 matrix...
Uh...
Its diagonal part...
That is to say...
My first group and the first group...
What is the probability...
Between 0 and 1...
To...
To connect.
And then...
For example...
I want Assaultitative now...
So I might...
It might be closer to 1.
For example, 0.9.
And then connect.
And then...
And off-diagonal...
It might be...
A little smaller.
So I can do this...
Connect a...
A network like this...
Assaultitative.
So just now...
The kind of...
Um...
How to connect between groups...
This kind of matrix...
It is...
Uh...
Formalized...
What we just said...
This kind of...
Large-scale structure...
The...
The...
Concept...
And then...
Uh...
For this work...
Uh...
Now in...
Physical Review 1...
Is a...
Uh...
Physics...
And then...
Uh...
I think...
Our...
More important contribution is...
We have a...
Uh...
I...
I...
Highlight these things now...
And then...
Uh...
This is also the focus I will emphasize today.
And then...
Uh...
The first one is we...
We have...
After having said SBM just now...
We want to have...
Have a...
Bayesian...
The...
Uh...
Formulation...
So...
And then...
Uh...
So I will talk about...
How to...
Estimate...
The so-called SBM Ensemble.
And then...
We will have...
We will still...
Fit this...
Model...
And then...
But the way we fit...
We will use...
MCMC...
And then...
Dynamic Programming...
And finally...
Uh...
We would like to see...
The model we proposed...
In the end...
Is it really better in statistics?
So...
I will say some examples...
In terms of...
It is more sensitive...
Or...
More sensitive...
That is to say...
It...
Is more likely to...
Really...
More likely to detect...
Smaller...
Uh...
Network structure...
Uh...
With...
Um...
If it is with...
Um...
Not us...
Um...
If it is with...
Um...
Before my work...
The...
More general...
Method...
When...
It...
Usually has a higher...
Um...
Posterior probability...
Post-check probability...
So...
It is actually a...
More...
Um...
Accurate model...
Okay...
And then...
Um...
This work...
Was done...
Four years...
Probably since 2016...
And then...
At that time...
I was still in Beijing...
And then...
I...
Um...
Participated in a project...
Called...
aminer.org
It is a...
A bit like...
For...
Researchers...
Facebook...
And then...
Um...
I...
Was assigned the task...
To draw...
A...
Um...
Network...
Using...
Um...
The following paper method...
And then...
Um...
This network is to say...
Um...
Which researcher...
He often goes to...
Which meetings...
So...
I now have a researcher meeting...
A...
A...
A matrix...
And then...
I want to say...
I...
I...
I...
How do I distribute...
For example...
I choose a meeting now...
I can know...
Hey...
The researchers who often come to this meeting...
Those people...
Maybe...
Do something more similar...
But...
The data is often sparse...
So every time...
Factorize...
It seems to encounter some...
Um...
Divided by zero...
This kind of problem...
Then...
Can't do it...
Then...
Or I don't know...
I want to divide it into several groups...
Then...
Then...
I saw a...
Um...
Work related to SBM...
Then...
Um...
There is a...
There is a Python library...
Called GraphTool...
He...
Directly implement this SBM...
Then it feels very useful...
Then...
And then...
Um...
Right...
This map...
Um...
The work on the right side of this map...
In 2014...
He is...
Um...
Um...
Um...
Use...
Bipartite SBM to...
Um...
He proposed a...
A...
A kind of...
SBM called Bipartite SBM...
Then he said...
Um...
This kind of Bipartite SBM...
Can...
Strictly stipulate...
You can't find...
A network solution...
And this solution...
He is...
Not Bipartite...
Because it will violate...
Your assumption of data...
Like this...
So...
For example...
Um...
Um...
Um...
If I...
Want my system...
Um...
Now see this network...
If I want my system...
To fit...
A k equals 5...
Numbers...
Numbers of community...
Then...
If it is...
The original SBM...
He may find a result...
Yes...
Yes...
Now see like this...
But...
Um...
His...
He put that...
Um...
Different types...
Between...
Um...
Different types are linked together...
He is not a Bipartite system...
Then...
Whereas...
If...
We have one...
Can really distinguish different types...
Then you can find a solution...
He is Bipartite...
And he actually has...
Um...
Better...
Likelihood...
So this is the system...
The work of that paper...
Then I thought at that time...
Um...
I...
This paper does not seem to tell me...
How to choose...
That k...
That is...
In the end...
How to go...
Yes...
Then...
And at that time I want to apply...
Um...
Doctorate...
So I...
Um...
Write a...
Write an email to this article...
The first author is...
Laramore...
Now also become...
One of my boss...
Then...
Say...
I want to do this work with him...
Then I want to ask him...
Say...
In the end...
Um...
This work...
Um...
This work does not seem to tell us...
How to choose k...
That...
If...
Say...
Can we use...
Um...
This method...
To...
To choose k...
In this case...
We are equal to...
Um...
Extended his...
Past work...
This way...
Then later he...
Um...
Promised me...
Then we...
Connected...
Then we started...
A series of work...
Then...
Um...
Although this work is published in the physics journal...
But he...
Some features with...
Statistics with...
With...
Machine learning...
Related...
This way...
Then I...
On Twitter...
See...
This way...
Yes...
Then...
Um...
By the way...
I just talked to Yen-Ping...
Um...
The difference between pure numbers and hard numbers...
Then there is an interesting...
That tweet...
Um...
Can...
Connected...
This way...
Then...
My work...
This...
This...
This...
Parameter...
Theoretical...
Our work is...
Then...
Prediction...
Um...
It can be predicted...
But...
Prediction will encounter a problem...
That is...
Is it consistent?
Is it...
Because...
Because I never know...
My data is...
Um...
Unless a specific system...
But I may be hard to know...
My data is...
How is it really generated?
So...
How suitable is this model?
This data is...
It is not certain...
Then...
At that time...
Maybe prediction...
It will not be good...
Then...
We did not do anything...
Decision making...
Then...
The model can be explained...
Because it is a statistical model...
Then...
Um...
Decision...
It seems...
Not very sure...
Then...
But we do have...
Um...
Model...
Regularization...
Regularization is...
To standardize the model...
I don't want my model to be too complicated...
So I want to...
Know that...
I want to choose...
How much K...
So I have to...
Um...
Convince it...
Like this...
There is a theory...
Then...
Cosality...
It seems not...
Then...
We do provide code...
Then...
Scalability...
It seems also...
Ordinary...
Like this...
Because there is no special...
Then...
Not sure...
How to make it...
Become an online...
An...
An...
Algorithm...
Then...
In short...
This is our job...
Then...
My outline...
Um...
I will...
Introduce what is SBM...
Then...
Do Bayesian inference on it...
Then...
Um...
Contribution...
It is...
We have a model...
This model considers...
With...
Original model...
The difference is...
We have a...
Um...
The prior...
It is bipartite...
Then...
Um...
Um...
Um...
Then...
There is a search method...
It is...
With...
Dynamic programming...
With...
MCMC...
Multicolor...
Algorithm...
Related...
Then...
Um...
Um...
The previous work did not...
Say...
Use dynamic programming to...
Fit this...
Model...
Then...
Um...
Will talk about some...
Results in statistics...
Then...
Then...
Finally...
Some...
Outlook...
Okay...
So far...
Do you have any questions?
Um...
Um...
Ahem...
Sorry...
Okay...
It is...
Um...
I look at the dialogue window...
No...
Problem...
But there is a...
Comment...
He said...
SBM looks like...
Random model...
ERGM...
Similar...
Uh...
Do you want to comment?
I...
Always...
Not familiar...
ERGM...
That piece...
Correct...
I think it is similar...
I think he...
In...
Um...
Now...
I will talk about this slide...
Say...
Um...
Uh...
My...
How is BN generated?
But I think...
He...
If I...
If my impression is correct...
I think he is...
Um...
For BN...
How to generate...
Statistics assumption...
Not the same...
And...
There seems to be no...
The concept of community inside...
But...
Just...
The...
Um...
Um...
I have not...
I have not studied that aspect...
ERGM...
Okay...
Then I...
Oh...
That...
Sorry...
There is another...
Now there is a person...
Asked a question...
Speaking of this...
Um...
That...
Let me finish the question first...
That...
That...
Jeffrey Wong is...
Originally asked the ERGM...
He said...
Okay, thank you...
Then he will continue to listen...
Then...
Win...
Win said...
Uh...
Yes...
ERGM...
It does not look like a community...
But that Peter...
Asked a question...
That is...
Oh...
By the way...
Before asking this question...
Um...
If you have any questions...
Of course...
I can also read it...
Oh...
Peter asked the question...
Please ask...
Uh...
If there is no...
Ground truth...
How to...
Validate the generated model...
Um...
If you want to answer Peter's question...
I will say...
Um...
We must...
Um...
When comparing models...
Those models have the same...
Assumption...
Then...
At this time...
Um...
If it is Bayesian framework...
I can actually look directly...
One thing called...
Uh...
Posterior probability...
Posterior probability...
Posterior probability...
Um...
Then...
Um...
If it is high...
I will say...
Um...
This model is more like...
The model used to generate this data...
Um...
This way...
Um...
Um...
Um...
Um...
So it is a...
From...
Model as the center...
To start...
To understand the data...
This way...
Um...
Ok...
He said...
Ok, thank you...
Ok, do you have any questions?
If...
Uh...
I have a question...
That is...
Please say...
Uh...
I still don't understand...
Why must...
Be related to bipartite...
Because it looks like...
Is...
Is in classification...
Similar to...
Is to divide community...
Then why...
Must be a bipartite...
Network...
Um...
Um...
Um...
The context is that...
Um...
Um...
At...
Um...
Before our work...
Before work method...
This is the original SBN...
In fact, it can...
Take it to...
Feed...
Bipartite network...
This way...
And then...
Um...
But, in the original SBN...
Um...
It is also at the base end...
But, because there is...
The assumption of priority...
It allows...
Uh...
Do not bipartite...
This thing happens...
So, um...
So, um...
It can...
It can be predicted, but...
Because...
The system is bipartite...
It will not appear...
That kind...
You...
Think it should have...
Priority...
Those spaces...
So, at this time...
It will have...
Some...
Waste...
For example, it may not be able to...
Have a better...
Uh...
Resolution...
Then...
Mainly this part...
Oh...
OK...
Thank you...
So...
Um...
Next, I will talk about...
We...
Um...
How to modify the original...
Uh...
The original SBN work...
Then...
Which places...
Actually...
We...
This kind of prior...
It is more frugal...
Or this kind of prior...
Is more...
More...
More...
More...
Sensitive...
So...
Thank you...
Um...
OK...
OK...
Then we will continue...
Thank you...
Um...
Continue...
Then...
OK...
So, what I want to talk about now is...
Uh...
A certain version of SBN...
Then...
Um...
First, SBN is a...
Generated model...
That is...
You tell it some...
Um...
After the parameters...
You can say...
I want to...
Like a dice...
Make a...
The dice is to make a point...
But...
SBN is to generate a network...
That's it...
And this network will have...
What you just said...
The odds you want...
Then...
Um...
Um...
So...
So...
Um...
What are the parameters in it?
It has...
Um...
The upper right corner...
That...
Those parameters...
The upper right corner...
It is a...
Um...
P...
Is to say...
You...
Given N...
Given K...
Given N...
Or given...
Omega...
After...
Um...
You want to generate a...
Specific A...
What is the probability?
That's it...
Okay...
So...
What does it mean?
It is to say...
For example...
Um...
Now...
I draw a picture...
Assuming this is...
I want...
Just now...
I said...
These parameters are generated...
Then...
My first step is to choose first...
How many points do I want?
After choosing...
For example, I have 25 points here...
I choose 25 points...
Then I say...
I deliberately want to divide into two groups...
K equals 2...
Okay...
Next...
I want to tell my system...
K equals 2...
Then...
I want to know...
How many percent...
K equals 1...
How many percent...
K equals 2...
K equals 1...
So...
I can know...
Who is in which group...
The third...
Is the proof I just said...
Because now...
I only choose two groups...
So it is a 2x2 proof...
Then...
Um...
The number in this proof...
Is also 0 to 1...
Then...
Um...
Every time...
Um...
When generating...
I still look at...
Two points...
Do you want to generate an edge?
Then...
So...
Um...
If I choose this type of...
Omega...
Because Diagonal is bigger...
So it...
Maybe it...
Um...
Um...
1 and 1 are connected...
It will be more dense...
2 and 2 are more dense...
Okay...
So I can...
After selecting...
I can...
Generate a network...
I decided N...
Then...
I...
I...
Didn't do much...
Image processing work...
So...
Um...
There is already an edge...
Decided by me...
But...
You can know many times...
Then every time the edge...
Will be different...
Then...
Um...
Um...
In the past...
The work is...
Um...
You still have to know...
Say...
How many K's does this system have?
Then...
After I know...
I...
To...
Use...
A method called...
Maximum...
Maximum Likelihood...
Then...
To know...
Given my data...
And after I choose K...
Um...
Um...
Um...
I...
Um...
Um...
Um...
I want to infer...
Um...
This N here...
With this Omega...
Then...
Um...
Because I want to know...
Its...
Structure...
The connection structure...
Then...
Um...
The work in the past...
Is to use this method...
To understand...
The large-scale structure of that data...
That is the Omega...
But...
Um...
Because there is a problem here...
I don't know how to choose K...
Then...
Um...
There is a point...
I won't use it now...
But I will use it later...
That is...
Um...
Um...
Every time...
Um...
After generating this network...
This network actually has some...
Um...
It is a certain network...
It is over there...
Generated by you...
Then...
On that certain network...
There are some...
Parameters that I can use to describe it...
Called...
I will explain later...
Called B...
Called E...
Called K...
It's a bit like...
Um...
Um...
Now...
The one in the upper right corner...
Um...
Generative model...
Correspondence...
Then...
I will explain later...
Then...
Then...
We will call it...
A micro canonical model...
Because it's called canonical now...
It is...
Um...
The parameters in the upper right corner...
If it is reasonable...
It's because you have to generate many...
After you generate an ensemble...
It will be reasonable...
But...
In the micro canonical...
You don't have to...
After you say it...
You generate those...
Um...
Ok...
Then...
Um...
Why...
I don't know how to choose...
There are a few groups...
It's a problem...
It's because...
Um...
You will overfitting...
Um...
Now here...
This...
P of G...
G is the one just now...
A is the graph...
Then...
B...
E...
K...
It's the parameters I just said...
Um...
Except that K...
But...
It's basically the same thing...
Then...
Um...
Let's take a look now...
We want...
Um...
Find...
Um...
After we decide K...
We have to find...
That...
Correspondence...
Correspondence...
E...
And K...
That value...
Then...
We...
Want...
Because we have to understand the network structure...
So I decided K...
Then I look at the network structure...
How is it?
Then...
Is it good?
Let's take a look...
My...
Maximum likelihood of fitting...
How big is that P?
Like this...
Then...
Um...
If the P is bigger...
I took the log now...
Add addition...
It will be smaller...
Then...
There is a parameter called S...
Um...
In short...
That's my goal...
My goal is...
I want to fit it...
Then look at the largest P...
Or the smallest S...
The place...
Then...
We can take a network to fit it...
Then I said...
Today I think this network is B equals 2...
Then I fit it...
Then I got a likelihood...
Then...
Eh...
It has a relative S...
So much...
Then...
Eh...
Then I look at the fit...
How about B equals 3?
Then I fit it again...
Then...
Because...
Because B equals 3...
It...
Um...
There are more groups in it...
So at this time...
Um...
The kind of Omega parameter I just said...
It becomes...
From 2 times 2...
Becomes 3 times 3...
So...
The one you can adjust...
There are more...
As a result...
You will find that you...
The fit is better...
So...
P is getting bigger...
Then S becomes smaller...
Then...
You do it like this...
You find...
When you make B bigger and bigger...
Its...
S is getting smaller...
Its...
Um...
Likelihood is getting higher...
Until...
That...
The number you want it to group...
Equals...
Every...
Um...
Equals this system...
Um...
Equals the number of nodes in this system...
Then...
You get a...
Omega matrix...
This matrix is n times n...
And that n times n...
Just like your...
Adjacency matrix...
Either 0 or 1...
You...
Predicted super well...
But...
You...
Overfitting...
So...
Um...
This is a...
Um...
If you don't regularize...
Your model...
The parameter...
Um...
Will encounter a problem...
Okay...
So...
Um...
We need to introduce...
Bayesian method...
What we saw just now is a...
Um...
P of G...
It's a likelihood...
And then...
Um...
We need to use...
Bayesian method to...
Um...
Regularize...
Um...
Some parameters in this likelihood...
For example...
This...
This thing...
Um...
In the usual...
In the usual...
Historical statistics...
Will...
Will be used...
For example...
You have a lot of points...
And then...
You say...
I'm going to choose...
A few...
A few bins to divide...
And then you choose so many bins...
And then you choose a little more...
You seem to see more structures...
But...
But you don't really know...
In the end...
How many bins do you want to choose...
Is...
Um...
Really see the structure you want...
This way...
So...
Choosing a bin is a...
It's also a problem...
This way...
And then...
Um...
So...
I want to know...
The data I see...
What are the odds...
Will appear...
And then...
I use a model...
To look at it...
That is to say...
After I set the odds...
Um...
How well does this model fit...
We call it posterior...
And then...
There is a...
Uh...
Bayesian theorem...
It can expand the posterior...
To become...
The following...
And then...
Uh...
We have a...
It's called...
Join likelihood...
And the following...
P of X...
It's called...
The odds of data appearing...
And then...
The problem is usually...
It's a certain data...
So it usually...
Will...
Disappear...
And then...
Um...
This join likelihood...
It can be expanded by two...
One is...
The likelihood we just talked about...
That one...
The glasses are...
We just saw...
A lot of parameters...
That thing...
And then...
Because the parameters are moving...
We're not sure...
Will we move to some...
In fact, there is no need for parameters...
So...
I want to put that...
The space for the parameter to move...
Consider it...
At this time...
There is that...
Prior...
When we look at...
When we look at this system...
This...
Um...
This system...
The odds of being seen by my eyes...
How is it?
The same thing...
It's the same on the Internet...
Because...
My X becomes my A...
Addresses in matrix...
And then...
Select different groups...
There are different...
Um...
Structures in it...
And then...
Um...
SBM is...
Um...
There are some parameters in it...
And then...
We're going to put what we just said...
That kind of...
Um...
Likelihood...
It's the green one...
The green thing below...
I put it...
Again...
Specify it...
Add that...
One...
Um...
Prior...
And then...
The last thing I want to see...
The biggest thing I want to see...
It's...
The one in front...
Um...
In front...
This P in the front...
And then...
It's that posterior...
And then I...
My goal becomes...
From just now...
I want to go...
Um...
Adjust my glasses...
Look at my...
Likelihood...
The biggest value...
It becomes...
Not only adjust the glasses...
And adjust my...
I want to adjust my prior...
And then look at the last...
The biggest value of the posterior...
And then at this time...
Um...
Usually there will be better...
Um...
Um...
Um...
The result of the statistics...
Like this...
Okay...
And then...
Okay...
So...
Um...
I just said...
There's one...
I haven't explained yet...
P of...
K, E, and B...
Like this...
And then...
This is a...
Um...
One...
With these three...
A function related to variables...
And then...
Um...
We're not sure how to...
To...
To calculate its value...
And then...
Um...
But...
Um...
In the lower right corner of this...
Um...
The article of PyChato...
It says...
Um...
You can use this...
Factorization method...
To...
Um...
Factorize it...
And then you can ask for three different items...
And then at this time...
Um...
Three different items...
There are some...
E...
Can be calculated by you...
For example...
Um...
The first item is called B...
That is...
You can observe that...
It is a specific partition...
That is...
The one I just said...
The length is N...
And then if there are two groups...
It's like the sum between 0 and 1...
This kind of thing...
Um...
You want to know...
Your specific partition...
What is the probability of it appearing?
Node partition...
And then...
Um...
After you know the node partition...
Because every point...
It has already determined...
Which group it is in...
And then...
You have to connect again...
And then...
Um...
After the connection...
There will be a matrix called E...
This E...
It is a matrix...
It...
It...
It is a bit like...
The omega...
Um...
Micro-canonical extension...
And then at this time...
You say...
Um...
In E...
Um...
Which group and which group...
How many lines are connected in the end?
Not probability...
How many lines are connected?
And then at this time...
You say...
It is its edge count...
Matrix...
And then you can also say...
Hey...
I...
I stipulate that I want to have...
Um...
Two groups...
And then...
How many...
Edge...
In this case...
My 2x2 matrix...
It...
Um...
What is its distribution?
This can be calculated...
Because...
These things are all discrete things...
And then...
After having these two just now...
I can...
Go again...
Um...
Um...
Go again...
To its...
Um...
Degree distribution...
Do the modeling...
I haven't explained what a degree is yet...
Degree...
It means a point...
How many edges did it connect?
This number is its degree...
And then...
Um...
Um...
And then...
Some context...
I didn't say...
Um...
Um...
Um...
Degrees are usually...
Things that everyone will put into this model...
Because degrees are often...
With a lot of...
On the Internet...
There is a dynamic phenomenon...
Connected...
Like this...
So...
They also hope to be able to...
Capture the degree...
Like this...
So...
Um...
After deciding the B and E that I just said...
In fact, I can also put...
The degree distribution...
Write it out...
Finally...
It is...
This network...
The...
The...
Likelihood...
That is...
I just explained...
The canonical version of the SVM...
Likelihood...
How does this thing become A?
Like this...
And then...
So we have a model summary...
Write it down first...
But it looks terrible...
It...
Each item can correspond to each other...
For example, this is the likelihood...
For example, this is...
Um...
The prior of the degree distribution...
And then...
This is the...
That...
Um...
The prior of the edge count matrix...
The prior...
And then...
This is...
Partition prior...
And then...
Although this thing looks so complicated...
And then we have K...
There are E and B...
A is...
I already knew...
Because it is my input data...
Um...
But there is an interesting point...
That is...
Oh...
I...
I want to talk about the words in the upper right corner...
That is...
The interesting point is...
Um...
K and E are actually a...
It's actually a B function...
Because if you think about it...
If you change B...
That...
Corresponding to that...
The one I just said...
Between the group and the group...
How many lines are there...
That...
Matrix E...
Will also change...
With K...
So...
K and E...
Can be written as...
B functions...
So...
Um...
Although I wrote this matrix so long...
But the whole bunch of things...
Actually...
It's still the same as just now...
Calculating modularity...
Just change that B...
That's it...
Just change every point...
Its...
Community...
That's it...
And then...
Um...
This work...
One of the contributions...
Is that we...
Put...
Um...
What I wrote now...
These...
Priors...
Put...
Put...
Not...
Bipartite...
Deduct the contribution...
That's it...
So...
Um...
Because after I deduct it...
Um...
The model is more concise...
So at this time...
My priors will be more...
The number will be bigger...
Will be more sensitive...
That's about it...
Okay...
Then...
Um...
Just...
You can look at the color...
Is...
Relative...
Partition...
Then...
Counts...
Degrees...
And then finally...
This is the network likelihood...
That's it...
And then there's a lot of work in here...
Because...
Um...
I need to know...
I gave the ones I just said...
Um...
Discrete parameters...
For example, B...
For example, E...
For example, K...
And then how do I know...
After giving these...
He in the end...
Um...
There are also these...
Parameters...
So many networks...
How many are there?
That's it...
One by one...
Is to know how many...
Then...
Um...
This...
Calculate...
How many...
This method...
Is...
Um...
Okay...
Um...
Why...
Calculate how many...
Is important?
Because we hope...
We can...
Um...
Just use...
Just being Bayesian...
That is, we can...
No...
Um...
For a specific state...
We don't have...
Special preferences...
We hope...
He...
Every state...
The probability is the same...
That's it...
So any network...
That's what I just calculated...
How many...
One...
That's it...
If I finally...
Calculate how many...
It's actually very small...
Actually, I found a good model...
Because...
I'm from that Ensembl...
Just pick it up...
It's like me...
To...
Um...
Observe that network...
That's it...
Okay...
Is there a problem so far?
Okay, then...
Just...
I think this is a very useful...
That library...
He really put...
SBM...
Put it in...
Then...
The bottom layer is C++...
That's it...
What I just said...
Um...
Um...
Ensembl counting work...
The author...
Paishoto...
Um...
His work...
Okay...
Then we...
We have to recap now...
What the hell did we do?
That is...
Uh...
Our job is to...
We want to maximize a posterior...
That is...
The upper left corner...
Um...
The blue thing...
Then we can...
Um...
Put it...
Um...
Use that...
Uh...
Bayes Theorem...
Write it like this...
Then...
We have a...
Uh...
Fixed box...
Because my data is fixed...
That's it...
Then...
So the important thing is...
With that...
Um...
The so-called total likelihood...
Put B and A...
Put it in...
This probability is related...
Then...
Uh...
What the hell is it?
He put it...
Um...
In the past...
Under the framework of Bayesian...
He put you...
Um...
There will be...
Those B and A...
Those...
Um...
Usually you have to estimate this...
Posterior likelihood...
You...
You have to calculate such points...
That's it...
Then...
Um...
This thing is on SBN...
It's...
It's hard to do...
The reason is because...
Um...
It's not easy to find...
Uh...
In the past, doing this point...
Usually in...
Likelihood...
To find some mathematical functions...
Then...
Go...
Then use the corresponding...
Um...
Conjugate prior...
Then...
You can get a point like this...
A...
A...
A...
Um...
On SBN...
This thing is very difficult...
Because...
Um...
Even if there is that prior...
May be distributed with data...
Also far away...
This way...
Then...
Um...
This is what I just said...
That is...
Um...
So...
Uh...
We just want to say...
Can we...
Because it is discrete...
Then I...
Um...
All right...
All the states...
The number is calculated...
Then...
I will minimize that number...
That's it...
Then...
At this time...
In fact, it is also with...
Um...
Um...
With the whole Bayesian framework...
Is consistent...
This way...
Then...
Um...
Petroto said yes...
Because...
Um...
You can take...
This is a recap...
That is, you can take the one I just said...
Um...
Join likely...
I divided it into two...
Then these two...
You can calculate it discretely...
So at this time...
I don't need to do points...
I just...
But I can still...
I still need to...
Have some assumptions about its prior...
Okay...
Then we will start...
Calculate likely...
Um...
I won't go through the details...
But I will talk about the concept...
Because of time...
Then...
Um...
Likely would...
That is...
I want to know...
I want...
After fixing these parameters...
Encounter a network...
How big is my parameter...
The chance of generating this network...
That's it...
So...
It's actually a...
Um...
The space that these parameters open...
The...
One of the...
Because I hope...
Every instance in the space it opens...
All...
The same probability appears...
That's it...
So it's actually a...
Um...
A...
Um...
Ensemble...
Called...
Um...
Omega...
Big Omega...
One-third...
That's it...
Then it can be completely from...
Um...
Those degree distributions...
Or...
That...
The edge count...
Matrix...
Calculate with the adjacency matrix...
Then...
Um...
When you are calculating...
Um...
You still hope to know...
That...
This Omega...
Um...
How to be the smallest...
That's it...
But...
Usually because Omega...
It will follow your...
Um...
You change every node...
It...
Which group is it related to...
So...
You will encounter...
You have a lot of space to search...
That's it...
Then...
So...
Usually everyone will use...
Um...
MCMC to sample this space...
That's it...
Then...
Um...
Here I have to say some...
Introduce a new term...
That is...
Um...
They will put...
Um...
We will put...
This...
Ensemble size...
Take a log...
Called Entropy...
That's it...
So...
Um...
Although what we see here is just...
Likelihood...
But the same thing...
Will also...
In...
Prior...
Um...
Happened once...
That is to say...
Um...
I will say...
Um...
The one I want to maximize in the end...
Posterior probability...
Probability...
In fact, it corresponds to...
A certain area...
Parameter space...
Then...
When I want to maximize...
That one...
Posterior probability...
This thing...
In fact, it is equivalent to...
I want to minimize...
The parameter space I opened...
Then...
The number of parameters I opened...
Take a log...
I call it Entropy...
That's it...
Or some people call it...
Um...
Description length...
Description length...
Okay...
So...
Just now...
These two things are...
Um...
One body, two sides...
That's it...
So...
Um...
Um...
Maximize...
Place in posterior...
At the time...
It seems to be in a...
Um...
The so-called minimum...
Minimum...
Description length...
Under the principle of work...
Find the simplest model...
Okay...
Then...
Um...
We have...
Just now...
Calculate the...
Likelihood...
That is...
Decide...
After determining the parameters...
The space it opens...
How big...
But...
We can also take a look at this...
Prior...
Have...
Have...
Um...
We can also look at this...
Prior...
How to calculate...
Then this Prior...
That is to say...
Um...
How many reasonable...
Um...
Parameters...
Parameters can change...
Then this time...
Calculate the space...
How big...
Okay...
For example...
Um...
We can have a...
Prior for Node Partition...
That is...
Um...
For example, now it is a...
Um...
There is a network of four groups...
Like this...
It is not yet a network...
But a thing of four groups...
Then I have to allocate...
One, two, three, four...
Four numbers on...
N points...
Then my question is...
Um...
Assuming...
Uh...
One, two, three, four...
These four numbers...
Divide to N points...
I...
Whatever...
Then...
After each division...
I will get a...
Partition...
Like this...
Then...
How do I describe this...
All of this...
When I decide...
How much N...
When I decide...
B is equal to four...
How big is this partition...
Like this...
Um...
This thing is...
I...
In the specification...
I...
Um...
Allow parameters to...
Um...
Change space...
Then...
So...
Um...
Um...
Um...
The above general prior...
Is...
Um...
The previous work is...
Um...
For example...
We...
We can put...
After deciding B...
After deciding B...
Um...
Okay...
Then...
I...
I have some...
That...
Um...
That is...
I didn't do a good job in the video...
I explain now...
That is...
Everyone sees B and sees K...
They are the same thing...
Then...
Um...
B is a group...
Then K is also a group...
But usually...
When I first do this work...
I will put...
I will use K as B...
Then K will be divided into KA or KB...
They represent it in...
Um...
Different types...
Different types in bipartite...
Numbers like this...
Okay...
Then...
Um...
In general prior...
I can put...
Um...
This partition...
Just separate it like this...
Then...
At this time...
I have a...
For example, I see a P of...
No mouse...
A little inconvenient...
That is...
P of B...
Given...
P of B...
Given N...
This kind of thing...
Like this...
It is...
Um...
This N is...
This N is not capital N...
It is small n...
Small n means...
Um...
How many points are there in each group...
Like this...
Not how many points in total...
How many points are there in each group...
That is...
Um...
Um...
P of B...
Given N...
It means...
Um...
When I determined...
In each group...
Um...
In my...
Um...
When I determined...
How many points are there in each group...
Um...
How many...
Um...
P of B...
Like this...
So it is actually...
Um...
Um...
N class...
Then...
In each group...
I can travel...
It is equivalent...
So it is...
N class...
Given...
A...
Um...
Product of...
A lot of...
N class...
This whole thing...
This whole thing...
This whole thing is a size...
It is a relatively large number...
Then I count it...
It becomes a probability...
Like this...
So...
Um...
This probability is...
It counts like this...
Then...
Then, for example, the second...
P of N...
Given...
B...
Big B is...
The number of groups...
It is...
Um...
I have...
After the number of groups...
How many possibilities do I have...
Can allow...
I have every...
How many points...
Put in each group...
Like this...
Then there are some...
Subtract 1...
It means...
It means...
Um...
It means...
I can't allow...
There is that...
Empty group...
So I at least put...
A point...
Put in a group...
Then...
By the way, the K over there is also B...
So...
When I put at least one point...
When I put at least one point...
When I put at least one point...
In a group...
Um...
How many...
The point is put in each group...
The point is put in each group...
This thing is a...
Um...
How many...
How many subtraction problems...
For example, it is...
N minus 1...
Because I put one in it...
N minus 1 subtract...
Uh...
K minus 1...
So many numbers...
Then because I want to subtract...
So many numbers...
Then because I want to subtract...
So I subtract it...
Then P has B...
Then it means...
I...
Um...
The same B is also the K...
That is...
Um...
I...
Assuming that each of my B...
The probability of appearance is the same...
So it's 1 per B...
That's it...
So at this time...
I have a general prior...
But...
Um...
In fact...
Um...
In fact, there is...
Count less...
Um...
In fact, there are some things...
For...
Uh...
For bipartite networks...
It's a waste...
For example...
Um...
For bipartite networks...
Because...
Uh...
Different networks...
It has to reside...
In their own type...
Inside...
So...
Um...
I'm counting...
Um...
When there is no empty group...
When there is no empty group...
I'll...
I'll count twice...
I can't have an empty group in...
I can't have an empty group in...
Type A...
I can't have an empty group in Type B...
I can't have an empty group in Type A...
So...
Um...
Um...
There's something in the middle...
I can...
Um...
The second one I just talked about...
That...
Prior...
Can...
Um...
Write a little more accurately...
This...
And then...
Um...
Last P of B...
Now P of two...
Two B's...
B is K...
So...
And...
It's not the same...
So...
Ok, but we...
The point is we put...
This thing is more accurate...
After that, actually...
The posterior I got in the end...
It will be...
Larger than general...
Bigger...
Ok...
And then...
Um...
this one...
I'm going to skip over...
To be honest...
In general prior...
Because what I have to consider is...
That P of E...
That is...
How to connect between groups...
Things like this...
But in general prior...
It has considered...
Um...
The connection of the group in the same type...
It also counts it in...
Um...
The ensemble I just said...
But it's actually a waste...
Like this...
Um...
Then we have to put it...
The waste part is deducted...
Like this...
So we have a...
A more frugal model...
So...
When we do this...
I can still get one...
Um...
Bigger...
Bigger...
Prior...
Then...
Um...
The degree is the same...
But the degree is quite involved...
I'll skip over...
Then...
But the degree is basically...
Um...
It has nothing to do with the group...
So...
We use the old method...
Like this...
So in the end...
Um...
Our prior is...
Um...
We...
Contribute...
Um...
One of the contributions to work is...
This prior...
It has a bigger...
value...
In various parameters...
Like this...
Ok...
Then...
Um...
We are now...
Already talked about...
The red part above...
It's Lightning...
With the yellow part on the right...
Prior...
So...
Together...
Although there are many parameters...
But in fact, it's all with...
Um...
The so-called B...
That is...
Every point is related to which group...
Then we...
We want to know...
Um...
How to change B...
Find the maximum...
That...
P...
Like this...
Then...
Um...
When I was doing that job...
Um...
About three...
Two or three years ago...
Um...
At that time...
Um...
The...
I didn't do too much...
For MCMC...
Method of...
Innovation...
Then...
At that time, MCMC...
Is...
You have to...
Decide...
After you have analyzed the group...
Then...
Um...
Every node...
To change its group...
Then...
Look again...
You want to maximize P...
How to maximize...
Then...
Um...
Starting this year...
Um...
You can...
There is MCMC...
He can...
Open it...
Or merge the group...
Then...
At this time...
Um...
Um...
MCMC...
Will be more efficient...
Then...
Um...
I specifically listed here...
Say...
A search algorithm...
I want...
Um...
I already have MCMC...
But MCMC is...
Temporarily in...
Specific...
B...
Then...
How do I...
In the space of B...
Move...
That part...
We call it a search algorithm...
Okay...
Then...
I haven't talked about MCMC...
Um...
What is it...
Then...
Here is a...
I think it's interesting...
That is...
How do we MCMC...
Then...
Why MCMC...
In SVM Inference...
It is useful...
But it is also quite difficult...
Then...
Um...
Usually...
Um...
When doing MCMC...
Is...
I want...
Um...
Seek a...
There are many parameters...
Function...
Then...
This...
This function...
It is a...
I know those parameters...
I know how to calculate...
The function...
But...
I may not know how to...
Write it...
Detailed...
Write it down...
So I can't use...
The method of microgram...
To...
To find the value...
So at this time...
I can only...
Um...
But...
This is a very useful method...
That is...
I can only...
Randomly change...
Those parameters of this function...
Then...
After each change...
For example, I want to maximize...
The value of that parameter...
Then...
After each change...
This...
The value may become larger or smaller...
Then...
Um...
I will say...
Ah...
If...
Um...
If I change it later...
This value becomes larger...
I will accept this change...
Uh...
If...
My change...
My random change...
If I make this value smaller...
I will...
Accept that change...
With conditions...
So...
Then...
That so-called condition...
Is a...
Called...
Metropolis-Hastings algorithm...
Uh...
Very useful...
Is...
When you choose that kind of specific...
Rejected some...
After some proposal conditions...
You finally came out with this...
You want to use...
But you don't know how to use...
That network...
Uh...
That...
That function...
The distribution...
Will be very close to the real distribution...
Then you can do it on top...
Um...
Maximum operation...
Like this...
So MCMC is very useful for me...
Because I...
I just need to change...
Every group...
It...
Uh...
Every node...
Its group...
Then after the change...
Let me see...
Uh...
My likelihood...
In fact, it is a posterior likelihood...
How to change...
I will decide if I want to accept it...
Like this...
But...
But there are many problems...
I'm not sure at the beginning...
For example, B3...
I'm not sure at the beginning...
What kind of partition do I want...
Then...
Uh...
Uh...
This is actually an example...
You are from...
Uh...
Number of sweeps...
Is your proposal every time...
Every sweep...
Is every time you...
There are N points...
Just start from 0...
Change once...
Change once...
Change once...
After N times...
This is one less time...
It's called a sweep...
Like this...
Then...
Number of sweeps...
You are many times less...
Like this...
You expect...
Because I want...
From random...
Uh...
Random system...
Down...
Then...
But...
Usually...
There are some systems...
It may be in...
This kind of...
Uh...
It's stuck in a certain...
Metastable state...
Then you...
You want to look at the entropy...
You are not sure...
Uh...
This...
Markov chain...
Converge...
Like this...
Then...
You are not sure how long you have to wait...
Then...
So this is unknown mixing time...
Then...
And...
Finally...
Uh...
After a while...
You have to...
Um...
Because I still want to know...
My...
What is the maximum P?
Then...
Um...
At this time...
You have to regulate the chain...
Let it...
Don't move so fast...
So there are some...
Unlinking schemes...
Then I'm not sure what to do...
Um...
There is an unlinking scheme...
Like this...
Then...
Unlinking schemes...
There will be a problem...
Then...
In short...
Um...
There is a lot of room here...
Then...
Um...
It's the paper in the lower right corner...
Also...
A lot of things...
Like this...
Okay...
Then...
Um...
Okay...
So far...
I just talked about...
Um...
How...
Um...
After determining B...
After determining the group...
How do I change those...
The one at each node...
Then look at...
The value of P...
But I haven't...
Said...
How do I...
Move in the space of the group...
Like this...
Then...
Um...
We proposed a search algorithm...
This search algorithm...
There is a concept called...
Dynamic Programming...
Then...
The idea at that time was like this...
Just...
I was thinking...
At that time...
I was developing this...
This...
Search algorithm...
I was thinking...
Um...
Uh...
If we start from random...
It's like putting a messy room...
Tidy up...
Then...
Um...
Um...
Um...
When tidying up...
I can...
I...
I can have a way...
Um...
I just...
Take a good one...
Take a piece of paper...
Write it down...
Say...
Um...
What should be put where...
What should be put where...
Then...
Finished...
Then I said...
I wrote a hundred things...
Because there are not a hundred things in the room...
Then I said...
Ah...
The room looks neat...
Yes...
But...
Um...
But you will find that...
When you do this...
Found...
Hey...
It seems that there are some things...
For example...
Um...
Books and pens may need...
Put it closer...
So you want to say...
I may not need to be so detailed...
Book and pen...
The...
The position...
So I can...
I can...
Um...
I said...
Um...
The position of the book and the pen...
If I did it at the beginning...
Very complicated...
After the search...
Um...
Next time...
I want to do...
Not so complicated...
That is...
I...
I...
The first time I want to divide it into 100 groups...
But the second time I may...
That 99 groups...
I can not say...
I want to divide 99 groups...
But from that 100 groups...
Then...
Say...
Who is closer to whom...
I will divide it together...
This way...
Then...
I want to see if doing this...
Um...
Will make me...
Much better...
Then...
Um...
The conclusion is...
It will be much worse...
But...
Um...
You can...
Standardize it...
You say...
If you are lazy...
I do not want to be serious...
Use what I just said...
That very complicated MCMC to divide...
But I...
Lazy to say...
Ah...
What is close to what...
I will write it together...
This thing...
Sorry...
I want to skip the video...
That is...
In the lower right corner...
This...
Matrix...
Merge...
That is...
This is a...
That makes me much worse...
Then...
Um...
Um...
If this way...
You can...
Then I can...
That...
The whole algorithm...
Become faster...
Say...
Then...
Um...
The answer is...
This is OK...
Then...
Um...
We do it this way...
And after we do it...
Ah...
Also using...
Um...
Um...
The way of design algorithm...
The way of algorithm is...
Dynamic programming...
That is...
I want in that...
Um...
Um...
This kind of...
Um...
Two K or two B...
Move in space...
But I don't want to...
Calculate all the values...
Now this is...
I deliberately calculated all the values...
I want to be inside...
Effective movement...
Then...
Um...
Um...
Because I know...
I often repeat...
Calculate something...
Then...
So this is...
My upper right corner...
Um...
The second point is...
Overlapping subproblems...
Then...
With...
Um...
With one thing is...
Um...
I know this system...
There is a system called...
Optimal substructure...
That is...
I want to find...
I just saw this...
Very complicated landscape...
Minimum value...
Then...
Um...
I can't find it all...
Because it is very expensive...
So I can only find...
This pink...
This little block...
Then...
Um...
If this system has...
Optimal substructure...
That is...
Say...
I found a lot of little blocks...
Then...
Um...
Um...
This little block...
A lot of little blocks...
Minimum value...
Um...
Really can represent...
Um...
If I...
Seriously find the minimum value...
Then...
That is...
Your substructure...
That is...
With optimal value...
You can use it...
Then...
So usually...
If there is this kind of value...
Problem...
We can design a...
Dynamic programming...
To...
To solve it...
Then...
We will...
Um...
Put MCMC and...
In...
B space...
Search these two...
Decouple...
Then...
We propose a algorithm...
Then...
Um...
The time is...
Not enough...
But...
Um...
When we compare...
We compare...
Um...
Another...
One...
On...
SVM...
The model above...
It...
For...
The edge count we just said...
With...
Um...
Partition...
These two prior...
Um...
Do extension...
Then...
Um...
Turn it into a...
Layered model...
Then...
This layered model...
Not like...
All...
All...
Has the same probability...
But...
It assumes...
Each state has a specific probability...
And this specific probability is...
A slightly upper...
Network...
Then...
Also generated by SVM...
Then...
This network...
It has a higher-level network...
Generated...
Then...
Until...
The top...
Layer...
It is a...
Um...
Not like I said...
Um...
Intentionally assume...
Um...
Who is who...
Is a...
Um...
Um...
Parameter distribution...
Then...
Um...
There was a work that said...
Ah...
If you do this...
Then use a layered network...
To feed it...
You will get...
Um...
A better...
Posterior...
Then we did compare with it...
Like this...
Then there is an interlude...
Just the one I just said...
This...
Hiacusvm...
In fact...
Can also be used in topic modeling...
Then...
It is...
Better than LDA...
On math...
Better...
It's pretty interesting...
Then...
Um...
We...
Have to our system...
Resolution limit...
Is that we...
Intentionally use a lot of this...
Bipartite clique...
It...
Every small network is bipartite...
But it has a lot of quantity...
Then we want to see...
Um...
For example...
If the number is small...
It should be able to...
Divide to the number I want...
But the number is getting more and more...
Is it...
Will...
More to a certain extent...
I can't see...
Such a small bipartite clique...
Then we have it...
With...
Um...
Um...
General model...
Comparison...
Then...
We know that...
Where is the boundary...
Then...
We also...
Um...
To see it...
Because our...
Um...
Contribution has...
The prior...
With the search...
Then we...
Compare some models...
Then...
Change the parameters...
Then we...
Um...
Generate...
The EC test on the left...
With...
The hard test on the right...
Um...
As...
Um...
Model network...
Then we look at it...
Um...
Finally...
Um...
The hard test network...
I...
I...
Planned it...
I...
I built it by hand...
Then I know that...
Is my method good or not...
Then we do...
Related comparison...
Then on the real network...
We...
Have...
Compare with...
The methods I just said...
Then we...
Have some results...
Um...
If the network is smaller...
By spam...
There will be better...
That...
Posterior...
Um...
Here to compare...
Is...
The smaller the better...
Because...
Um...
Is the...
Entropy...
Then...
If it's a larger network...
And...
Um...
The bipartite SPM is better...
Then this network is usually better...
Technical...
We get...
This result...
Then...
You can look at paper...
Then...
Um...
Here I...
Did very little...
Um...
Um...
The so-called predicted...
What kind of group is the node...
Thing...
C...
Then...
We have a...
Um...
There is a view...
Is...
Um...
David Walpert...
Walpert...
Um...
Proved a theorem...
Called...
There is no stupid lunch theorem...
He...
Say...
Um...
Um...
He made a pretty strong statement...
Then...
Say...
Um...
Uh...
We have...
Different...
Machine learning models...
Then...
Um...
All machine learning models...
If you put it...
Um...
His...
Um...
His performance...
To...
His input data...
If you do the average...
Then...
All...
Um...
Machine learning models...
Their...
Um...
Performance is the same...
On average...
For all data...
On average, it's the same...
So...
Uh...
In other words...
Uh...
Our point of view on this issue is...
Compare the model itself...
Uh...
Uh...
No special...
Say...
Is it in which data...
He...
To perform better...
Um...
So...
Um...
Um...
For example, there is a machine learning model...
He may be...
He predicted some type of data well...
But he may be...
To...
Other...
The type of data is not well predicted...
So...
Um...
So...
What everyone sells...
It's better to say what he sells...
Um...
Okay...
Then...
Almost...
It's over...
Uh...
What did we learn?
Uh...
If we have a good constraint...
Then we have a more economical model...
Then...
Um...
Our design...
Our design method...
Is...
Um...
There is nothing to predict...
It's a soft model...
Then he can...
Can...
Uh...
Look at the distribution...
Can look at...
Can look at...
Um...
The probability of parameters...
Then...
And then...
Uh...
Um...
There are many possibilities of new methods...
Then...
Then...
What can we do?
We can have...
What...
Core proof structure...
It's a type of structure...
Then...
Uh...
We can also look at...
The assumption of SBM at the beginning...
Then...
There is some work...
Then...
Uh...
What I just said...
The landscape of B and B...
Uh...
We can also look at...
His landscape...
Then...
Some recent work...
Then...
Um...
Finally...
Um...
Just...
Um...
Recently there is...
There is that MCMC algorithm...
He can...
Separate the group directly...
Or merge the group...
Then...
Usually the efficiency is very high...
But the interesting thing is...
Um...
Although he...
Um...
Um...
Um...
Although he...
Um...
Can guarantee...
Just mathematically guarantee...
He has a better mixing time...
That is...
He...
MCMC is more quickly trained...
In some specific networks...
He...
Um...
Uh...
Uh...
After the training...
But...
Um...
The result of the training...
Just...
Think about it...
Um...
Just...
He...
He...
We have...
We have...
Um...
Um...
Just...
This work has said...
Just...
Um...
Some type of data...
Although you...
Uh...
Have a better MCMC algorithm...
You can...
Fit it...
However...
Um...
Often...
It still needs a long training time...
So in other words...
Um...
Maybe some data is...
Not suitable...
SBM to fit...
Although SBMs...
It looks like too...
Too...
Too...
Too simple like this...
So there is the model consistency problem...
Okay...
Then...
Um...
Um...
You can also ask...
If there is a change in the network...
Then...
Uh...
At the time of change...
The distribution of the node is...
What kind of...
Uh...
Maybe...
Is there any...
Physical...
Low...
In the high-end...
Like this...
So this is...
If you get two snapshots of the network...
Then the middle will change...
You can...
Say you want...
Interpolation...
Like this...
Find a model...
To fit the middle...
Change...
Okay...
Okay...
Then...
Because our work...
Is related to different data...
So...
I think a takeaway...
Is to be curious about...
New experiments and...
Applications...
Okay...
Then...
I have one last slide...
Is that I want to thank...
Um...
These people are...
Um...
Um...
Um...
From not applying for a PhD...
To...
Later...
The people I met in the PhD class...
Then...
Many people are related to this work...
Like this...
Then...
Um...
Dan is my instructor...
Then...
Joshua in the lower right corner...
Is my other instructor...
Then...
Um...
The one on the right is...
Paishoto...
Okay...
Then...
Um...
Finally...
I want to give some...
Is...
Um...
This...
Is...
To show everyone...
This community...
Then...
Um...
I have...
Um...
Participated in a project...
Called...
Taiwan Internet Science Education...
It is currently a website...
But I hope to put...
A lot of...
Internet data related to Taiwan...
Because...
Um...
To develop a model...
You need to...
Look at it with data...
Then...
Um...
I hope everyone can give me...
Suggestions...
About this...
Website...
How to go...
Will...
Affect more people...
Then...
We translated...
Two small books...
Called...
Complex systems...
Yes, you don't know...
And...
Network adoption...
I specifically separate the network and the network...
Because I think the network is the Internet...
But the network is...
Um...
Complex networks...
Although...
It is not a commonly used word in Taiwan...
A word...
Yes...
Then...
This...
Is...
Um...
Is a...
Um...
A website that collects various Internet data...
Yes...
Then...
Um...
There is also a seminar similar to Taiwan...
Um...
Seminar...
That is...
Women in Network Science Seminar...
Then...
Um...
Usually it is a weekly speech...
Everyone can...
To...
To...
To subscribe to their messages...
Yes...
Then...
Um...
In the lower right corner...
This...
Um...
Very special...
With many CS conferences...
A very different place is...
Um...
Many...
Um...
People who go there...
Um...
He...
Um...
That job may not be a completed job...
Then...
He...
Um...
The community is quite active...
Everyone...
More willing...
Vote for a...
Um...
I haven't finished my work yet...
But go there to exchange opinions with everyone...
Yes...
Then...
That...
NETSCI conference...
It has some symposiums...
Then...
I think with...
The two are related to networking science...
Yes...
Such as...
Um...
This Network Science in Education...
This one...
Um...
It may be mentioned a lot...
Um...
The actual desktop of teaching Internet Science...
Even have...
Um...
Education in the high school this period...
Then diversify NETSCI...
Yes...
Then...
As for the upper right corner...
This is a that...
Um...
Society of Young Network Scientists...
He...
As long as he considers...
That you are young...
You can...
You can join...
Yes...
Then...
Um...
Now most of them are...
Early career...
Professor...
Then postdoc...
With PhD students...
Then...
We have a Slack...
If you...
If anyone wants to join...
You can email me...
Then...
My report will end here...
Sorry...
More than an hour...
Um...
Hi...
It doesn't matter...
That...
That...
Before opening the question...
Let me first represent everyone...
With a warm applause...
Um...
Thank you...
Ziqi...
Today is very...
Um...
Wow...
OK...
Ok...
Thank you everyone...
Um...
Are there any questions?
I think...
I think the content is very rich...
And you...
Hold on...
Let everyone think about it...
Have a question...
Then let me ask one...
This is more relevant to the content...
Excuse me...
You...
So many things...
You wrote it in one piece...
Um...
Um...
I don't know if it's...
The relationship I talked about...
It has changed a lot...
But...
He was in one piece...
Then...
That piece...
Has 17 pages...
Including references...
Oh...
But I think it's a lot...
Because...
Because in fact, every link...
Of course, it's all linked together...
That's right...
But every link is interesting...
Then...
Every link is basically...
The contribution of the main force...
Of course, they help each other...
There is nothing wrong...
Anyway...
Anyway, I just think it's very good...
Um...
OK...
Do you have any questions?
Um...
Um...
Um...
Um...
Um...
Um...
Um...
Um...
Um...
Um...
Um...
Um...
I want to ask a question...
Um...
You just say it directly...
Can you hear me?
Yes...
Um...
Thank you very much...
Thank you for the sharing...
Then...
I think...
Very...
Useful...
And...
The meaning of this education is great...
That is...
You are not just...
What are you doing from this article?
That is...
From the beginning...
The whole context...
It's a complete narrative...
I think it's great...
Then...
There are two questions I want to ask...
The first one is...
Um...
Because I haven't read this article...
That is...
I want to ask...
Are you testing...
There is...
Synthetic...
Or...
Simulated network...
That is...
You make it yourself...
Some...
Um...
You have already controlled it...
This parameter...
This network...
Just like...
You are not from the real world...
To search for information...
You are...
Synthetic...
Come...
Then you go to test...
Under what circumstances...
Your algorithm performance is better...
Under what circumstances...
Your performance is not so good...
Because...
You say...
I...
I...
That...
Or signed...
That is to say...
It has positive signs...
Or even...
Temporal...
That...
This model...
Can you extend it?
Because I have one here...
Contents of Congress...
It is...
The legislative and proposals of the proposal...
Then...
It is a...
Weighted...
Then there is also a sign...
Because it has...
Different parties...
I don't know if...
Directly apply you...
Framework of research...
Or...
Packaging...
Can use...
Good...
These two questions...
Um...
The first question is easier to answer...
Um...
Is in my work...
Use...
Synthetic network...
Then I thought of it most directly...
Uh...
I'm talking about a faster projection...
I hope to know it...
In the end...
Um...
How much support...
So...
This horizontal is...
Uh...
My...
Uh...
Synthetic network noise...
That...
Then...
That...
That...
Adjacency matrix is...
Its structure...
It looks like that...
But...
You see a lot of...
Noise...
Noise points...
That is...
I deliberately let that noise point...
From small to large...
Then I want to see...
To what extent...
Can't...
So...
This is what I think of...
Uh...
Useful synthetic network...
Then...
Um...
As for...
Um...
The second question...
Um...
Um...
Um...
As far as I know...
Not yet...
Um...
At the same time there is...
Bipartite...
That is...
What I said now...
Uh...
My paper...
These priors...
Plus...
Uh...
Weighted...
The edge...
Or signed edge...
The SBM...
Um...
But now there is...
Very good...
Weighted SBM...
Um...
And there is...
Not too bad...
Prior...
Come...
Come...
Can be used immediately...
Just in that graph tool...
Inside...
The work of the architecture...
Then...
But there is no...
Let me think...
Uh...
Oh, no...
That is...
Inside...
For that edge...
It is a weighted edge...
Then it can be...
Weighted...
It can also be...
Categorical...
So...
Um...
Um...
The framework is the same...
But...
Um...
Because it is weighted or...
Categorical...
It is the same...
Um...
Corresponding...
Uh...
Prior...
Inside...
Then it can be used directly...
So...
I...
I don't know...
I think...
Um...
I don't know if...
Deliberately put...
That kind of...
Already have...
Weighted SBM...
Inside...
Bipartite place...
Adjust again...
Adjust to...
Say...
I really want to use a...
Weighted Bipartite Structure...
Will have...
How much improvement...
Otherwise...
I...
I think this is a job...
Not wrong...
But I think...
Um...
Direct off the shell...
In that graph tool...
I think...
Should be very good...
I don't know...
So ask again...
The one you said is...
Off the shell is...
It already has...
Weighted...
But there is no bipartite...
Or...
There is weighted...
Then there is bipartite...
Um...
Weighted...
No bipartite...
But...
Weighted...
And there is hierarchical...
Um...
Um...
If you take it directly...
Hierarchical...
Directly...
Um...
Directly hierarchical...
With...
Bipartite...
Together on...
Bipartite...
If you compare...
Um...
Um...
Except for some...
I think...
Not yet understood by everyone...
Example...
Except...
I think...
Hierarchical will be better...
Um...
Correct...
Understand...
Then my last sentence is...
We have real evidence here...
Is...
Taiwan Congress...
Legislation...
Proposal...
Joint data...
Extend to...
Weighted...
Bipartite...
Welcome you to do...
Then our data can let you do...
Realistic research...
Is...
Because I want to say...
You should be...
Just a few steps away...
Can be promoted to...
Weighted...
Or even...
Sign...
If you are interested...
Extended...
Is that we can...
Cooperate in this area...
Thank you...
Sorry for taking up too much time...
Need to rub a hand...
Does everyone have any questions?
Does everyone have any questions?
Can you hear me talking like this?
Can...
OK...
Because of time...
If you have any questions...
This...
Just like I said at the beginning...
Welcome everyone...
Go to...
Ziqi's website...
Then I believe that Ziqi will...
Very happy to contact you...
Then this video...
Will also be placed on...
Hera's website...
This...
This video...
Will also be placed on...
Hera's website...
So...
Everyone can still go and see...
Last round of applause...
Thank you Ziqi...
For sharing today...
Thank you...
Thank you...
Thank you...
Thank you...
