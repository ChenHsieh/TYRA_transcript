start	end	text
0	8000	Welcome back to today's Tarot Talk.
8000	15000	We are honored to have Joseph here to talk about blockchain research.
15000	20000	Before he starts, let me introduce him.
20000	28000	Joseph is a Ph.D. student at the University of Waterloo.
28000	34000	His major is Electrical and Computer Engineering, Faculty of Engineering.
34000	39000	Before he came to the University of Waterloo,
39000	51000	he was leading a new banking system design team in Taiwan.
51000	58000	He also collaborated with Microsoft Taiwan on medical system research.
58000	63000	His doctoral research is about blockchain.
63000	71000	He is also interested in IoT, edge computing, security, and AI.
71000	76000	In addition to his doctoral research,
76000	86000	he also volunteered to mentor Taiwanese students to apply for Ph.D. programs.
86000	101000	He also volunteers to mentor new graduate students in his department
101000	106000	and the Taiwan Global Ambassador of UW's Office of Advancement.
106000	114000	He encourages Taiwanese students to jump out of their comfort zone and pursue their dreams.
114000	124000	Today's topic is
124000	134000	An Implementation of Fake News Prevention by Blockchain and Entropy-Based Incentive Mechanism.
134000	142000	Let's welcome Joseph with a round of applause.
164000	166000	Thank you for giving my presentation.
166000	174000	If you have any questions, feel free to ask in Chinese or English.
174000	180000	I'm so glad to give you this presentation.
180000	186000	It's about how we use blockchain techniques to combat fake news.
186000	200000	Before we get into the research, I would like to introduce my background and to let you understand me more.
200000	203000	This is my background.
203000	205000	Actually, I was born in Taiwan.
205000	209000	When I graduated, I worked in Taiwan for several years.
210000	215000	As you can see, I originally worked in Hsinchu.
215000	220000	Then I went back to Taipei to do some software development.
220000	224000	Then I went to the banks, Yishan Bank and China Trust.
224000	233000	Before I joined UW, I collaborated with Microsoft Taiwan as a technical consultant.
234000	243000	My educational background is that I have a master's degree in computer science at National Taiwan Ocean University.
243000	252000	For this project, I introduced that in my master's thesis.
252000	259000	We developed a facial recognition system that you could input the sentences and your image.
260000	269000	Then our system could generate an animation like you and the animation can say some sentences.
269000	274000	It's a pretty interesting application in that year.
274000	281000	As you can see right now, a lot of applications like this are published and very famous.
282000	284000	That was a joke.
284000	290000	If we put this product on the market, maybe I'm a millionaire now, right?
290000	301000	When I graduated, for engineering students, seniors told you, you could go to Hsinchu to get a job.
301000	303000	Then you can get a lot of stocks.
303000	305000	Then you can retire very soon.
305000	307000	It was my dream.
307000	311000	When I graduated, I went to Hsinchu.
311000	318000	But in that year, we met some financial crisis.
318000	327000	So I came back to Taipei to do software development.
327000	334000	At that time, I was very interested in mobile development.
334000	343000	In that year, Microsoft collaborated with Nokia to deliver the new devices.
343000	346000	There's Nokia and Windows Phone on the market.
346000	355000	So I joined one company as a product manager and did a team to develop some mobile apps.
356000	360000	As you can see, the interesting one is the repost.
360000	367000	We transfer the Nokia's games.
367000	369000	It's a Barbie game, right?
369000	375000	If you're a child and you use this app, you can change the process of the app to us in these games.
375000	381000	We transfer it from the iPhone and Android system to Windows Phone.
381000	384000	And then we do some applications.
384000	388000	We did some applications for, you know, the care, right?
388000	393000	And two for Taiwan Taxi and also for EasyTag.
393000	394000	All these applications.
394000	403000	I was so excited that I did this application because, you know, in apps, if you are a programmer,
403000	408000	the apps could give you a very straightforward feedback from customers.
408000	410000	So it's very interesting.
410000	417000	However, the life gave me another joke.
417000	423000	It's that the Windows Phone did not succeed at that time.
423000	437000	So, yeah, after I did this job for more than one year, I found that, yeah, no, this job cannot help me to earn enough money for me to buy.
437000	446000	So I joined two banks and to run the directives, some financial products, and then we built a system.
446000	448000	It's called the mirrorless.
448000	452000	And this mirrorless is a very powerful system that the banking operations.
452000	460000	If you are a trader, you could use this system to record any financial products in this system.
460000	466000	And all the time in the first year, the banks use this system.
466000	468000	So I went to China Trust.
468000	476000	And then I, when the Hunter invited me to join EasyTag to introduce mirrorless to their banks.
476000	483000	And then when I finished this mirrorless project, yeah, we made a big project in Taiwan.
483000	488000	It's that the Asian Pacific organization came to Taiwan.
488000	497000	They do some resource to how does Taiwan do in the anti-money laundry, which is a fine teaching.
497000	502000	And so when I, I was kidding that.
502000	507000	So, yeah, I am one of the Taiwan representative, the national teams in Taiwan.
507000	511000	It's very honest for us, but it's very tough to do that.
511000	514000	We need to finish it out of the requirements in a very short time.
514000	517000	But, yeah, the final result is very good.
517000	521000	And I also got a promotion in Eastland Bank.
521000	525000	So, so far, it looks so good.
525000	528000	So why I want to go study abroad.
528000	536000	Yeah, that's because I was thinking about that if I still stay in Taiwan.
537000	544000	Maybe I could gather good jobs and to be promoted, continue to be promoted.
544000	559000	But, yeah, I was very, I heard about some great new technology skills or some new products from Silicon Bay or from North America.
559000	564000	So it was always my, be my dream to study abroad.
564000	567000	So I was so hesitant.
567000	574000	But, yeah, I just had to give myself a chance to do this.
574000	579000	And so right now I was thinking about that.
579000	584000	The thing is that if you have a dream, it only could stop you.
584000	585000	It's just that yourself.
586000	597000	So I just want to encourage all of my friends or all the participants or some young students in this meeting.
597000	606000	You could just to check your dreams or not to limit yourself.
606000	608000	And so right now I'm here in Canada.
608000	613000	So I, yeah, I saw a lot of beautiful things.
613000	622000	And, but, yeah, the period is also when I prepare for this study, I met a lot of difficulties.
622000	627000	And, yeah, but, yeah, it's worth it, I think.
628000	642000	And, yeah, this is my another my motto is that actually adventure may hurt you, but monotony, which means that you always do the same things will kill you, kill you creative or other things.
642000	645000	So, yeah, that's all my background introduction.
645000	650000	So I will stop here for a while.
650000	652000	If you have any question, you could ask me right now.
657000	676000	Okay, I think I think you have any question feel free to ask me, and we can discuss more.
677000	682000	And so I wrote some my experiences in my blog and some medium.
682000	686000	If you have, you could check it and to get more detail.
686000	696000	If you also want to trust your dreams, or you have some dream career, or if you could also find me on those pages.
696000	701000	And, yeah, we could discuss more about your patient.
701000	707000	Okay, so let's get to our main research topic today.
707000	716000	Today we're going to talk about how to use the blockchain to prevent the tech news.
716000	738000	Yeah, actually, what is the tech news. It is a concrete, complete fabrication, which means people to approach something or to mimic something and that is similar to a real, you are not able to confirm on the fact is, what is the real, what is the fact.
738000	760000	And so, what is the actually the great effect news is not in the long future. It happens right now, you can see this slide in the depth and upside is the US stock market in one day, you can see the picture is dropped sharply.
760000	781000	But why, because at that day, a news says that the Trump's, the Obama was dead, but it's a rumor, right? But however, the stock market to drop very sharply by this tech news and other examples in this slide.
782000	784000	We all know that the Cambridge Analytics.
784000	798000	They collect the users previous and to make some news or articles that you are interested in, and to affect your opinion in some specific issues.
798000	813000	And also, the Russian used some propaganda to affect the results of the crisis. So, all these are the current threat to our society and democracy and the economy.
813000	835000	Also, in Taiwan is the same. Actually, in the research, Taiwan is the most, you know, we got the most weight from the whole world. We are number one. So, it's very, like I said, it's not a long, not a straight in a long distance is the current.
836000	838000	It happened currently right now.
839000	858000	So, why people invest in tech news? That is because they can get a lot of the news. That's a very lower cost. Right now, the cost of producing the tech news is very cheap.
859000	872000	But if you can affect a big topic, like the U.S. president election or some very huge topic, you could get a huge, huge revenues. Right?
873000	890000	So, why don't we do that? If I can impact a lot of people by a very low cost, actually, the malicious people will, of course, they will do that.
891000	907000	And also, AI technology makes things worse. Because in my this slide, that this is Jordan Peele. He's a U.S. actor.
907000	925000	He collaborated with technology companies that use the AI technique to, you know, the deep tech technique to produce the video and to let Obama to say some words they want him to say.
925000	929000	So, AI makes the tech news to be produced more easier.
930000	945000	And so, but in a traditional ways, our research mainly focus on how to use the computer to detect the tech news automatically.
945000	954000	How we do that? You see this picture that we have different algorithms, but mainly we use some machine learning algorithms.
954000	970000	We find some clues. For example, if you are the tech news comes in and we find whether the IP address is correct or not, and its content has some weird things or not.
971000	991000	So, we use some patterns to detect and to decide whether this tech news is, whether this news is correct or not. But like I said, AI makes things very similar to real. The algorithms does not have the great performance right now.
992000	1010000	So, some big companies like Google or Facebook, they begin to use some human beings power capabilities to help AI to check the content's authenticity.
1011000	1021000	How they do that? They build some teams, they collaborate with the local professionals, they have the ability to check the content is correct or not.
1021000	1047000	And Facebook also did the same things. They built a team internal, and those teams will check some news, the authenticity, and to make your news, when news push to your device, they will be real, and they will build some tech news.
1051000	1073000	And in Taiwan, we also have one organization, and you could see that is the Taiwan TechTrack Center, and you could go on their website and to see, they will check some news and tell you this is correct or incorrect.
1074000	1100000	Okay, so I stop here, and for these solutions, like Google or Facebook, or the Taiwan TechTrack Center, those are good solutions, but these solutions are managed by a central organization, right?
1100000	1114000	We don't know how they work. It's like a black box. If they do not do their job well, you do not know, right? You just can trust them.
1114000	1135000	So, in our solution, we used decentralized solutions, which means that we will form a team, form a core, a lot of people, they are advisors, and they will verify some contents.
1136000	1154000	But this solution does not manage by only one company. It's managed by everyone who joined this system. So, this is our paper, and just accepted by DTNs 2021 in November.
1154000	1167000	So, after the meeting, if you have more interest in detail, you could, but it's not published yet, maybe in December. So, in December, you can check this content in more detail.
1168000	1187000	Okay, so I think we can get into the detail. So, this work is from another paper that was invented by some researchers in the UK.
1187000	1204000	As you can see, in the middle of this architecture, there is a group that we call the appraising actors. These are some professionals, they have ability to check the content is correct or not.
1204000	1223000	And in that side, content creators are people who create the digital content. Maybe you are a writer or you are a photographer, and you make some content, and you could send your content into our system.
1224000	1244000	And then, those appraising actors will verify your content to check, oh, your content is real or fake, and then they will send back to you the appraising result, and then those appraising results will be saved in the blockchain network.
1245000	1255000	After that, the right-hand side users, when they come into our system, they could check the authenticity of the contents from our blockchain network.
1256000	1274000	So, that's an overview of the architecture of the system. Our system's architecture is similar to this architecture, but in the following slide, I will introduce the differences between our architecture and this.
1275000	1294000	And this slide is interesting. Like I said, if we all rely on Facebook or Google to help us to check the content, how do I know they do their job very well?
1295000	1317000	Maybe they did not do very seriously, so I still could see some fake news. But in this system, you could select the people or some authorities, media you trust, and then let them be your appraising actors.
1318000	1342000	So, for example, in this case, I selected one is the BBC. It's a media, right? And I also selected some people, Professor Smith, Mr. John, maybe some celebrities you trust. If they say this news is real, you will trust that, right?
1343000	1359000	So, maybe, for example, in Taiwan, maybe you are very confident, right? If he says this news is real, okay, I will trust him. So, you could select these people to be your trusted parties.
1359000	1375000	And when the news comes in, we call it a quorum, a group of appraising actors. They will give you their opinion. Is this news correct or not?
1375000	1393000	And so, but why we need to use the blockchain in this system? Why don't we just use the traditional databases? We just accept the appraising results. We don't need to use the blockchain, right?
1393000	1412000	But actually, blockchain has some characteristics that if once your data is set in blockchain, you cannot modify that because everyone has one ledger in their node. Is this possible to modify all the ledgers, right?
1413000	1435000	And another case is that if you modify the ledger in only one node, it will be easier to be fine because we have some, you know, some hash tag in computer science. The hash point is that, for example, in my node, I have a hash point to point to your node.
1435000	1457000	But if you modify, the hash point is based on your content. If you modify the content, the hash point will be modified. So, it will be easier to be found that somebody modified the data. So, blockchain will be a great solution to maintain the correctness of the data.
1458000	1477000	And we go back to give you some knowledge. How does the blockchain work? Actually, you can see this picture. And if you have a transaction in this system, and the nodes will compete to try to verify it.
1477000	1495000	And once this data is verified, you will be set to the blockchain like this. In the bottom, we have some red blocks, and they are ticked by the hash point, right? So, this is how the blockchain works.
1495000	1519000	And then it's just, there are many, many blockchains right now. So, it's a very simple classification. One is the public, and another is the private. Public means that everyone could send a transaction in this blockchain network, like Bitcoin.
1519000	1540000	People know about Bitcoin and all of it. But in private blockchain, only people approved can send the transaction into the system. And so, this private blockchain often be used in the corporations, not for the public.
1541000	1564000	And another thing you need to know in blockchain is the proof of work and proof of stake. What is proof of work? This means that if we have a transaction and we have a lot of nodes, which node can record this data into blockchain?
1564000	1585000	And if you are proof of work mechanism, the nodes will compete to solve a complicated math problem. And this process, we call it mining, right? So, it's a famous word, or it's very popular to be heard. If you solve this problem, then you get the right to set data into blockchain.
1586000	1601000	But, yeah, and the proof of work is right now the blockchain and the Ethereum used. And the proof of stake is that, oh, no, we don't to solve the math problem. We just use some stake. What does that mean?
1601000	1617000	What does that mean is that if you want to record data into blockchain, you need to pay some fees. And if you pay higher, you will get higher chance to get the right, to be selected to get the right to set data into blockchain.
1617000	1638000	So, these two solutions could resist the hackers, because in the proof of work, we want to control our right to record data in blockchain. You need to control more than 51% computers in the world.
1639000	1649000	And in proof of stake, you need to pay more than 51% stakes, then you could get the right to record data.
1650000	1666000	And then, so, you can see that. So, in blockchain, actually, I think people heard about one news in Taiwan, that one bank, some people buy a lottery, right?
1666000	1682000	Then they collect a lot of people to buy the lottery together. But one man denied, he denied that, said, oh, no, this lottery only belongs to me.
1682000	1695000	But if you use blockchain, the solution, the problem will not exist, because in blockchain, everything will vary transparency, and we could just update the transaction.
1695000	1715000	So, that's the one of the blockchain could support our day life. Actually, they can solve a lot of things, like if you're trying to vote in, or to test some fishery or some products, the original prices of the product by using a blockchain.
1715000	1736000	And then, in this system, you could, I would like to introduce the hybrid ledger framework. It's the blockchain that proposed by IBM. And this is the permission, a private blockchain that I said is for the companies, not for the public.
1737000	1752000	And how they work, you can see this flow. If you have a transaction, you could propose to the execution chain code. The chain code is the smart contract in the blockchain network.
1752000	1767000	I think people heard about smart contract often, very often. And then the smart contract will check your content to check is there any mistake in your content, and then send back to you.
1767000	1784000	And when you get a permission, you can submit your transaction to the auditing services. And then the auditing services will decide how to record your data into the blockchain. And it's different to the Bitcoin.
1784000	1809000	The hybrid ledger framework does not use the proof-of-work mechanism, which means that we don't need to spend a lot of energy or money to do the mining process. So, it will be faster and save energy than the blockchain, than the Bitcoin.
1810000	1831000	So, in our paper, this is our main architecture. The main difference is that you can see that in the data persistent layer, our blockchain, we use the hybrid ledger framework. But in the previous work, they did not mention which blockchain they used.
1832000	1854000	Also, we have incentive to let the oppressing actors to do things well. We invented incentive mechanisms to let people willing to do things very well, not to produce some dishonest behaviors.
1855000	1872000	And then our system will tell you that if the content creator sends one content into our system, our system will tell you how much percent of this content is real or fake.
1872000	1893000	Like this slide shows, the content is 66% is real and 80% is fake. This opinion is from the people you trust, in your trust list. So, you could use this oppressing result to judge this news is fake or not.
1894000	1921000	And so, this is our main flow, and we refer to the history on that. In the beginning, our system will accept the participant to add some new stakes, so they will get some credit point. And then they can use the credit point to do some oppressing, and then we can get a trust score.
1922000	1941000	And then when we get a trust score, so we could, for example, in the previous, this example, most of the majority thinks that this content is real. So, the minority, why this minority think this content is fake?
1941000	1967000	So, our system will give reward to those people that are, belongs to the majority, and to get the punishment to people who are in the minority, right? Because all people are professions, why you think this content is fake, not correct, not similar to the majority's opinion.
1968000	1979000	And then before I get into how to calculate the trust score, I want to introduce one concept is that we call it entropy.
1980000	1996000	And yeah, don't think it too complicated. I don't want to introduce some to academia or some to top concept. You can only see that this concept is from the chemistry.
1997000	2011000	And from, yeah, example is that if you have a glass of water and you put ink into the water, the water will, the ink will from one point and will spread to everywhere, right?
2012000	2029000	So, entropy is to measure this phenomenon of a sense. Is it a disorder or not? If the entropy is high, because it means that the sense is more disordered.
2030000	2045000	I could put here. And so, in this slide, you could see that the red point is really, really similar to, you could imagine that the one red point is one people.
2046000	2060000	Every day, our opinion on the same. It's just like a little entropy, right? They will stay together very closely. But if their opinion divergent, they will have high entropy, right? So, their opinion will not the same.
2061000	2078000	So, I'll continue begin to tell you how we figure the trust school, and I will put some equations there but you don't need to, to look very detail into equations, you just need to look at the numbers.
2079000	2088000	Okay, but for this example, if we have five oppressors, and three of them agree, and the two of them reject.
2089000	2110000	So we could get that the possibility of radio is three divided by this on this this is a possibility, and this is three divided five so it's the 0.5, and the P fact will be true divided five will be 0.4.
2111000	2119000	And then we put this the period and P fact into this equation, then we can get to the entropy for this case will be the raw point.
2120000	2123000	Okay, let's just remember the number.
2123000	2133000	And another case is that if we have the, the four oppressors agree, only one of reject.
2134000	2147000	You could get that, you know, in this case, the people have the more consistent opinion right so the entropy will lower and in the example one entropy is higher.
2148000	2154000	So our system, encourage people to let their opinion, consistent.
2155000	2177000	I entropy, our reward or punishment will lower, but in the, in the example to our reward and the punishment will be higher.
2177000	2190000	So, I will give you a table and to show you how we calculate the trust score. And so, if you remember the metaphor, I said before, in the beginning.
2191000	2212000	This is our five people, and they have some opinions in the specific news, and then their initial stakes is that a one put 5000 into the system, a to put 1000, and then a total stakes is the 21,000.
2212000	2216000	So, you could get the, the credit point or oppressor.
2216000	2229000	And there's those is the 5000 to divide 21,000 so we could get the age appraisers credit point of credit point over process.
2229000	2230000	Right.
2230000	2236000	So it's just a simple, simple math.
2236000	2257000	Okay, so we right now we have no how the point of the appraisers, and then when they get when the appraisers gave opinion to the content they could say that, oh, I'm 100% to throw that this news is back on.
2258000	2261000	But, like, a to.
2261000	2275000	He could also say that, oh, this, the content is a little bit tough for me to charge. So I, I just have the 70% confidence that it is true.
2276000	2294000	So our trust score will be the CPOA to multiply the confidence. So you could see that this SOA is the score of authentic is the older people agrees and their SOA the summation of the older SOA.
2295000	2307000	And here is the sum score of egg is the same as a straw score of score, score of authentic. And we also submission.
2307000	2318000	All the people's degrees, and the summation the SOF to get the final score. So is the trust the how we calculate trust score.
2319000	2325000	And then when we know our trust score, like I said, our system will decide.
2325000	2340000	We will reflect the reward or punishment to each appraisers, and we define that our basic reward is the total stake to modify the 0.1 percentage.
2340000	2352000	So, in this case, will be 21 and basic punishment will be higher or higher than the reward because we want to stop the malicious knows.
2353000	2362000	So, the basic punishment will be total stake, multiply the 10%. So, it will be the 2100.
2362000	2376000	And then the reward of the content, which means that when content comes in, and how many reward we will reflect to all the appraisers who did their jobs.
2376000	2380000	So, our equation will be.
2380000	2401000	We use the basic reward to modify the value of the one minus the entropy s. So, in, you could see that in this case, we know that, you know, our example one, our entropy is 0.9697.
2401000	2404000	So, the RLC or PLC will be.
2405000	2408000	6.3 and 6.3 respectively.
2412000	2413000	Okay.
2414000	2427000	So, then we know that the basic reward, and we don't know the reward of content and the punishment of content and the how we reflect to each appraisers.
2427000	2433000	And we could, we will to calculate the ratio of which means that.
2434000	2448000	For example, in SOA, we have 0.66 and for A1, we will know that there are 36% of the trust score.
2448000	2452000	The score of authentic is come from.
2452000	2455000	It's come from the appraiser one, right?
2455000	2462000	So, we will use this ratio to reflect the reward to each appraiser.
2462000	2465000	And for the punishment, it's the same.
2465000	2472000	We will know that how you contribute to the trust of the fake, right?
2472000	2482000	And then if you contribute, or you occupy more percentage, you will get more reward or more punishment.
2486000	2487000	Okay, so.
2487000	2500000	In here, you can see we calculate a ratio of the RLC and the ratio of PLC, which each appraisers need to be reflected to be added or be reduced.
2500000	2513000	And then, when we reflect it, you can see that for, for example, in for A1, the initial is initial state is 5,000 and after state is the 5,000 point.
2513000	2522000	23, so we could get all the after stakes, and then we can calculate the after credit point.
2522000	2536000	So, this is a cycle when every content, when we finish the appraising process, we will have the new stakes and the new credit point, and then they will get ready to accept content.
2537000	2550000	And if any appraisers, their credit point is close to zero, which means that their stakes already, they get, they got a lot of the punishment and their credit point.
2551000	2568000	Their stakes is equal to zero, and we will go back to our main floor, the initial process that to accept to ask this appraisers, do we need to add more stakes to continue to be the appraisers?
2569000	2583000	So, that's how, so we only finish the, I think that the most difficult part in our presentation is to calculate the trust score.
2584000	2603000	And in this slide that we do some experiments that to improve, to prove that our system could be a scale on a lot of the users, you could see that, and in the left side, the picture shows that our system could be in our testing environment.
2603000	2628000	We use the 4 servers and the throughput is still goes well, but in our system, we set the contents, the maxima of the content is the 8 megabytes, because you could imagine that if we could compress the content for videos or some, we use the in-camera size.
2628000	2631000	There is the technique you could use the content to.
2632000	2635000	As the input, and we have the output is a very short.
2635000	2648000	Um, or some contents, and it will not to occupy a lot of size. So, our intention of the content will be the 8 megabytes.
2649000	2668000	And the right hand side, the picture shows that we all know that the computer system is rely on the tolerance, which means that if the main server fails, our systems should still work well. Right? You could see that.
2668000	2697000	Okay, so I think we can get into.
2697000	2698000	Conclusion.
2699000	2705000	So, yeah, so our system, I asked to summarize some take home points for all of you.
2706000	2714000	Yeah, as you remember that I've said that the AI technique is getting better and better. So.
2715000	2722000	The traditional algorithm does not get the great performance to detect the automatic.
2723000	2728000	So, if we use the human's capabilities, we will be a better solution.
2729000	2732000	Um, then the traditional algorithms.
2733000	2741000	But, of course, we could combine the advantage for the AI algorithms and the human based capability. For example, we can.
2742000	2754000	Use AI technique to do the 1st day of filters and people can get into the check the results. Right? But the most important part is the blockchain network.
2755000	2760000	Because it's the decentralized not to manage it by only 1 companies.
2761000	2766000	It's a transparency and very easier to find some data is being modified.
2767000	2771000	And so in this experience, we know that our system.
2772000	2775000	It's the architecture is peaceful, and we could.
2776000	2780000	It has potential to be deployed in some sense of machines.
2781000	2786000	And the most important part is that our incentive mechanism.
2787000	2790000	Offers this architecture commercial potential.
2790000	2791000	You can imagine that to.
2792000	2796000	If you are a writer, or you are a photographer, you could use our system.
2797000	2799000	To get some proof.
2800000	2805000	That your content is for you, and your, your reputation will getting better. Right?
2806000	2809000	And you are a journalist, or you are some.
2810000	2813000	If people doing the fact checking.
2814000	2816000	And you can get reward for our system.
2816000	2818000	So, there is also.
2819000	2824000	Then this will get some commercial and purpose and for building.
2825000	2828000	On this, the technical precision systems.
2830000	2831000	Okay.
2832000	2833000	So, that's.
2834000	2841000	It's a long presentation, so I stopped here, and I actually here is that this research is.
2842000	2848000	Supported by report is the payment protocol.
2849000	2850000	And also.
2851000	2853000	By our national science and engineering.
2854000	2856000	Research Council of Canada.
2857000	2859000	And also thanks my mentor to give me.
2861000	2863000	A lot of suggestions and help when I.
2864000	2865000	And to.
2866000	2867000	Find some.
2868000	2870000	Research ideas or some.
2872000	2873000	No more people how to do well.
2874000	2877000	In a program, so I will introduce.
2879000	2880000	Facebook and.
2881000	2885000	You got a lot of great things for all of you who wants to.
2886000	2888000	Study for, or you have some.
2889000	2894000	If you are women is that is the best for you to check.
2894000	2896000	So, I put it here and out of thing.
2897000	2900000	To be my mentor and give me more.
2901000	2902000	Help.
2904000	2905000	So.
2906000	2909000	That's all the presentation and if you have a question.
2910000	2911000	You could ask me right now.
2912000	2913000	Or.
2913000	2917000	After the presentation, you could also drop me a mail or find me.
2918000	2919000	I also.
2920000	2921000	We will discuss with you.
2922000	2924000	Or detail or any questions you have.
2925000	2926000	Thank you so much.
2928000	2933000	So, yeah, actually, so before we start our QA session, please turn on your microphones again.
2934000	2938000	So, let's thank for the very interesting talk.
2941000	2942000	Thank you.
2943000	2944000	So.
2945000	2951000	I'm not sure, but I'm guessing that some of our attendees would like to ask in Mandarin Chinese.
2952000	2957000	Probably, I guess, but I mean, any kind of language as long as you can come in, that will all be fine.
2958000	2960000	So, yeah, so please feel free to ask questions.
2962000	2963000	Anyone.
2965000	2966000	Hello.
2966000	2967000	Hi, I'm.
2968000	2973000	And I would like to know, because I noticed that actually you have no experience starting a board, right?
2974000	2975000	It's your first time.
2976000	2978000	How do you how do you practice English?
2979000	2981000	Sorry, it's not related to your content, but I'm just curious.
2982000	2983000	Yeah.
2983000	2986000	Oh, yeah, so interesting question is that I think.
2987000	2991000	I joined some English speaking practice in crosses in.
2992000	2995000	So, yeah, they gave me a lot of practice to.
2997000	2998000	To.
3003000	3009000	I to join, which some master the crops near.
3010000	3013000	So, there's that option for me to practice in English.
3013000	3014000	Yeah.
3017000	3018000	Is that answer your question?
3019000	3023000	Oh, yes, and I heard some people say that we should prepare.
3024000	3025000	Maybe we should join the, like.
3027000	3029000	It's faster than we prepare English fast.
3030000	3031000	How do you think about that?
3032000	3037000	If I cannot reach the score that I want, and did I need to apply for the language?
3037000	3046000	Or English, and I'm here, I get a score, get a total capital, and then apply for the school.
3051000	3054000	Okay, this is not an interesting questions yet.
3055000	3058000	Actually, I will suggest that this depends.
3059000	3060000	If you.
3061000	3064000	I will suggest to prepare English.
3065000	3067000	And try your best to pass the top.
3069000	3072000	In Taiwan, because if you.
3073000	3076000	Go study abroad and you go to some language schools.
3077000	3079000	The cost is to leave is very high.
3080000	3082000	And then it will spend a lot of money.
3083000	3086000	But, but the most important thing is the.
3086000	3089000	If you sometimes.
3090000	3095000	Some people are struggling to passing to pass the exams.
3096000	3098000	So, if you found that you.
3099000	3100000	You cannot concentrate to.
3101000	3102000	Prepare for it.
3103000	3106000	And you stuck on the example.
3108000	3114000	And also the schools offer some opportunities that they say you could go to the university.
3114000	3119000	They say you could go to the university to take the language courses, and then you can get over.
3120000	3122000	I would say you just go because.
3124000	3125000	The time is important.
3127000	3128000	Do not to stack 1.
3130000	3135000	1 stage for a long time. Yeah, I was, I think that would be my answer.
3141000	3143000	Yeah, so I have a question.
3144000	3145000	So, um.
3145000	3152000	So, if we go back to your title, I think you're saying that this technique is mainly for.
3154000	3160000	Um, how do I say this for deploying against fake news, quote, unquote news, right?
3160000	3166000	But if you let the people decide what will be the thing that they recognize, right?
3166000	3168000	They basically just vote.
3169000	3172000	Let's say Barack Obama just died.
3172000	3178000	And if you let the Internet vote, probably 99% of the people will say that's a fake news.
3178000	3179000	Right.
3179000	3185000	But let's say objectively true thing that cannot be voted to become a truth in your system.
3185000	3186000	How do you comment about that?
3187000	3188000	Uh, yeah, like I said.
3189000	3193000	Um, we trust the majority opinion, right in our system.
3194000	3203000	We have the reward and the punishment mechanisms to make make you to do the correct and the honest.
3205000	3214000	Actually, in all of the production network, they have the specific incentive mechanism to prevent from some malicious notes.
3215000	3216000	So, if you joined our.
3217000	3220000	We have 2 mechanism to prevent.
3221000	3225000	This, uh, the notes to adjust them to vote.
3226000	3233000	Actually, they will do their best because the 1st, if they join our system, they need to pay for pay fees for.
3234000	3236000	I say the stakes, so it's a cost.
3237000	3241000	And then if we join the system, and you did not do your job very well.
3241000	3242000	You will get.
3243000	3246000	The, uh, the punishment, and then eventually.
3247000	3251000	You are not, you will be prohibited to be the oppressors in our system.
3252000	3254000	So, our mechanism of incentive.
3255000	3258000	It's a core concept to incentive people.
3260000	3263000	To check the content seriously and to fault.
3263000	3264000	Correct.
3268000	3274000	So, if I understand correctly, basically, the assumption that you're going with is that.
3275000	3282000	People generally have good intention, and they are answering based on their good intention, regardless of the truth.
3283000	3283000	
3283000	3285000	So, say, Obama just died.
3286000	3287000	Nobody knows.
3287000	3288000	And they basically just.
3289000	3290000	Like.
3291000	3297000	Saying it's not a good news because it's just not the thing that is well established that I know.
3298000	3299000	But it's true.
3299000	3300000	It's true.
3300000	3300000	
3300000	3308000	So, but so, in your system, it's basically working against malicious information, but not exactly news.
3309000	3310000	And I say that.
3311000	3319000	Um, I could say how to define the news is that the content should be.
3320000	3321000	Correct.
3321000	3322000	Right.
3322000	3326000	So, when people, when you say Obama is that.
3328000	3329000	If it's not a truth.
3330000	3334000	Our system will consider it is a back news.
3334000	3335000	Right.
3335000	3338000	Yeah, but in my scenario, I'm saying.
3338000	3341000	What if he really just died?
3342000	3349000	But nobody knows, like, 10 minutes ago, I go out and say, hey, guys, he just died.
3350000	3352000	That will be the fake news in your system.
3353000	3354000	Nobody knows if that's true.
3356000	3357000	Yeah, you're right, but.
3360000	3363000	If you say Obama is dead, but nobody knows.
3364000	3365000	Um.
3367000	3370000	I would say that this situation is really.
3372000	3373000	Hmm.
3373000	3375000	That's why this is news.
3377000	3378000	Yeah, so.
3379000	3385000	I think because the most oppressing actors are very capable to check the contents.
3386000	3394000	So, if they, if they hesitate to give some opinions, their confidence will lower.
3395000	3398000	So, the trust the score of the.
3399000	3403000	Agree authentic and the fact there will be very similar.
3404000	3407000	So, our system will not trade it in your case.
3408000	3409000	It's a very.
3410000	3413000	Can do your real effect will be a balance.
3414000	3416000	But we could give you.
3417000	3418000	Objective.
3419000	3420000	Opinion.
3420000	3421000	To check this content.
3424000	3425000	I see.
3426000	3427000	Okay.
3428000	3431000	I do have some comment about a previous question.
3431000	3432000	Just like.
3433000	3434000	Okay.
3434000	3435000	Okay.
3435000	3436000	First, thank you for a nice talk.
3436000	3440000	And for the token number design, I think that's about how you make a dispute system.
3441000	3444000	Because for now, it's like a 1 time votes and you just get the result done.
3444000	3445000	It's done.
3445000	3455000	But if you see, like, uh, in order for the prediction market, just like, if you predict and and like, whether Obama will die between, like, 2021st.
3455000	3460000	And people at 2021st votes, maybe 3% of the voter is enough to get.
3461000	3462000	Yes or no.
3462000	3469000	But if you don't, if you don't trust the result, you can put your, your state against that voting result.
3469000	3471000	And and then more people need to vote.
3471000	3478000	Like, if you stay, like, maybe 1% of the total, total, total, like, token, then all of the people need to vote.
3479000	3486000	So, so I guess you need to kind of, like, refine the, the appeal system or the tokenomic.
3486000	3497000	Because I think the previous, the previous, like, question is quite valid, because you assume that, like, the majority vote.
3497000	3500000	Well, well, well, not the truth, but maybe it takes some time, right?
3500000	3506000	So, but if you like, you want to label the news in 10 minutes, and definitely.
3506000	3515000	Well, it's hard to get information to propagate to, like, the society or civilization to get people votes and get the correct result.
3515000	3521000	So, I guess you just need to have a dispute system, I guess.
3521000	3522000	Yeah, sure.
3522000	3523000	Yeah.
3524000	3525000	Okay.
3525000	3526000	Yeah.
3526000	3537000	If you want to get a news in just 10 minutes to see if it's correct or not, yeah, it will be difficult to achieve that.
3537000	3543000	Because maybe, like Elon said, the news is not just known by minority people.
3544000	3547000	But, yeah.
3547000	3568000	For this case, I think for previous, the current solutions, I think people will have, like, we have put more stakes or to, we can to, to introduce more, the sub-parties, the libraries that, and we can ask their opinions.
3568000	3575000	So our speed will get in faster.
3575000	3576000	Yeah.
3576000	3582000	Well, I do think this is a very interesting, actually very valuable research.
3582000	3584000	I'm just being curious.
3584000	3585000	Like, yeah.
3585000	3592000	So, anyone has other questions?
3593000	3599000	Do you really, like, plan to launch this thing in a public chain?
3599000	3605000	Because I would say in the past, like, two years, hyperledger is not, like, really thriving.
3606000	3621000	And even though, like, the gas fee and the transaction fee is an issue for, like, Bitcoin and Ethereum, but for the new generation, like, high throughput blockchain, also support a smart contract, then it will be a much better, like, platform, I would say.
3621000	3636000	Because, well, hyperledger is definitely more, like, clean and more permission, but how to really scale the, how to really scale the platform, I would say, like, public chain is still better than the permission chain.
3636000	3637000	Yeah.
3637000	3638000	Yeah.
3638000	3640000	Actually, you're right.
3640000	3649000	We plan to introduce some digital tokens in our system because we need to get the reward or punishment.
3649000	3658000	So, Ethereum will be the one choice that we consider to transfer the hyperledger to the Ethereum.
3658000	3663000	And also, there are other tokens that are invented right now.
3663000	3669000	They have a great, because they, Ethereum and Bitcoin, their speed is too slow, right?
3669000	3674000	But there are lots of faster blockchain network.
3674000	3683000	So, we consider to, yeah, like you said, to transfer from the private blockchain to the public.
3683000	3684000	Yeah.
3685000	3686000	Okay.
3686000	3688000	Thank you for applying.
3688000	3689000	Yeah.
3689000	3690000	Thank you.
3690000	3691000	Thank you for asking.
3695000	3696000	Okay.
3696000	3697000	So, one last time.
3697000	3699000	Anybody has a question?
3699000	3700000	Okay.
3700000	3704000	If not, please, again, really turn on your microphone.
3704000	3707000	So, let's thank our speaker again.
3707000	3709000	That's a very interesting talk.
3709000	3710000	Thank you.
3710000	3711000	Thank you.
3711000	3714000	Do you also have any questions?
