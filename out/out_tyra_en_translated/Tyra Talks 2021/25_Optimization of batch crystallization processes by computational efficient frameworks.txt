Both are fine.
Do you want to introduce again?
It's okay, no need.
Let's start.
I hope the internet is okay later.
If it breaks again, let me know.
Is the full screen screen okay now?
Okay.
Okay, I'll start right away.
Thanks to the host, Wei Zhe, for the introduction.
I'm Pan Haoren from the Department of Chemical Engineering.
The topic of today's report is related to my research.
It is a high-efficiency algorithm for the optimization of P4 alkynes.
Before I start talking about today's topic,
I want to clarify one thing.
Because when you hear the word alkene,
if you are a material background,
or you may be a chemistry background,
or you are a chemistry background,
or you are a chemistry background,
you may want to discuss,
for example, the problem of alkyne arrangement,
for example, some morphology,
for example, I may want to play SRD,
and so on.
But in the field of procedural engineering,
that is, in the field of PSE,
we are referring to any process that can be obtained from the solution.
So today, even if it is just a sedimentary reaction,
we will also broadly call it crystallization.
Under this definition,
this crystallization is a process of separation and purification.
It can be done in a continuous way,
or in a batch way.
SRD is the so-called supersaturation.
It can be obtained in the following four ways.
What we are discussing today is the crystallization of batches.
Most of the crystallization of batches
is to make some high-added-value compounds,
such as specialized chemicals,
such as food additives,
such as drugs, and so on.
Compared to this continuous way,
its operability is relatively large.
It can also deal with some more difficult-to-deal with systems,
such as toxic chemicals,
such as the higher viscosity of the sedimentary solution,
such as the higher viscosity of the sedimentary solution,
such as the higher viscosity of the sedimentary solution,
Such crystallization has two basic mechanisms,
which are called nucleation, synthesis, and crystal growth.
Nucleation means the new crystal body is generated.
When a new crystal body is generated,
its volume is basically less than zero.
Crystal growth means that a new crystal body exists in a batch.
Because of the high-added-value compounds,
it grows bigger and bigger.
This is what we call crystal growth.
Nucleation can be divided into several different mechanisms.
The first one is homogeneous primary nucleation.
It is a spontaneous reaction in thermodynamics.
Nucleation can only occur in a high-added-value system.
The second one is heterogeneous primary nucleation.
It is a reaction of different particles.
For example, it may be caused by some pink particles.
The third one is secondary nucleation.
It means that nucleation is generated on the surface of the existing crystal body.
For example, in a batch, the crystal body may collide with each other.
There will be some friction.
When the friction occurs, its volume is basically less than zero.
This is what we call nucleation.
Let me give you an example.
If there is a batch today,
there is no crystal body in it at the beginning.
In the early stage of this batch,
because we want to have the crystal body from nothing,
it is called primary nucleation.
However, in the later stage,
because there are more crystal bodies,
crystal growth and secondary nucleation
will become the main mechanism.
In the operation of a batch,
we can usually add some crystals at the beginning,
which are called seed crystals,
or simply called seed.
The reason is that these seeds can quickly
exhaust the over-homogeneity.
Once it is exhausted,
it can inhibit the so-called primary nucleation.
What is the advantage of this?
The advantage is that primary nucleation has a feature
called stochastic,
which means that it is random.
So in this operation,
you can make this process more stable.
Next, let's talk about how to measure the quality of a product.
Of course, we know that the purity of the product
is a very important indicator.
But in addition to purity,
another important indicator is the so-called
crystal size distribution, or CSD.
A good CSD usually has the following features.
For example, it must be single-seam.
For example, the ratio of fine particles should be lower.
Next, because in some circumstances,
for example, if I want to make a drug today,
I would like this drug to be in the human body.
The conclusion curve is a curve that we want.
In this case, I may need to specify
the average filter of a product
and the so-called variance of this CSD.
If we can find a way to design a process
to make the average filter and variance of this product
meet such a requirement,
it will also be a situation that we want.
Of course, if today's CSD is not very good,
it may cause some problems.
For example, if today's product has the so-called double-seam CSD,
or its filter distribution,
or its fine particles are too high,
it may cause the following problems,
including excessive filter resistance,
including the need to use a lot of washing liquid
to clean the solid,
or even when it is sent out,
these small particles may be directly stuck on the tube wall.
These are all situations that we don't want to see.
So basically, what we want to do is
to suppress some excessive nucleation.
Next, if we talk about some
CSD control strategies,
it can be divided into two categories.
The first category is what we call model-free feedback control.
Its idea is that it will use a so-called feedback control loop,
and some corresponding
self-made analytical technology,
the so-called PAT,
to implement this.
Basically, such a control strategy,
I just need to know some basic features of the system,
for example, what kind of filter is OK.
The so-called model-free model here
refers to some kinetic energy of crystal growth and nucleation.
When we use this type of strategy,
we don't need to know the kinetic energy of these growth factors.
One of the so-called model-free strategies
is called supersaturation control.
Some people call it concentration control.
What it means is that
in the entire batch,
we control the saturation in a set value.
Of course, it will need some corresponding
analytical technology.
For example, like this,
it will need an online concentration measurement.
For example, you can use FTIR to do this.
This strategy is not bad,
because we don't need too much information.
But there is a drawback.
We still can't design this product's CSD directly.
Because it doesn't have a model to describe.
So in contrast,
another method is called model-based control.
It basically uses a function called population density function
to describe this CSD.
How does this CSD change in the batch?
We will use a population balance equation
to describe it.
In this strategy,
we usually multiply optimization problems
to find the so-called best control strategy.
The target function may include some
batch time,
or some characteristics of product CSD statistics.
Decision variable is the so-called
control strategy.
For example, it may be related to the temperature trajectory.
Or I may consider some so-called seed recipes,
which is how many seeds I want to add
and its average filter, and so on.
One of the challenges of using this model-based control
is that its burden will be greater.
Because the so-called PBE just now
belongs to a distributed city,
so its computational burden is relatively large.
This is also one of the purposes of our research.
We hope to use some high-performance computational methods
to improve the optimization of these P4 solution.
We will analyze several problems,
including some competition between target functions, trade-off.
Next, we want to find the seed recipe design
and some experience rules of control strategy,
the so-called rule of thumb.
Next, we will also explore
if what we use today is the seed coding process,
what impact will it have on the control of CSD?
Next, we want to introduce some theories.
At the beginning, we want to introduce the so-called
population density function.
As mentioned earlier, it is used to describe CSD.
Assuming that there is a system today,
we only use the feature length of a system to describe it.
For example, if the solution of this system today
is close to the sphere,
then we can use the diameter or radius
as the feature length.
If there is such a description,
we put the pdf to l,
and the result will be
the solution number in the batch.
Next, we can define a pdf moment.
This moment is not a moment.
It has some other translations in physics,
but I don't think it fits our situation.
So I just call it moment.
I won't translate it.
The definition of moment is as follows.
From mu0 to mu3,
these moments all have corresponding physical meanings.
The physical meaning of mu3 is the total volume of the solid.
After we define the pdf,
we can write the corresponding population balance equation.
In this study, we have some assumptions,
including the theory in the batch,
which is uniform,
and its volume is constant.
We also ignore some phenomena,
such as the collision of solids
and the splitting of solids.
This is called crystal breakage.
Or the collision of solids and solids.
This is called aggregation.
Let's ignore these assumptions.
Next is the volume of the solid.
It has a volume of 0 at the beginning.
Under these assumptions,
we can get a corresponding ppe.
As you can see, it is a Pythagorean equation.
Later, we will talk about
the physical meaning of the equation
and some calculations.
If we look at the ppe,
we can see that it contains
the momentum of growth and dissolution.
Therefore, we need to introduce the part of momentum.
As mentioned earlier,
the driving force of the crystal is super saturation.
There are many different definitions of super saturation.
Here are the two most common ones,
the definition of relative and absolute.
We can see that the definition of super saturation
is related to the difference between the density of the solution
and the density of the saturation.
The density of the solution itself
can be calculated by mass balance.
Let's assume that the ppe function
is a function of temperature.
Let's assume that
the ppe function is a function of temperature.
Therefore, the primary nucleation can be ignored.
In this case,
we can define the momentum of growth rate and nucleation rate.
At present, there is no complete theory
to describe this phenomenon in the academic world.
Therefore, we still use some experimental methods.
What we can notice is that
the growth rate is divided into two parts.
The first part is only related to super saturation.
The second part is only related to the size of the crystal.
If the second part is equal to 1,
we call it size-independent growth.
If the second part is not equal to 1,
we call it size-dependent growth.
For most systems,
the nucleation rate is more sensitive to super saturation
than the growth rate.
Therefore, the parameter b is greater than g.
Therefore, the parameter b is greater than g.
Next, I will introduce
how to solve the ppe function.
At the beginning,
we will divide the ppe function into two parts.
Mathematically,
we call it homogeneous and non-homogeneous.
Physically,
we call it homogeneous and non-homogeneous.
Why do we need to divide it into two parts?
There are two reasons.
The first reason is that
it is more convenient to apply some experimental methods.
The second reason is that
we can separate the seed and nucleation
to make a quantitative description.
As I just said,
we want to suppress nucleation.
Therefore, we make a quantitative description
to make a quantitative description.
Before we start to talk about
some quantitative methods,
let's take a look at a simple example.
In this example,
the ppe function has a so-called analysis solution.
By the way, let's understand
the physical meaning of some of the functions.
Let's assume that there is a system
whose growth rate and nucleation rate
are constant.
In this way,
we can write the corresponding ppe
and the corresponding analysis solution.
How can we judge the physical meaning
of the analysis solution?
If there are some seed crystals,
the pdf at the beginning
looks like an orange.
After a while,
they will grow bigger.
How big will they grow?
The size will increase
the growth rate times batch time.
Therefore,
the yellow part
is directly shifted to the right
to shift the growth rate times batch time.
In the case of nucleation,
the size of the seed crystal
at the beginning
will also be the same
as the growth rate times batch time.
Therefore,
the pdf will look like this.
Of course,
this is just a special case.
Let's talk about
some commonly used quantitative methods.
In general,
there are two types of quantitative methods.
The first type is called
the method of moment.
There are two versions,
standard and quadrature.
In the standard version,
ppe is converted to ODE.
It can only be used
in the size-independent
kinetic system.
In the quadrature version,
ppe is converted to
the differential algebraic equation.
In other words,
there is a linear equation
of the differential equation
that can only be used
in the size-independent
kinetic system.
The advantage of this method
is that it can quickly
solve the moment,
but it can't find
the complete pdf.
Let's take a look at
the standard version.
The ODE
will look like this.
As you can see,
the seed and nucleation
are solved.
The second method
involves
some linear methods.
There are many
methods,
including the method of
characteristic,
method of classes,
or the method of
finite volume.
The advantage of this method
is that it can
keep the pdf format
and the pdf format
will be based on
the accuracy of
the CSD analysis
or the accuracy of
the numerical method.
It will need a lot of
discretization points,
which will lead to
a greater burden
on the calculation.
In this study,
we use the NOCH method.
The idea is that
the characteristic curve
is represented
by the family
of ODE.
These are some
common numerical methods.
I'd like to introduce
a method that
greatly reduces
the burden on the calculation.
It's called
coordinate transformation.
This method was
proposed by two German
scholars.
As we just saw,
the fixed value
will reduce the burden
on the calculation.
However,
since the unit of
coordinate transformation
is speed,
can we use the
VT graph
that we learned in
high school
to represent
the area?
The area
is the size
of the crystal.
So, we define
a transformer time,
which is the
size of the crystal
from batch 1
to a certain time.
The transformer time
is a unit of
length.
We can use
the VT graph
to represent
the area
of the crystal.
The area
is the size
of the crystal
from batch 1
to a certain time.
The area
is the size
of the crystal
from batch 2
to a certain time.
We only need to use
the transformer time
to represent the
area of the crystal.
This way,
we can reduce the
burden on the calculation.
After introducing
the necessary theories,
we can start to look at
the case studies.
The first part
is the competition
As mentioned earlier,
the model-based control
is to find the best
control input
under the
constraint of
the target function.
There are three types
of target function.
The first type is
to minimize the
new creation moment.
The second type is
to maximize the
variable coefficient
of variation.
Another question is
how to define
the best strategy.
This is a big question.
There is a study
conducted by
my professor
and his team.
The study found
that the best
control input
seems to have a
strong relationship
with the
target function.
Does this mean
that there is a
strong competition
between the
target function
and the
control input?
Finally,
let's analyze
the relationship
between the
target function
and the
new creation moment
and the
variable coefficient.
How do we
analyze the best
strategy of
multi-target function?
We use the
weighting sum method.
The idea is to
write the target function
as a linear addition
and the
weighting vector
should be specified
before the
optimization problem.
After the specification,
we can get
the point
on the
Pareto front.
If α is 0 or 1,
we can get
the point on
the Pareto front.
If α is 0 or 1,
we can get
the point on
the Pareto front.
How do we choose
the beta?
Let's find the
point on the
Pareto front.
Then we define
the normalized
objective function.
The point on the
Pareto front
should be
between 0 and 1.
Then we can
define the
beta.
Another advantage is
that we can
find the best
solution
in the
multiple
Pareto optimal
solutions.
The shortest
distance is the
best solution
for the
multiple
Pareto optimal solutions.
The solution is
called
knee point.
Now we can
solve the
optimization problem.
We need to consider
independent kinetics.
We also need to
consider some
restrictions.
For example,
we need to consider
production rate,
batch time,
and control input.
The control input
is set as growth rate.
We can also set it as temperature
or super saturation.
They can be
considered as
equal values.
In this case,
we can write
the problem
formulation,
as shown in the video.
For the
optimization problem,
the input variable
and the state variable,
the growth rate and the
nucleation moment,
are all time functions.
This is the
dynamic optimization problem.
There are many ways
to solve the DLP problem.
In today's
study,
we use
the optimal control theory.
The optimal control theory
can be
introduced by
Pond-Stratton's
Minimum Principle.
This principle
is a two-point
boundary-variable problem.
It is a two-point
boundary-variable problem.
It is a two-point
boundary-variable problem.
It sounds good.
It can be introduced
as a necessary condition.
But it is challenging.
First,
I just need to write
the TP-BVP,
which is challenging.
The TP-BVP
is a two-point
boundary-variable problem.
We know that
there may be
some convergence problems
in the numerical method.
In today's study,
we just need to
use the coding
transformation
and some assumptions.
Then,
we can
apply the TP-BVP
and TPs
to solve
the problem.
The TP-BVP
is a two-point
boundary-variable problem.
It is a two-point
boundary-variable problem.
We can use the
TP-BVP
and TPs
to solve
the problem.
The TP-BVP
is a two-point
boundary-variable problem.
We can use the TP-BVP
and TPs
to solve
the problem.
The TP-BVP
is a two-point
boundary-variable problem.
We can use the TP-BVP
and TPs
to solve
the problem.
The TP-BVP
is a two-point
boundary-variable problem.
We can use the TP-BVP
and TPs
to solve
the problem.
The TP-BVP
is a two-point
boundary-variable problem.
We can use the TP-BVP
and TPs
to solve
the problem.
Remember that
for any value
in the U-star,
the u-star
must have
a minimum value
for the Hamiltonian.
In this section,
the state variable
is selected as
the movement
at increased process time.
The control input
is the growth rate.
The reason for this
is to make it easier
to push down the equation.
The simplification
we mentioned earlier
is assuming that
the nucleation rate
is no longer affected
by the nucleation crystal.
This is a reasonable assumption
because the size of the seed
should be greater than
the size of the nucleation.
Because of this simplification,
it is possible to
calculate the growth rate
in the same way.
The process is relatively long,
so I won't go into details.
You can read the paper
I published
for more details.
Let's take a look at
some case studies.
In this chapter,
we used two different systems.
One is the nucleation crystal.
The other is the
pericyclic crystal.
Their
kinetic parameters
are as shown.
Note that
J refers to
the relation between
the nucleation rate
and mu3,
which is the number
of crystals in the batch.
The nucleation rate
is proportional to mu3,
but the nucleation rate
is not related to mu3.
This will lead to some
differences in the nucleation rate.
I will talk about this later.
In addition to the nucleation rate,
we will also look at
some simple control trajectories
that will lead to
a control result.
We used a system that
included linear deceleration,
three-dimensional deceleration,
and the pericyclic crystal
we mentioned earlier.
This system is called
size-independent kinetics.
For the pericyclic crystal control trajectory,
the growth rate is measured
as a fixed value.
The result of these trajectories
is as shown in the picture.
Let's look at
the pericyclic crystal system.
If it is a linear deceleration,
it will lead to
early growth trajectory,
which means that
the growth rate will be higher
due to the
nucleation rate.
The early growth
of the pericyclic crystal
will cause
the growth rate to increase.
This will lead to
a larger
nucleation rate.
On the other hand,
a tubid cooling
can be considered a net growth
because it has a large growth rate
later on,
which will lead to a
larger nucleation rate.
If we look at the
PENTA system,
we will see a similar phenomenon.
It is worth mentioning that
if we look at the temperature
trajectory,
we will find that the temperature
after the linear cooling
is relatively low.
The reason is that
a lot of the solution
will be sucked out
from the system.
This will cause
the batch temperature
to be relatively low.
Next,
if we look at
the target function
set,
we will
divide the target function
into two categories.
The first category is the lower moment.
It is composed of mu0 to mu2.
The higher moment
is composed of mu3 to mu5.
In this case,
we can divide the target function
into groups
and use
different pairs
to analyze.
You can see
that we have analyzed four groups of target functions.
The first two groups
are a higher moment
and a lower moment.
The last two groups
are two higher moments
and two lower moments.
Let's take a look at
how the different target functions
are related to each other.
If we look at
the small triangle system,
we can draw
all four groups.
We will find that
the first two groups,
which are a higher moment
and a lower moment,
will cause a strong
trade-off.
This can be seen from the
width of the x-axis.
The x-axis represents
the volume of mu3n.
The first group
is about
5000 to 7000,
which is about 40%.
The second group is about
50,000 to 10,000,
which is almost double.
The trade-off between
3.3 and 3.4
is relatively weak.
If we look at
the yellow circle,
it represents
the target function
obtained by super-saturation control.
The value of this target function
is quite close
to the so-called need point.
So we can conclude that
super-saturation control
seems to get
a trade-off result.
If we look at
the Penta system,
the trade-off
is quite similar to
the super-saturation system.
What we need to pay attention to
is the 3.1 graph.
The yellow circle
obtained by super-saturation control
is directly
overlapped with the need point.
Why?
Because in the Penta system,
the nucleation rate and
the density of the
mu3 or batch
are not related.
This means that
I don't need to
change the value of
super-saturation control
before or after the batch.
I only need to make
the nucleation number
the lowest value.
If we look at
the growth rate trajectory,
let's look at
the single objective
of the Penta system.
We will find that
the early growth trajectory
seems to be
more friendly to
the lower moment
objective.
The reason is that
early growth can avoid
the nucleation rate
to become very large
if the nucleation rate
is small.
On the other hand,
if the nucleation rate
is small,
the nucleation rate
in the later stage
will not grow
much.
This will make
the nucleation rate
smaller,
so it will be more
friendly to the higher
objectives.
If we look at the
so-called knee point
trajectory,
we will find that
the growth rate
of 3.1 and 3.2
is relatively small.
This can be verified
that super-saturation control
has a good balance
between the higher moment
and lower moment
objectives.
If we look at the
3.3 and 3.4,
we will find that
3.3 has two higher
moment objectives,
so its trajectory
is more friendly
to the later growth.
On the other hand,
3.4 has two lower
moment objectives,
so its knee point
trajectory is more
friendly to the early growth.
We can conclude that
super-saturation control
can be used
as a rule of thumb
in the
Penta system.
In conclusion,
we use
the best control
theory
to solve
these problems
in a
self-explanatory
way.
We also analyzed
the tradeoff
and found that
the higher moment
and lower moment
objectives are more competitive.
It seems that
super-saturation control
is a pretty good
strategy in these two systems.
However,
the SSC
super-saturation control
is also a good idea.
However,
we still need to
explore other systems.
When we were
looking for this document,
we found that
the numerical range
of the K-parameter
is very wide,
and it can be as wide as
a dozen orders.
In addition,
the unit of
the K-parameter
is related to
the unit of
the J-parameter.
Another problem is that
we can't judge
the numerical range
of the system
directly from the value.
Another problem
is that
we also want to explore
the design of the C-recipe.
A group of Japanese scholars
did an experiment
to explore
several different crystalline systems.
They found that
as long as the C-loading ratio
is increased,
the definition of C-loading ratio
is a ratio of
C and product mass.
As long as the C-loading ratio
is increased,
and the C-mean size is reduced,
the numerical range
can be as limited as possible.
There are two problems here.
First, we know that
if the C-loading ratio
is higher,
and the C-mean size is
smaller,
then the average
numerical range of the product
will also be smaller.
Of course, the goals mentioned earlier
are contrary.
Next, if the C-loading ratio
is too high,
or if C is used too aggressively,
then the product recycle rate
will be higher.
This is not a good phenomenon.
In order to analyze
these problems,
we will use
a soundless model
to quickly analyze
these problems.
Let me briefly introduce
this soundless model.
First, in order to reduce
the number of parameters,
we combine
these parameters.
After combining,
the definition of Kb
is different.
The definition of Gamma
is the relative sensitivity
of the nucleation rate
and growth rate
to the saturation rate.
Next, we define
the reference quantity
with the underline ref.
A prime means
that this quantity
is a soundless quantity.
For example,
I want to define a soundless growth rate,
which is the original value
divided by the reference quantity.
Based on the following definition,
we can build
this soundless model.
In this model,
the soundless C-mass
will be the C-loading ratio.
In this way,
it is easier to see
how large the ratio is.
In terms of the
selection of the target function,
we just mentioned
the competition between
the higher-moment and lower-moment
target function,
so we choose one
as a representative.
We choose the volume of nucleation
and the number of nucleation
as the higher-moment and lower-moment
representative.
We can also write the target function
as a linear addition.
We use the same method
to solve this addition problem.
The process is similar,
so I won't go into details.
As for the city recipe,
we know that
there should be an infinite number
of city recipes
that can balance the nucleation.
We define the so-called
critical city recipe.
It is a combination of
C-loading ratio and
C-means size.
For example,
the nucleated volume
at the knee point
is equal to
1% of the city volume.
We consider 1%
to be negligible,
so we can use
C-loading ratio
to balance
the nucleation.
Next,
let's look at
the
kinetic parameter.
In our research,
we found 32 sets of
different crystallization systems.
We can see the relative
parameter gamma
and J.
In most systems,
gamma is between 1 and 2.5,
while J is between 0 and 1.
You can refer to
this paper
by me as
the first author
of the paper.
We consider the
solution of the crystallization system,
including many organic compounds,
non-organic compounds,
food additives,
and medicines.
You can refer to
these details in the paper.
In this case,
we can solve
the relative parietal front.
We find that
when the value of gamma
is higher than
the value of J,
the relative parietal front
will be wider.
On the other hand,
if the nucleation rate
and growth rate
are similar to
the sensitivity of
super-saturation,
gamma will be closer to 1,
and the relative parietal front
will be weaker.
At the same time,
the difference between
super-saturation and
nucleation rate
will be greater.
In other cases,
the difference will be smaller.
The reason is that
when J is between 0 and 1,
the super-saturation will
be minimized by
mu0n,
which is the maximum
reduction of nucleation rate.
In other cases,
if gamma is not large,
the relative parietal front
will not be wide,
and the result of
nucleation rate
and super-saturation
will not be much different.
This is another proof
that super-saturation
is a good control strategy
for most systems.
As for the series of design,
we will discuss
the impact of batch time
on the series of recipes.
The longer the batch time,
the smaller the
average super-saturation
and nucleation rate.
In this case,
we don't need to add so much seed.
However,
is it worth it?
Because the longer the batch time,
it is not a good thing.
Is there a CP value
to extend the batch time
to reduce the
amount of seed?
Is there a general rule
for different
systems?
Or is there a general rule?
We would like to
do an analysis
through this
random framework.
The result is
the extended batch time
affects the
seed loading ratio,
depending on
the system.
For example,
if I change the batch time
from 1 hour to 12 hours,
the amount of seed
changes from
100% to
20%.
However,
for system 3,
the extended batch time
basically has no effect.
This is called
system specific.
In this chapter,
we have analyzed
seed recipe design
and
seed loading ratio.
We will
use
a high-efficiency
random framework
to analyze
the best solution
by
using
a high-efficiency
random framework.
In the last part,
we consider
the donor stage.
In some documents,
it is mentioned that
if we include
the dissolution mechanism
into the operation,
we will have the opportunity
to control the
seed SSD
However,
if we use
the model-based method,
we will have to
analyze the crystal
disappearance.
In the process of
dissolution,
some crystals will
disappear.
In the past,
the model-based description
was described in
a rational way,
but in this chapter,
we want to solve this problem.
Since we want to
analyze the dissolution,
we have to analyze
the PPE.
Let's assume
there are many stages in the batch.
In each stage,
only growth
or dissolution occurs.
In addition,
this process always starts
with the dissolution.
In this way,
we can write the
number of growth stage
and dissolution stage,
and write the PPE.
The lower case k
is the stage number.
At the same time,
the kinetic expression
and the definition of
transform time
are based on
the growth stage expression
and the definition of
transform time expression.
In this way,
we can see that
the expression and
definition of transform time
are similar.
Let's divide the
PPE into
seed and nucleation part.
In the seed part,
we can solve
the expletive solution
in the tau domain.
We can see that
the expletive solution
can be expressed
in the tau domain
regardless of
growth stage
or dissolution stage.
Again,
we use a
high-performance method
to solve the seed PPE.
The nucleation part
is a bit more complicated
because it involves
the size-independent
growth and dissolution
kinetics.
The reason for this
is that
if we want to
use the
standard method
of moment
to solve the PPE,
this is a necessary
assumption.
At the same time,
it is a reasonable assumption
because the value of
GL and DL
are close to 1
when L is small.
When we
do the experiment
to fit the parameter,
most of the parameters
are used in the
seed-goron crystal experiment.
There is no evidence
to show that
small nucleated crystals
have the same kinetic behavior.
So we make the above assumption.
Under this assumption,
we can write the
corresponding PPE.
As you can see,
it is a continuous solution.
What is the benefit of this?
This can lead us to
directly use
the standard method of moment
to match the continuous expression
to deduce
the corresponding ODE.
What is the role of
the continuous expression?
Let's look at this graph.
Suppose we have a batch
with only two stages.
After the first stage is over,
the nucleated
CSD
looks like this.
The second stage
will move all the CSD
to the left
when it is about to
dissolve.
Once the crystal size
is reduced,
the continuous expression
can be used
as a reference
for the
disappearing crystal rate.
After discussing
some mathematical methods
and theories,
we can look at
the so-called
attenuable seed-goron CSD.
Why do we want to
find the CSD
The main idea is
that when we
select the crystal
or select the product,
we often use the
sieve to select the product.
In this way,
the average grain diameter of the product
and the width of the CSD
will be limited by the
size of the sieve.
So if we can
design a process
to control the grain
diameter and
the width of the CSD,
it will be the best.
So we can use
the algorithm
we just introduced
to do a quick analysis.
In this chapter,
we use a
simple system.
The simple system of kinetic
is shown below.
The next question is
how do we define
the so-called
attenuable region.
We use the
mean and
variance of the product
to define it.
We define the
seed CSD
and some constraints
for optimization.
The constraints include
initial condition,
seed loading,
solubility,
and production rate.
Some more detailed
explanations can be found
in another paper
by me.
I won't go into
too much detail here.
Next,
the constraints
for the product
mu0 and mu3
can be defined
by the
mean and
variance of the product
mu0 and mu3.
Finally,
when we define
the attenuable region,
we use the mean size
to define the
number volume mean.
We can then
use the constraints
we just mentioned
and
the number volume mean
of the target
to maximize or minimize
the product's sigma p
or variance.
In this way,
we can define the
attenuable region.
Note that
we only talked about the seed.
The PBE of the seed
can be expressed by tau.
Therefore,
we can see that
we haven't mentioned
the sum of saturation
of the seed.
This is the result.
First, let's look at
the attenuable region.
As a result,
if this is a single-stage process,
we will get an attenuable curve
instead of a region.
However,
if we have more stages,
the region will become wider.
However,
the width of the region
is basically
the same as
the width of the seed.
Therefore,
the CSD of
A, B, C
will become
wider and wider.
However,
the CSD of
D, E, F, G
will not change much.
Why is that?
Let's find out.
The result is
that the growth rate
and the dissolution rate
have different sensitivities
to the crystal size.
The growth rate
is more sensitive
while the dissolution rate
is only sensitive
in a small range.
Therefore,
if I want to make the CSD
narrower,
I can only do cycling
in a narrow region.
At the same time,
the mu3 change
in the growth stage
is relatively small.
Now,
let's talk about
the attenuable region.
If we have
the product
mean size
and sigma p,
and the value
is in the attenuable region,
we can use
the optimization problem
to find out
a set of CDO and
a set of TAUF.
There are many sets of solutions.
The patch time
and the nucleated volume
of each set
are different.
However, the patch time
and the nucleated volume
will depend on
the sequence of
the second function
and the constant function.
We can use
the optimization problem
to find out
the minimum patch time
under the conditions
of the product mean
and variance.
This is a trade-off.
The larger the set point
of the super-saturation,
the shorter the patch time.
However, the same nucleation rate
will be larger.
So, we draw
a curve
to analyze
the relationship.
At the same time,
we can deduce
the corresponding temperature.
Then, we can analyze
how large the set point
of the super-saturation
will be.
In this chapter,
we used the framework
to analyze
the C-CSD tailoring problem
using
gross dissolution cycles.
We can also
analyze the so-called
crystal disappearance.
As for
the expandability
of the attainable region,
it will be determined
by the kinetic.
We also discussed
the competitive relationship
between patch time
and inhibiting nucleation.
The conclusion of
the last chapter
is that
we analyzed the following
problems
using
coordinate transformation,
optimum control theory,
target number trade-off analysis,
rule-of-sum
control strategy,
C0-0 ratio
saving problem,
and attainable region
problem.
This kind of framework
can be used in
different fields.
These are some research directions
for the future,
including the analysis
of more complex
parameters,
including
crystal shape process,
and temperature dependent
systems.
We can also consider
the control of
model-based
control.
There are some
parameter uncertainty problems,
so we need
robustness analysis.
We can also
use optimum
control theory
in some advanced
control strategies,
such as
model predictive control.
This kind of
control strategy
requires higher
computational efficiency.
This is the
end of
today's lecture.
Thank you for
listening to my
presentation.
If you have any questions,
please feel free to ask.
Can you hear me?
Yes, we can.
This is a very
informative talk.
I have a
quick question.
You said robust analysis
is basically
like error analysis.
Let's say
there is a certain
uncertainty in the
kinetic parameter.
You can only give
a rough estimate.
If there is
such an uncertainty,
how much difference
will it make
if you run
it again?
Is that what you mean?
Yes.
I am just
curious
if this is
what you want to do
in the future.
I am curious
about the
kinetic parameter.
You are right.
I can understand.
I am a scientist.
It is difficult
to get a precise
kinetic parameter.
So I just want to know
what is the error.
According to
the best
data set
in the future.
If
there is such an
error,
how much difference
will it make
if you run
it again?
Will it cause
serious consequences?
If there is
a certain error,
we know
that the model
will be different.
I just want to know
your comment on this.
First of all,
this is the
future research direction.
We are still
doing some early
studies.
Indeed,
in the robust
control
or error analysis,
there is a lot of research.
Will it cause
disastrous consequences?
It may not.
It is because
the crystallization
process is quite long.
Of course, it may cause
some troubles
in post-processing.
Therefore,
some batch-to-batch
variations
or some
up-and-down errors
need to be studied.
As for the
systems we just mentioned,
we are currently
using other people's
experimental parameters
to do the so-called
unknown case analysis.
At present, we are
focusing on
some so-called
goal-tradeoff
algorithms.
We haven't
done this kind of
research in other
groups.
I see.
Since you are discussing
the crystallization process,
assuming that
there are some errors in
the kinetic parameters,
but the result is still
optimized,
it is
quite reasonable.
Indeed,
we may consider
some so-called nominal values
and some so-called robust values.
For example,
under some
kinetic errors,
what is the
variation of the result?
I see.
Have you ever done
any experiments
with the crystallization
process?
For example,
have you ever
compared some
experimental data
with the result of
profiling?
To be honest,
I haven't.
Frankly speaking,
we are
optimizing
and calculating
the result.
Thank you.
Thank you.
Any other questions?
OK.
I have the most questions.
I have a question.
But I am not sure
if I have mentioned it.
You mentioned
the multi-target
PBE.
I forgot which page.
You mentioned Alpha, Beta,
Beta 1 and Beta 2.
You mentioned
how to
look at Beta 1 and Beta 2.
Right.
When you
introduced the function,
I was curious
how you
decide the
weight.
It seems that
you have
linked
Phi 1 and Phi 2.
Have you considered
linking more?
Or
do you use
this expression?
Phi 1 and Phi 2.
Alpha 1
will be
considered as
Phi 1.
Alpha between 0 and 1
will be
Phi 1 and Phi 2,
any weight.
As long as
Alpha between 0 and 1
can be
analyzed.
Sorry.
You may have interrupted.
You can
continue.
OK.
Alpha between 0 and 1
will be considered as
Phi 1 or Phi 2.
As long as
Alpha between 0 and 1
can be considered as
Phi 1 or Phi 2,
any weight.
Right.
How do you know
which one is better?
If Alpha is bigger,
Phi 1 is more important.
If Alpha is smaller,
Phi 2 is more important.
How do you know
which one is more important?
OK.
Because
the first question is
it's hard to say.
For example, Phi 1 is twice as important as Phi 2.
It's hard to define.
So the idea is
we do a
so-called normalization
on page 22.
We compress Phi 1 and Phi 2
to be between 0 and 1.
Like this picture on the right.
These two points,
these two points correspond to
the best solution of Phi 1 and Phi 2.
After normalization,
it corresponds to
the two points
0 and 1.
So on such a plane,
the original point is
the best solution
of two goal functions at the same time.
But due to competition,
this is not possible.
So we want to
draw the cradle front first.
Then we find a point
that is the shortest distance
between this point and the original point.
It means that
the value of this point
in Phi 1 and Phi 2
and the percentage difference
between them
is the smallest.
And this point is
the best trade-off.
There is a
name in the multi-goal function
called knee point.
It can be used to describe such a point.
Basically, it is done
in this way.
I see.
What I just saw is
how to say
when you minimize
the distance from the original point,
my intuition is that
in terms of math,
that point also means
that the plus solution
you are looking for
should be minimized.
Or is it not necessary?
I'm just curious
if there is a link between them.
I should say
that there is a
so-called double-layer optimization
when solving.
The first layer is
to find any point
on this cradle front.
We can call any point
a plus solution.
It's just that
we need to find
the best solution.
Then we need to
minimize the distance.
So in fact,
when solving such a problem,
it is actually a double-layer optimization problem.
Double-layer may come from
you have two goal functions.
Because I just saw that
you have some different combinations,
maybe four combinations,
and each of them involves
two goal functions.
Of course,
in optimization,
the two goal functions
must be a little bit
equal to each other.
This is more meaningful.
Otherwise,
it's just that both are minimized
or both are maximized.
I can imagine
that if you have n goal functions,
your combination must be
c6x2 or something like that.
I'm just wondering
whether you have ever
considered
whether it is reasonable to
consider more than two goal functions,
or even three or more goal functions.
OK.
There are several ideas.
The first idea is
that of course we can
consider six goal functions.
Of course we can do
c6x2 or 15.
However,
is it necessary?
In fact,
between some goal functions,
how should I put it?
When doing the analysis,
we found that
the relationship between
the higher moment and lower moment
is significantly stronger.
So in this case,
when we are doing
the so-called tradeoff analysis
or when we want to find
the relationship between
the higher moment
and lower moment,
it is not necessary
to consider
the so-called
two higher moment
or two lower moment
goal functions.
Of course,
in fact,
it can be done,
and it can also be done mathematically.
However,
the relationship between the higher moment
and lower moment
may become plateau surface
or even higher.
I believe it can be done mathematically.
It's just that we didn't do
such a thing.
In addition,
when we usually
explore the
Piscitein's maximization,
we probably only see
at most two goal functions.
At present,
we only see more than three
goal functions.
Another idea may be that
these are some of the
characteristics of product CST,
so the two characteristics
can describe
the benefits and
disadvantages of this product.
I see.
So in most of your cases,
it sounds like
before you did it,
you probably knew
how to get
which two
or how to say
which two
goal functions
have a stronger
anti-alignment relationship.
Yes.
In fact, in some
earlier studies,
it was found that
for example,
the moment of the goal function
is relatively large,
its trajectory will become
larger in the later stage of the growth rate.
On the contrary,
if the goal function
is composed of small moments,
its trajectory will be
larger in the early stage of the growth rate.
Just like this,
there will be some
ideas that
whether the
early stage is larger,
and the later stage is larger,
its trade-off is constant.
There is such an idea.
But the reason why
earlier studies
can't draw a
plateau front like this
is because
in fact,
if I set an alpha,
I have to solve the
optimization problem once.
If I use
some different
so-called solving methods,
it may involve
some non-variable.
In this way,
I only need one point,
and I may die.
Not to mention that
I have to draw a complete plateau front,
and I have to find the knee point.
So earlier studies
had this idea,
but due to
computational burden,
they didn't do this in the early stage.
Until later,
the German
approach
was introduced,
we realized that
we can quickly
measure the
trade-off.
I'm just curious.
You probably know
the cost of
calculation
and
the complexity
of the time
to draw a
plateau front.
I'm not sure.
But I can be sure that
if today's situation
is like this,
because of
the time limit of
this video,
I didn't show it.
In fact,
the so-called
best
U-star
solution
can be written out.
It can be written out
under some assumptions.
But if it's written out,
it's just
a question of
point ODE.
And the point
refers to
the initial value problem.
So it becomes a very simple
solution
that can be solved
with some
false-order solvers.
This method
is a very
competitive reason.
I see.
Thank you.
Anyone else
have any questions
for the presentation?
I actually have
some questions
and some
ideas.
For example,
in our lab,
there is a senior
who is going to
parameterize
some
particles'
positions.
In the process of
parameterization,
he is going to
predict
the two parameters
of Lennard-Jones
in a particle.
He may
have four
experimental data
that he can
take down
to fit.
At that time,
he didn't know
which of the four
should be taken
or not.
So this is
a little bit
like a
multi-target function.
I'm not sure
whether this
method
can be
applied in your field.
But it reminds me that
our senior used
Bayesian inference.
There is an important
thing in Bayesian inference,
which is the
base factor.
The base factor
can estimate
the accuracy
after adding
a parameter.
Sometimes,
adding one more parameter
may not be
as good as
expected.
So we use
the base factor
for multi-target function.
I'm curious about the complexity of time.
Sometimes, when we think about
high-end problems,
the cost of
calculation
may not be linear.
So the senior's
method may not be
applicable to everything.
Sometimes, the cost of
optimizing is too high.
If we consider two parameters
and get 90% accuracy,
the cost may not be
as high as
expected.
Actually,
we have encountered
this problem recently.
It may be
related to this screen.
For example,
when we are doing
production cooperation,
we often need to
fit some live data.
For example,
today,
a junior
is doing a
high-polymer analysis
project.
The reaction of
high-polymer analysis
is very complicated.
There are only 20 or 30
power rooms,
not to mention
each power room
has its own parameters.
So we are thinking
how to do fitting
so that
the model
can be built
with Aspen Plus
or something else.
At that time,
a junior
used a
set of
Bayesian optimization
in Python.
I don't know if you mentioned
this just now.
There are some projects
that have
achieved some success
with Bayesian
optimization.
I think it should be the same.
I haven't used Python,
but I guess it's
similar in theory.
Yes,
I have
encountered a similar problem.
For example,
we want to build
a Cydia model.
The problem with this model
is that it takes
quite a long time
to calculate
with Aspen.
One and a half minutes
is not too long,
but there are
a lot of
fittings needed.
The performance
of the computer is also limited.
I don't know
which parameter combination
is the best,
so I'm trying to
find a better
fitting method.
Whether it's
good parameters
or some
uncertainty in code data,
it's quite big.
It's quite a headache for me.
It sounds like
the idea
of machine learning
can also be used
in some cases.
Do you want to consider
machine learning?
No,
fitting is just a part
of your job.
I'm just kidding.
Everyone loves machine learning.
Actually,
since three years ago,
some cases
of ASP
have been
doing machine learning.
About two years ago,
ASP
proposed
an idea
in the AI era.
They asked
every department
to propose a
improvement case
and the improvement
would directly affect
the popularity of the final exam.
At that time,
the result was
that the manufacturers
could only
vote
to see if
they could do it.
At that time,
we didn't know much about it,
so we went to
the Department of Mechanical Engineering
and proposed a case.
It turned out
to be a team
of a chemical plant,
a chemical plant,
and a factory.
However,
we found that
there were some shortcomings
or uncertainties
in the sample data
or some damage to the sample data.
So,
the final result
was too limited.
In my experience,
what we could do
was quite limited.
The results were not very good,
but I don't know if it was because
we were not
well-versed enough.
I don't know if it was because
we were not well-versed enough.
It was indeed in the development stage.
Because in the end,
you still have to go back to physics.
How do you connect
with physics in the end?
This is also a topic
that we have been working on.
In addition,
this data-driven model
is not physically meaningful,
so
there are a lot of
limitations in use.
For example, if you want to do a
on-site energy saving analysis,
it will become
impossible to use
the data-driven model,
because it will
make some
disruptive changes
to the existing operating conditions.
This will make it impossible
to use the data-based model.
It is also quite difficult
to handle.
If you are doing
a machine learning
that is not chained,
it will result in
destructive results.
So it becomes
that for the cases
we have received,
the extent to which
it can be used is quite limited.
Although they are still
in the so-called
AI era,
in the end,
some cases will
result in
the so-called
data-driven model,
which will
result in
disruptive changes
to the existing
operating conditions.
In the end,
some cases will
result in
disruptive changes
to the existing
operating conditions.
I will now
wrap it up.
Hello?
Hello?
Hello?
Hello?
Hello?
Hello?
Hello?
Hello?
Hello?
AI in
cases like ours
is quite limited.
In other words,
some cases
are not AI,
but we still have to
wrap it up.
Yes, that's another point.
What do you think
about MLD?
It's a bit too flamboyant.
Yes.
It's easy to
wrap it up.
It's easy to
learn about
quantum mechanics.
It's easy to
become flamboyant.
Yes, it's a bit
too flamboyant.
It's data-driven,
and it can't return to physics.
It's not reasonable.
If it returns to physics,
it's still
very limited.
Yes.
Of course,
this has nothing to do with
today's topic,
but we did encounter
some
problems.
Are there any other questions?
If not,
it's about time
to wrap up.
Thank you,
Haoran,
for your presentation.
Thank you,
everyone.
It's been a long time
since I've heard
this kind of thing.
It's about time
to have
supper or lunch.
If you want to
participate in our
next half year's
presentation,
you can sign up for
Tera's membership.
We'll have more
information about it.
Thank you, everyone.
Good night, everyone.
Thank you, everyone.
Bye-bye.
Bye-bye, everyone.
Why did you suddenly
stop talking?
I just wanted to
show my presence.
