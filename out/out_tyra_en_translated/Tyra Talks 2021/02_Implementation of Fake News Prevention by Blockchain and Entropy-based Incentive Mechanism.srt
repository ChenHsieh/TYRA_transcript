1
00:00:00,000 --> 00:00:08,000
Welcome back to today's Tarot Talk.

2
00:00:08,000 --> 00:00:15,000
We are honored to have Joseph here to talk about blockchain research.

3
00:00:15,000 --> 00:00:20,000
Before he starts, let me introduce him.

4
00:00:20,000 --> 00:00:28,000
Joseph is a Ph.D. student at the University of Waterloo.

5
00:00:28,000 --> 00:00:34,000
His major is Electrical and Computer Engineering, Faculty of Engineering.

6
00:00:34,000 --> 00:00:39,000
Before he came to the University of Waterloo,

7
00:00:39,000 --> 00:00:51,000
he was leading a new banking system design team in Taiwan.

8
00:00:51,000 --> 00:00:58,000
He also collaborated with Microsoft Taiwan on medical system research.

9
00:00:58,000 --> 00:01:03,000
His doctoral research is about blockchain.

10
00:01:03,000 --> 00:01:11,000
He is also interested in IoT, edge computing, security, and AI.

11
00:01:11,000 --> 00:01:16,000
In addition to his doctoral research,

12
00:01:16,000 --> 00:01:26,000
he also volunteered to mentor Taiwanese students to apply for Ph.D. programs.

13
00:01:26,000 --> 00:01:41,000
He also volunteers to mentor new graduate students in his department

14
00:01:41,000 --> 00:01:46,000
and the Taiwan Global Ambassador of UW's Office of Advancement.

15
00:01:46,000 --> 00:01:54,000
He encourages Taiwanese students to jump out of their comfort zone and pursue their dreams.

16
00:01:54,000 --> 00:02:04,000
Today's topic is

17
00:02:04,000 --> 00:02:14,000
An Implementation of Fake News Prevention by Blockchain and Entropy-Based Incentive Mechanism.

18
00:02:14,000 --> 00:02:22,000
Let's welcome Joseph with a round of applause.

19
00:02:44,000 --> 00:02:46,000
Thank you for giving my presentation.

20
00:02:46,000 --> 00:02:54,000
If you have any questions, feel free to ask in Chinese or English.

21
00:02:54,000 --> 00:03:00,000
I'm so glad to give you this presentation.

22
00:03:00,000 --> 00:03:06,000
It's about how we use blockchain techniques to combat fake news.

23
00:03:06,000 --> 00:03:20,000
Before we get into the research, I would like to introduce my background and to let you understand me more.

24
00:03:20,000 --> 00:03:23,000
This is my background.

25
00:03:23,000 --> 00:03:25,000
Actually, I was born in Taiwan.

26
00:03:25,000 --> 00:03:29,000
When I graduated, I worked in Taiwan for several years.

27
00:03:30,000 --> 00:03:35,000
As you can see, I originally worked in Hsinchu.

28
00:03:35,000 --> 00:03:40,000
Then I went back to Taipei to do some software development.

29
00:03:40,000 --> 00:03:44,000
Then I went to the banks, Yishan Bank and China Trust.

30
00:03:44,000 --> 00:03:53,000
Before I joined UW, I collaborated with Microsoft Taiwan as a technical consultant.

31
00:03:54,000 --> 00:04:03,000
My educational background is that I have a master's degree in computer science at National Taiwan Ocean University.

32
00:04:03,000 --> 00:04:12,000
For this project, I introduced that in my master's thesis.

33
00:04:12,000 --> 00:04:19,000
We developed a facial recognition system that you could input the sentences and your image.

34
00:04:20,000 --> 00:04:29,000
Then our system could generate an animation like you and the animation can say some sentences.

35
00:04:29,000 --> 00:04:34,000
It's a pretty interesting application in that year.

36
00:04:34,000 --> 00:04:41,000
As you can see right now, a lot of applications like this are published and very famous.

37
00:04:42,000 --> 00:04:44,000
That was a joke.

38
00:04:44,000 --> 00:04:50,000
If we put this product on the market, maybe I'm a millionaire now, right?

39
00:04:50,000 --> 00:05:01,000
When I graduated, for engineering students, seniors told you, you could go to Hsinchu to get a job.

40
00:05:01,000 --> 00:05:03,000
Then you can get a lot of stocks.

41
00:05:03,000 --> 00:05:05,000
Then you can retire very soon.

42
00:05:05,000 --> 00:05:07,000
It was my dream.

43
00:05:07,000 --> 00:05:11,000
When I graduated, I went to Hsinchu.

44
00:05:11,000 --> 00:05:18,000
But in that year, we met some financial crisis.

45
00:05:18,000 --> 00:05:27,000
So I came back to Taipei to do software development.

46
00:05:27,000 --> 00:05:34,000
At that time, I was very interested in mobile development.

47
00:05:34,000 --> 00:05:43,000
In that year, Microsoft collaborated with Nokia to deliver the new devices.

48
00:05:43,000 --> 00:05:46,000
There's Nokia and Windows Phone on the market.

49
00:05:46,000 --> 00:05:55,000
So I joined one company as a product manager and did a team to develop some mobile apps.

50
00:05:56,000 --> 00:06:00,000
As you can see, the interesting one is the repost.

51
00:06:00,000 --> 00:06:07,000
We transfer the Nokia's games.

52
00:06:07,000 --> 00:06:09,000
It's a Barbie game, right?

53
00:06:09,000 --> 00:06:15,000
If you're a child and you use this app, you can change the process of the app to us in these games.

54
00:06:15,000 --> 00:06:21,000
We transfer it from the iPhone and Android system to Windows Phone.

55
00:06:21,000 --> 00:06:24,000
And then we do some applications.

56
00:06:24,000 --> 00:06:28,000
We did some applications for, you know, the care, right?

57
00:06:28,000 --> 00:06:33,000
And two for Taiwan Taxi and also for EasyTag.

58
00:06:33,000 --> 00:06:34,000
All these applications.

59
00:06:34,000 --> 00:06:43,000
I was so excited that I did this application because, you know, in apps, if you are a programmer,

60
00:06:43,000 --> 00:06:48,000
the apps could give you a very straightforward feedback from customers.

61
00:06:48,000 --> 00:06:50,000
So it's very interesting.

62
00:06:50,000 --> 00:06:57,000
However, the life gave me another joke.

63
00:06:57,000 --> 00:07:03,000
It's that the Windows Phone did not succeed at that time.

64
00:07:03,000 --> 00:07:17,000
So, yeah, after I did this job for more than one year, I found that, yeah, no, this job cannot help me to earn enough money for me to buy.

65
00:07:17,000 --> 00:07:26,000
So I joined two banks and to run the directives, some financial products, and then we built a system.

66
00:07:26,000 --> 00:07:28,000
It's called the mirrorless.

67
00:07:28,000 --> 00:07:32,000
And this mirrorless is a very powerful system that the banking operations.

68
00:07:32,000 --> 00:07:40,000
If you are a trader, you could use this system to record any financial products in this system.

69
00:07:40,000 --> 00:07:46,000
And all the time in the first year, the banks use this system.

70
00:07:46,000 --> 00:07:48,000
So I went to China Trust.

71
00:07:48,000 --> 00:07:56,000
And then I, when the Hunter invited me to join EasyTag to introduce mirrorless to their banks.

72
00:07:56,000 --> 00:08:03,000
And then when I finished this mirrorless project, yeah, we made a big project in Taiwan.

73
00:08:03,000 --> 00:08:08,000
It's that the Asian Pacific organization came to Taiwan.

74
00:08:08,000 --> 00:08:17,000
They do some resource to how does Taiwan do in the anti-money laundry, which is a fine teaching.

75
00:08:17,000 --> 00:08:22,000
And so when I, I was kidding that.

76
00:08:22,000 --> 00:08:27,000
So, yeah, I am one of the Taiwan representative, the national teams in Taiwan.

77
00:08:27,000 --> 00:08:31,000
It's very honest for us, but it's very tough to do that.

78
00:08:31,000 --> 00:08:34,000
We need to finish it out of the requirements in a very short time.

79
00:08:34,000 --> 00:08:37,000
But, yeah, the final result is very good.

80
00:08:37,000 --> 00:08:41,000
And I also got a promotion in Eastland Bank.

81
00:08:41,000 --> 00:08:45,000
So, so far, it looks so good.

82
00:08:45,000 --> 00:08:48,000
So why I want to go study abroad.

83
00:08:48,000 --> 00:08:56,000
Yeah, that's because I was thinking about that if I still stay in Taiwan.

84
00:08:57,000 --> 00:09:04,000
Maybe I could gather good jobs and to be promoted, continue to be promoted.

85
00:09:04,000 --> 00:09:19,000
But, yeah, I was very, I heard about some great new technology skills or some new products from Silicon Bay or from North America.

86
00:09:19,000 --> 00:09:24,000
So it was always my, be my dream to study abroad.

87
00:09:24,000 --> 00:09:27,000
So I was so hesitant.

88
00:09:27,000 --> 00:09:34,000
But, yeah, I just had to give myself a chance to do this.

89
00:09:34,000 --> 00:09:39,000
And so right now I was thinking about that.

90
00:09:39,000 --> 00:09:44,000
The thing is that if you have a dream, it only could stop you.

91
00:09:44,000 --> 00:09:45,000
It's just that yourself.

92
00:09:46,000 --> 00:09:57,000
So I just want to encourage all of my friends or all the participants or some young students in this meeting.

93
00:09:57,000 --> 00:10:06,000
You could just to check your dreams or not to limit yourself.

94
00:10:06,000 --> 00:10:08,000
And so right now I'm here in Canada.

95
00:10:08,000 --> 00:10:13,000
So I, yeah, I saw a lot of beautiful things.

96
00:10:13,000 --> 00:10:22,000
And, but, yeah, the period is also when I prepare for this study, I met a lot of difficulties.

97
00:10:22,000 --> 00:10:27,000
And, yeah, but, yeah, it's worth it, I think.

98
00:10:28,000 --> 00:10:42,000
And, yeah, this is my another my motto is that actually adventure may hurt you, but monotony, which means that you always do the same things will kill you, kill you creative or other things.

99
00:10:42,000 --> 00:10:45,000
So, yeah, that's all my background introduction.

100
00:10:45,000 --> 00:10:50,000
So I will stop here for a while.

101
00:10:50,000 --> 00:10:52,000
If you have any question, you could ask me right now.

102
00:10:57,000 --> 00:11:16,000
Okay, I think I think you have any question feel free to ask me, and we can discuss more.

103
00:11:17,000 --> 00:11:22,000
And so I wrote some my experiences in my blog and some medium.

104
00:11:22,000 --> 00:11:26,000
If you have, you could check it and to get more detail.

105
00:11:26,000 --> 00:11:36,000
If you also want to trust your dreams, or you have some dream career, or if you could also find me on those pages.

106
00:11:36,000 --> 00:11:41,000
And, yeah, we could discuss more about your patient.

107
00:11:41,000 --> 00:11:47,000
Okay, so let's get to our main research topic today.

108
00:11:47,000 --> 00:11:56,000
Today we're going to talk about how to use the blockchain to prevent the tech news.

109
00:11:56,000 --> 00:12:18,000
Yeah, actually, what is the tech news. It is a concrete, complete fabrication, which means people to approach something or to mimic something and that is similar to a real, you are not able to confirm on the fact is, what is the real, what is the fact.

110
00:12:18,000 --> 00:12:40,000
And so, what is the actually the great effect news is not in the long future. It happens right now, you can see this slide in the depth and upside is the US stock market in one day, you can see the picture is dropped sharply.

111
00:12:40,000 --> 00:13:01,000
But why, because at that day, a news says that the Trump's, the Obama was dead, but it's a rumor, right? But however, the stock market to drop very sharply by this tech news and other examples in this slide.

112
00:13:02,000 --> 00:13:04,000
We all know that the Cambridge Analytics.

113
00:13:04,000 --> 00:13:18,000
They collect the users previous and to make some news or articles that you are interested in, and to affect your opinion in some specific issues.

114
00:13:18,000 --> 00:13:33,000
And also, the Russian used some propaganda to affect the results of the crisis. So, all these are the current threat to our society and democracy and the economy.

115
00:13:33,000 --> 00:13:55,000
Also, in Taiwan is the same. Actually, in the research, Taiwan is the most, you know, we got the most weight from the whole world. We are number one. So, it's very, like I said, it's not a long, not a straight in a long distance is the current.

116
00:13:56,000 --> 00:13:58,000
It happened currently right now.

117
00:13:59,000 --> 00:14:18,000
So, why people invest in tech news? That is because they can get a lot of the news. That's a very lower cost. Right now, the cost of producing the tech news is very cheap.

118
00:14:19,000 --> 00:14:32,000
But if you can affect a big topic, like the U.S. president election or some very huge topic, you could get a huge, huge revenues. Right?

119
00:14:33,000 --> 00:14:50,000
So, why don't we do that? If I can impact a lot of people by a very low cost, actually, the malicious people will, of course, they will do that.

120
00:14:51,000 --> 00:15:07,000
And also, AI technology makes things worse. Because in my this slide, that this is Jordan Peele. He's a U.S. actor.

121
00:15:07,000 --> 00:15:25,000
He collaborated with technology companies that use the AI technique to, you know, the deep tech technique to produce the video and to let Obama to say some words they want him to say.

122
00:15:25,000 --> 00:15:29,000
So, AI makes the tech news to be produced more easier.

123
00:15:30,000 --> 00:15:45,000
And so, but in a traditional ways, our research mainly focus on how to use the computer to detect the tech news automatically.

124
00:15:45,000 --> 00:15:54,000
How we do that? You see this picture that we have different algorithms, but mainly we use some machine learning algorithms.

125
00:15:54,000 --> 00:16:10,000
We find some clues. For example, if you are the tech news comes in and we find whether the IP address is correct or not, and its content has some weird things or not.

126
00:16:11,000 --> 00:16:31,000
So, we use some patterns to detect and to decide whether this tech news is, whether this news is correct or not. But like I said, AI makes things very similar to real. The algorithms does not have the great performance right now.

127
00:16:32,000 --> 00:16:50,000
So, some big companies like Google or Facebook, they begin to use some human beings power capabilities to help AI to check the content's authenticity.

128
00:16:51,000 --> 00:17:01,000
How they do that? They build some teams, they collaborate with the local professionals, they have the ability to check the content is correct or not.

129
00:17:01,000 --> 00:17:27,000
And Facebook also did the same things. They built a team internal, and those teams will check some news, the authenticity, and to make your news, when news push to your device, they will be real, and they will build some tech news.

130
00:17:31,000 --> 00:17:53,000
And in Taiwan, we also have one organization, and you could see that is the Taiwan TechTrack Center, and you could go on their website and to see, they will check some news and tell you this is correct or incorrect.

131
00:17:54,000 --> 00:18:20,000
Okay, so I stop here, and for these solutions, like Google or Facebook, or the Taiwan TechTrack Center, those are good solutions, but these solutions are managed by a central organization, right?

132
00:18:20,000 --> 00:18:34,000
We don't know how they work. It's like a black box. If they do not do their job well, you do not know, right? You just can trust them.

133
00:18:34,000 --> 00:18:55,000
So, in our solution, we used decentralized solutions, which means that we will form a team, form a core, a lot of people, they are advisors, and they will verify some contents.

134
00:18:56,000 --> 00:19:14,000
But this solution does not manage by only one company. It's managed by everyone who joined this system. So, this is our paper, and just accepted by DTNs 2021 in November.

135
00:19:14,000 --> 00:19:27,000
So, after the meeting, if you have more interest in detail, you could, but it's not published yet, maybe in December. So, in December, you can check this content in more detail.

136
00:19:28,000 --> 00:19:47,000
Okay, so I think we can get into the detail. So, this work is from another paper that was invented by some researchers in the UK.

137
00:19:47,000 --> 00:20:04,000
As you can see, in the middle of this architecture, there is a group that we call the appraising actors. These are some professionals, they have ability to check the content is correct or not.

138
00:20:04,000 --> 00:20:23,000
And in that side, content creators are people who create the digital content. Maybe you are a writer or you are a photographer, and you make some content, and you could send your content into our system.

139
00:20:24,000 --> 00:20:44,000
And then, those appraising actors will verify your content to check, oh, your content is real or fake, and then they will send back to you the appraising result, and then those appraising results will be saved in the blockchain network.

140
00:20:45,000 --> 00:20:55,000
After that, the right-hand side users, when they come into our system, they could check the authenticity of the contents from our blockchain network.

141
00:20:56,000 --> 00:21:14,000
So, that's an overview of the architecture of the system. Our system's architecture is similar to this architecture, but in the following slide, I will introduce the differences between our architecture and this.

142
00:21:15,000 --> 00:21:34,000
And this slide is interesting. Like I said, if we all rely on Facebook or Google to help us to check the content, how do I know they do their job very well?

143
00:21:35,000 --> 00:21:57,000
Maybe they did not do very seriously, so I still could see some fake news. But in this system, you could select the people or some authorities, media you trust, and then let them be your appraising actors.

144
00:21:58,000 --> 00:22:22,000
So, for example, in this case, I selected one is the BBC. It's a media, right? And I also selected some people, Professor Smith, Mr. John, maybe some celebrities you trust. If they say this news is real, you will trust that, right?

145
00:22:23,000 --> 00:22:39,000
So, maybe, for example, in Taiwan, maybe you are very confident, right? If he says this news is real, okay, I will trust him. So, you could select these people to be your trusted parties.

146
00:22:39,000 --> 00:22:55,000
And when the news comes in, we call it a quorum, a group of appraising actors. They will give you their opinion. Is this news correct or not?

147
00:22:55,000 --> 00:23:13,000
And so, but why we need to use the blockchain in this system? Why don't we just use the traditional databases? We just accept the appraising results. We don't need to use the blockchain, right?

148
00:23:13,000 --> 00:23:32,000
But actually, blockchain has some characteristics that if once your data is set in blockchain, you cannot modify that because everyone has one ledger in their node. Is this possible to modify all the ledgers, right?

149
00:23:33,000 --> 00:23:55,000
And another case is that if you modify the ledger in only one node, it will be easier to be fine because we have some, you know, some hash tag in computer science. The hash point is that, for example, in my node, I have a hash point to point to your node.

150
00:23:55,000 --> 00:24:17,000
But if you modify, the hash point is based on your content. If you modify the content, the hash point will be modified. So, it will be easier to be found that somebody modified the data. So, blockchain will be a great solution to maintain the correctness of the data.

151
00:24:18,000 --> 00:24:37,000
And we go back to give you some knowledge. How does the blockchain work? Actually, you can see this picture. And if you have a transaction in this system, and the nodes will compete to try to verify it.

152
00:24:37,000 --> 00:24:55,000
And once this data is verified, you will be set to the blockchain like this. In the bottom, we have some red blocks, and they are ticked by the hash point, right? So, this is how the blockchain works.

153
00:24:55,000 --> 00:25:19,000
And then it's just, there are many, many blockchains right now. So, it's a very simple classification. One is the public, and another is the private. Public means that everyone could send a transaction in this blockchain network, like Bitcoin.

154
00:25:19,000 --> 00:25:40,000
People know about Bitcoin and all of it. But in private blockchain, only people approved can send the transaction into the system. And so, this private blockchain often be used in the corporations, not for the public.

155
00:25:41,000 --> 00:26:04,000
And another thing you need to know in blockchain is the proof of work and proof of stake. What is proof of work? This means that if we have a transaction and we have a lot of nodes, which node can record this data into blockchain?

156
00:26:04,000 --> 00:26:25,000
And if you are proof of work mechanism, the nodes will compete to solve a complicated math problem. And this process, we call it mining, right? So, it's a famous word, or it's very popular to be heard. If you solve this problem, then you get the right to set data into blockchain.

157
00:26:26,000 --> 00:26:41,000
But, yeah, and the proof of work is right now the blockchain and the Ethereum used. And the proof of stake is that, oh, no, we don't to solve the math problem. We just use some stake. What does that mean?

158
00:26:41,000 --> 00:26:57,000
What does that mean is that if you want to record data into blockchain, you need to pay some fees. And if you pay higher, you will get higher chance to get the right, to be selected to get the right to set data into blockchain.

159
00:26:57,000 --> 00:27:18,000
So, these two solutions could resist the hackers, because in the proof of work, we want to control our right to record data in blockchain. You need to control more than 51% computers in the world.

160
00:27:19,000 --> 00:27:29,000
And in proof of stake, you need to pay more than 51% stakes, then you could get the right to record data.

161
00:27:30,000 --> 00:27:46,000
And then, so, you can see that. So, in blockchain, actually, I think people heard about one news in Taiwan, that one bank, some people buy a lottery, right?

162
00:27:46,000 --> 00:28:02,000
Then they collect a lot of people to buy the lottery together. But one man denied, he denied that, said, oh, no, this lottery only belongs to me.

163
00:28:02,000 --> 00:28:15,000
But if you use blockchain, the solution, the problem will not exist, because in blockchain, everything will vary transparency, and we could just update the transaction.

164
00:28:15,000 --> 00:28:35,000
So, that's the one of the blockchain could support our day life. Actually, they can solve a lot of things, like if you're trying to vote in, or to test some fishery or some products, the original prices of the product by using a blockchain.

165
00:28:35,000 --> 00:28:56,000
And then, in this system, you could, I would like to introduce the hybrid ledger framework. It's the blockchain that proposed by IBM. And this is the permission, a private blockchain that I said is for the companies, not for the public.

166
00:28:57,000 --> 00:29:12,000
And how they work, you can see this flow. If you have a transaction, you could propose to the execution chain code. The chain code is the smart contract in the blockchain network.

167
00:29:12,000 --> 00:29:27,000
I think people heard about smart contract often, very often. And then the smart contract will check your content to check is there any mistake in your content, and then send back to you.

168
00:29:27,000 --> 00:29:44,000
And when you get a permission, you can submit your transaction to the auditing services. And then the auditing services will decide how to record your data into the blockchain. And it's different to the Bitcoin.

169
00:29:44,000 --> 00:30:09,000
The hybrid ledger framework does not use the proof-of-work mechanism, which means that we don't need to spend a lot of energy or money to do the mining process. So, it will be faster and save energy than the blockchain, than the Bitcoin.

170
00:30:10,000 --> 00:30:31,000
So, in our paper, this is our main architecture. The main difference is that you can see that in the data persistent layer, our blockchain, we use the hybrid ledger framework. But in the previous work, they did not mention which blockchain they used.

171
00:30:32,000 --> 00:30:54,000
Also, we have incentive to let the oppressing actors to do things well. We invented incentive mechanisms to let people willing to do things very well, not to produce some dishonest behaviors.

172
00:30:55,000 --> 00:31:12,000
And then our system will tell you that if the content creator sends one content into our system, our system will tell you how much percent of this content is real or fake.

173
00:31:12,000 --> 00:31:33,000
Like this slide shows, the content is 66% is real and 80% is fake. This opinion is from the people you trust, in your trust list. So, you could use this oppressing result to judge this news is fake or not.

174
00:31:34,000 --> 00:32:01,000
And so, this is our main flow, and we refer to the history on that. In the beginning, our system will accept the participant to add some new stakes, so they will get some credit point. And then they can use the credit point to do some oppressing, and then we can get a trust score.

175
00:32:02,000 --> 00:32:21,000
And then when we get a trust score, so we could, for example, in the previous, this example, most of the majority thinks that this content is real. So, the minority, why this minority think this content is fake?

176
00:32:21,000 --> 00:32:47,000
So, our system will give reward to those people that are, belongs to the majority, and to get the punishment to people who are in the minority, right? Because all people are professions, why you think this content is fake, not correct, not similar to the majority's opinion.

177
00:32:48,000 --> 00:32:59,000
And then before I get into how to calculate the trust score, I want to introduce one concept is that we call it entropy.

178
00:33:00,000 --> 00:33:16,000
And yeah, don't think it too complicated. I don't want to introduce some to academia or some to top concept. You can only see that this concept is from the chemistry.

179
00:33:17,000 --> 00:33:31,000
And from, yeah, example is that if you have a glass of water and you put ink into the water, the water will, the ink will from one point and will spread to everywhere, right?

180
00:33:32,000 --> 00:33:49,000
So, entropy is to measure this phenomenon of a sense. Is it a disorder or not? If the entropy is high, because it means that the sense is more disordered.

181
00:33:50,000 --> 00:34:05,000
I could put here. And so, in this slide, you could see that the red point is really, really similar to, you could imagine that the one red point is one people.

182
00:34:06,000 --> 00:34:20,000
Every day, our opinion on the same. It's just like a little entropy, right? They will stay together very closely. But if their opinion divergent, they will have high entropy, right? So, their opinion will not the same.

183
00:34:21,000 --> 00:34:38,000
So, I'll continue begin to tell you how we figure the trust school, and I will put some equations there but you don't need to, to look very detail into equations, you just need to look at the numbers.

184
00:34:39,000 --> 00:34:48,000
Okay, but for this example, if we have five oppressors, and three of them agree, and the two of them reject.

185
00:34:49,000 --> 00:35:10,000
So we could get that the possibility of radio is three divided by this on this this is a possibility, and this is three divided five so it's the 0.5, and the P fact will be true divided five will be 0.4.

186
00:35:11,000 --> 00:35:19,000
And then we put this the period and P fact into this equation, then we can get to the entropy for this case will be the raw point.

187
00:35:20,000 --> 00:35:23,000
Okay, let's just remember the number.

188
00:35:23,000 --> 00:35:33,000
And another case is that if we have the, the four oppressors agree, only one of reject.

189
00:35:34,000 --> 00:35:47,000
You could get that, you know, in this case, the people have the more consistent opinion right so the entropy will lower and in the example one entropy is higher.

190
00:35:48,000 --> 00:35:54,000
So our system, encourage people to let their opinion, consistent.

191
00:35:55,000 --> 00:36:17,000
I entropy, our reward or punishment will lower, but in the, in the example to our reward and the punishment will be higher.

192
00:36:17,000 --> 00:36:30,000
So, I will give you a table and to show you how we calculate the trust score. And so, if you remember the metaphor, I said before, in the beginning.

193
00:36:31,000 --> 00:36:52,000
This is our five people, and they have some opinions in the specific news, and then their initial stakes is that a one put 5000 into the system, a to put 1000, and then a total stakes is the 21,000.

194
00:36:52,000 --> 00:36:56,000
So, you could get the, the credit point or oppressor.

195
00:36:56,000 --> 00:37:09,000
And there's those is the 5000 to divide 21,000 so we could get the age appraisers credit point of credit point over process.

196
00:37:09,000 --> 00:37:10,000
Right.

197
00:37:10,000 --> 00:37:16,000
So it's just a simple, simple math.

198
00:37:16,000 --> 00:37:37,000
Okay, so we right now we have no how the point of the appraisers, and then when they get when the appraisers gave opinion to the content they could say that, oh, I'm 100% to throw that this news is back on.

199
00:37:38,000 --> 00:37:41,000
But, like, a to.

200
00:37:41,000 --> 00:37:55,000
He could also say that, oh, this, the content is a little bit tough for me to charge. So I, I just have the 70% confidence that it is true.

201
00:37:56,000 --> 00:38:14,000
So our trust score will be the CPOA to multiply the confidence. So you could see that this SOA is the score of authentic is the older people agrees and their SOA the summation of the older SOA.

202
00:38:15,000 --> 00:38:27,000
And here is the sum score of egg is the same as a straw score of score, score of authentic. And we also submission.

203
00:38:27,000 --> 00:38:38,000
All the people's degrees, and the summation the SOF to get the final score. So is the trust the how we calculate trust score.

204
00:38:39,000 --> 00:38:45,000
And then when we know our trust score, like I said, our system will decide.

205
00:38:45,000 --> 00:39:00,000
We will reflect the reward or punishment to each appraisers, and we define that our basic reward is the total stake to modify the 0.1 percentage.

206
00:39:00,000 --> 00:39:12,000
So, in this case, will be 21 and basic punishment will be higher or higher than the reward because we want to stop the malicious knows.

207
00:39:13,000 --> 00:39:22,000
So, the basic punishment will be total stake, multiply the 10%. So, it will be the 2100.

208
00:39:22,000 --> 00:39:36,000
And then the reward of the content, which means that when content comes in, and how many reward we will reflect to all the appraisers who did their jobs.

209
00:39:36,000 --> 00:39:40,000
So, our equation will be.

210
00:39:40,000 --> 00:40:01,000
We use the basic reward to modify the value of the one minus the entropy s. So, in, you could see that in this case, we know that, you know, our example one, our entropy is 0.9697.

211
00:40:01,000 --> 00:40:04,000
So, the RLC or PLC will be.

212
00:40:05,000 --> 00:40:08,000
6.3 and 6.3 respectively.

213
00:40:12,000 --> 00:40:13,000
Okay.

214
00:40:14,000 --> 00:40:27,000
So, then we know that the basic reward, and we don't know the reward of content and the punishment of content and the how we reflect to each appraisers.

215
00:40:27,000 --> 00:40:33,000
And we could, we will to calculate the ratio of which means that.

216
00:40:34,000 --> 00:40:48,000
For example, in SOA, we have 0.66 and for A1, we will know that there are 36% of the trust score.

217
00:40:48,000 --> 00:40:52,000
The score of authentic is come from.

218
00:40:52,000 --> 00:40:55,000
It's come from the appraiser one, right?

219
00:40:55,000 --> 00:41:02,000
So, we will use this ratio to reflect the reward to each appraiser.

220
00:41:02,000 --> 00:41:05,000
And for the punishment, it's the same.

221
00:41:05,000 --> 00:41:12,000
We will know that how you contribute to the trust of the fake, right?

222
00:41:12,000 --> 00:41:22,000
And then if you contribute, or you occupy more percentage, you will get more reward or more punishment.

223
00:41:26,000 --> 00:41:27,000
Okay, so.

224
00:41:27,000 --> 00:41:40,000
In here, you can see we calculate a ratio of the RLC and the ratio of PLC, which each appraisers need to be reflected to be added or be reduced.

225
00:41:40,000 --> 00:41:53,000
And then, when we reflect it, you can see that for, for example, in for A1, the initial is initial state is 5,000 and after state is the 5,000 point.

226
00:41:53,000 --> 00:42:02,000
23, so we could get all the after stakes, and then we can calculate the after credit point.

227
00:42:02,000 --> 00:42:16,000
So, this is a cycle when every content, when we finish the appraising process, we will have the new stakes and the new credit point, and then they will get ready to accept content.

228
00:42:17,000 --> 00:42:30,000
And if any appraisers, their credit point is close to zero, which means that their stakes already, they get, they got a lot of the punishment and their credit point.

229
00:42:31,000 --> 00:42:48,000
Their stakes is equal to zero, and we will go back to our main floor, the initial process that to accept to ask this appraisers, do we need to add more stakes to continue to be the appraisers?

230
00:42:49,000 --> 00:43:03,000
So, that's how, so we only finish the, I think that the most difficult part in our presentation is to calculate the trust score.

231
00:43:04,000 --> 00:43:23,000
And in this slide that we do some experiments that to improve, to prove that our system could be a scale on a lot of the users, you could see that, and in the left side, the picture shows that our system could be in our testing environment.

232
00:43:23,000 --> 00:43:48,000
We use the 4 servers and the throughput is still goes well, but in our system, we set the contents, the maxima of the content is the 8 megabytes, because you could imagine that if we could compress the content for videos or some, we use the in-camera size.

233
00:43:48,000 --> 00:43:51,000
There is the technique you could use the content to.

234
00:43:52,000 --> 00:43:55,000
As the input, and we have the output is a very short.

235
00:43:55,000 --> 00:44:08,000
Um, or some contents, and it will not to occupy a lot of size. So, our intention of the content will be the 8 megabytes.

236
00:44:09,000 --> 00:44:28,000
And the right hand side, the picture shows that we all know that the computer system is rely on the tolerance, which means that if the main server fails, our systems should still work well. Right? You could see that.

237
00:44:28,000 --> 00:44:57,000
Okay, so I think we can get into.

238
00:44:57,000 --> 00:44:58,000
Conclusion.

239
00:44:59,000 --> 00:45:05,000
So, yeah, so our system, I asked to summarize some take home points for all of you.

240
00:45:06,000 --> 00:45:14,000
Yeah, as you remember that I've said that the AI technique is getting better and better. So.

241
00:45:15,000 --> 00:45:22,000
The traditional algorithm does not get the great performance to detect the automatic.

242
00:45:23,000 --> 00:45:28,000
So, if we use the human's capabilities, we will be a better solution.

243
00:45:29,000 --> 00:45:32,000
Um, then the traditional algorithms.

244
00:45:33,000 --> 00:45:41,000
But, of course, we could combine the advantage for the AI algorithms and the human based capability. For example, we can.

245
00:45:42,000 --> 00:45:54,000
Use AI technique to do the 1st day of filters and people can get into the check the results. Right? But the most important part is the blockchain network.

246
00:45:55,000 --> 00:46:00,000
Because it's the decentralized not to manage it by only 1 companies.

247
00:46:01,000 --> 00:46:06,000
It's a transparency and very easier to find some data is being modified.

248
00:46:07,000 --> 00:46:11,000
And so in this experience, we know that our system.

249
00:46:12,000 --> 00:46:15,000
It's the architecture is peaceful, and we could.

250
00:46:16,000 --> 00:46:20,000
It has potential to be deployed in some sense of machines.

251
00:46:21,000 --> 00:46:26,000
And the most important part is that our incentive mechanism.

252
00:46:27,000 --> 00:46:30,000
Offers this architecture commercial potential.

253
00:46:30,000 --> 00:46:31,000
You can imagine that to.

254
00:46:32,000 --> 00:46:36,000
If you are a writer, or you are a photographer, you could use our system.

255
00:46:37,000 --> 00:46:39,000
To get some proof.

256
00:46:40,000 --> 00:46:45,000
That your content is for you, and your, your reputation will getting better. Right?

257
00:46:46,000 --> 00:46:49,000
And you are a journalist, or you are some.

258
00:46:50,000 --> 00:46:53,000
If people doing the fact checking.

259
00:46:54,000 --> 00:46:56,000
And you can get reward for our system.

260
00:46:56,000 --> 00:46:58,000
So, there is also.

261
00:46:59,000 --> 00:47:04,000
Then this will get some commercial and purpose and for building.

262
00:47:05,000 --> 00:47:08,000
On this, the technical precision systems.

263
00:47:10,000 --> 00:47:11,000
Okay.

264
00:47:12,000 --> 00:47:13,000
So, that's.

265
00:47:14,000 --> 00:47:21,000
It's a long presentation, so I stopped here, and I actually here is that this research is.

266
00:47:22,000 --> 00:47:28,000
Supported by report is the payment protocol.

267
00:47:29,000 --> 00:47:30,000
And also.

268
00:47:31,000 --> 00:47:33,000
By our national science and engineering.

269
00:47:34,000 --> 00:47:36,000
Research Council of Canada.

270
00:47:37,000 --> 00:47:39,000
And also thanks my mentor to give me.

271
00:47:41,000 --> 00:47:43,000
A lot of suggestions and help when I.

272
00:47:44,000 --> 00:47:45,000
And to.

273
00:47:46,000 --> 00:47:47,000
Find some.

274
00:47:48,000 --> 00:47:50,000
Research ideas or some.

275
00:47:52,000 --> 00:47:53,000
No more people how to do well.

276
00:47:54,000 --> 00:47:57,000
In a program, so I will introduce.

277
00:47:59,000 --> 00:48:00,000
Facebook and.

278
00:48:01,000 --> 00:48:05,000
You got a lot of great things for all of you who wants to.

279
00:48:06,000 --> 00:48:08,000
Study for, or you have some.

280
00:48:09,000 --> 00:48:14,000
If you are women is that is the best for you to check.

281
00:48:14,000 --> 00:48:16,000
So, I put it here and out of thing.

282
00:48:17,000 --> 00:48:20,000
To be my mentor and give me more.

283
00:48:21,000 --> 00:48:22,000
Help.

284
00:48:24,000 --> 00:48:25,000
So.

285
00:48:26,000 --> 00:48:29,000
That's all the presentation and if you have a question.

286
00:48:30,000 --> 00:48:31,000
You could ask me right now.

287
00:48:32,000 --> 00:48:33,000
Or.

288
00:48:33,000 --> 00:48:37,000
After the presentation, you could also drop me a mail or find me.

289
00:48:38,000 --> 00:48:39,000
I also.

290
00:48:40,000 --> 00:48:41,000
We will discuss with you.

291
00:48:42,000 --> 00:48:44,000
Or detail or any questions you have.

292
00:48:45,000 --> 00:48:46,000
Thank you so much.

293
00:48:48,000 --> 00:48:53,000
So, yeah, actually, so before we start our QA session, please turn on your microphones again.

294
00:48:54,000 --> 00:48:58,000
So, let's thank for the very interesting talk.

295
00:49:01,000 --> 00:49:02,000
Thank you.

296
00:49:03,000 --> 00:49:04,000
So.

297
00:49:05,000 --> 00:49:11,000
I'm not sure, but I'm guessing that some of our attendees would like to ask in Mandarin Chinese.

298
00:49:12,000 --> 00:49:17,000
Probably, I guess, but I mean, any kind of language as long as you can come in, that will all be fine.

299
00:49:18,000 --> 00:49:20,000
So, yeah, so please feel free to ask questions.

300
00:49:22,000 --> 00:49:23,000
Anyone.

301
00:49:25,000 --> 00:49:26,000
Hello.

302
00:49:26,000 --> 00:49:27,000
Hi, I'm.

303
00:49:28,000 --> 00:49:33,000
And I would like to know, because I noticed that actually you have no experience starting a board, right?

304
00:49:34,000 --> 00:49:35,000
It's your first time.

305
00:49:36,000 --> 00:49:38,000
How do you how do you practice English?

306
00:49:39,000 --> 00:49:41,000
Sorry, it's not related to your content, but I'm just curious.

307
00:49:42,000 --> 00:49:43,000
Yeah.

308
00:49:43,000 --> 00:49:46,000
Oh, yeah, so interesting question is that I think.

309
00:49:47,000 --> 00:49:51,000
I joined some English speaking practice in crosses in.

310
00:49:52,000 --> 00:49:55,000
So, yeah, they gave me a lot of practice to.

311
00:49:57,000 --> 00:49:58,000
To.

312
00:50:03,000 --> 00:50:09,000
I to join, which some master the crops near.

313
00:50:10,000 --> 00:50:13,000
So, there's that option for me to practice in English.

314
00:50:13,000 --> 00:50:14,000
Yeah.

315
00:50:17,000 --> 00:50:18,000
Is that answer your question?

316
00:50:19,000 --> 00:50:23,000
Oh, yes, and I heard some people say that we should prepare.

317
00:50:24,000 --> 00:50:25,000
Maybe we should join the, like.

318
00:50:27,000 --> 00:50:29,000
It's faster than we prepare English fast.

319
00:50:30,000 --> 00:50:31,000
How do you think about that?

320
00:50:32,000 --> 00:50:37,000
If I cannot reach the score that I want, and did I need to apply for the language?

321
00:50:37,000 --> 00:50:46,000
Or English, and I'm here, I get a score, get a total capital, and then apply for the school.

322
00:50:51,000 --> 00:50:54,000
Okay, this is not an interesting questions yet.

323
00:50:55,000 --> 00:50:58,000
Actually, I will suggest that this depends.

324
00:50:59,000 --> 00:51:00,000
If you.

325
00:51:01,000 --> 00:51:04,000
I will suggest to prepare English.

326
00:51:05,000 --> 00:51:07,000
And try your best to pass the top.

327
00:51:09,000 --> 00:51:12,000
In Taiwan, because if you.

328
00:51:13,000 --> 00:51:16,000
Go study abroad and you go to some language schools.

329
00:51:17,000 --> 00:51:19,000
The cost is to leave is very high.

330
00:51:20,000 --> 00:51:22,000
And then it will spend a lot of money.

331
00:51:23,000 --> 00:51:26,000
But, but the most important thing is the.

332
00:51:26,000 --> 00:51:29,000
If you sometimes.

333
00:51:30,000 --> 00:51:35,000
Some people are struggling to passing to pass the exams.

334
00:51:36,000 --> 00:51:38,000
So, if you found that you.

335
00:51:39,000 --> 00:51:40,000
You cannot concentrate to.

336
00:51:41,000 --> 00:51:42,000
Prepare for it.

337
00:51:43,000 --> 00:51:46,000
And you stuck on the example.

338
00:51:48,000 --> 00:51:54,000
And also the schools offer some opportunities that they say you could go to the university.

339
00:51:54,000 --> 00:51:59,000
They say you could go to the university to take the language courses, and then you can get over.

340
00:52:00,000 --> 00:52:02,000
I would say you just go because.

341
00:52:04,000 --> 00:52:05,000
The time is important.

342
00:52:07,000 --> 00:52:08,000
Do not to stack 1.

343
00:52:10,000 --> 00:52:15,000
1 stage for a long time. Yeah, I was, I think that would be my answer.

344
00:52:21,000 --> 00:52:23,000
Yeah, so I have a question.

345
00:52:24,000 --> 00:52:25,000
So, um.

346
00:52:25,000 --> 00:52:32,000
So, if we go back to your title, I think you're saying that this technique is mainly for.

347
00:52:34,000 --> 00:52:40,000
Um, how do I say this for deploying against fake news, quote, unquote news, right?

348
00:52:40,000 --> 00:52:46,000
But if you let the people decide what will be the thing that they recognize, right?

349
00:52:46,000 --> 00:52:48,000
They basically just vote.

350
00:52:49,000 --> 00:52:52,000
Let's say Barack Obama just died.

351
00:52:52,000 --> 00:52:58,000
And if you let the Internet vote, probably 99% of the people will say that's a fake news.

352
00:52:58,000 --> 00:52:59,000
Right.

353
00:52:59,000 --> 00:53:05,000
But let's say objectively true thing that cannot be voted to become a truth in your system.

354
00:53:05,000 --> 00:53:06,000
How do you comment about that?

355
00:53:07,000 --> 00:53:08,000
Uh, yeah, like I said.

356
00:53:09,000 --> 00:53:13,000
Um, we trust the majority opinion, right in our system.

357
00:53:14,000 --> 00:53:23,000
We have the reward and the punishment mechanisms to make make you to do the correct and the honest.

358
00:53:25,000 --> 00:53:34,000
Actually, in all of the production network, they have the specific incentive mechanism to prevent from some malicious notes.

359
00:53:35,000 --> 00:53:36,000
So, if you joined our.

360
00:53:37,000 --> 00:53:40,000
We have 2 mechanism to prevent.

361
00:53:41,000 --> 00:53:45,000
This, uh, the notes to adjust them to vote.

362
00:53:46,000 --> 00:53:53,000
Actually, they will do their best because the 1st, if they join our system, they need to pay for pay fees for.

363
00:53:54,000 --> 00:53:56,000
I say the stakes, so it's a cost.

364
00:53:57,000 --> 00:54:01,000
And then if we join the system, and you did not do your job very well.

365
00:54:01,000 --> 00:54:02,000
You will get.

366
00:54:03,000 --> 00:54:06,000
The, uh, the punishment, and then eventually.

367
00:54:07,000 --> 00:54:11,000
You are not, you will be prohibited to be the oppressors in our system.

368
00:54:12,000 --> 00:54:14,000
So, our mechanism of incentive.

369
00:54:15,000 --> 00:54:18,000
It's a core concept to incentive people.

370
00:54:20,000 --> 00:54:23,000
To check the content seriously and to fault.

371
00:54:23,000 --> 00:54:24,000
Correct.

372
00:54:28,000 --> 00:54:34,000
So, if I understand correctly, basically, the assumption that you're going with is that.

373
00:54:35,000 --> 00:54:42,000
People generally have good intention, and they are answering based on their good intention, regardless of the truth.

374
00:54:43,000 --> 00:54:43,000


375
00:54:43,000 --> 00:54:45,000
So, say, Obama just died.

376
00:54:46,000 --> 00:54:47,000
Nobody knows.

377
00:54:47,000 --> 00:54:48,000
And they basically just.

378
00:54:49,000 --> 00:54:50,000
Like.

379
00:54:51,000 --> 00:54:57,000
Saying it's not a good news because it's just not the thing that is well established that I know.

380
00:54:58,000 --> 00:54:59,000
But it's true.

381
00:54:59,000 --> 00:55:00,000
It's true.

382
00:55:00,000 --> 00:55:00,000


383
00:55:00,000 --> 00:55:08,000
So, but so, in your system, it's basically working against malicious information, but not exactly news.

384
00:55:09,000 --> 00:55:10,000
And I say that.

385
00:55:11,000 --> 00:55:19,000
Um, I could say how to define the news is that the content should be.

386
00:55:20,000 --> 00:55:21,000
Correct.

387
00:55:21,000 --> 00:55:22,000
Right.

388
00:55:22,000 --> 00:55:26,000
So, when people, when you say Obama is that.

389
00:55:28,000 --> 00:55:29,000
If it's not a truth.

390
00:55:30,000 --> 00:55:34,000
Our system will consider it is a back news.

391
00:55:34,000 --> 00:55:35,000
Right.

392
00:55:35,000 --> 00:55:38,000
Yeah, but in my scenario, I'm saying.

393
00:55:38,000 --> 00:55:41,000
What if he really just died?

394
00:55:42,000 --> 00:55:49,000
But nobody knows, like, 10 minutes ago, I go out and say, hey, guys, he just died.

395
00:55:50,000 --> 00:55:52,000
That will be the fake news in your system.

396
00:55:53,000 --> 00:55:54,000
Nobody knows if that's true.

397
00:55:56,000 --> 00:55:57,000
Yeah, you're right, but.

398
00:56:00,000 --> 00:56:03,000
If you say Obama is dead, but nobody knows.

399
00:56:04,000 --> 00:56:05,000
Um.

400
00:56:07,000 --> 00:56:10,000
I would say that this situation is really.

401
00:56:12,000 --> 00:56:13,000
Hmm.

402
00:56:13,000 --> 00:56:15,000
That's why this is news.

403
00:56:17,000 --> 00:56:18,000
Yeah, so.

404
00:56:19,000 --> 00:56:25,000
I think because the most oppressing actors are very capable to check the contents.

405
00:56:26,000 --> 00:56:34,000
So, if they, if they hesitate to give some opinions, their confidence will lower.

406
00:56:35,000 --> 00:56:38,000
So, the trust the score of the.

407
00:56:39,000 --> 00:56:43,000
Agree authentic and the fact there will be very similar.

408
00:56:44,000 --> 00:56:47,000
So, our system will not trade it in your case.

409
00:56:48,000 --> 00:56:49,000
It's a very.

410
00:56:50,000 --> 00:56:53,000
Can do your real effect will be a balance.

411
00:56:54,000 --> 00:56:56,000
But we could give you.

412
00:56:57,000 --> 00:56:58,000
Objective.

413
00:56:59,000 --> 00:57:00,000
Opinion.

414
00:57:00,000 --> 00:57:01,000
To check this content.

415
00:57:04,000 --> 00:57:05,000
I see.

416
00:57:06,000 --> 00:57:07,000
Okay.

417
00:57:08,000 --> 00:57:11,000
I do have some comment about a previous question.

418
00:57:11,000 --> 00:57:12,000
Just like.

419
00:57:13,000 --> 00:57:14,000
Okay.

420
00:57:14,000 --> 00:57:15,000
Okay.

421
00:57:15,000 --> 00:57:16,000
First, thank you for a nice talk.

422
00:57:16,000 --> 00:57:20,000
And for the token number design, I think that's about how you make a dispute system.

423
00:57:21,000 --> 00:57:24,000
Because for now, it's like a 1 time votes and you just get the result done.

424
00:57:24,000 --> 00:57:25,000
It's done.

425
00:57:25,000 --> 00:57:35,000
But if you see, like, uh, in order for the prediction market, just like, if you predict and and like, whether Obama will die between, like, 2021st.

426
00:57:35,000 --> 00:57:40,000
And people at 2021st votes, maybe 3% of the voter is enough to get.

427
00:57:41,000 --> 00:57:42,000
Yes or no.

428
00:57:42,000 --> 00:57:49,000
But if you don't, if you don't trust the result, you can put your, your state against that voting result.

429
00:57:49,000 --> 00:57:51,000
And and then more people need to vote.

430
00:57:51,000 --> 00:57:58,000
Like, if you stay, like, maybe 1% of the total, total, total, like, token, then all of the people need to vote.

431
00:57:59,000 --> 00:58:06,000
So, so I guess you need to kind of, like, refine the, the appeal system or the tokenomic.

432
00:58:06,000 --> 00:58:17,000
Because I think the previous, the previous, like, question is quite valid, because you assume that, like, the majority vote.

433
00:58:17,000 --> 00:58:20,000
Well, well, well, not the truth, but maybe it takes some time, right?

434
00:58:20,000 --> 00:58:26,000
So, but if you like, you want to label the news in 10 minutes, and definitely.

435
00:58:26,000 --> 00:58:35,000
Well, it's hard to get information to propagate to, like, the society or civilization to get people votes and get the correct result.

436
00:58:35,000 --> 00:58:41,000
So, I guess you just need to have a dispute system, I guess.

437
00:58:41,000 --> 00:58:42,000
Yeah, sure.

438
00:58:42,000 --> 00:58:43,000
Yeah.

439
00:58:44,000 --> 00:58:45,000
Okay.

440
00:58:45,000 --> 00:58:46,000
Yeah.

441
00:58:46,000 --> 00:58:57,000
If you want to get a news in just 10 minutes to see if it's correct or not, yeah, it will be difficult to achieve that.

442
00:58:57,000 --> 00:59:03,000
Because maybe, like Elon said, the news is not just known by minority people.

443
00:59:04,000 --> 00:59:07,000
But, yeah.

444
00:59:07,000 --> 00:59:28,000
For this case, I think for previous, the current solutions, I think people will have, like, we have put more stakes or to, we can to, to introduce more, the sub-parties, the libraries that, and we can ask their opinions.

445
00:59:28,000 --> 00:59:35,000
So our speed will get in faster.

446
00:59:35,000 --> 00:59:36,000
Yeah.

447
00:59:36,000 --> 00:59:42,000
Well, I do think this is a very interesting, actually very valuable research.

448
00:59:42,000 --> 00:59:44,000
I'm just being curious.

449
00:59:44,000 --> 00:59:45,000
Like, yeah.

450
00:59:45,000 --> 00:59:52,000
So, anyone has other questions?

451
00:59:53,000 --> 00:59:59,000
Do you really, like, plan to launch this thing in a public chain?

452
00:59:59,000 --> 01:00:05,000
Because I would say in the past, like, two years, hyperledger is not, like, really thriving.

453
01:00:06,000 --> 01:00:21,000
And even though, like, the gas fee and the transaction fee is an issue for, like, Bitcoin and Ethereum, but for the new generation, like, high throughput blockchain, also support a smart contract, then it will be a much better, like, platform, I would say.

454
01:00:21,000 --> 01:00:36,000
Because, well, hyperledger is definitely more, like, clean and more permission, but how to really scale the, how to really scale the platform, I would say, like, public chain is still better than the permission chain.

455
01:00:36,000 --> 01:00:37,000
Yeah.

456
01:00:37,000 --> 01:00:38,000
Yeah.

457
01:00:38,000 --> 01:00:40,000
Actually, you're right.

458
01:00:40,000 --> 01:00:49,000
We plan to introduce some digital tokens in our system because we need to get the reward or punishment.

459
01:00:49,000 --> 01:00:58,000
So, Ethereum will be the one choice that we consider to transfer the hyperledger to the Ethereum.

460
01:00:58,000 --> 01:01:03,000
And also, there are other tokens that are invented right now.

461
01:01:03,000 --> 01:01:09,000
They have a great, because they, Ethereum and Bitcoin, their speed is too slow, right?

462
01:01:09,000 --> 01:01:14,000
But there are lots of faster blockchain network.

463
01:01:14,000 --> 01:01:23,000
So, we consider to, yeah, like you said, to transfer from the private blockchain to the public.

464
01:01:23,000 --> 01:01:24,000
Yeah.

465
01:01:25,000 --> 01:01:26,000
Okay.

466
01:01:26,000 --> 01:01:28,000
Thank you for applying.

467
01:01:28,000 --> 01:01:29,000
Yeah.

468
01:01:29,000 --> 01:01:30,000
Thank you.

469
01:01:30,000 --> 01:01:31,000
Thank you for asking.

470
01:01:35,000 --> 01:01:36,000
Okay.

471
01:01:36,000 --> 01:01:37,000
So, one last time.

472
01:01:37,000 --> 01:01:39,000
Anybody has a question?

473
01:01:39,000 --> 01:01:40,000
Okay.

474
01:01:40,000 --> 01:01:44,000
If not, please, again, really turn on your microphone.

475
01:01:44,000 --> 01:01:47,000
So, let's thank our speaker again.

476
01:01:47,000 --> 01:01:49,000
That's a very interesting talk.

477
01:01:49,000 --> 01:01:50,000
Thank you.

478
01:01:50,000 --> 01:01:51,000
Thank you.

479
01:01:51,000 --> 01:01:54,000
Do you also have any questions?

