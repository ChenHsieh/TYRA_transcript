Thank you, Yen-Yung.
We have talked about how to join Project Terra.
Today, we are going to start the lecture.
Before we start, let me introduce the speaker.
Today's speaker is Professor Hsieh Ming-Hsiu from Sydney University of Science and Technology.
Professor Hsieh is also a senior at NTU.
In 2008, he got his Ph.D. from the University of Sydney.
He has worked in Japan and Cambridge.
His research direction from Ph.D. to now is related to quantum computer and quantum information.
His more detailed direction is related to quantum information.
From the beginning of error correction to focus on quantum machine learning and different projects.
Next, Professor Hsieh will introduce his research.
By the way, I would like to promote that NTU will have a team this year.
I hope we can work on quantum computing together.
If you are interested, I believe Professor Hsieh can introduce the team later.
There are electrical engineering and physics.
I think this is a big project.
I hope Professor Hsieh can introduce the team later.
I believe this project is good for Taiwan's research.
Professor Hsieh, you can start.
Hello everyone, my name is Hsieh.
Thank you, Hsieh Ming-Hsiu.
I am very happy to share with you today.
The Minister of Science and Technology has a SOC in NTU.
System on Chip.
They have a front page.
They will find some front page topics every year.
This year they chose quantum computation.
They invited me to give a speech.
Ted just mentioned that.
Now Taiwan is starting to pay attention to quantum computation.
Hardware or software development.
The Ministry of Science and Technology has started to invest.
Of course, there are many in Taiwan.
But there are very few overseas.
This year, there are about 7,000 to 7,500 million.
Now they have set up a center for quantum technology in NTU.
It's a five-year plan.
There are 50 million in a year.
The Ministry of Science and Technology has 25 million.
The Ministry of Education has 25 million.
I am also in charge of the International Advisory Board.
After Taiwan started to invest in this place.
There are many opportunities.
If you happen to be in the field.
In fact, there are many people in this field.
Because quantum computing is a very cross-field.
You can also find it in my talk later.
It also has a lot to do with CS.
It also needs to do these things now.
For example, quantum control needs to be micro-wave controlled.
So traditional micro-wave people can also come in.
Then look at what your process is.
Semiconductor talents can also come in.
Superconductor talents can also come in.
These classic physics people can also come in.
So no matter what field you are.
Engineering, science, or electronics.
As long as you are interested in quantum computing.
You can come in very quickly.
In fact, I also studied a master's degree in Taiwan University.
It's a telecommunications company.
Then I went to Nanyang University to study for a doctorate.
I just switched to quantum error correction.
Then slowly do some information computation.
Then I did a lot of other things.
So what is it like to cross-field with students from science and engineering colleges?
Some background introduction.
Then there's my talk.
Because I mainly prepared for the SOC workshop.
It's only 40 minutes.
So I probably only have 40 minutes of material.
Then you can use Q&A later.
See what kind of questions you have about this direction.
You can bring it up.
Of course, in the middle of my report later.
You can also bring up any questions you have.
Let me remind you.
Sorry.
Because we have a lot of participants today.
So we have a prediction.
The microphone is off when everyone enters.
So if you want to ask a question.
Please click on the microphone icon above.
This way you won't feel ignored.
We usually welcome questions at any time.
But I'm afraid there's a lot of responses today.
So when you ask a question.
Please click on the icon.
Okay, let's get started.
Okay, I'll continue my report.
This is my title page.
I mainly want to introduce some.
Because we know that machine learning.
Now it's a very popular topic.
Quantum Computation is another popular topic.
Quantum Computation.
Basically, everyone has a vague idea about it.
That is, it can be used to do some.
High-performance operations.
So, uh.
So, uh.
About three or five years ago.
In fact, earlier.
Someone started to notice that.
How to use quantum computers.
To do some machine learning tasks.
But it was said that the early development was slower.
Because everyone said.
Especially in academia.
People will think that.
Your effect on your quantum mechanics.
Then your quantum computer.
The power of this computation.
Your definitions are not very clear yet.
To do this quantum machine learning.
It seems to be a bit, uh.
A little bit of this.
Uh, it's a little unrealistic.
So, uh.
So, uh.
Before 2010.
That is, in this quantum machine learning topic.
In fact, very few people do it.
Then, uh.
The main results are also relatively limited.
So, uh.
Until the last three or five years.
That is, everyone has been from this.
Uh, this technology.
In this news.
For general public news.
You also find that.
Uh, all the big companies have invested.
Google, IBM.
Microsoft, Intel.
Especially like Intel, IBM, Google.
They all have more than 50 qubits of machine.
That is to say, their, this.
Highway development can already be 50 qubits.
In fact, you can already do a little bit.
A little something like this.
So, uh.
In the academic world, everyone started to go back and say.
In fact, quantum computers may still be used.
To do some.
Uh, the operation of this big data.
So that's why this quantum machine learning.
In the last few years.
Very, uh.
Very hot.
So, uh.
Uh, so in 2017.
In Nature.
Uh, this journal, it has.
Two reviewed articles.
That is, two of the same.
Uh, the same, uh.
The same, uh, the same value.
And then it just happened to be the first two.
Uh, it's all in the review of this quantum machine.
Uh, uh, uh.
What about me?
This slide is from.
One of the review articles.
It was taken out, it was by Monta, this person.
Uh, it's written like this.
He actually said.
He did some summary, which means he took it to.
Before 2017, that is to say.
Uh, which one.
You can do it with quantum computation.
And then?
What kind of acceleration does it have?
Uh, on the far left.
He's actually just.
It's just, uh, it's classification.
What are the problems like this?
For example, he has that dangerous inference.
Uh.
Uh, let's say perceptron perceptron.
It's like classification.
Uh, let's say.
The left and right, or if it's linear.
That's what you're going to do.
Uh, and then.
Uh.
The third is fitting.
It's similar to what we used to have in curve fitting.
You'll see your curve is probably.
If it's a very complicated curve.
For example, you can use linearity or sine wave to.
To fit this curve.
Uh, then.
Uh, the fourth is the Boltzmann machine.
We're probably just saying this is more statistical.
Uh.
That's classical Boltzmann machine.
Of course, there's a corresponding quantum Boltzmann machine.
Uh, that Boltzmann machine is also this.
Uh, uh.
Uh, similar to neural network.
It's actually a very similar concept.
And then there's PCA.
Uh, Principle Component Analysis.
Uh, this is basically.
PCA is also a machine learning.
Uh, one.
Uh, a very weird study.
A problem like this.
You have a covariance matrix.
Then you have to put it in a right angle.
After that, you just need to focus on.
A relatively large value.
So they're called, uh.
Principle component.
The biggest one, the biggest one, the biggest one.
It means that it affects your distribution.
Is relatively large.
So you cut off some of the smaller ones.
Uh, the value.
Uh, this is what classical PCA is doing.
Uh, uh, and then there's SVM.
This is actually called a vector.
Uh, supporting vector.
Basically, it's also part of this.
Uh, classification problem.
Um, the second column?
The second row on the left is to say that this is the acceleration on the value.
Uh, uh, uh.
Uh, it means, uh,
for example, uh,
Perceptron, it has this, uh,
Square root, it's called the acceleration of square root n.
It's your sample size.
If you have a point, you're going to do the classification.
Then use the value, it will have one.
Probably, uh,
n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n
So there's this acceleration.
Some of the questions it has a value acceleration.
Log n is the value of the value of the value.
And then, uh,
You see some signals in this place.
In fact, this signal is
It's actually these accelerations.
There are some conditions.
It has some assumptions, so it's not
If you're going to
Uh, from this classical
To the classical
Its acceleration will not
To the value of the value
It's just that, uh, my classical
Data, I have to map it first.
Map to a quantum state
Quantum state is your quantum
A type that the computer can handle
Is, uh, your input
To a quantum machine
You must have one, just say this
An input that the machine can accept
That's it, so you're from classical
Data to map to this quantum data
Where is this place
In fact, there is a, uh
There is a trade-off
It's basically impossible to do very well now
Because you can imagine
The classical data, I must have every data
I have to read it once, so you can
Map it over, right?
Uh, so
In this place, you must be
With your data size
There is already a proportional relationship
What about these accelerations
It's usually just looking at
I already have a preprocessing
Map it to the quantum state
And then it can
What about the calculation of these quantities
It has this index acceleration
This is what this signal means
That's it
But if there is no signal, this is
Hey, uh, it's
Overall, I can have an index
The acceleration of this square root n
Okay, what about this a
It's actually a call
This algorithm
Then I will introduce it later
It's actually a global
Uh, a more advanced version of global search
Then I will introduce
Global search
Then this place means y
That is to say, in these methods
It actually uses this
Amplitude amplification or global
Search like this
Then the other one is hhl
It's a quantum
Very famous algorithm now
I'll introduce it later. It's actually used to solve the linear equation.
If you say
Quantum computation
Quantum algorithm to solve the linear equation
It will have an index acceleration
Then we know that machine learning
Uh, a lot of problems
You can map it to the solution
So you can do this
You can just use this hhl algorithm
To solve
I'm not familiar with this piece
So I didn't go to
Go to
I don't explain much
That's what I just said
You have to put classical data
Map to quantum state
Basically you need an encoding step
Then this is what qrem is doing
N means
This method, for example, the first
Bayesian inference, it doesn't use
This qrem place
Because qrem is actually a
It's a more complicated operation
So you can jump as much as you can
You can try not to use it
Don't use it
Ok
Is there a problem?
No problem, I'll go down
Introduce this
From here
I will introduce some quantum algorithms
Why
Why is there an index acceleration?
Basically because there are these
These are more basic
This quantum algorithm
As your
This
From
This
The more complicated algorithm
Then do more complicated things
Then I introduced these
These are more fundamental
There are a few
Basically you are going to read these textbooks
There are already introductions in the textbooks
I basically know here
Everyone is not doing this
Quantum information or computation
So I basically only talk about the results
Everyone can basically have an impression
The first one is Fourier transform
Why is quantum computation
It's so useful because
Quantum
You can let you go
Do
Do the action of superposition
Your state
Your state is
For example, we are 0 1, right?
Quantum state is 0 1
Superposition
What about this superposition
That is, you can use parallel operations
You can
You can do a 0 operation
Do a 1 operation
Then correspond to it
Its superposition will correspond to
Your operation will superposition
Quantum mechanics is a linear
Linear
What about this place
So why
Quantum computer
Why is it so effective
Because of this quantum Fourier transform
If we talk about science students
Everyone knows what Fourier transform is doing
That is, he wants to put a state
Plus this
Plus this face
This is called Fourier coefficient
This omega n xk
If you say there are n bits
Then your capital n
Is the square root of 2n
This is what you do to n bits
A Fourier transform
Then my notation is actually
Quantum notation
Basically this x
There is a
This place
This place
It's actually a quantum state
You can think of it as a vector
This is a
That is to say there are 2n
One
A vector element that is a
This technical detail
It doesn't matter if you don't need to know
Then
I can't introduce this Fourier transform
The introduction is too detailed, I just want to tell you
What is the difference between it and classical
Our science students
Everyone knows that classical Fourier transform
How many operations do you need
If you say you have n
n bits
The operation you need is n
Multiplied by the value of 2
n square
So it is
It's not just it's polynomial
In number of bits
It's exponential in number of bits
This is what classical Fourier transform needs
It's n times 2n
So many operations
You need so many operations
But in quantum Fourier transform
You only need
n square
That is to say, n is your number of bits
So you have a place
Uh, uh, the acceleration of the index
Uh, exponential speed
Uh, exponential speed
That
Uh, the reason for this
Of course, it just happens to be this operation
Can be divided into this bit
Single qubit operation and this two qubit operation
Then you add it up
That is to say, its operation
You can decompose it into this elementary quantum gate
Then it just needs n
n square and so many
Yeah, if you're interested in this
You can go to Wikipedia
It's also very simple
If you have a background in physics
It's easy to understand
Uh, so this place has an index
So your algorithm if you need
If you use quantum Fourier transform
You just have a big acceleration
And then this
Uh, and then you
This, uh, quantum Fourier transform
It will be used in some
More complicated this is called phase estimation
In this algorithm
I will introduce phase in a moment
I will introduce phase estimation in the next slide
Because phase estimation is a very important algorithm
Okay, what is phase estimation doing?
What is phase estimation doing?
It is to estimate your eigenvalue
That is to say, suppose I have a unitary
U, and then
I have one, uh, this vector
This is the eigenvector of U
Psi, right?
Then we know that your U
Multiply by your eigenvector
You will take your eigenvalue
It will be equal to the eigenvalue multiplied by your eigenvector
Right?
Then this phase estimation is to estimate
Because of your eigenvalue
Because it is unitary, it is only related to this theta
Right? So we're going to estimate this theta
Right? So we're going to estimate this theta
Uh, what about this?
What about this estimation?
That is to say, this place is also
That is, uh, this take-home message is like this
Uh, there is a quantum algorithm
Uh, there is a quantum algorithm
It can estimate your eigenvalue
Can estimate your eigenvalue
It's very efficient
It's very efficient
How about this?
How about this?
It can use all 1 over
It can use all 1 over
It can use all 1 over
If you say, if your requirement is
If you say, if your requirement is
It just needs all 1 over
to calculate this eigenvalue
Then we know
In fact, many later algorithms
In fact, many later algorithms
The most important part of it is
I have a matrix
Because quantum mechanics
You can use this matrix to express
You can use this matrix to express
A matrix
What do you need to focus on?
Just the place where its eigenvalue is large
Just the place where its eigenvalue is large
So we can use some
Fastest method
Make some changes
Then let it just estimate this
Bigger eigenvalue
Then you can
Smaller eigenvalue
Truncate it
Then this truncate
It basically doesn't affect
Because if you look at its distance
For example, trace distance
It's basically just comparing its eigenvalue
So if you truncate a smaller eigenvalue
So if you truncate a smaller eigenvalue
And then
Like this Fastest method
It uses, for example, I just mentioned
Hhl is the solution
Linear equation algorithm
It should be the most famous
It should be the most famous
It can be used to do
The value factor is highly efficient
It can be used to do
The value factor is highly efficient
It can
It can
It's so effective
That's it
That's it
That's it
That's it
Here
Here
Here
This
This
This
This
This
This place is in classical
Actually there is no way to avoid
We know that we used to be on
It should be high school
Is the Gauss elimination method
I have n variables n equations
I just delete one variable first
Then slowly delete all the variables
Until the last variable and then push it back
This is the Gauss elimination method
Then, but in quantum
If you say this x is a quantum state
It already has superposition to help you
Then v is also a quantum state
To do this
You only need to log n
n is the size of your matrix
The matrix of this a
Is n by n
Then you only need to log n so much
Then we know that the Gauss elimination method
You probably need n squared
Steps like this
Then this
This place is actually in major
This matrix a is
How regular
How should it be said
The biggest eigenvalue and the smallest eigenvalue
You don't want it to be too different
So this kappa is a bit
This linear equation
In the end
His
What is his behavior
So this is actually a technical condition
It doesn't matter if you don't care first
Everyone basically only compares this log n
Then
In the best classic algorithm
In fact, it is n
If your matrix is n by n
You don't need to do so many operations
Basically with the Gauss elimination method
A little better than the Gauss elimination method
This is actually a teacher at MIT
Aaron Harrow
They are all at MIT
They proposed it in 2009
You see, it's less than 10 years
This breakthrough within 10 years
This article until now
It seems that I have almost 600 citations
It's because of the quantum algorithm
It's all up
Then this machine learning also got up
A lot of this step
This matrix
Inversion step, so now citation
This paper is very high
So it's the same
It's like this
Linear equation
The quantum algorithm has an index acceleration
That's it
Okay, then
The other one is that in addition to HHL
The other one is most commonly used in machine learning
Is global search
What is global search?
Then this data space
You don't need to have some structure
It's an unsorted search
Then use this global search
What about this quantum algorithm
It will have the acceleration of square root n
What about this square root n
n is your
This size of the database
Sorry
Then basically global search
What are you doing?
If my omega is an n-bit sequence
I wrote it into this vector form
It's like this
Then this
Every omega is a 0 1 bit
Every omega i
Is a 0 1 bit
Then I just want from this
All of this bit sequence
This ps below me is all of the bit sequence
The superposition
I want to identify from the inside
A particular
This omega
From a uniform superposition
I want to identify this omega
This is what global search is doing
That global is basically
Very simple
It uses two very elementary
This operation
This is basically
These two are reflection
Is to reverse
I have a picture below
That is
It is a
Geometry picture like this
I use this picture to show
It's called omega
This y-axis
This omega is what I want to estimate
What about this s
Is all of the state
Uniform superposition
Because omega is also in this s
So
Sorry
There is a call
Because omega is also in this s
An element
This omega is positive
But you can
It's positive with this omega
This
OK
That
What about the two operations just now
One is US, one is UW
That is, there are two operations in the previous slide
What are these two operations doing
That is, this UW
This operation is for this s
This one
This plane
It is s
You use this u
Omega this operation is used in s
It's actually s, this s, this place
Do a flip for s prime
So when you do the first step, it will flip here
The following gray line
Can everyone see it?
Should be right
I can see it
Because it's too quiet
I don't know if I can see it like this
OK
Do the first flip
It will become the gray line below
Right then
It's the second flip
Is to flip to s
So it's now this gray line
Will run to the top here
If you assume that at first
s and s prime
s and s prime
Is the angle of theta over 2
Then you do the first flip
You will be here below
Here is also theta over 2
Then you flip to s now
It will go up here
There is a theta angle, right?
That is to say, you did a round
That is to say, u omega and us
After doing a round
You actually put s this angle
s this s this vector
Up to the direction of omega
Then you just need it now
Your goal now
Is to let you repeat this step
Let the last
It can be close to omega
Right, you just let it keep flipping
To omega flip to where you want
Then you can search
Search to the value you want
This global
In fact, it is very interesting that he is not doing quantum information
He only has one
One to two pieces of quantum paper
He basically proposed this global search
Classical
He is a classic CS person
Then he mentioned this very interesting result
Then he is very famous now
This is a very fundamental
A quantum algorithm
Because this is a search algorithm
And it is your data space
No need to have
No need to have assumptions
So it is an unsorted search
So it is very useful
It is in many algorithms
This is a global search
Ok
Uh
Uh
I need to introduce one
Because the progress of quantum machine learning
Very fast
Then the progress of 2017
This review article
In fact, it has been a bit
There have been a lot of results in the past year
There is a very interesting result
Is the recommendation system
Uh
Recommendation system I think everyone knows
You do online shopping
Or you watch netflix
You watch netflix
After you choose
You can finally say
Give him a star
See if you like five stars
Or three stars
Do you like it
He has an algorithm
Will recommend you a similar movie
Online shopping is the same
What did you buy
Will recommend you something like
So this is the recommendation system
Uh system is doing
That uh
Recommendation system
You use
Describe it in math
Describe what it is
I have n users
This n
N
Every user
He has m things
You can choose, right
I have a movie, I have a database
Then every user can choose
All the movies in this database
Uh
You can put this thing
Think of it as a big evidence
This evidence is really big because
For example, I have a million movies
So my m is
1 million
If I say 1 million users
Then my evidence is 1 million times
Such a big size
Of course, usually
Most of the entries are empty
Because you can't say 1 million movies
All finished
So you can only watch
For example, 10,000 movies
So this rank is very important
Most of this matrix
Uh, the rank is very low
What about the recommendation system
What is this algorithm basically doing
He just wants to reconstruct
This matrix
I already chose some elements
Then I want to be on you
I want to reconstruct other places
This is what the recommendation system does
Then this place is very interesting
In 2017
What about us
Uh, in this quantum information
Quantum machine learning
Uh, where
They sent a big message
They have a algorithm now
Just need
Log m n so much
This is very impressive
Why
Because before this algorithm
All classic
Do recommendation system
Algorithm
All need at least
Polynomial
In m and n
So you can see this quantum algorithm
There is an increase in the number of points
Right, this recommendation system
Is a very important algorithm
Because all your online shopping
As long as it is related to online
Maybe this
Uh, this
What are they advertising
Also have this recommendation system
Build in
That is to say, everyone felt at that time
Quantum algorithm is very useful
Uh, but
This year in 2018
There is a student in Texas
It seems to be
Scott Aaronson
It's not a student
It's like a specialty
This kind of student
He should be only 18 years old
Uh
He just
He read this quantum algorithm paper
Found it
What about this quantum algorithm paper
In fact, this quantum algorithm paper
It's actually not entirely
Classic recommendation system
With classic recommendation system
Basically not doing the same thing
It's actually just estimating this
Uh, this is inside
The most likely
Entry
This low probability entry
Basically, you don't need to reconstruct
What about their quantum algorithm
Do it like this
Quantum algorithm
Can go to
These high probability values
Then the second
You don't need to reconstruct the whole
All elements
So this will have this index
That's it
This 18-year-old
He actually
What about this quantum idea
He found this
Sampling can be done
That is, in 2018, this is about a few months ago
Oh, no, it's July
Uh
He also uses a
Classic algorithm can do exactly the same
Uh, this
Efficiency
So in the recommendation system
This quantum algorithm is completely
No acceleration, it's the same as the classic algorithm
It's exactly the same
But this is another interesting place
That is, in this quantum information and quantum computation
What about
No matter what field you are in, you can come in
You have classical knowledge
You can take your classical knowledge
To improve your
Classical algorithm
Uh, and then you
You're starting on both sides
It's kind of like a competition
Keep pushing the algorithm on both sides
The efficiency keeps going up
This is a very interesting result
So good
Uh, and then
Uh, because I've been doing some of this lately
This aspect of work
So I took this opportunity
Uh, this opportunity
I also told you
I'm wearing
The place
What kind of questions did this place have just now
Next is that I want to introduce
My my
Some of the results on this quantum machine learning
Is there a problem
It seems like
Some people want to ask questions
But that one
There's a jump
Oh, really? I didn't notice
Good good
Now we're going to
Because it's another one
It's more like my topic
We can pause a little bit
Let everyone ask
These algorithms
Hello
My question is
That is
Maybe in the short term
We might just
Rely on dozens of qubits
Or I don't know
Something like 100 qubits
For this kind of really
Strong is really
These
For example, like recommendation system
This kind of thing
Is there a way to do this
Or a few hundred qubits
Test
Uh
Uh
This may not be
But your question is very good
Because my next report is basically
Focus on this question
So that's it
We
What I do with my students
Now this
Hardware development
Can do this
That is to say, we need this
The circuit size is actually very small
I'll give you one later
This is a more specific number
Good good
Thank you
Because of these
Most of the algorithms
All
I need a bigger scale
This quantum computer can do
A negative quantum computer
That is to say
Uh he can't
Just a small device
You can do these things
Right
I also want to ask you
Is the circuit size talking about the circuit depth?
You can do a few gates
This thing
There are still others
Or qubit size
Or circuit depth
I'll give you one later
My example is
You will see later
How many qubits do you need
How many gates do you need
Any other questions
Can you hear that
Can
Let me introduce myself
Sorry
I am that Guo Enrui
Now in
University of Maryland
Hello
Just like me
As far as I know
For example, like Shor's algorithm
It has exponential acceleration
Then like Gravel's search
At least
Even if
At least you can reach the square root of n
I want to ask
Now everyone is mainly doing quantum algorithms
Is studying quantum algorithms
In different
Different aspects
Expansion of different problems
That is, now everyone is mainly studying
In this
Part of the algorithm
Right algorithm
Actually from
From this 1980s
That one
Everyone started to think
Should it be possible to have a quantum computer
Do some
That's the one
He mentioned that maybe we can
Can have a quantum device to do the operation
From that time on
Quantum algorithm is a very, very
Important topic
Its influence is basically
Very comprehensive
For example, we know
In this crypto
In the early days, we were very focused on
This cryptography
Cryptography basically has some
Assumption of computation
Right, that is, I want to assume
What kind of problem
Not good
Then
That
This is the assumption of computation
Many places may
It means that if you say there is a good one
You have a good quantum algorithm
Your assumption of computation is not true
Not established
The most classic is this rsa
Because it assumes this
Prime number is
It's not easy to be affected
It's not easy to find its
The value factor
That's why it's so famous
Another reason
So
You design a good quantum algorithm
You may be able to break a lot
A lot of things
This is why the quantum algorithm is very important
A reason
Thank you
Thank you
That
I'll continue
We'll have one last time
Q&A, you can ask any questions
Okay, that's it
University of Sydney
I'm from the University of Sydney
I happen to be with
The last author
Tao Dacheng
In this classical machine learning
Is a very famous person
This group
He is also interested in this machine learning
Quantum machine learning is very interesting
So I took his students
The first author is his student
From the University of Sydney
Our paper
Just put it on the archive a few days ago
Just put it on the archive in September
We basically
The idea is basically
Combine some classical
Classical machine learning
Then combine some
Quantum algorithm
The idea
Then our
Our target is that we can
Now this hardware
We can do this
Proposal like this
Okay, that's the first one
Let me do a little background introduction
I just introduced global search
Right global search
It can be used basically
Machine learning to implement
How to implement
Basically global search has to do one thing
What is it
I want to find a state
This size, this file, this state
Let it get closer
Omega is the direction I want
I want him to get closer to Omega
The better
I just want to
Basically, this is me
I am doing this place
What are you doing
I want to
The closer he gets to Omega, the better
I want him
The less part perpendicular to Omega
Right, this is basically
What is this equation doing
Then we just said
That's why we can
We can come and learn this
This state
We just use
We use circuit to learn him
Because just that global
The algorithm is that I have two operations
I'm just going to flip it, right
Keep doing it and then flip it to a certain state
Just say about n
This is probably this
This step
I can flip it to the state I want
Right
Then this variational
This idea is basically
Anyway, I'm going to find a state
The closer to this
Omega is better, right?
I just want to
I can use machine learning to recognize this state
Right, this is also this
Biomonte
This nature just wrote that nature
Review article
A thought from their group
This is also a paper in May
This development is very fast
Then he went to recognize this
Recognize this circuit
How does he recognize it?
Hey, I designed two operations
One of them is this P Omega
This operation is what I just had in that slide
And then another PS is just
That is to say, there are two at the beginning
This flip operation
One is Omega and the other is S
It's not exactly
Do P Omega and PS
He can do P Omega again
Give him an alpha angle
Give him a beta angle
What about the angle of this beta
This can be used
This can be used
Optimization
To update your alpha and beta
Right
We are doing
We can all use some mms
Right to do this
To do update
Then he is to update this alpha and beta
Angle to let you next round
Your alpha and beta angles are different
Right then
Then they thought
Hey, if I use this machine
What about the method of learning
What is the comparison with Global Search
Very interesting, look at the next picture
This is their numerical experiment
They just used four cubits
I used four cubits here
And then
And then this blue dot
It's called Global Search
Global Search means
We just said you need to flip, right
You need this
Omega and S do a round like this
Right
Is this a round
One round is two steps, right
And then you need to do a few rounds
This Global Search probably tells you
Probably four rounds
Is eight operations
In fact, it is
2n plus 1, so it should be nine
Right
Nine words
Look at this
Horizontal
X-axis, you will find
Several times
This point is done nine times
Global Search
It can get the biggest
The biggest chance you see is 1
You see you are in this Oracle P
This is equal to 9
Your chance is 1, right
This means that Global Search
At this point, you can get the biggest chance
And then at other points
You see this probability is fluctuating
Right, it's actually like this
Up and down, right, it's only in
The probability is the biggest
And then in this part of the red cross
They got it by machine learning
Very interesting
At nine
It can be the same as Global Search
The result
But in other places
It can be better than Global Search
It's more robust
That is to say
You won't be because of yours
For example, I was supposed to be in the nine
I'm going to stop
But I stopped early
For example, I stopped at 8
I'm this up and down
I did it eight times
What if you use Variation
It will be better, right
It will be robust to your system design
What about this idea
That is to say, we
We want to say that this idea
Perception
Perception is basically
That is to say, you are also going to search
So basically you can put this
Global Search Embedded in your Perception
You just have to identify you
Mislabeled this value
Right, that is to say
That is to say, we
Use this idea
This is our step
Give me a database
This database
I have so many points
i from 1 to n
I have so many points
Then every point of mine
Xi is my input
Every point of mine
I have m so many features
I just want to go from feature
To identify my
To identify my label
This yi is my label
It may be positive or negative
Then, of course, the first step
I want to put this classical data
Encode to a quantum state
What about this quantum state
What do we do
That is to say, I put this feature
I put this
Xi
Encode to my input
Then
Then I have another
This index register
I just came to keep track
I have a few
A few samples
This i this place
I just need to say I have n
N so many
I need to log n
Because
Because you know
Quantum state can be superposition
So I just need to log n so many
I can keep track
This n
This n
Such a big data
Data set
The same
If I say there are m features
This feature register
I just need to log n so many
That is to say, it actually has a
Index compression
Of course, this step is actually
I'll tell you from the beginning
It actually needs a
A more non-trivial algorithm
But I'll come back to this issue later
Uh, and then
The first step is to say
I'm going to do this UK operation
What about this UK operation
What am I going to do
I want to identify this K
Basically, this
My state basically means
My K
Has been mislabeled
So I'm going to mislabel
This place introduces a
Face
What about this UK
I want to do this thing
But what about me
I'm going to design a
Learning algorithm, I'm going to learn
I'm going to
Approximate this UK
With this
With this low depth
Quantum circuit
Approximate UK and then achieve this
Basically this is
And then
Because this is different from global search
Because we have feature register
With index register
Feature register is for you
To recognize your
What does your distribution look like
Right?
There is no feature
You just need to do your index register
Right?
But because
We're in front of this
Because you're done
This
Give you this
Add a face
Because he actually
With this index
He actually has one
These two registers
It actually exists
You do the operation for K
You will affect you
Feature this register
So we need to do one more step
We need to do this step
We need to put the feature
This register
With your index register
Disentangle it
So you see this is now
It means that there is no correlation in the middle
There is no correlation
That is, we
What about our circuit
Disentangle
This operation
These two operations
We're going to use the method of machine learning
And then
Basically, this is my two
Basic operation
This picture is a demo
This ul1 is what I just did
And then, no, ul1
Plus this control g
Basically, this is what I just did
And then
This ul1
In fact, because this is just learning
Your feature, so it's just in this feature
Register
Disentangle can also be used
In this feature register
And then disentangle
This unit
This is basically just
The second inversion of this global search
For this s
An inversion of this plane
What about this inversion
Because it's going to do an inversion of your index
So it's just an index register
This is a
This is a
An iteration
Then I need a total of about
All n so many iterations
And then you'll find
That's what we're actually doing here
This quantum
Quantum state
This quantum circuit
I use this classical
Optimization to do
Just like this variation of global search
It actually has an angle that can be controlled
I'm using classical
To do optimization
That is to say, there are some MMD
This is more
Anyway, this is some classical optimization
Machine learning often does some things
Then I update and do it again
I've been doing square root n so many times
Ah
Just like global search
Square root n so many times
I can adjust it to the angle I want
Like this
This is our
Simple experiment
Then we can
We use experiments to implement
That
What about this place
I
Ah
Say
Inside this square
I told you that every one of me
Every one of this
How big is this
Our circuit
It's probably three
Three three three
Three layers like this
Or five layers
Because of our
Our
Feature is probably
Just
Our example
It's actually quite small
We'll have a bigger example later
Our feature size is probably
Four
There are four features in each sample
Then you will find
My success probability
Can be up to
About 80%
Ah
The one on the left is this
You identify your
Mislabel index
This probability
The error rate is probably
It can be up to 80%
90% like this
Classification
This place
Then
Perception can only do this
Linear partition
What if we want to
How do we do it
We just have to combine these
Perception
I don't know if there is
Is there any of this
More familiar with machine learning
We are
There is one called Ensemble Learning
Basically the idea is
I have a big
Ah database
I have a bigger sample
I don't need to
I don't need to use this whole sample
What can I do
I can
I can sample from the inside
Some smaller data
Then I use these smaller data
Set to train
My perception
Then I use my
A few weaker perceptions
I can combine it into one
Strong perception
This idea is called
This is the idea of ​​Ensemble Learning
Then
Doing this is sub-sampling
Because I don't need to
Such a big database
I just sample some smaller data
This is the concept of sub-sampling
So if I say I can train
My capital T
Then I can
This yt
Is the outcome of each vt
Then my ct
Is my preset threshold
Then I can use this thing
To do a better
This
Ah
Then we want to know
Will this combine be better than
My weak
Right
Then we will do a numerical
Conclusion
What are the benefits of this
It can
You can put your bigger
Training set to break it down into
Smaller training set
Then you train this weak classifier
Then this just fits
This quantum machine learning now
Because we mentioned at the beginning
What is the bottleneck of quantum machine learning
What is it
Classical data
Quantum state
This is almost all
Quantum machine learning proposal
Then we use
The idea of sub-sampling
The idea of ensemble learning
We can overcome this problem
Because you are actually a small data set
You are still quite efficient
You don't need to say
I have a hundred thousand points
With me a
This complexity
It's not just a hundred thousand
Divided by one hundred so much
It's actually every step
After the complexity you need
The complexity is multiplied by the index
So we use
This concept is actually very
Very fit for now
The development of quantum hardware
Good
Then
Next is a more
One of CS
We want to characterize our efficiency
If we say every data set
Every data set
Because we want to train some
Weak classifier
If I say
Log square root n so many points
Then we
Query complexity
This capital T means that I have a few
I have a total
Weak classifier number is
This capital T so many
Then you will find that because it is actually
Still related to this global algorithm
So it is with you
This square root of the data set
So if you say it's log n
Square root is probably
Multiply by this quantity
This m is the size of your feature
This is related to me
How many steps do you need to do
How many steps do you need to do
Then this is probably
Heuristic is probably
Log n so many steps
We will demonstrate later
If I say I choose log n
So many classical optimization
Then every data set of mine
I'll have one later
Performance comparison
Run time is probably
With log n
Multiply this
Polynomial relationship
So basically it is very efficient
it is good
it is good
This is our
A practical example we run
A classical database
Tens of thousands of points
10,000 points
Then every point
This feature is four
Is four features
Can be encoded on two qubits
Then we sub-sampling
We just sample
We just make it a sample of four smaller databases
Then every database
Only eight points
So this ratio is very small
It's about 8 out of 10,000
Then we found that if you use 8 out of 10,000
You can follow
You can get it directly from this
From this 10,000 points
To train
Your perception
Your combined efficiency is basically
With you
10,000 points to do the same
So you look at the last of this table
The last column
The last column means that I have combined
I have four
This week
This week's perception
This is my last combined success probability
You will find that it is
It can be up to about 80%
Of course with
It's not so good
But
Hardware can be implemented now
Basically this is already a very
Very
Very good proposal because it is now
Hardware can also be implemented
Ah
This is my example
Ah oh yes
This place
There is a caveat
This is
We
We can also another one
If you say you have a way to let you
Ah sample
Ah this data set
If you say your subsampling
You can sample away
From this
Perception of each week
Because perception is basically
Classify left or classify right
If you say your sample
When you do subsampling
You can avoid this
These points
This probability can also be increased to 92%
Say this depends on you
This method of subsampling
At the most
Say at the most extreme
If you say even if you use uniform sampling
Then we can still get 80%
If you say your sampling you can adjust
You can get 9 times 2
9 times 2 is already very impressive
I think so
I don't know his community
Reason
I think his community can do very well
So at least in the quantum community
No one can do it like us
So good
This place is just that
Ah
The question asked by the agent is
How many operations do we need
In the numerical experiment we just did
We actually use that regated
Cloud platform they have a library
Then we implement
We all use his standard library
If they want they can take our code
Can compile to their
This circuit can be run like this
Then
We say that in encoding
We want to put classical map to quantum state
If you use subsampling you only need to use
29 operations
Probably use 29 operations
Then this is all from him
That's what I just said
You use their library
You can
After you input the database
He will tell you your
How many gates do you need
So we
Input it about 29
On average 29
Then the operations behind us
Don't we have a few operations
That is, you have to flip your face
And then you have to disentangle
Right then
What about the operation we need in total
Ah
That's what we need
About 36
Can control your angle
Then
Then
Then
Then
Then
You can
So if you say
Decomposing
We probably need a total of 146
Jingle with
You can do our experiment
Like this
Okay, this is probably the details of our experiment
Then
Well, this one after this
The latter is more complicated
I won't talk about it first
Anyway, when the time comes, everyone has an opinion
Maybe I can come back later
Give other talk like this
Okay, I don't know
Do you have any questions
Like this
Thank you
Hello
Can you ask a question
Related to your talk
Okay, no problem
Because you mentioned it at the beginning
Let me introduce myself first
My name is Chen Yuan, I am a fourth grade
Then
You mentioned that Taiwan is planning
Want to
It's also related to computation
This is more famous
Like Microsoft
Should be
Or google
Buy john martini's lab
That Taiwan
Have decided to use
Which one
Which direction
To do
This is a good question
Basically
Taiwan's tendency is more
It should be said that the high-level
Government technology minister
Their tendency is more
Hope to use semiconductor
Yes, the semiconductor
The big company has not yet invested
I can't say that either
The big company is now
It may have a main force
But he didn't give up
Other proposals
The best semiconductor is Australia
Australia
University of New South Wales
University of New South Wales
They are leading the world
Semiconductor technology
Their Qubit is using
Semiconductor process
So then
Taiwan has TSMC
Everyone still hopes to use semiconductor
So the government department technology department
Preferred approach is also
Semiconductor
Then Taiwan University
Now it is said that the Taiwan University of Physics
Then with the Taiwan University of Electrical Engineering
Li Junyun
With
Teacher Chen Shiyuan
They are now saying that they have
A team is now
Follow is to say that they want to
Mainly to work with
Cooperate with University of New South Wales
They want to develop semiconductor technology
Then now this Tsinghua University
This research center
Inside a
Project to do superconductivity
In fact, the Central Academy of Sciences has a teacher doing superconductivity
Qubit
So
In this Tsinghua University
There are people doing superconductivity experiments
Of course there is also light and snow
There is also a group of people doing light and snow
Qubit
Light and snow, of course, also need
Sorry
Sorry
I just want to ask if this is in the field
Generally speaking, which direction
Is the most likely to be done
The most robust and then the easiest
Large-scale production
It's not easy
Because these are all super low temperatures
Superconductivity now needs
Hundreds of
Mini-K
It's almost absolute zero
Then semiconductor
Can go to
Maybe
Around 4K
Now
Large-scale is almost
At present
No idea
It's not like our computer can be at room temperature
These devices all need a
A big refrigerator
Cool it down to almost zero
Absolute zero
So
At present, everyone's business model
It's still more like crowd computing
For example, they said
Regetti IBM
Or Microsoft, they are basically
The future trend is that I have one
Is cloud
Then let you access
You can have some more complicated
You can upload it to my server
Then my quantum computer will help you calculate
The current business model is like this
Thank you
I thought
Most of the people in Caltech
Are doing theory
Caltech is now doing experiments
Which direction
Caltech itself
It doesn't seem to be experimenting with quantum computation
We have an institution called IQIM
This is quantum information
It's called
It's called
It's all theory
The group of people
Yes
Let me add that I am now a graduate of Dongda
What we do here is
Just the semiconductor
How to make a semiconductor
To do Qubit
That's true
New South Wales
Very leading
Especially recently
From the end of last year
About 80 million
Establish a new company
And their country's funds
Invest in this area
That 80 million is US dollars
That's right
So I think
All directions are
Very difficult
I was before
When I was in PhD
Also use
But
My personal idea is
It's not that easy
He's probably
A bottleneck
I don't know yet
How to overcome
Yes, but because I do experiments
So if
If you want to discuss
Let's continue to discuss
But for now
For the content of today's speech
Is there a problem
Yes, I want to remind you
If you want to see that
Senior's video
I have a link below
That link
If you want to find
If you don't know the problem
You can click on the link
In the chat
There is a link
My slides are all online
So you can watch it anytime
It will always be there
I have a question
Sorry
My question
I remember
I'm watching now
Because I'm not doing theory
But I know
It seems to be in the algorithm
For
If you don't know how to deal with it
At least there is
Grover
To do a basic acceleration
But we want
What you want
Things should be
Want to do
Not just polynomial
But exponential acceleration
So inside
Inside
Several algorithms
Recommendation now
I saw it before
Austin
This news
I think it's super cool
Found
Is a
I thought only quantum computers could do it
Found out
Can be done
What do they call it
Quantum Inspired Algorithm
I think it's really cool
Can I make a comment
Yes
This is basically
If you say now
Strict
Complexity Theory
For those who do theoretical computer science
They actually mean
You can't define it yet
Quantum Computation
Really
Really
Very strict proof of acceleration
Even like Grover
Even like this Schultz Algorithm
We know he has it now
He now has this
But it's just
The best classic algorithm
Then we know
What about this factoring
He is not a
He is a question of mouth NP
He said
Not to say
Ah
It should be said that the current bottleneck is
Classical may
I don't know how to do it
Maybe one day there will be one
There is also a Schultz algorithm
This classical algorithm
Schultz Algorithm
Yes
Because of Complexity Class
Not to say this
Factoring is the most difficult problem
He is not a question of NP
He is not a question of NP
So
It's all like this now
Although we have some
Some
Quantum algorithms are better than modern classic algorithms
But the point is
This is to say
There will be a classic algorithm
Will improve
This is also
Very interesting
I also encourage if
People who do CS
You should be able to say
Bring your classical knowledge
Let's take a look at these quantum algorithms
Or
Or to discuss
You are interested in the corresponding quantum version
Maybe it's like this
Maybe one day you can also
A very good way
This way
Yes
Do you have any questions?
Please say
I want to ask you again
At the beginning of your quantum
Principle Component Analysis
Is it the HHL algorithm?
Or later
There are other extensions
Uh
It is possible
It is possible to use this HHL
But I didn't go to see it
Because I just entered this field
Not long
Basically this principle component
It is definitely a face estimation
Because
Basically it is to identify these
The covariance matrix is relatively large
This eigenvalue
Maybe you can put HHL
Do it like this
Hello i have a question
Ok you ask
Self-introduction
Uh
Uh
I did a high-performance experiment
Is
I am doing dark matter
But because I did
Is actually with
Very close
So when I was in college
I went to the teacher's class
I almost did it
My question is like this
Uh actually
You just
Is what you said
In fact, it is more like
Is there
Strict mathematical logic
Machine learning
But now the most powerful machine learning is some kind of fossil logic
Neural net
Now quantum
Everyone is in this direction
Use quantum to do fossil logic
What is the situation now
Will be
Traditionally more powerful
Still don't know
Because I don't know
I don't know
Ok ok
Since you ask these questions
I will continue to talk about it
In fact, one of the results behind me
In fact, it may be related to this
That is to say
Can everyone see it
Can see
Basically
Me and my student
The question we want to ask is
Because you know that we actually use this
Circuit to learn
You are actually your quantum circuit
You have a parameter in it
Then you can use this optimization
To adjust your parameters to produce the state you want
Right
There is a question that you want to know
Such a quantum circuit
We call parametric quantum circuit
His
Expressive power because we know
Machine learning has two types, one is classification
One is generative model
That is to say, I want to produce a distribution
I want to produce a target distribution
Then we want to know this quantum circuit
This
The ability to generate this
Generative ability is how good
Right
So I did it with my student
Did the following thing
That is to say
If I say I can have
L block
This every
A block of this square
Then
In fact, the last row
Is measurement
After measurement, it is a classical data
Then I can put this
Do some classical optimization
Put this
Parameter feedback
Because this is all adjustable
Adjustable parameters
Then I want to know
This type of circuit
I finally generated these classical distributions
With
We know that the Boltzmann machine
Can also be used to generate distributions
Right
You just mentioned these like this neural network
Basically, the Boltzmann machine is like this
That is to say, the most general Boltzmann machine
The picture in the middle
That is to say, it has a visible layer and a hidden layer
Then all the layers are fully connected
Right
Then we all know that this kind of fully connected
Very powerful
This is basically Deep Neural Network
Everything is connected
Then
Of course there is another type of Boltzmann machine
That is to say, it is actually
It only has layers and layers
Connected
Every
The middle of your own layer is not
This is not connected
Right, this is Restricted Boltzmann Machine
These two are classic methods
Can be generated
Then the picture on my left
That is to say this is
Another version of this picture
That is to say
The layout of each block of the circuit is different
I'm on the left
The layout of each circuit on the left
The layout of each block of the circuit is the same
The quantum circuit on my right
It actually has one
Its every layout
In fact, the first layout is the most powerful
Because it
The first one
The first one is that it has a lot of C-NOT gates
Can be connected
The second one, it's actually your C-NOT gate
It's already less
And it's
It's every block
Then all the way to the end
This
Connectivity is actually very limited
Then we want to know
These circuit layouts
This
From the perspective of deep distribution
Which one is the most powerful
Just these two quantum
With these two classical
That's basically it
The most powerful is the one on the left
This quantum circuit
This quantum circuit
Each of them is
The same layout
Then each layout actually has this
It has a C-NOT between any two points
Can be connected
This is the most powerful
It's more powerful than the general Boltzmann machine
It's more powerful than the general Boltzmann machine
In terms of deep
Distribution
Then this general Boltzmann machine
It's better than this quantum circuit
The ability to generate
Because this quantum circuit
In fact, it
It's actually more limited
It's not all
All of this
There's a connection between the lines
So the general Boltzmann machine
Will be stronger than this circuit
This circuit will be stronger than this
Boltzmann machine
This is what we haven't published
A result of the paper
Maybe next week it will be put on the archive
If all goes well
This is
On the one hand
Also answer your question
In fact, the quantum circuit
It's in the perspective of deep distribution
It's actually
Classic
The classic Boltzmann machine is stronger
Of course
Maybe there are some specific tasks
Now we don't know yet
How does the quantum circuit compare with the Boltzmann machine
This is still unknown
Senior, can I ask you
Can you go back to the page above
it is good
Because like this
The two pictures above
Does it really correspond to the physical
His connection
Like this
It might be
Near snapper coupling
So he can only follow the side
He's gonna do a bunch of swaps
Right, right, right
The second picture
It's kind of like
With this actual
Implementation set up
Consider
Because there are some set ups
So in that
When you use
Every one of them
Like he's doing
One swap is three CNOT
Every CNOT
He can now
About how much
Their
Fidelity is not high
I heard it's low, but I don't know
How much
I remember the best is only 80%
May be worse
Because he's actually every
Connectivity between qubits
Different
A little worse
Because I think the above
They just got one million
One hundred million
This investment
Very powerful
They
That's pretty important to them
Because they even
They do it themselves
But I think the above two pictures
It's like that
Compared with that
Really
Like that
He is
A series of times he can
As long as he can
With five
That
CNOT
So I think
At least
Why
At least for people
Ask them
The biggest idea is
Where is the biggest advantage
I think this is what they can do
Emphasize
Like their CNOT
It should be up to 97
Around 98
It seems like someone has 99
But at least I think
Their advantage
Yeah yeah yeah
Mr. Cao
Gate is lower but
The chance is bigger
Any other questions
It seems like there's no problem
Okay
If there's no problem today
Thank you for your participation
Thank you everyone
Thank you
My personal website is here
If you have any questions
You're welcome to contact me
We will
Put the link of the video
And
Senior's personal website
On our announcement
On our website
Thank you
Our recording will be
In a day or two
We will put it on
Pyra's
Pyra's website
No problem
Yeah
Okay, thank you everyone
Thank you
Bye bye
Bye bye
