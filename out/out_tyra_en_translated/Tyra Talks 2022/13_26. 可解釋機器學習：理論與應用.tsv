start	end	text
0	4000	Can you see it now?
4000	6000	Yes.
6000	8000	OK.
8000	10000	Hello, everyone.
10000	12000	I'm John Yu-nan.
12000	14000	You can call me Alan.
14000	16000	I'm a Ph.D. student at Rice University.
16000	20000	I'm studying computer science.
20000	22000	I'm in my second year.
22000	24000	I'm doing research on
24000	30000	interpretable machinery
30000	34000	or explainable artificial intelligence.
34000	38000	My goal is to explain
38000	40000	why this machine learning model
40000	42000	can make such predictions.
42000	44000	I want to explain
44000	46000	and convince people
46000	48000	that this prediction result
48000	52000	is not random.
52000	54000	OK.
54000	56000	Let me introduce myself.
56000	58000	I used to study in Taiwan.
58000	60000	I'm a master's student.
60000	62000	I used to work in
62000	64000	a recommendation system.
64000	66000	If you have questions
66000	68000	about the recommendation system,
68000	70000	you can ask me.
70000	72000	I'll try to answer them.
72000	74000	OK.
74000	76000	I studied math in college.
76000	78000	Before I graduated from
78000	80000	my senior year,
80000	82000	I didn't know how to code.
82000	84000	I didn't even understand
84000	86000	a line of code.
86000	88000	I want to encourage
88000	90000	those who still want to code
90000	92000	not to give up.
92000	94000	Everyone has a chance.
94000	96000	Keep working hard.
96000	98000	Everyone has a chance
98000	100000	to do this.
100000	102000	OK.
102000	104000	Today,
104000	106000	OK.
106000	108000	I'm going to talk about
108000	110000	I just want to say
110000	112000	it's too inspirational at the beginning.
112000	114000	Anyway,
114000	116000	I'm not a master today.
116000	118000	I'm just an ordinary
118000	120000	sophomore PhD student.
120000	122000	If you want to interrupt me,
122000	124000	just turn on your microphone
124000	126000	and interrupt me.
126000	128000	OK.
128000	130000	Let's get started.
130000	132000	OK.
132000	134000	Today's outline
134000	136000	focuses on four points.
136000	138000	First,
138000	140000	explainable artificial intelligence.
140000	142000	Next,
142000	144000	explainable machine learning.
144000	146000	Next,
146000	148000	explainable machine learning.
148000	150000	There are two types of machine learning.
150000	152000	First,
152000	154000	explainable machine learning.
154000	156000	For example,
156000	158000	you may have heard of
158000	160000	decision tree.
160000	162000	In decision tree,
162000	164000	we can see
164000	166000	the model itself
166000	168000	is explainable.
168000	170000	is explainable.
170000	172000	The other type is
172000	174000	neural network.
174000	176000	The other type is
176000	178000	deep neural network.
178000	180000	It's fully connected.
180000	182000	It's fully connected.
182000	184000	It's a black box model.
184000	186000	It's not explainable.
186000	188000	You don't know
188000	190000	the learning weight.
190000	192000	You don't know the learning weight.
192000	194000	You don't know the learning weight.
194000	196000	You don't know the learning weight.
196000	198000	So,
198000	200000	we need an explanation
200000	202000	after learning the model.
202000	204000	After learning the model,
204000	206000	we need to learn
206000	208000	an explanation model
208000	210000	to explain the model.
210000	212000	This type of learning method
212000	214000	is called post hoc.
214000	216000	post hoc is after prediction process.
216000	218000	OK.
218000	220000	After this,
220000	222000	I'm going to talk about
222000	224000	the application of XAI.
224000	226000	If you are not a computer science
226000	228000	background,
228000	230000	you may wonder
230000	232000	what's the use of XAI.
232000	234000	I'm going to talk about
234000	236000	the open source package
236000	238000	to make an explainable model.
238000	240000	to make an explainable model.
240000	242000	Now, everyone uses Python.
242000	244000	I'm going to talk about
244000	246000	two open source packages.
246000	248000	I'm going to tell you
248000	250000	what is XAI.
250000	252000	XAI is not an unimaginable thing.
252000	254000	If you have a prediction model,
254000	256000	and you want to make it explainable,
256000	258000	everyone has a chance
258000	260000	to step into this field.
260000	262000	OK.
262000	264000	Now,
264000	266000	I'm going to talk about
266000	268000	what is XAI.
268000	270000	Artificial intelligence
270000	272000	has many applications
272000	274000	in our daily life.
274000	276000	For example,
276000	278000	deep mind
278000	280000	has created AlphaGo
280000	282000	to play Go.
282000	284000	It can scare
284000	286000	human experts.
286000	288000	Yes.
288000	290000	A while ago,
290000	292000	deep mind gave
292000	294000	AlphaTensor and
294000	296000	AlphaProtein.
296000	298000	In fact,
298000	300000	they are for reinforcement learning.
300000	302000	They are for
302000	304000	reinforcement learning.
304000	306000	They can be applied
306000	308000	in daily life.
308000	310000	For example,
310000	312000	medical diagnosis.
312000	314000	For example,
314000	316000	I give you an X-ray,
316000	318000	and you hope the machine learning model
318000	320000	can tell if you have a brain tumor.
320000	322000	Then it will tell you
322000	324000	if you have a brain tumor.
324000	326000	Or you can tell
326000	328000	the machine learning model
328000	330000	if you have a cough
330000	332000	or symptoms.
332000	334000	Tesla Autopilot
334000	336000	is an
336000	338000	autonomous vehicle.
338000	340000	It will not detect
340000	342000	any object.
342000	344000	It will give you the best way
344000	346000	to drive the car.
346000	348000	Next is voice recognition.
348000	350000	For example, Alexa.
350000	352000	I don't know if you have used
352000	354000	a smart speaker.
354000	356000	When you speak to the speaker,
356000	358000	it will detect your voice
358000	360000	and convert it
360000	362000	to English.
362000	364000	Then it will
364000	366000	request the service
366000	368000	from the online server.
368000	370000	Recently,
370000	372000	there is a popular
372000	374000	speech-to-speech
374000	376000	in Meta.
376000	378000	This kind of
378000	380000	AI application
380000	382000	is becoming more and more common.
382000	384000	It is hard for us
384000	386000	to know why
386000	388000	these models make these judgments.
388000	390000	For example,
390000	392000	if you go to a doctor
392000	394000	and he presented a
394000	396000	medical diagnosis model.
396000	398000	You input all your symptoms
398000	400000	into this model.
400000	402000	He will say,
402000	404000	you just have a cold today.
404000	406000	Why do you make this judgment?
406000	408000	Or if Tesla gives you
408000	410000	the best
410000	412000	autonomous driving
412000	414000	route plan,
414000	416000	do you really think
416000	418000	he won't be in trouble?
418000	420000	In this decision-making process,
420000	422000	if you can provide
422000	424000	what features
424000	426000	this model learned today,
426000	428000	or if it senses
428000	430000	a pedestrian passing by
430000	432000	and stops,
432000	434000	or if it detects a green light
434000	436000	in front of it.
436000	438000	If this kind of
438000	440000	why process is provided to users,
440000	442000	it can actually
442000	444000	increase people's
444000	446000	credibility in
446000	448000	machine learning models.
448000	450000	So,
450000	452000	the whole XAI
452000	454000	is actually doing one thing.
454000	456000	It wants to provide
456000	458000	a reasonable reason to users.
458000	460000	For example,
460000	462000	what features did this model learn?
462000	464000	Or what did this model see?
464000	466000	Or did this model learn
466000	468000	two or three features
468000	470000	that led me to make
470000	472000	this prediction result?
472000	474000	XAI is improving
474000	476000	this process.
476000	478000	For example,
478000	480000	this autopilot on the left.
480000	482000	What does XAI want to do?
482000	484000	We actually want to provide
484000	486000	why this autopilot
486000	488000	will give you
488000	490000	a safe route plan.
490000	492000	So, you have to tell users
492000	494000	that because my model
494000	496000	detects people,
496000	498000	my model detects
498000	500000	no cars within 50 meters.
500000	502000	For example,
502000	504000	the medical diagnosis
504000	506000	on the right.
506000	508000	The medical diagnosis
508000	510000	is like
510000	512000	persuading a doctor
512000	514000	or a patient
514000	516000	that because I
516000	518000	see a tumor
518000	520000	in the X-ray of your brain,
520000	522000	I frame it,
522000	524000	so I judge your brain tumor.
524000	526000	This action
526000	528000	doesn't seem
528000	530000	so necessary
530000	532000	in the whole
532000	534000	prediction process.
534000	536000	But if you provide
536000	538000	this step to users,
538000	540000	why this step,
540000	542000	users will feel
542000	544000	very confident
544000	546000	about your decision
546000	548000	model.
548000	550000	Of course, there are many applications
550000	552000	that I haven't mentioned,
552000	554000	such as stock market trading.
554000	556000	For example, if you buy
556000	558000	a stock,
558000	560000	the price of the stock
560000	562000	has already fallen
562000	564000	to the point of fracture.
564000	566000	If you sell the stock,
566000	568000	the price will not
568000	570000	fall below $100.
570000	572000	If the model is wrong,
572000	574000	it will ask you to sell it
574000	576000	because the price of the stock
576000	578000	may fall below $100.
578000	580000	You have to convince
580000	582000	the trader
582000	584000	that because I see
584000	586000	a tumor
586000	588000	in the X-ray of your brain,
588000	590000	your prediction model
590000	592000	is credible.
592000	594000	There are several points
594000	596000	related to this.
596000	598000	First, you have to convince users.
598000	600000	Second, you have to provide security.
600000	602000	Another important thing
602000	604000	is some legal things,
604000	606000	such as GDPR.
606000	608000	A while ago,
608000	610000	Facebook was called
610000	612000	to the European Parliament
612000	614000	or Google CEO
614000	616000	was called to the U.S.
616000	618000	House of Representatives
618000	620000	was called to scold.
620000	622000	In fact,
622000	624000	these privacy data
624000	626000	or personal data
626000	628000	are not provided
628000	630000	to users
630000	632000	in a reasonable
632000	634000	or safe way.
634000	636000	You can't use these features
636000	638000	unconditionally
638000	640000	as a basis
640000	642000	for training models.
642000	644000	If you can provide these methods
644000	646000	to users through XAI
646000	648000	and say
648000	650000	I use these features
650000	652000	today
652000	654000	and provide you
654000	656000	some personal
656000	658000	recommendations or predictions.
658000	660000	In fact, users
660000	662000	will be more at ease
662000	664000	that you have used these things.
664000	666000	Even if it is privacy data,
666000	668000	it is better than
668000	670000	using XAI.
670000	672000	This is the reason
672000	674000	why XAI has attracted
674000	676000	more and more people
676000	678000	in the past 1 or 2 years
678000	680000	or 3 or 4 years.
680000	682000	OK.
682000	684000	Next,
684000	686000	those are
686000	688000	some high-level
688000	690000	things.
690000	692000	Let's talk about
692000	694000	some
694000	696000	more
696000	698000	scientific things.
698000	700000	If we are doing
700000	702000	image classification,
702000	704000	we are judging
704000	706000	the model
706000	708000	whether it is good or not.
708000	710000	We are training
710000	712000	a model of image classification.
712000	714000	We can't say
714000	716000	this model
716000	718000	is good or not.
718000	720000	Of course, we can say
720000	722000	it is a prediction.
722000	724000	For example,
724000	726000	we can predict
726000	728000	the label of this image.
728000	730000	However,
730000	732000	we can say
732000	734000	this image is a frog.
734000	736000	Does this model
736000	738000	really learn the characteristics of a frog?
738000	740000	In other words,
740000	742000	does it really learn
742000	744000	the head of a frog?
744000	746000	Does it really learn
746000	748000	the head of a frog?
748000	750000	No one knows.
750000	752000	It may learn
752000	754000	that most of the frog images
754000	756000	are because the frog
756000	758000	stands on the pond and the lotus.
758000	760000	So it catches the characteristics
760000	762000	of the pond and the lotus.
762000	764000	We don't know this.
764000	766000	Why?
766000	768000	When a model like this
768000	770000	learns the model of
770000	772000	pond and frog,
772000	774000	will it predict the wrong?
774000	776000	We don't know this.
776000	778000	Through this AI,
778000	780000	we can see
780000	782000	which part is important.
782000	784000	In the end,
784000	786000	if we get the final explanation
786000	788000	that the reason
788000	790000	the model is a frog
790000	792000	is because it has
792000	794000	a very representative head,
794000	796000	then we know
796000	798000	this model is actually learning
798000	800000	the right direction,
800000	802000	instead of catching some
802000	804000	background loopholes
804000	806000	or shortcuts
806000	808000	to improve
808000	810000	the AI.
810000	812000	For example,
812000	814000	let's say
814000	816000	there is a test
816000	818000	to see if you are
818000	820000	a COVID classifier.
820000	822000	As we all know,
822000	824000	there is a high probability
824000	826000	that you have a fever or a sore throat.
826000	828000	Let's say you only have a cold.
828000	830000	If you only have a fever
830000	832000	or a sore throat,
832000	834000	the test says you are a COVID classifier.
834000	836000	Then you are panicking.
836000	838000	If we want to
838000	840000	judge whether
840000	842000	the machine learning model
842000	844000	really learns
844000	846000	the features we want,
846000	848000	for example,
848000	850000	we provide some X-ray photos.
850000	852000	It will say that
852000	854000	the COVID diagnosis
854000	856000	is not only based on
856000	858000	the two features,
858000	860000	sore throat and fever.
860000	862000	If we
862000	864000	provide
864000	866000	this prediction result
866000	868000	to the user,
868000	870000	we can detect
870000	872000	the features
872000	874000	and the user
874000	876000	will be happier.
876000	878000	Okay.
878000	880000	Let's drink some water.
884000	886000	To sum up,
886000	888000	AI and XAI
888000	890000	are actually
890000	892000	a cycle
892000	894000	of prediction.
894000	896000	If we don't have
896000	898000	AI prediction,
898000	900000	we don't need to
900000	902000	detect the features.
902000	904000	XAI will not appear.
904000	906000	XAI is
906000	908000	mainly
908000	910000	trained by
910000	912000	a model.
912000	914000	The prediction model
914000	916000	has the ability to detect
916000	918000	the features.
918000	920000	You can see
920000	922000	a dog here.
922000	924000	There is a prediction model here.
924000	926000	It detects that it is a dog.
926000	928000	Why?
928000	930000	Maybe the dog is lying on the ground.
930000	932000	Most dogs are lying on the ground.
932000	934000	I learned the features
934000	936000	on the side.
936000	938000	It detects that it is a dog.
938000	940000	Or maybe
940000	942000	it is orange.
942000	944000	It detects that it is a dog.
944000	946000	We don't know.
946000	948000	It is a black box model.
948000	950000	We need another
950000	952000	XAI model
952000	954000	to open the black box.
954000	956000	We need another XAI model
956000	958000	to open the black box.
958000	960000	Then we can see
960000	962000	why the picture is a dog.
962000	964000	Because the dog has thin eyes
964000	966000	and special ears.
966000	968000	Because of the learning
968000	970000	of this feature,
970000	972000	the picture is
972000	974000	judged to be a dog.
974000	976000	At this time,
976000	978000	XAI model is very credible.
978000	980000	Because we know
980000	982000	it learns in the right direction.
982000	984000	Instead of
984000	986000	learning other features
986000	988000	to create the prediction result.
988000	990000	This model,
990000	992000	prediction model,
992000	994000	is more credible.
994000	996000	OK.
996000	998000	That was
998000	1000000	an explanation of
1000000	1002000	a big concept of XAI.
1002000	1004000	Let's go deeper.
1004000	1006000	I am not sure
1006000	1008000	if you are all
1008000	1010000	CS background students,
1010000	1012000	professors,
1012000	1014000	or PhD students.
1014000	1016000	Today,
1016000	1018000	I am going to give a high-level
1018000	1020000	talk.
1020000	1022000	We are going to
1022000	1024000	learn more about
1024000	1026000	the current
1026000	1028000	XAI
1028000	1030000	SOTA techniques
1030000	1032000	to execute
1032000	1034000	the explanation.
1034000	1036000	OK.
1036000	1038000	Before that,
1038000	1040000	let's take a look
1040000	1042000	at the different scopes.
1042000	1044000	There are global and local.
1044000	1046000	I am going to talk about
1046000	1048000	the global.
1048000	1050000	We are looking at the model.
1050000	1052000	For example,
1052000	1054000	the image classifier
1054000	1056000	in total
1056000	1058000	cares about the features.
1058000	1060000	For example,
1060000	1062000	let's train
1062000	1064000	a model
1064000	1066000	for dog
1066000	1068000	classifier.
1068000	1070000	We want to know
1070000	1072000	what this model cares about.
1072000	1074000	For example,
1074000	1076000	dogs have different ears.
1076000	1078000	So, ears are the features
1078000	1080000	this model cares about.
1080000	1082000	This is the global scope.
1082000	1084000	What is local?
1084000	1086000	Local is to explain
1086000	1088000	what each image cares about.
1088000	1090000	For example,
1090000	1092000	if we want to
1092000	1094000	do the global,
1094000	1096000	we have to look at the classifier
1096000	1098000	in all dogs.
1098000	1100000	What features does it care about?
1100000	1102000	But in the local scope,
1102000	1104000	we have to look at
1104000	1106000	each image.
1106000	1108000	This image cares about
1108000	1110000	the dog's face.
1110000	1112000	This image cares about
1112000	1114000	the dog's tail or ears.
1114000	1116000	In the local scope,
1116000	1118000	each image cares about
1118000	1120000	each image.
1120000	1122000	Each has its own advantages.
1122000	1124000	If we want to look at
1124000	1126000	whether this model is credible,
1126000	1128000	we can use global.
1128000	1130000	For example,
1130000	1132000	in total,
1132000	1134000	the autopilot model
1134000	1136000	detects
1136000	1138000	traffic lights,
1138000	1140000	pedestrians,
1140000	1142000	or obstacles
1142000	1144000	on the road.
1144000	1146000	In total,
1146000	1148000	we use global.
1148000	1150000	In the local scope,
1150000	1152000	in this scene,
1152000	1154000	in this second,
1154000	1156000	the sensor detects
1156000	1158000	this street scene.
1158000	1160000	Because it has something,
1160000	1162000	it can provide something in the next second.
1162000	1164000	When we analyze
1164000	1166000	the street scene
1166000	1168000	every second,
1168000	1170000	we have to use the local model.
1170000	1172000	The local model
1172000	1174000	provides a more
1174000	1176000	restrictive
1176000	1178000	explanation to the user.
1178000	1180000	This is global and local.
1180000	1182000	Next, there are two
1182000	1184000	different manners.
1184000	1186000	One is intrinsic and the other is post-hoc.
1186000	1188000	Intrinsic means
1188000	1190000	the model itself has something to explain.
1190000	1192000	What is something to explain?
1192000	1194000	For example,
1194000	1196000	decision tree.
1196000	1198000	At the end,
1198000	1200000	the model predicts the result is C.
1200000	1202000	We can start from C
1202000	1204000	and go back to C.
1204000	1206000	For example,
1206000	1208000	when the model makes a decision,
1208000	1210000	what are the conditions
1210000	1212000	to reach the
1212000	1214000	final prediction result?
1214000	1216000	The model itself
1216000	1218000	has something to explain.
1218000	1220000	We can know
1220000	1222000	from the model's decision
1222000	1224000	what features it has
1224000	1226000	or what condition it has.
1226000	1228000	This is the intrinsic model.
1228000	1230000	On the right,
1230000	1232000	if you have heard
1232000	1234000	some Machine Learning lectures
1234000	1236000	or courses,
1236000	1238000	you should know about BERT.
1238000	1240000	In BERT,
1240000	1242000	there is a
1242000	1244000	tension score
1244000	1246000	for each word
1246000	1248000	in the model.
1248000	1250000	For example,
1250000	1252000	I put a sentence
1252000	1254000	in the BERT model.
1254000	1256000	For each word,
1256000	1258000	there is a positive or negative sentiment.
1258000	1260000	Each word
1260000	1262000	has an added value
1262000	1264000	in terms of
1264000	1266000	positive or negative sentiment.
1266000	1268000	For example,
1268000	1270000	love is positive.
1270000	1272000	Love is a positive
1272000	1274000	tense.
1274000	1277000	This
1277000	1279000	high-level
1279000	1281000	contribution
1281000	1283000	is the tension weight
1283000	1285000	in the model.
1285000	1287000	So we can use
1287000	1289000	the importance of words
1289000	1291000	in terms of
1291000	1293000	model prediction
1293000	1295000	to determine
1295000	1297000	why this sentence
1297000	1299000	is a
1299000	1301000	positive
1301000	1303000	prediction result.
1303000	1305000	We can use
1305000	1307000	the importance score
1307000	1309000	to make the prediction result.
1309000	1311000	The two models
1311000	1313000	I just talked about
1313000	1315000	are Intrinsic
1315000	1317000	and Prediction.
1317000	1319000	Next is
1319000	1321000	Post-Hoc.
1321000	1323000	I will talk about
1323000	1325000	why I focus on
1325000	1327000	Post-Hoc.
1327000	1329000	Most
1329000	1331000	machine learning models
1331000	1333000	use black box models.
1333000	1335000	For example,
1335000	1337000	I build a deep learning network
1337000	1339000	and stack some blocks
1339000	1341000	and
1341000	1343000	connect them to
1343000	1345000	a convolution.
1345000	1347000	These prediction models
1347000	1349000	have good results,
1349000	1351000	but I don't know
1351000	1353000	what they do inside.
1353000	1355000	For example, I don't know
1355000	1357000	what features they learn.
1357000	1359000	This black box learning
1359000	1361000	requires
1361000	1363000	Post-Hoc.
1363000	1365000	After the model training,
1365000	1367000	I need to provide an
1367000	1369000	explanation
1369000	1371000	for the prediction model.
1371000	1373000	This model
1373000	1375000	is common
1375000	1377000	in daily life.
1377000	1379000	For example,
1379000	1381000	let's build a
1381000	1383000	recommendation system.
1383000	1385000	We use
1385000	1387000	neural network
1387000	1389000	to train the model.
1389000	1391000	At this time,
1391000	1393000	the recommendation system
1393000	1395000	is not explainable.
1395000	1397000	So we need a
1397000	1399000	explainable model
1399000	1401000	to explain the model.
1401000	1403000	The Post-Hoc model
1403000	1405000	has two scopes.
1405000	1407000	One is global,
1407000	1409000	the other is local.
1409000	1411000	For the global model,
1411000	1413000	I need to see
1413000	1415000	what features
1415000	1417000	the neural network
1417000	1419000	focuses on.
1419000	1421000	For the local model,
1421000	1423000	the neural network
1423000	1425000	focuses on
1425000	1427000	18 years old,
1427000	1429000	male,
1429000	1431000	Asian,
1431000	1433000	Chinese,
1433000	1435000	and Taiwanese.
1435000	1437000	So I need
1437000	1439000	a high level
1439000	1441000	model.
1441000	1443000	Each model
1443000	1445000	has different features.
1445000	1447000	I need to provide
1447000	1449000	different models
1449000	1451000	for each model.
1451000	1453000	In conclusion,
1453000	1455000	these two models
1455000	1457000	are the mainstream.
1457000	1459000	Intrinsic and Post-Hoc.
1459000	1461000	Today,
1461000	1463000	I will focus on
1463000	1465000	the Post-Hoc explanation.
1465000	1467000	Later on,
1467000	1469000	I will focus on
1469000	1471000	the local model.
1471000	1473000	Local model is
1473000	1475000	common in daily life.
1475000	1477000	For example,
1477000	1479000	the recommendation system.
1479000	1481000	OK.
1481000	1483000	Now,
1483000	1485000	let's talk about
1485000	1487000	Intrinsic and Local.
1487000	1489000	Here is a picture.
1489000	1491000	This is an article.
1491000	1493000	For example,
1493000	1495000	I say this article
1495000	1497000	is important.
1497000	1499000	Why?
1499000	1501000	Because it detects
1501000	1503000	ENT23 or
1503000	1505000	natural tenacity.
1505000	1507000	So,
1507000	1509000	through this,
1509000	1511000	I can tell
1511000	1513000	which word is important.
1513000	1515000	I can tell
1515000	1517000	users
1517000	1519000	why I use this article
1519000	1521000	to make a prediction.
1521000	1523000	It's because
1523000	1525000	my model detects
1525000	1527000	ENT23
1527000	1529000	or natural tenacity.
1529000	1531000	This way,
1531000	1533000	users can be convinced
1533000	1535000	that their prediction
1535000	1537000	is correct.
1537000	1539000	OK.
1539000	1541000	This is
1541000	1543000	what Intrinsic
1543000	1545000	provides.
1545000	1547000	OK.
1547000	1549000	What is Intrinsic Global?
1549000	1551000	As I said,
1551000	1553000	Global focuses on
1553000	1555000	the model.
1555000	1557000	For example,
1557000	1559000	we design a CNN model
1559000	1561000	which is
1561000	1563000	a representation
1563000	1565000	of image classification.
1565000	1567000	OK.
1567000	1569000	Why does CNN
1569000	1571000	successfully detect
1571000	1573000	the image of a cat?
1573000	1575000	It's because
1575000	1577000	it detects the cat's head.
1577000	1579000	If we extract
1579000	1581000	a layer
1581000	1583000	of model weight
1583000	1585000	from CNN,
1585000	1587000	we can see
1587000	1589000	that this model
1589000	1591000	does a convolution
1591000	1593000	to some features
1593000	1595000	resulting in
1595000	1597000	a model prediction.
1597000	1599000	OK.
1599000	1601000	If we extract
1601000	1603000	this feature map
1603000	1605000	from the model,
1605000	1607000	users can see
1607000	1609000	that
1609000	1611000	their prediction
1611000	1613000	is correct.
1613000	1615000	This is
1615000	1617000	the process
1617000	1619000	of Intrinsic.
1619000	1621000	Intrinsic is
1621000	1623000	a model
1623000	1625000	which has
1625000	1627000	a function
1627000	1629000	to make a prediction.
1629000	1631000	Or a model
1631000	1633000	can extract
1633000	1635000	a component
1635000	1637000	to make a prediction.
1637000	1639000	This is
1639000	1641000	the process of Intrinsic.
1641000	1643000	OK.
1643000	1645000	The other thing
1645000	1647000	we focus on
1647000	1649000	is PostHoc Explanation.
1649000	1651000	PostHoc Explanation
1651000	1653000	is divided into two steps.
1653000	1655000	The first step is
1655000	1657000	the black box model,
1657000	1659000	which is our prediction model.
1659000	1661000	We have data and model
1661000	1663000	to train.
1663000	1665000	The next step is
1665000	1667000	the explainer,
1667000	1669000	which is our machine learning
1669000	1671000	model to explain
1671000	1673000	the prediction model.
1673000	1675000	OK.
1675000	1677000	Then we need
1677000	1679000	to know
1679000	1681000	whether the explainer
1681000	1683000	is correct or not.
1683000	1685000	We don't need to
1685000	1687000	trust the explainer.
1687000	1689000	We can use
1689000	1691000	many different metrics
1691000	1693000	to determine
1693000	1695000	whether the explainer
1695000	1697000	is correct or not.
1697000	1699000	The most famous metric
1699000	1701000	is the sharp E-value.
1701000	1703000	If you have
1703000	1705000	a background in economics
1705000	1707000	you need to calculate
1707000	1709000	the contribution of each feature
1709000	1711000	to the prediction model.
1711000	1713000	We use
1713000	1715000	the sharp E-value
1715000	1717000	in Game Theory
1717000	1719000	to create
1719000	1721000	the correct answer
1721000	1723000	and do the evaluation.
1723000	1725000	OK.
1725000	1727000	The explainer has many
1727000	1729000	methods, such as SHARP
1729000	1731000	and LIME.
1731000	1733000	These methods
1733000	1735000	have one
1735000	1737000	big drawback.
1737000	1739000	If we want to do
1739000	1741000	the local explanation,
1741000	1743000	we need to provide
1743000	1745000	an explanation for each picture.
1745000	1747000	The most common
1747000	1749000	method
1749000	1751000	is to
1751000	1753000	provide
1753000	1755000	an explanation
1755000	1757000	for each picture.
1757000	1759000	We need to have
1759000	1761000	100,000 pictures
1761000	1763000	to provide
1763000	1765000	an explanation for each picture.
1765000	1767000	We need to provide
1767000	1769000	an explanation for each picture.
1769000	1771000	This process
1771000	1773000	is very slow.
1773000	1775000	These methods
1775000	1777000	have some drawbacks.
1777000	1779000	Of course,
1779000	1781000	some people say
1781000	1783000	we have a good
1783000	1785000	SHARP
1785000	1787000	and Game Theory
1787000	1789000	I will explain
1789000	1791000	why we can't
1791000	1793000	use SHARP
1793000	1795000	to provide an explanation.
1795000	1797000	OK.
1801000	1803000	What is SHARP E-Value?
1803000	1805000	SHARP E-Value
1805000	1807000	is a tool
1807000	1809000	to provide
1809000	1811000	an explanation
1811000	1813000	for model prediction.
1813000	1815000	To create SHARP E-Value,
1815000	1817000	we need to know
1817000	1819000	the important score
1819000	1821000	of each feature.
1821000	1823000	We need to know
1823000	1825000	the important score
1825000	1827000	of each feature.
1827000	1829000	What do we need to do
1829000	1831000	to know the important score
1831000	1833000	of each feature?
1833000	1835000	It's simple.
1835000	1837000	Let's say
1837000	1839000	we have 4 features.
1839000	1841000	5. 1, 2, 3, 4, 5.
1841000	1843000	We need to know
1843000	1845000	the important score
1845000	1847000	of each feature.
1847000	1849000	We need to know
1849000	1851000	the important score
1851000	1853000	of each feature.
1853000	1855000	For example,
1855000	1857000	we need to know
1857000	1859000	the important score
1859000	1861000	of Job and
1861000	1863000	Marital Status.
1863000	1865000	We need to know
1865000	1867000	the important score
1867000	1869000	of each feature.
1869000	1871000	We need to use
1871000	1873000	the only combination
1873000	1875000	of all features.
1875000	1877000	We need to use
1877000	1879000	the only combination
1879000	1881000	of all features.
1881000	1883000	For example,
1883000	1885000	we need to use
1885000	1887000	the only combination
1887000	1889000	of Job and Marital Status.
1889000	1891000	We need to know
1891000	1893000	the important score
1893000	1895000	of each feature
1895000	1897000	in the Sharp Eval.
1897000	1899000	What will happen?
1899000	1901000	If there are only
1901000	1903000	5 features,
1903000	1905000	all combinations
1905000	1907000	will be 2 to the power of 5.
1907000	1909000	So, it's 2 to the power of 5.
1909000	1911000	If I have
1911000	1913000	2,000 features
1913000	1915000	in the recommendation system
1915000	1917000	in a big company,
1917000	1919000	it will be 2 to the power of 2,000.
1919000	1921000	A user needs
1921000	1923000	2 to the power of 2,000.
1923000	1925000	I have 1 billion users
1925000	1927000	today.
1927000	1929000	The calculation process
1929000	1931000	is endless.
1931000	1933000	So,
1933000	1935000	why can't we use the Sharp Eval?
1935000	1937000	It's an NP-hard problem.
1937000	1939000	Its complexity
1939000	1941000	is too high.
1941000	1943000	We can't use
1943000	1945000	a low-complexity method
1945000	1947000	to verify
1947000	1949000	our calculations.
1949000	1951000	So,
1951000	1953000	we use the word
1953000	1955000	Sharp Eval
1955000	1957000	to explain
1957000	1959000	the important score.
1959000	1961000	It's possible, but not realistic.
1961000	1963000	Why?
1963000	1965000	It takes too much time
1965000	1967000	and complexity
1967000	1969000	to calculate.
1969000	1971000	So,
1971000	1973000	we can't use it.
1973000	1975000	Oh, I forgot to mention
1975000	1977000	the following.
1977000	1979000	For example,
1979000	1981000	I only care about Job and Marito.
1981000	1983000	If I remove the other three,
1983000	1985000	what will happen
1985000	1987000	to my model prediction?
1987000	1989000	What will happen
1989000	1991000	to my model prediction?
1991000	1993000	If I remove
1993000	1995000	Balance, Job,
1995000	1997000	and Marito,
1997000	1999000	and only consider Age and Education
1999000	2001000	in my prediction model,
2001000	2003000	the prediction result
2003000	2005000	can't be returned,
2005000	2007000	but it can be returned.
2007000	2009000	What does it mean?
2009000	2011000	These three features are important.
2011000	2013000	So,
2013000	2015000	we can use
2015000	2017000	this iterative process
2017000	2019000	to find
2019000	2021000	the important score
2021000	2023000	for each feature.
2023000	2025000	This is Sharp Eval.
2025000	2027000	If you know economics,
2027000	2029000	you will feel
2029000	2031000	this is quite
2031000	2033000	intimate.
2033000	2035000	OK.
2035000	2037000	If we can't use
2037000	2039000	Sharp Eval,
2039000	2041000	what can we do?
2041000	2043000	This is my research.
2043000	2045000	I want to
2045000	2047000	speed up
2047000	2049000	the process of
2049000	2051000	explanation.
2051000	2053000	Because
2053000	2055000	when we do
2055000	2057000	local explanation,
2057000	2059000	each user must
2059000	2061000	provide a customized
2061000	2063000	explanation result.
2063000	2065000	As I mentioned,
2065000	2067000	each customized explanation
2067000	2069000	is a model
2069000	2071000	to explain the prediction model.
2071000	2073000	This process
2073000	2075000	takes a lot of time.
2075000	2077000	For example,
2077000	2079000	if you give a recommended result
2079000	2081000	and ask the user to wait
2081000	2083000	10 seconds,
2083000	2085000	the user can't
2085000	2087000	accept it.
2087000	2089000	If you ask the user to wait
2089000	2091000	10 seconds,
2091000	2093000	the user will click
2093000	2095000	the upper left corner.
2095000	2097000	This is the work
2097000	2099000	I did last year.
2099000	2101000	This work
2101000	2103000	aims to
2103000	2105000	speed up
2105000	2107000	the operation of
2107000	2109000	Sharp Eval
2109000	2111000	and provide the
2111000	2113000	user with an
2113000	2115000	explanation result.
2115000	2117000	As I mentioned,
2117000	2119000	I want to
2119000	2121000	go through all the combinations
2121000	2123000	of the feature set.
2123000	2125000	For example,
2125000	2127000	each feature set
2127000	2129000	is important to the model,
2129000	2131000	isn't it?
2131000	2133000	Some features
2133000	2135000	are not important
2135000	2137000	to the model.
2137000	2139000	Can we get this information
2139000	2141000	after training the model?
2141000	2143000	Yes, we can.
2143000	2145000	We can take out
2145000	2147000	the weight of the model
2147000	2149000	and see that
2149000	2151000	feature 1 and feature 2
2151000	2153000	are not important.
2153000	2155000	Feature 1 and feature 3
2155000	2157000	are not important.
2157000	2159000	Feature 1 and feature 5
2159000	2161000	are not important at all.
2161000	2163000	We can take out
2163000	2165000	feature 1 and feature 5
2165000	2167000	from the combination
2167000	2169000	of Sharp Eval.
2169000	2171000	For example,
2171000	2173000	we want to have
2173000	2175000	2 squares,
2175000	2177000	so we need 32 squares.
2177000	2179000	We can take out
2179000	2181000	the number of times
2181000	2183000	that we need to calculate.
2183000	2185000	It may be 2, 2, 4, 6, 8,
2185000	2187000	4, 5, 4, 2... 14 times.
2187000	2189000	It's actually half the process.
2189000	2191000	This process can
2191000	2193000	speed up the calculation
2193000	2195000	of Sharp Eval
2195000	2197000	and turn this into
2197000	2199000	a user's acceptable
2199000	2201000	waiting time.
2201000	2203000	Finally, provide the user
2203000	2205000	with a faster
2205000	2207000	explanation result.
2207000	2209000	This mathematician
2209000	2211000	is actually expressing that
2211000	2213000	in Sharp Eval,
2213000	2215000	we need to go through
2215000	2217000	all the features.
2217000	2219000	We turn it into
2219000	2221000	the importance of
2221000	2223000	the interaction between
2223000	2225000	feature and feature
2225000	2227000	through the weight of the model.
2227000	2229000	We take out the unimportant
2229000	2231000	and calculate the approximate
2231000	2233000	Sharp Eval.
2233000	2235000	We take this Sharp Eval
2235000	2237000	and provide the important score
2237000	2239000	of the explanation to the user.
2239000	2241000	This is the spirit
2241000	2243000	of this work.
2243000	2245000	As you can see,
2245000	2247000	this work
2247000	2249000	has an application scenario.
2249000	2251000	Today, we trained
2251000	2253000	a prediction model
2253000	2255000	to predict
2255000	2257000	whether the income
2257000	2259000	is high or low.
2259000	2261000	We can use our model
2261000	2263000	to explain the trained
2263000	2265000	prediction model
2265000	2267000	and then provide
2267000	2269000	the reason.
2269000	2271000	For example,
2271000	2273000	if you are a part-time worker,
2273000	2275000	you will be judged
2275000	2277000	as a high-paying worker.
2277000	2279000	This model
2279000	2281000	can be used
2281000	2283000	in a faster way
2283000	2285000	to provide
2285000	2287000	an explanation
2287000	2289000	to the user.
2289000	2291000	You can see
2291000	2293000	the blue bar on the left
2293000	2295000	is the
2295000	2297000	ground truth
2297000	2299000	Sharp Eval.
2299000	2301000	We use the method of
2301000	2303000	economics and theory
2303000	2305000	to calculate the
2305000	2307000	Sharp Eval value
2307000	2309000	for this feature.
2309000	2311000	As you can see,
2311000	2313000	even if we take out
2313000	2315000	the unimportant feature
2315000	2317000	and interaction,
2317000	2319000	the Sharp Eval
2319000	2321000	score
2321000	2323000	is very close to
2323000	2325000	the Sharp Eval.
2325000	2327000	We can't say it's exactly the same
2327000	2329000	because we took out
2329000	2331000	some combinations.
2331000	2333000	It's faster,
2333000	2335000	but we don't lose
2335000	2337000	the Sharp Eval accuracy.
2337000	2339000	When we provide
2339000	2341000	the importance of each feature
2341000	2343000	to the user,
2343000	2345000	we can say that
2345000	2347000	for our model,
2347000	2349000	whether it's high-paying
2349000	2351000	or low-paying,
2351000	2353000	we can provide
2353000	2355000	the score
2355000	2357000	or the important
2357000	2359000	ranking result
2359000	2361000	to the user
2361000	2363000	as a reference
2363000	2365000	for whether
2365000	2367000	they want to believe
2367000	2369000	the prediction model.
2369000	2371000	The next step
2371000	2373000	is to
2373000	2375000	speed up
2375000	2377000	the calculation
2377000	2379000	of the Sharp Eval.
2381000	2383000	Instead of
2383000	2385000	speeding up the process,
2385000	2387000	we can train
2387000	2389000	a model to learn
2389000	2391000	the distribution
2391000	2393000	of the Sharp Eval.
2393000	2395000	This can
2395000	2397000	provide
2397000	2399000	a faster
2399000	2401000	prediction result.
2401000	2403000	Why?
2403000	2405000	For a traditional
2405000	2407000	DNN model,
2407000	2409000	each user
2409000	2411000	needs a
2411000	2413000	customized result,
2413000	2415000	so we need
2415000	2417000	an explanation model.
2417000	2419000	To predict 100,000 users,
2419000	2421000	we need 100,000 models.
2421000	2423000	Here, we only need
2423000	2425000	one model
2425000	2427000	to predict all users'
2427000	2429000	explanation results.
2429000	2431000	This process
2431000	2433000	is what we call
2433000	2435000	an explanation process.
2435000	2437000	Why?
2437000	2439000	For a deep learning
2439000	2441000	network,
2441000	2443000	we can send 100,000 users
2443000	2445000	as data
2445000	2447000	to the network
2447000	2449000	and the network
2449000	2451000	will produce 100,000 different results.
2451000	2453000	We don't
2453000	2455000	need
2455000	2457000	an independent
2457000	2459000	explainer
2459000	2461000	to
2461000	2463000	respond to all users.
2463000	2465000	This training
2465000	2467000	process is another
2467000	2469000	part of my work.
2469000	2471000	It is about
2471000	2473000	whether we can
2473000	2475000	learn from
2475000	2477000	positive and negative
2477000	2479000	samples.
2479000	2481000	For example,
2481000	2483000	if we
2483000	2485000	cover up
2485000	2487000	an important feature
2487000	2489000	such as
2489000	2491000	a dog's eye,
2491000	2493000	the model will
2493000	2495000	still make the same
2495000	2497000	prediction
2497000	2499000	when it
2499000	2501000	detects
2501000	2503000	a dog.
2503000	2505000	This can
2505000	2507000	help us
2507000	2509000	explain.
2509000	2511000	For example,
2511000	2513000	we want a model
2513000	2515000	to detect a dog.
2515000	2517000	We can give
2517000	2519000	an explanation
2519000	2521000	that the dog
2521000	2523000	cares about
2523000	2525000	these features,
2525000	2527000	so the model will
2527000	2529000	make the same prediction.
2529000	2531000	We can also give
2531000	2533000	a negative example.
2533000	2535000	For example,
2535000	2537000	the model will
2537000	2539000	know that
2539000	2541000	the dog's
2541000	2543000	negative sample
2543000	2545000	is a dog.
2545000	2547000	When the model
2547000	2549000	detects positive
2549000	2551000	and negative samples,
2551000	2553000	we can learn from
2553000	2555000	the model.
2555000	2557000	We want the model
2557000	2559000	to get closer to
2559000	2561000	the positive sample
2561000	2563000	and further away from
2563000	2565000	the negative sample.
2565000	2567000	This process
2567000	2569000	is called
2569000	2571000	contrastive learning.
2571000	2573000	We want to know
2573000	2575000	what is right
2575000	2577000	and what is wrong.
2577000	2579000	We want to
2579000	2581000	have a better
2581000	2583000	distance between
2583000	2585000	right and wrong.
2585000	2587000	Why is there
2587000	2589000	a fine-tuning process?
2589000	2591000	It is because
2591000	2593000	the shoppy value
2593000	2595000	has a strong
2595000	2597000	support of
2597000	2599000	economic theory,
2599000	2601000	so
2601000	2603000	if we can
2603000	2605000	create
2605000	2607000	important scores
2607000	2609000	that have
2609000	2611000	shoppy value,
2611000	2613000	we can
2613000	2615000	provide
2615000	2617000	at least
2617000	2619000	to
2619000	2621000	machine learning engineers
2621000	2623000	or higher-ups
2623000	2625000	to evaluate
2625000	2627000	whether the
2627000	2629000	explanation is
2629000	2631000	believable.
2631000	2633000	First, we can make
2633000	2635000	people happier.
2635000	2637000	Second, we
2637000	2639000	want to keep
2639000	2641000	shoppy value
2641000	2643000	away from
2643000	2645000	fine-tuning.
2645000	2647000	We don't
2647000	2649000	use any
2649000	2651000	shoppy value
2651000	2653000	as a label.
2653000	2655000	We can use
2655000	2657000	shoppy value
2657000	2659000	as a label
2659000	2661000	to keep
2661000	2663000	shoppy value
2663000	2665000	away from
2665000	2667000	explanation.
2667000	2669000	We can use
2669000	2671000	the result
2671000	2673000	to make people
2673000	2675000	believe that
2675000	2677000	we are not
2677000	2679000	creating
2679000	2681000	important scores
2681000	2683000	randomly,
2683000	2685000	so
2685000	2687000	we can
2687000	2689000	make people
2689000	2691000	happy.
2691000	2693000	Third, we
2693000	2695000	want to
2695000	2697000	keep shoppy value
2697000	2699000	away from
2699000	2701000	fine-tuning.
2701000	2703000	We don't
2703000	2705000	use any
2705000	2707000	shoppy value
2707000	2709000	as a label.
2709000	2711000	We can
2711000	2713000	keep
2713000	2715000	shoppy value
2715000	2717000	away from
2717000	2719000	fine-tuning.
2719000	2721000	We can
2721000	2723000	keep
2723000	2725000	shoppy value
2725000	2727000	away from
2727000	2729000	fine-tuning.
2729000	2731000	We can
2731000	2733000	keep
2733000	2735000	shoppy value
2735000	2737000	away from
2737000	2739000	fine-tuning.
2739000	2741000	We can
2741000	2743000	keep
2743000	2745000	shoppy value
2745000	2747000	away from
2747000	2749000	fine-tuning.
2749000	2751000	We can
2751000	2753000	keep
2753000	2755000	shoppy value
2755000	2757000	away from
2757000	2759000	fine-tuning.
2759000	2761000	We can
2761000	2763000	keep
2763000	2765000	shoppy value
2765000	2767000	away from
2767000	2769000	fine-tuning.
2769000	2771000	We can
2771000	2773000	keep
2773000	2775000	shoppy value
2775000	2777000	away from
2777000	2779000	fine-tuning.
2779000	2781000	We can
2781000	2783000	keep
2783000	2785000	shoppy value
2785000	2787000	away from
2787000	2789000	fine-tuning.
2789000	2791000	We can
2791000	2793000	keep
2793000	2795000	shoppy value
2795000	2797000	away from
2797000	2799000	fine-tuning.
2799000	2801000	We can
2801000	2803000	keep
2803000	2805000	shoppy value
2805000	2807000	away from
2807000	2809000	fine-tuning.
2809000	2811000	We can
2811000	2813000	keep
2813000	2815000	shoppy value
2815000	2817000	away from
2817000	2819000	fine-tuning.
2819000	2821000	We can
2821000	2823000	keep
2823000	2825000	shoppy value
2825000	2827000	away from
2827000	2829000	fine-tuning.
2829000	2831000	We can
2831000	2833000	keep
2833000	2835000	shoppy value
2835000	2837000	away from
2837000	2839000	fine-tuning.
2839000	2841000	We can
2841000	2843000	keep
2843000	2845000	shoppy value
2845000	2847000	away from
2847000	2849000	fine-tuning.
2849000	2851000	We can
2851000	2853000	keep
2853000	2855000	shoppy value
2855000	2857000	away from
2857000	2859000	fine-tuning.
2859000	2861000	We can
2861000	2863000	keep
2863000	2865000	shoppy value
2865000	2867000	away from
2867000	2869000	fine-tuning.
2869000	2871000	We can
2871000	2873000	keep
2873000	2875000	shoppy value
2875000	2877000	away from
2877000	2879000	fine-tuning.
2879000	2881000	We can
2881000	2883000	keep
2883000	2885000	shoppy value
2885000	2887000	away from
2887000	2889000	fine-tuning.
2889000	2891000	We can
2891000	2893000	keep
2893000	2895000	shoppy value
2895000	2897000	away from
2897000	2899000	fine-tuning.
2899000	2901000	We can
2901000	2903000	keep
2903000	2905000	shoppy value
2905000	2907000	away from
2907000	2909000	fine-tuning.
2909000	2911000	We can
2911000	2913000	keep
2913000	2915000	shoppy value
2915000	2917000	away from
2917000	2919000	fine-tuning.
2919000	2921000	We can
2921000	2923000	keep
2923000	2925000	shoppy value
2925000	2927000	away from
2927000	2929000	fine-tuning.
2929000	2931000	We can
2931000	2933000	keep
2933000	2935000	shoppy value
2935000	2937000	away from
2937000	2939000	fine-tuning.
2939000	2941000	We can
2941000	2943000	keep
2943000	2945000	shoppy value
2945000	2947000	away from
2947000	2949000	fine-tuning.
2949000	2951000	We can
2951000	2953000	keep
2953000	2955000	shoppy value
2955000	2957000	away from
2957000	2959000	fine-tuning.
2959000	2961000	We can
2961000	2963000	keep
2963000	2965000	shoppy value
2965000	2967000	away from
2967000	2969000	fine-tuning.
2969000	2971000	We can
2971000	2973000	keep
2973000	2975000	shoppy value
2975000	2977000	away from
2977000	2979000	fine-tuning.
2979000	2981000	We can
2981000	2983000	keep
2983000	2985000	shoppy value
2985000	2987000	away from
2987000	2989000	fine-tuning.
2989000	2991000	We can
2991000	2993000	keep
2993000	2995000	shoppy value
2995000	2997000	away from
2997000	2999000	fine-tuning.
2999000	3001000	We can
3001000	3003000	keep
3003000	3005000	shoppy value
3005000	3007000	away from
3007000	3009000	fine-tuning.
3009000	3011000	We can
3011000	3013000	keep
3013000	3015000	shoppy value
3015000	3017000	away from
3017000	3019000	fine-tuning.
3019000	3021000	We can
3021000	3023000	keep
3023000	3025000	shoppy value
3025000	3027000	away from
3027000	3029000	fine-tuning.
3029000	3031000	We can
3031000	3033000	keep
3033000	3035000	shoppy value
3035000	3037000	away from
3037000	3039000	fine-tuning.
3039000	3041000	We can
3041000	3043000	keep
3043000	3045000	shoppy value
3045000	3047000	away from
3047000	3049000	fine-tuning.
3049000	3051000	We can
3051000	3053000	keep
3053000	3055000	shoppy value
3055000	3057000	away from
3057000	3059000	fine-tuning.
3059000	3061000	We can
3061000	3063000	keep
3063000	3065000	shoppy value
3065000	3067000	away from
3067000	3069000	fine-tuning.
3069000	3071000	We can
3071000	3073000	keep
3073000	3075000	shoppy value
3075000	3077000	away from
3077000	3079000	fine-tuning.
3079000	3081000	We can
3081000	3083000	keep
3083000	3085000	shoppy value
3085000	3087000	away from
3087000	3089000	fine-tuning.
3089000	3091000	We can
3091000	3093000	keep
3093000	3095000	shoppy value
3095000	3097000	away from
3097000	3099000	fine-tuning.
3099000	3101000	We can
3101000	3103000	keep
3103000	3105000	shoppy value
3105000	3107000	away from
3107000	3109000	fine-tuning.
3109000	3111000	We can
3111000	3113000	keep
3113000	3115000	shoppy value
3115000	3117000	away from
3117000	3119000	fine-tuning.
3119000	3121000	We can
3121000	3123000	keep
3123000	3125000	shoppy value
3125000	3127000	away from
3127000	3129000	fine-tuning.
3129000	3131000	We can
3131000	3133000	keep
3133000	3135000	shoppy value
3135000	3137000	away from
3137000	3139000	fine-tuning.
3139000	3141000	We can
3141000	3143000	keep
3143000	3145000	shoppy value
3145000	3147000	away from
3147000	3149000	fine-tuning.
3149000	3151000	We can
3151000	3153000	keep
3153000	3155000	shoppy value
3155000	3157000	away from
3157000	3159000	fine-tuning.
3159000	3161000	We can
3161000	3163000	keep
3163000	3165000	shoppy value
3165000	3167000	away from
3167000	3169000	fine-tuning.
3169000	3171000	We can
3171000	3173000	keep
3173000	3175000	shoppy value
3175000	3177000	away from
3177000	3179000	fine-tuning.
3179000	3181000	We can
3181000	3183000	keep
3183000	3185000	shoppy value
3185000	3187000	away from
3187000	3189000	fine-tuning.
3189000	3191000	We can
3191000	3193000	keep
3193000	3195000	shoppy value
3195000	3197000	away from
3197000	3199000	fine-tuning.
3199000	3201000	We can
3201000	3203000	keep
3203000	3205000	shoppy value
3205000	3207000	away from
3207000	3209000	fine-tuning.
3209000	3211000	We can
3211000	3213000	keep
3213000	3215000	shoppy value
3215000	3217000	away from
3217000	3219000	fine-tuning.
3219000	3221000	We can
3221000	3223000	keep
3223000	3225000	shoppy value
3225000	3227000	away from
3227000	3229000	fine-tuning.
3229000	3231000	We can
3231000	3233000	keep
3233000	3235000	shoppy value
3235000	3237000	away from
3237000	3239000	fine-tuning.
3239000	3241000	We can
3241000	3243000	keep
3243000	3245000	shoppy value
3245000	3247000	away from
3247000	3249000	fine-tuning.
3249000	3251000	We can
3251000	3253000	keep
3253000	3255000	shoppy value
3255000	3257000	away from
3257000	3259000	fine-tuning.
3259000	3261000	We can
3261000	3263000	keep
3263000	3265000	shoppy value
3265000	3267000	away from
3267000	3269000	fine-tuning.
3269000	3271000	We can
3271000	3273000	keep
3273000	3275000	shoppy value
3275000	3277000	away from
3277000	3279000	fine-tuning.
3279000	3281000	We can
3281000	3283000	keep
3283000	3285000	shoppy value
3285000	3287000	away from
3287000	3289000	fine-tuning.
3289000	3291000	We can
3291000	3293000	keep
3293000	3295000	shoppy value
3295000	3297000	away from
3297000	3299000	fine-tuning.
3299000	3301000	We can
3301000	3303000	keep
3303000	3305000	shoppy value
3305000	3307000	away from
3307000	3309000	fine-tuning.
3309000	3311000	We can
3311000	3313000	keep
3313000	3315000	shoppy value
3315000	3317000	away from
3317000	3319000	fine-tuning.
3319000	3321000	We can
3321000	3323000	keep
3323000	3325000	shoppy value
3325000	3327000	away from
3327000	3329000	fine-tuning.
3329000	3331000	We can
3331000	3333000	keep
3333000	3335000	shoppy value
3335000	3337000	away from
3337000	3339000	fine-tuning.
3339000	3341000	We can
3341000	3343000	keep
3343000	3345000	shoppy value
3345000	3347000	away from
3347000	3349000	fine-tuning.
3349000	3351000	We can
3351000	3353000	keep
3353000	3355000	shoppy value
3355000	3357000	away from
3357000	3359000	fine-tuning.
3359000	3361000	We can
3361000	3363000	keep
3363000	3365000	shoppy value
3365000	3367000	away from
3367000	3369000	fine-tuning.
3369000	3371000	We can
3371000	3373000	keep
3373000	3375000	shoppy value
3375000	3377000	away from
3377000	3379000	fine-tuning.
3379000	3381000	We can
3381000	3383000	keep
3383000	3385000	shoppy value
3385000	3387000	away from
3387000	3389000	fine-tuning.
3389000	3391000	We can
3391000	3393000	keep
3393000	3395000	shoppy value
3395000	3397000	away from
3397000	3399000	fine-tuning.
3399000	3401000	We can
3401000	3403000	keep
3403000	3405000	shoppy value
3405000	3407000	away from
3407000	3409000	fine-tuning.
3409000	3411000	We can
3411000	3413000	keep
3413000	3415000	shoppy value
3415000	3417000	away from
3417000	3419000	fine-tuning.
3419000	3421000	We can
3421000	3423000	keep
3423000	3425000	shoppy value
3425000	3427000	away from
3427000	3429000	fine-tuning.
3429000	3431000	We can
3431000	3433000	keep
3433000	3435000	shoppy value
3435000	3437000	away from
3437000	3439000	fine-tuning.
3439000	3441000	We can
3441000	3443000	keep
3443000	3445000	shoppy value
3445000	3447000	away from
3447000	3449000	fine-tuning.
3449000	3451000	We can
3451000	3453000	keep
3453000	3455000	shoppy value
3455000	3457000	away from
3457000	3459000	fine-tuning.
3459000	3461000	We can
3461000	3463000	keep
3463000	3465000	shoppy value
3465000	3467000	away from
3467000	3469000	fine-tuning.
3469000	3471000	We can
3471000	3473000	keep
3473000	3475000	shoppy value
3475000	3477000	away from
3477000	3479000	fine-tuning.
3479000	3481000	We can
3481000	3483000	keep
3483000	3485000	shoppy value
3485000	3487000	away from
3487000	3489000	fine-tuning.
3489000	3491000	We can
3491000	3493000	keep
3493000	3495000	shoppy value
3495000	3497000	away from
3497000	3499000	fine-tuning.
3499000	3501000	We can
3501000	3503000	keep
3503000	3505000	shoppy value
3505000	3507000	away from
3507000	3509000	fine-tuning.
3509000	3511000	We can
3511000	3513000	keep
3513000	3515000	shoppy value
3515000	3517000	away from
3517000	3519000	fine-tuning.
3519000	3521000	We can
3521000	3523000	keep
3523000	3525000	shoppy value
3525000	3527000	away from
3527000	3529000	fine-tuning.
3529000	3531000	We can
3531000	3533000	keep
3533000	3535000	shoppy value
3535000	3537000	away from
3537000	3539000	fine-tuning.
3539000	3541000	We can
3541000	3543000	keep
3543000	3545000	shoppy value
3545000	3547000	away from
3547000	3549000	fine-tuning.
3549000	3551000	We can
3551000	3553000	keep
3553000	3555000	shoppy value
3555000	3557000	away from
3557000	3559000	fine-tuning.
3559000	3561000	We can
3561000	3563000	keep
3563000	3565000	shoppy value
3565000	3567000	away from
3567000	3569000	fine-tuning.
3569000	3571000	We can
3571000	3573000	keep
3573000	3575000	shoppy value
3575000	3577000	away from
3577000	3579000	fine-tuning.
3579000	3581000	We can
3581000	3583000	keep
3583000	3585000	shoppy value
3585000	3587000	away from
3587000	3589000	fine-tuning.
3589000	3591000	We can
3591000	3593000	keep
3593000	3595000	shoppy value
3595000	3597000	away from
3597000	3599000	fine-tuning.
3599000	3601000	We can
3601000	3603000	keep
3603000	3605000	shoppy value
3605000	3607000	away from
3607000	3609000	fine-tuning.
3609000	3611000	We can
3611000	3613000	keep
3613000	3615000	shoppy value
3615000	3617000	away from
3617000	3619000	fine-tuning.
3619000	3621000	We can
3621000	3623000	keep
3623000	3625000	shoppy value
3625000	3627000	away from
3627000	3629000	fine-tuning.
3629000	3631000	We can
3631000	3633000	keep
3633000	3635000	shoppy value
3635000	3637000	away from
3637000	3639000	fine-tuning.
3639000	3641000	We can
3641000	3643000	keep
3643000	3645000	shoppy value
3645000	3647000	away from
3647000	3649000	fine-tuning.
3649000	3651000	We can
3651000	3653000	keep
3653000	3655000	shoppy value
3655000	3657000	away from
3657000	3659000	fine-tuning.
3659000	3661000	We can
3661000	3663000	keep
3663000	3665000	shoppy value
3665000	3667000	away from
3667000	3669000	fine-tuning.
3669000	3671000	We can
3671000	3673000	keep
3673000	3675000	shoppy value
3675000	3677000	away from
3677000	3679000	fine-tuning.
3679000	3681000	We can
3681000	3683000	keep
3683000	3685000	shoppy value
3685000	3687000	away from
3687000	3689000	fine-tuning.
3689000	3691000	We can
3691000	3693000	keep
3693000	3695000	shoppy value
3695000	3697000	away from
3697000	3699000	fine-tuning.
3699000	3701000	We can
3701000	3703000	keep
3703000	3705000	shoppy value
3705000	3707000	away from
3707000	3709000	fine-tuning.
3709000	3711000	We can
3711000	3713000	keep
3713000	3715000	shoppy value
3715000	3717000	away from
3717000	3719000	fine-tuning.
3719000	3721000	We can
3721000	3723000	keep
3723000	3725000	shoppy value
3725000	3727000	away from
3727000	3729000	fine-tuning.
3729000	3731000	We can
3731000	3733000	keep
3733000	3735000	shoppy value
3735000	3737000	away from
3737000	3739000	fine-tuning.
3739000	3741000	We can
3741000	3743000	keep
3743000	3745000	shoppy value
3745000	3747000	away from
3747000	3749000	fine-tuning.
3749000	3751000	We can
3751000	3753000	keep
3753000	3755000	shoppy value
3755000	3757000	away from
3757000	3759000	fine-tuning.
3759000	3761000	We can
3761000	3763000	keep
3763000	3765000	shoppy value
3765000	3767000	away from
3767000	3769000	fine-tuning.
3769000	3771000	We can
3771000	3773000	keep
3773000	3775000	shoppy value
3775000	3777000	away from
3777000	3779000	fine-tuning.
3779000	3781000	We can
3781000	3783000	keep
3783000	3785000	shoppy value
3785000	3787000	away from
3787000	3789000	fine-tuning.
3789000	3791000	We can
3791000	3793000	keep
3793000	3795000	shoppy value
3795000	3797000	away from
3797000	3799000	fine-tuning.
3799000	3801000	We can
3801000	3803000	keep
3803000	3805000	shoppy value
3805000	3807000	away from
3807000	3809000	fine-tuning.
3809000	3811000	We can
3811000	3813000	keep
3813000	3815000	shoppy value
3815000	3817000	away from
3817000	3819000	fine-tuning.
3819000	3821000	We can
3821000	3823000	keep
3823000	3825000	shoppy value
3825000	3827000	away from
3827000	3829000	fine-tuning.
3829000	3831000	We can
3831000	3833000	keep
3833000	3835000	shoppy value
3835000	3837000	away from
3837000	3839000	fine-tuning.
3839000	3841000	We can
3841000	3843000	keep
3843000	3845000	shoppy value
3845000	3847000	away from
3847000	3849000	fine-tuning.
3849000	3851000	We can
3851000	3853000	keep
3853000	3855000	shoppy value
3855000	3857000	away from
3857000	3859000	fine-tuning.
3859000	3861000	We can
3861000	3863000	keep
3863000	3865000	shoppy value
3865000	3867000	away from
3867000	3869000	fine-tuning.
3869000	3871000	We can
3871000	3873000	keep
3873000	3875000	shoppy value
3875000	3877000	away from
3877000	3879000	fine-tuning.
3879000	3881000	We can
3881000	3883000	keep
3883000	3885000	shoppy value
3885000	3887000	away from
3887000	3889000	fine-tuning.
3889000	3891000	We can
3891000	3893000	keep
3893000	3895000	shoppy value
3895000	3897000	away from
3897000	3899000	fine-tuning.
3899000	3901000	We can
3901000	3903000	keep
3903000	3905000	shoppy value
3905000	3907000	away from
3907000	3909000	fine-tuning.
3909000	3911000	We can
3911000	3913000	keep
3913000	3915000	shoppy value
3915000	3917000	away from
3917000	3919000	fine-tuning.
3919000	3921000	We can
3921000	3923000	keep
3923000	3925000	shoppy value
3925000	3927000	away from
3927000	3929000	fine-tuning.
3929000	3931000	We can
3931000	3933000	keep
3933000	3935000	shoppy value
3935000	3937000	away from
3937000	3939000	fine-tuning.
3939000	3941000	We can
3941000	3943000	keep
3943000	3945000	shoppy value
3945000	3947000	away from
3947000	3949000	fine-tuning.
3949000	3951000	We can
3951000	3953000	keep
3953000	3955000	fine-tuning.
3955000	3957000	We can
3957000	3959000	keep
3959000	3961000	fine-tuning.
3961000	3963000	We can
3963000	3965000	keep
3965000	3967000	fine-tuning.
3967000	3969000	We can
3969000	3971000	keep
3971000	3973000	fine-tuning.
3973000	3975000	We can
3975000	3977000	keep
3977000	3979000	fine-tuning.
3979000	3981000	We can
3981000	3983000	keep
3983000	3985000	fine-tuning.
3985000	3987000	We can
3987000	3989000	keep
3989000	3991000	fine-tuning.
3993000	3995000	Finally,
3995000	3997000	I saw
3997000	3999000	a meeting
3999000	4001000	No.
4001000	4003000	Our study is
4003000	4005000	based on
4005000	4007000	this individual feature.
4009000	4011000	Thank you.
4011000	4013000	Thank you, Alan.
4023000	4025000	Someone at the group
4025000	4027000	asked about
4027000	4029000	mask size.
4029000	4031000	Mask size?
4031000	4033000	You mean
4033000	4035000	the size of the mask?
4035000	4037000	Yes.
4037000	4039000	For example,
4039000	4041000	a dog's eyes.
4041000	4043000	If
4043000	4045000	your mask
4045000	4047000	is
4047000	4049000	smaller than the eyes,
4049000	4051000	this mask is valid.
4051000	4053000	OK.
4053000	4055000	When we do this,
4055000	4057000	we can't
4057000	4059000	make it smaller than the nose.
4059000	4061000	For example,
4061000	4063000	if you ask
4063000	4065000	the size of the mask,
4065000	4067000	we don't
4067000	4069000	restrict
4069000	4071000	the mask size.
4071000	4073000	But I know
4073000	4075000	someone restricts
4075000	4077000	the mask size.
4077000	4079000	Why?
4079000	4081000	If the mask size is too large,
4081000	4083000	it will cause
4083000	4085000	a negative effect.
4085000	4087000	Or
4087000	4089000	it has different
4089000	4091000	purposes.
4091000	4093000	Or
4093000	4095000	it only selects
4095000	4097000	a few important features.
4097000	4099000	If the mask size is
4099000	4101000	too small,
4101000	4103000	it will cause
4103000	4105000	a negative effect.
4105000	4107000	But if the mask size
4107000	4109000	is the same as
4109000	4111000	the original image,
4111000	4113000	it will cause a negative effect.
4115000	4117000	OK.
4117000	4119000	Do you have any questions?
4125000	4127000	OK. Thank you.
4127000	4129000	I think it depends on the person.
4129000	4131000	For example,
4131000	4133000	if the mask size
4133000	4135000	is
4135000	4137000	the smallest
4137000	4139000	feature
4139000	4141000	of the eye,
4141000	4143000	it will exceed
4143000	4145000	a pixel.
4149000	4151000	I don't know.
4151000	4153000	I don't know how to answer this question.
4153000	4155000	Or
4155000	4157000	sometimes I feel
4157000	4159000	the size choice
4159000	4161000	is subjective.
4161000	4163000	This choice
4163000	4165000	is made by someone.
4165000	4167000	Someone
4167000	4169000	knows
4169000	4171000	the components
4171000	4173000	are large.
4173000	4175000	For example, the eyes are large.
4175000	4177000	For example, the mask
4177000	4179000	is not random for every pixel.
4179000	4181000	It is random for every 4x4 pixel.
4181000	4183000	Random for every 6x6 pixel.
4183000	4185000	Someone
4185000	4187000	does it this way.
4187000	4189000	But I think
4189000	4191000	we don't know
4191000	4193000	the pixel model cares about.
4193000	4195000	It may be the upper right and lower left
4195000	4197000	or the upper left and lower right.
4197000	4199000	Something like this.
4199000	4201000	If we give such a strong assumption,
4201000	4203000	I think model training
4203000	4205000	will be difficult.
4205000	4207000	This explanation model
4207000	4209000	will be difficult.
4209000	4211000	Of course, what you said
4211000	4213000	is exactly right.
4213000	4215000	It is really like this.
4215000	4217000	In fact, someone criticized
4217000	4219000	this kind of explanation.
4219000	4221000	It is about
4221000	4223000	covering
4223000	4225000	the eyes.
4225000	4227000	In fact, it is possible that
4227000	4229000	this kind of mask was not done well
4229000	4231000	at the beginning,
4231000	4233000	so the final result
4233000	4235000	is not so comprehensive.
4235000	4237000	Yes.
4237000	4239000	This is possible.
4239000	4241000	I think this question is super good.
4247000	4249000	OK.
4249000	4251000	It feels like
4251000	4253000	there are no new questions
4253000	4255000	in the chat room.
4255000	4257000	Then
4257000	4259000	let's thank
4259000	4261000	YuNan again
4261000	4263000	for his speech today.
4263000	4265000	Thank you, everyone.
4265000	4267000	How do I stop?
4269000	4271000	Let me clap my hands.
4271000	4273000	Thank you.
4273000	4275000	Thank you for your support.
4275000	4277000	How many people are there today?
4277000	4279000	I can't count.
4279000	4281000	Today, I see
4281000	4283000	the highest number of people
4283000	4285000	is 32.
4285000	4287000	That's a lot.
4287000	4289000	I found a lot of people.
4289000	4291000	Is it related to the field?
4291000	4293000	But no one helped me
4293000	4295000	to post on Facebook.
4295000	4297000	I'm kidding.
4297000	4299000	For example,
4299000	4301000	I have given a speech before.
4301000	4303000	I am a physics major.
4303000	4305000	There are five people in total.
4305000	4307000	Maybe
4307000	4309000	everyone is curious about this.
4309000	4311000	I don't know how to start.
4311000	4313000	I don't know either.
4313000	4315000	But I'm glad to have this opportunity
4315000	4317000	to share my knowledge.
4317000	4319000	Otherwise, I have to contribute
4319000	4321000	to the social resources.
4321000	4323000	Social resources.
4323000	4325000	Are you Gao Hong-An?
4327000	4329000	No, I'm not.
4329000	4331000	I'm not from Taiwan.
4331000	4333000	It's okay.
4333000	4335000	Okay.
4335000	4337000	Thank you.
4337000	4339000	I have a question.
4339000	4341000	I suddenly want to ask you a question.
4341000	4343000	For example,
4343000	4345000	in your first video,
4345000	4347000	This one?
4347000	4349000	Yes, that one.
4349000	4351000	The one about the matrix.
4351000	4353000	This one?
4353000	4355000	Yes, this one.
4355000	4357000	Do you automatically ignore
4357000	4359000	all the x1, x1, x2, x2
4359000	4361000	in the matrix?
4361000	4363000	We don't discuss
4363000	4365000	our relationship with ourselves.
4365000	4367000	We don't discuss our relationship with ourselves.
4367000	4369000	We don't discuss our relationship with ourselves.
4369000	4371000	There is no meaning
4371000	4373000	in the definition of this question.
4373000	4375000	Yes, because ShopEvalue
4375000	4377000	is still ourselves.
4377000	4379000	Yes.
4379000	4381000	Our group is still ourselves.
4381000	4383000	So we don't discuss it.
4383000	4385000	Of course, our relationship with ourselves
4385000	4387000	is highly related.
4387000	4389000	Yes.
4389000	4391000	Actually, I don't know
4391000	4393000	how to share it.
4395000	4397000	Okay.
4397000	4399000	Let me think about it.
4399000	4401000	I might stop recording now.
4401000	4403000	Okay.
