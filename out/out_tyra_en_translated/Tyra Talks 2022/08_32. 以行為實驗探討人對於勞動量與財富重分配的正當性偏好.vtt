WEBVTT

00:00.000 --> 00:07.000
Hello everyone, welcome to today's Tarot Talk.

00:07.000 --> 00:14.000
We are honored to have Pei-Hsueh to give us a talk.

00:14.000 --> 00:22.000
The topic of today's talk has been shown in his introduction video.

00:22.000 --> 00:32.000
Does the deservingness heuristic explain the effect of effort, luck, and need in a redistribution experiment?

00:32.000 --> 00:42.000
Before we start the talk, let me introduce Pei-Hsueh.

00:42.000 --> 00:47.000
Pei-Hsueh is a PhD student from the Department of Political Science at Stony Brook.

00:47.000 --> 00:54.000
His research focuses on the social preferences of people based on experimental methods and computational social science,

00:54.000 --> 00:58.000
and how these preferences affect people's choices in political issues.

00:58.000 --> 01:06.000
Before we start, let's give a warm welcome to Pei-Hsueh for his wonderful talk.

01:06.000 --> 01:10.000
Oh, the microphone is set to be turned off.

01:10.000 --> 01:14.000
If you want Pei-Hsueh to hear the applause, you have to turn on the microphone first.

01:14.000 --> 01:21.000
Let's give a warm round of applause to Pei-Hsueh for his wonderful talk.

01:21.000 --> 01:24.000
Now it's your turn.

01:24.000 --> 01:31.000
This English is a bit awkward, but what I want to talk about today is

01:31.000 --> 01:38.000
how people's preferences affect people's choices in equality and fairness.

01:38.000 --> 01:48.000
I did this research with my advisor, Ruben Klein.

01:48.000 --> 01:56.000
We're almost done, but I plan to do more analysis.

01:56.000 --> 02:00.000
If you have any comments, please feel free to ask.

02:00.000 --> 02:03.000
Let's get started.

02:03.000 --> 02:08.000
First of all, there's a long-standing problem with social scientists.

02:08.000 --> 02:11.000
Do people care about equality?

02:11.000 --> 02:16.000
Many experiments have shown that people do care about equality.

02:16.000 --> 02:24.000
For example, in a dictatorial election, the dictator gets a sum of money.

02:25.000 --> 02:30.000
Even if there's no reason to give their partner any money,

02:30.000 --> 02:33.000
they'll still choose to give a portion.

02:33.000 --> 02:37.000
Even in a non-dictatorial election,

02:37.000 --> 02:45.000
different experiments have shown that people do care about equality.

02:45.000 --> 02:51.000
Scientists have also found that in a non-industrialized society,

02:51.000 --> 02:56.000
people will punish unfair distributors.

02:56.000 --> 03:00.000
So people do care about equality.

03:00.000 --> 03:07.000
If we use a mathematical model to show that people care about equality,

03:07.000 --> 03:11.000
there are several main models.

03:11.000 --> 03:14.000
The first one is called Inequality Aversion.

03:14.000 --> 03:18.000
It was proposed by Feyer and Schmidt in 1999.

03:18.000 --> 03:22.000
Their concept is very straightforward.

03:22.000 --> 03:29.000
Let's say a person's wealth has not changed.

03:29.000 --> 03:32.000
If he has more money than others,

03:32.000 --> 03:37.000
he might feel embarrassed and unhappy.

03:37.000 --> 03:40.000
If he has less money than others,

03:40.000 --> 03:43.000
he might feel even more unhappy.

03:43.000 --> 03:49.000
If he has the same amount of money as others,

03:49.000 --> 03:54.000
he'll be the happiest if his wealth hasn't changed.

03:54.000 --> 03:58.000
For example, if he has more or less money than others,

03:58.000 --> 04:02.000
he'll be happy because of inequality.

04:02.000 --> 04:06.000
It sounds straightforward and doesn't require a mathematical model.

04:06.000 --> 04:09.000
However, it's better to use a mathematical model

04:09.000 --> 04:14.000
to compare parameters and predictions.

04:14.000 --> 04:23.000
The other model is called Inequality Aversion.

04:23.000 --> 04:27.000
It was proposed by Angiolini and Miller in 2002.

04:27.000 --> 04:30.000
The concept is more straightforward.

04:30.000 --> 04:35.000
People care about their happiness and preferences.

04:35.000 --> 04:37.000
It sounds vague.

04:37.000 --> 04:39.000
People care about their preferences.

04:39.000 --> 04:42.000
It sounds like people don't care about themselves,

04:42.000 --> 04:44.000
but care about others.

04:44.000 --> 04:46.000
It's impossible, right?

04:46.000 --> 04:48.000
Most people are not like this.

04:48.000 --> 04:51.000
The basic idea of this model is that

04:51.000 --> 04:54.000
people will consider others' preferences.

04:54.000 --> 04:57.000
No matter what, people will consider others' preferences.

04:57.000 --> 05:06.000
How can we push people to care about equality?

05:06.000 --> 05:12.000
Let me give you an example.

05:12.000 --> 05:25.000
For most of the wealth,

05:25.000 --> 05:27.000
whether it's money or something else,

05:27.000 --> 05:29.000
of course not everything,

05:29.000 --> 05:35.000
but for most of the wealth,

05:36.000 --> 05:40.000
if you add up all the wealth in one unit,

05:40.000 --> 05:42.000
people will reduce the amount.

05:42.000 --> 05:46.000
For example, if I have $100,000,

05:46.000 --> 05:48.000
and I get $100,000,

05:48.000 --> 05:49.000
I will be happy.

05:49.000 --> 05:51.000
But if I have $1 million,

05:51.000 --> 05:53.000
and I get $100,000,

05:53.000 --> 05:54.000
I will still be happy.

05:54.000 --> 05:59.000
But I won't be as happy as when I only have $100,000.

05:59.000 --> 06:01.000
But if I have $100,000,

06:01.000 --> 06:03.000
and I get $100,000,

06:03.000 --> 06:05.000
and I get $100,000,

06:05.000 --> 06:09.000
I might feel that I don't have a good balance,

06:09.000 --> 06:13.000
but I won't be as happy as when I only have $100,000.

06:13.000 --> 06:16.000
In other words,

06:16.000 --> 06:21.000
if someone is already very rich,

06:21.000 --> 06:23.000
and he gets $100,000,

06:23.000 --> 06:25.000
he might be quite happy.

06:25.000 --> 06:29.000
But if this person cares about another person,

06:29.000 --> 06:31.000
no matter how much he cares,

06:31.000 --> 06:34.000
if this $100,000 is for another poor person,

06:34.000 --> 06:36.000
and this person only has $100,000,

06:36.000 --> 06:38.000
and he cares about this person,

06:38.000 --> 06:41.000
then the other person getting $100,000

06:41.000 --> 06:44.000
might bring him more happiness

06:44.000 --> 06:47.000
than getting $100,000 himself.

06:47.000 --> 06:50.000
So assuming that the editor's use of land

06:50.000 --> 06:53.000
and the user's use of other people's use

06:53.000 --> 06:55.000
these two conditions,

06:55.000 --> 06:58.000
it can also be inferred that

06:58.000 --> 07:01.000
people care about equality.

07:01.000 --> 07:03.000
This model can also explain

07:03.000 --> 07:05.000
why people care about equality.

07:05.000 --> 07:07.000
This model can also explain

07:07.000 --> 07:09.000
why people care about equality.

07:09.000 --> 07:11.000
But the question is,

07:11.000 --> 07:13.000
do people really care about equality?

07:13.000 --> 07:15.000
If we find that,

07:15.000 --> 07:17.000
like the big boss might tell you

07:17.000 --> 07:19.000
that there is no bad luck,

07:19.000 --> 07:21.000
only bad luck,

07:21.000 --> 07:23.000
and then he is not willing to donate his money

07:23.000 --> 07:25.000
or ask for tax, right?

07:25.000 --> 07:27.000
Or some politicians might tell you

07:27.000 --> 07:29.000
that some people refuse to work,

07:29.000 --> 07:31.000
and then use this to blow votes.

07:31.000 --> 07:33.000
Does this mean that people

07:33.000 --> 07:35.000
might not care about equality?

07:35.000 --> 07:37.000
What people care about is actually

07:37.000 --> 07:39.000
the so-called fairness.

07:39.000 --> 07:41.000
Of course, fairness can have many definitions,

07:41.000 --> 07:43.000
but what people care about

07:43.000 --> 07:45.000
is how much effort

07:45.000 --> 07:47.000
people should make

07:47.000 --> 07:49.000
to get paid.

07:49.000 --> 07:51.000
What people care about is how much effort

07:51.000 --> 07:53.000
people should make

07:53.000 --> 07:55.000
to get paid.

07:55.000 --> 07:57.000
If the income in the lab

07:57.000 --> 07:59.000
is determined by effort,

07:59.000 --> 08:01.000
is determined by effort,

08:01.000 --> 08:03.000
is determined by effort,

08:03.000 --> 08:05.000
and not by luck,

08:05.000 --> 08:07.000
and not by luck,

08:07.000 --> 08:09.000
the beneficiary will choose

08:09.000 --> 08:11.000
to redistribute less.

08:11.000 --> 08:13.000
In other words,

08:13.000 --> 08:15.000
the beneficiary will choose

08:15.000 --> 08:17.000
not to redistribute the wealth

08:17.000 --> 08:19.000
according to certain taxes.

08:19.000 --> 08:21.000
In other words,

08:21.000 --> 08:23.000
the beneficiary will choose

08:23.000 --> 08:25.000
not to redistribute the wealth

08:25.000 --> 08:27.000
according to certain taxes.

08:27.000 --> 08:29.000
In other words,

08:29.000 --> 08:31.000
if the income is determined by luck,

08:31.000 --> 08:33.000
the beneficiary will have more social benefits

08:33.000 --> 08:35.000
and higher tax payments.

08:35.000 --> 08:37.000
and higher tax payments.

08:37.000 --> 08:39.000
and higher tax payments.

08:39.000 --> 08:41.000
If the beneficiary

08:41.000 --> 08:43.000
believes that

08:43.000 --> 08:45.000
everyone's income

08:45.000 --> 08:47.000
is determined by effort,

08:47.000 --> 08:49.000
not by luck,

08:49.000 --> 08:51.000
the beneficiary will not

08:51.000 --> 08:53.000
receive social benefits

08:53.000 --> 08:55.000
and lower taxes.

08:55.000 --> 08:57.000
and lower taxes.

08:57.000 --> 08:59.000
Recently,

08:59.000 --> 09:01.000
I've been

09:01.000 --> 09:03.000
talking about

09:03.000 --> 09:05.000
why people

09:05.000 --> 09:07.000
prefer an unequal society.

09:07.000 --> 09:09.000
Why do people think

09:09.000 --> 09:11.000
an unequal society is fair?

09:17.000 --> 09:19.000
Today,

09:19.000 --> 09:21.000
we're going to talk about

09:21.000 --> 09:23.000
whether people

09:23.000 --> 09:25.000
only care about

09:25.000 --> 09:27.000
equality or fairness.

09:27.000 --> 09:29.000
Of course not.

09:29.000 --> 09:31.000
Most people

09:31.000 --> 09:33.000
care about equality and fairness.

09:33.000 --> 09:35.000
Today,

09:35.000 --> 09:37.000
we want to

09:37.000 --> 09:39.000
build a model

09:39.000 --> 09:41.000
that can explain

09:41.000 --> 09:43.000
people's preference for equality

09:43.000 --> 09:45.000
and fairness.

09:45.000 --> 09:47.000
Let's go back

09:47.000 --> 09:49.000
to our model.

09:49.000 --> 09:51.000
Let's assume

09:51.000 --> 09:53.000
that this model

09:53.000 --> 09:55.000
only cares about

09:55.000 --> 09:57.000
other people.

09:57.000 --> 09:59.000
Of course,

09:59.000 --> 10:01.000
the degree of care

10:01.000 --> 10:03.000
may vary

10:03.000 --> 10:05.000
depending on the situation

10:05.000 --> 10:07.000
and individual personality

10:07.000 --> 10:09.000
and experience.

10:09.000 --> 10:11.000
Today,

10:11.000 --> 10:13.000
we want to

10:13.000 --> 10:15.000
put the degree of effort

10:15.000 --> 10:17.000
into the model.

10:17.000 --> 10:19.000
Let's say

10:19.000 --> 10:21.000
there is a decision-maker

10:21.000 --> 10:23.000
and

10:23.000 --> 10:25.000
there are two other people.

10:25.000 --> 10:27.000
Let's say

10:27.000 --> 10:29.000
one person is very hard-working.

10:29.000 --> 10:31.000
The decision-maker

10:31.000 --> 10:33.000
cares about the hard-working person more.

10:33.000 --> 10:35.000
Let's say

10:35.000 --> 10:37.000
the wealth of these two people

10:37.000 --> 10:39.000
is unequal,

10:39.000 --> 10:41.000
and the decision-maker is rich.

10:41.000 --> 10:43.000
Let's say

10:43.000 --> 10:45.000
the wealth of this person is not equal,

10:45.000 --> 10:47.000
but the decision-maker is hard-working.

10:47.000 --> 10:49.000
The decision-maker cares about the recipient,

10:49.000 --> 10:51.000
and decides to

10:51.000 --> 10:53.000
redistribute the wealth

10:53.000 --> 10:55.000
to the recipient.

10:55.000 --> 10:57.000
On the other hand,

10:57.000 --> 10:59.000
if the recipient

10:59.000 --> 11:01.000
is not hard-working,

11:01.000 --> 11:03.000
the decision-maker

11:03.000 --> 11:05.000
will not care about the recipient's preference

11:05.000 --> 11:07.000
and choose to give the recipient

11:07.000 --> 11:09.000
less money.

11:09.000 --> 11:11.000
In order

11:11.000 --> 11:13.000
to verify

11:13.000 --> 11:15.000
the prediction of our model,

11:15.000 --> 11:17.000
we designed

11:17.000 --> 11:19.000
a laboratory experiment.

11:19.000 --> 11:21.000
The experiment

11:21.000 --> 11:23.000
was conducted

11:23.000 --> 11:25.000
by the students

11:25.000 --> 11:27.000
of Stony Brook University.

11:27.000 --> 11:29.000
In this experiment,

11:29.000 --> 11:31.000
we will give them

11:31.000 --> 11:33.000
different amounts of money

11:33.000 --> 11:35.000
based on their decisions.

11:35.000 --> 11:37.000
So,

11:37.000 --> 11:39.000
the money they can get

11:39.000 --> 11:41.000
after the experiment

11:41.000 --> 11:43.000
is determined

11:43.000 --> 11:45.000
by their decisions

11:45.000 --> 11:47.000
in the laboratory.

11:47.000 --> 11:49.000
This is our laboratory,

11:49.000 --> 11:51.000
but this is just a demo.

11:51.000 --> 11:53.000
This is not my experiment.

11:53.000 --> 11:55.000
So, the people here

11:55.000 --> 11:57.000
look a little old.

11:57.000 --> 11:59.000
This is the professor,

11:59.000 --> 12:01.000
and this seems to be

12:01.000 --> 12:03.000
a demo for the website.

12:03.000 --> 12:05.000
This experiment

12:05.000 --> 12:07.000
was not conducted

12:07.000 --> 12:09.000
in the laboratory,

12:09.000 --> 12:11.000
because the recipient

12:11.000 --> 12:13.000
was in the middle of the epidemic.

12:13.000 --> 12:15.000
So, the experiment was conducted

12:15.000 --> 12:17.000
on-site.

12:17.000 --> 12:19.000
This is the design

12:19.000 --> 12:21.000
of our laboratory.

12:21.000 --> 12:23.000
At the beginning

12:23.000 --> 12:25.000
of the experiment,

12:25.000 --> 12:27.000
the recipient

12:27.000 --> 12:29.000
can choose

12:29.000 --> 12:31.000
to give the recipient

12:31.000 --> 12:33.000
less money

12:33.000 --> 12:35.000
based on their decisions.

12:35.000 --> 12:37.000
At the beginning of the experiment,

12:37.000 --> 12:39.000
the recipient

12:39.000 --> 12:41.000
can decide whether

12:41.000 --> 12:43.000
to do this task,

12:43.000 --> 12:45.000
which we call

12:45.000 --> 12:47.000
the counting zero task.

12:47.000 --> 12:49.000
They need to write

12:49.000 --> 12:51.000
the answer here

12:51.000 --> 12:53.000
and send it out.

12:53.000 --> 12:55.000
When they get it right,

12:55.000 --> 12:57.000
they will get a reward.

12:57.000 --> 12:59.000
The reward for getting it right

12:59.000 --> 13:01.000
is random.

13:01.000 --> 13:03.000
Each recipient

13:03.000 --> 13:05.000
can decide

13:05.000 --> 13:07.000
how many tasks

13:07.000 --> 13:09.000
they want to do.

13:09.000 --> 13:11.000
They can choose

13:11.000 --> 13:13.000
not to do it

13:13.000 --> 13:15.000
and skip this step.

13:15.000 --> 13:17.000
They can choose

13:17.000 --> 13:19.000
to do up to 10 tasks.

13:19.000 --> 13:21.000
After this task,

13:21.000 --> 13:23.000
we will randomly

13:23.000 --> 13:25.000
pair the recipients.

13:25.000 --> 13:27.000
One recipient

13:27.000 --> 13:29.000
will be designated as

13:29.000 --> 13:31.000
the decider.

13:31.000 --> 13:33.000
The other recipient

13:33.000 --> 13:35.000
will be designated as the recipient.

13:35.000 --> 13:37.000
The decider can decide

13:37.000 --> 13:39.000
how much money

13:39.000 --> 13:41.000
they want to earn

13:41.000 --> 13:43.000
from this task

13:43.000 --> 13:45.000
to the recipient.

13:45.000 --> 13:47.000
The decider will know

13:47.000 --> 13:49.000
how much the recipient's salary is

13:49.000 --> 13:51.000
and how many tasks they need to complete.

13:51.000 --> 13:53.000
In this experiment,

13:53.000 --> 13:55.000
we use a method

13:55.000 --> 13:57.000
called the strategy method.

13:57.000 --> 13:59.000
This method

13:59.000 --> 14:01.000
allows the recipient

14:01.000 --> 14:03.000
to answer

14:03.000 --> 14:05.000
the combination of

14:05.000 --> 14:07.000
all the salary and tasks.

14:07.000 --> 14:09.000
There are two salaries

14:09.000 --> 14:11.000
and 11 different

14:11.000 --> 14:13.000
effort levels.

14:13.000 --> 14:15.000
There are 22 combinations.

14:15.000 --> 14:17.000
The decider

14:17.000 --> 14:19.000
needs to answer

14:19.000 --> 14:21.000
the question

14:21.000 --> 14:23.000
how much money

14:23.000 --> 14:25.000
they want to earn

14:25.000 --> 14:27.000
from this task

14:27.000 --> 14:29.000
to the recipient.

14:29.000 --> 14:31.000
In the end,

14:31.000 --> 14:33.000
the decider

14:33.000 --> 14:35.000
will know

14:35.000 --> 14:37.000
how much money

14:37.000 --> 14:39.000
the recipient

14:39.000 --> 14:41.000
earns

14:41.000 --> 14:43.000
from this task

14:43.000 --> 14:45.000
to the recipient.

14:45.000 --> 14:47.000
The decider will know

14:47.000 --> 14:49.000
how much money

14:49.000 --> 14:51.000
the recipient

14:51.000 --> 14:53.000
earns

14:53.000 --> 14:55.000
from this task

14:55.000 --> 14:57.000
to the recipient.

14:57.000 --> 14:59.000
In the end,

14:59.000 --> 15:01.000
the decider

15:01.000 --> 15:03.000
will know

15:03.000 --> 15:05.000
how much money

15:05.000 --> 15:07.000
the recipient

15:07.000 --> 15:09.000
earns

15:09.000 --> 15:11.000
from this task

15:11.000 --> 15:13.000
to the recipient.

15:13.000 --> 15:15.000
Our model

15:15.000 --> 15:17.000
is called

15:17.000 --> 15:19.000
the conditional model.

15:19.000 --> 15:21.000
In this model,

15:21.000 --> 15:23.000
this is

15:23.000 --> 15:25.000
the income

15:25.000 --> 15:27.000
of the decider.

15:27.000 --> 15:29.000
T is how much money

15:29.000 --> 15:31.000
the decider wants to give to the recipient.

15:31.000 --> 15:33.000
M is

15:33.000 --> 15:35.000
the salary

15:35.000 --> 15:37.000
of the recipient,

15:37.000 --> 15:39.000
$1 or $2.

15:39.000 --> 15:41.000
E is how many tasks

15:41.000 --> 15:43.000
the decider has completed.

15:43.000 --> 15:45.000
T is how much money

15:45.000 --> 15:47.000
the receiver earns.

15:47.000 --> 15:49.000
Theta is

15:49.000 --> 15:51.000
the weight of the decider.

15:51.000 --> 15:53.000
Theta is the weight of the decider.

15:53.000 --> 15:55.000
Theta is the weight of the decider.

15:55.000 --> 15:57.000
In this case,

15:57.000 --> 15:59.000
Theta is the value of

15:59.000 --> 16:01.000
the function of labor.

16:01.000 --> 16:03.000
Alpha is

16:03.000 --> 16:05.000
the parameter of

16:05.000 --> 16:07.000
the marginal cost of the decision.

16:07.000 --> 16:09.000
Therefore, the alpha determines

16:09.000 --> 16:11.000
the marginal cost of the decision.

16:11.000 --> 16:13.000
The higher the alpha,

16:13.000 --> 16:15.000
the lower the value.

16:15.000 --> 16:17.000
In other words,

16:17.000 --> 16:19.000
the higher the alpha,

16:19.000 --> 16:21.000
the lower the marginal cost of the decision.

16:21.000 --> 16:23.000
We can determine

16:23.000 --> 16:25.000
how much money

16:25.000 --> 16:27.000
the decider

16:27.000 --> 16:29.000
will give

16:29.000 --> 16:31.000
to the recipient

16:31.000 --> 16:33.000
based on

16:33.000 --> 16:35.000
each factor.

16:35.000 --> 16:37.000
We can determine

16:37.000 --> 16:39.000
how much money

16:39.000 --> 16:41.000
the decider

16:41.000 --> 16:43.000
will give

16:43.000 --> 16:45.000
to the recipient

16:45.000 --> 16:47.000
based on

16:47.000 --> 16:49.000
each factor.

16:49.000 --> 16:51.000
based on

16:51.000 --> 16:53.000
each factor.

16:53.000 --> 16:55.000
There are

16:55.000 --> 16:57.000
several predictions

16:57.000 --> 16:59.000
under this model.

16:59.000 --> 17:01.000
The first prediction is that

17:01.000 --> 17:03.000
as long as the weight

17:03.000 --> 17:05.000
is greater than or equal to 0,

17:05.000 --> 17:07.000
the income of the decider

17:07.000 --> 17:09.000
and the degree of labor of the recipient

17:09.000 --> 17:11.000
will remain the same

17:11.000 --> 17:13.000
or less when the income of the receiver

17:13.000 --> 17:15.000
is greater than or equal to 0.

17:15.000 --> 17:17.000
This is called

17:17.000 --> 17:19.000
inequality effect.

17:19.000 --> 17:21.000
The second prediction

17:21.000 --> 17:23.000
is that

17:23.000 --> 17:25.000
as long as the income of the decider

17:25.000 --> 17:27.000
and the degree of labor of the recipient

17:27.000 --> 17:29.000
is greater than or equal to 0,

17:29.000 --> 17:31.000
the income of the decider

17:31.000 --> 17:33.000
will remain the same or less.

17:33.000 --> 17:35.000
In our experiment,

17:35.000 --> 17:37.000
we found that

17:37.000 --> 17:39.000
90% of the receivers

17:39.000 --> 17:41.000
met the first

17:41.000 --> 17:43.000
inequality effect.

17:43.000 --> 17:45.000
inequality effect.

17:45.000 --> 17:47.000
In addition,

17:47.000 --> 17:49.000
we found that

17:49.000 --> 17:51.000
85% of the receivers

17:51.000 --> 17:53.000
met the

17:53.000 --> 17:55.000
deservingness effect.

17:55.000 --> 17:57.000
deservingness effect.

17:57.000 --> 17:59.000
deservingness effect.

17:59.000 --> 18:01.000
In addition,

18:01.000 --> 18:03.000
we ran a random effect regression.

18:03.000 --> 18:05.000
We found that

18:05.000 --> 18:07.000
as long as the

18:07.000 --> 18:09.000
income of the viewers

18:09.000 --> 18:11.000
is higher

18:11.000 --> 18:13.000
than or equal to 0,

18:13.000 --> 18:15.000
the income of the

18:15.000 --> 18:17.000
recipients

18:17.000 --> 18:19.000
is also higher.

18:19.000 --> 18:21.000
If the

18:21.000 --> 18:23.000
income of the

18:23.000 --> 18:25.000
智lr celebrities

18:25.000 --> 18:27.000
is better than

18:27.000 --> 18:29.000
or equal to 0,

18:29.000 --> 18:31.000
all providers

18:31.000 --> 18:33.000
will offer

18:33.000 --> 18:35.000
a higher

18:35.000 --> 18:37.000
income than

18:37.000 --> 18:39.000
or equal to 0.

18:39.000 --> 18:41.000
The other

18:41.000 --> 18:43.000
prediction is

18:43.000 --> 18:45.000
that

18:45.000 --> 18:47.000
we actually

18:47.000 --> 18:49.000
specify some parameters

18:49.000 --> 18:51.000
for the model.

18:51.000 --> 18:53.000
We actually

18:53.000 --> 18:55.000
assume

18:55.000 --> 18:57.000
the weight

18:57.000 --> 18:59.000
function.

18:59.000 --> 19:01.000
We assume

19:01.000 --> 19:03.000
it is linear.

19:03.000 --> 19:05.000
If the weight

19:05.000 --> 19:07.000
is a baseline weight,

19:07.000 --> 19:09.000
that is,

19:09.000 --> 19:11.000
when the collector

19:11.000 --> 19:13.000
does not work hard at all,

19:13.000 --> 19:15.000
how much is the

19:15.000 --> 19:17.000
basic weight?

19:17.000 --> 19:19.000
How much does the

19:19.000 --> 19:21.000
collector care about

19:21.000 --> 19:23.000
the recipient?

19:23.000 --> 19:25.000
Another parameter is

19:25.000 --> 19:27.000
that once the

19:27.000 --> 19:29.000
recipient increases

19:29.000 --> 19:31.000
how much does the

19:31.000 --> 19:33.000
weight increase?

19:33.000 --> 19:35.000
We use two different

19:35.000 --> 19:37.000
parameters as an

19:37.000 --> 19:39.000
assumption.

19:39.000 --> 19:41.000
The first parameter

19:41.000 --> 19:43.000
is that the

19:43.000 --> 19:45.000
weight baseline is very high.

19:45.000 --> 19:47.000
When the recipient

19:47.000 --> 19:49.000
does not work hard at all,

19:49.000 --> 19:51.000
the collector cares

19:51.000 --> 19:53.000
very much about the

19:53.000 --> 19:55.000
recipient.

19:55.000 --> 19:57.000
The picture on the

19:57.000 --> 19:59.000
left shows

19:59.000 --> 20:01.000
that the

20:01.000 --> 20:03.000
weight baseline

20:03.000 --> 20:05.000
is very low.

20:05.000 --> 20:07.000
When the

20:07.000 --> 20:09.000
recipient does not

20:09.000 --> 20:11.000
work at all,

20:11.000 --> 20:13.000
the collector does

20:13.000 --> 20:15.000
not care about

20:15.000 --> 20:17.000
the recipient.

20:17.000 --> 20:19.000
However,

20:19.000 --> 20:21.000
when the

20:21.000 --> 20:23.000
recipient increases

20:23.000 --> 20:25.000
by one unit,

20:25.000 --> 20:27.000
the weight

20:27.000 --> 20:29.000
is the

20:29.000 --> 20:31.000
income of the

20:31.000 --> 20:33.000
recipient.

20:33.000 --> 20:35.000
The weight

20:35.000 --> 20:37.000
determines

20:37.000 --> 20:39.000
how much

20:39.000 --> 20:41.000
the collector

20:41.000 --> 20:43.000
has to pay

20:43.000 --> 20:45.000
to the recipient.

20:45.000 --> 20:47.000
The higher the

20:47.000 --> 20:49.000
recipient's income,

20:49.000 --> 20:51.000
the higher

20:51.000 --> 20:53.000
the recipient's

20:53.000 --> 20:55.000
salary.

20:55.000 --> 20:57.000
As you can see,

20:57.000 --> 20:59.000
the two lines

20:59.000 --> 21:01.000
on the left

21:01.000 --> 21:03.000
are overlapping.

21:03.000 --> 21:05.000
In other words,

21:05.000 --> 21:07.000
the collector

21:07.000 --> 21:09.000
does not care

21:09.000 --> 21:11.000
about the

21:11.000 --> 21:13.000
recipient's salary,

21:13.000 --> 21:15.000
but the weight

21:15.000 --> 21:17.000
baseline is very high,

21:17.000 --> 21:19.000
so the salary

21:19.000 --> 21:21.000
does not matter.

21:21.000 --> 21:23.000
However,

21:23.000 --> 21:25.000
the higher the

21:25.000 --> 21:27.000
recipient's income,

21:27.000 --> 21:29.000
the less

21:29.000 --> 21:31.000
the collector

21:31.000 --> 21:33.000
has to pay

21:33.000 --> 21:35.000
to the recipient.

21:35.000 --> 21:37.000
On the right,

21:37.000 --> 21:39.000
the two lines

21:39.000 --> 21:41.000
on the left

21:41.000 --> 21:43.000
are overlapping.

21:43.000 --> 21:45.000
The higher the

21:45.000 --> 21:47.000
recipient's income,

21:47.000 --> 21:49.000
the higher

21:49.000 --> 21:51.000
the collector

21:51.000 --> 21:53.000
has to pay

21:53.000 --> 21:55.000
to the recipient.

21:55.000 --> 21:57.000
The higher the

21:57.000 --> 21:59.000
recipient's income,

21:59.000 --> 22:01.000
the more

22:01.000 --> 22:03.000
the collector

22:03.000 --> 22:05.000
has to pay

22:05.000 --> 22:07.000
to the recipient.

22:07.000 --> 22:09.000
According to our

22:09.000 --> 22:11.000
experiment results,

22:11.000 --> 22:13.000
we can divide

22:13.000 --> 22:15.000
these collectors

22:15.000 --> 22:17.000
into four types.

22:17.000 --> 22:19.000
The first type

22:19.000 --> 22:21.000
is sensitive

22:21.000 --> 22:23.000
to the recipient's

22:23.000 --> 22:25.000
salary.

22:25.000 --> 22:27.000
This type

22:27.000 --> 22:29.000
accounts for

22:29.000 --> 22:31.000
25% of the collectors.

22:31.000 --> 22:33.000
The second type

22:33.000 --> 22:35.000
is insensitive

22:35.000 --> 22:37.000
to the

22:37.000 --> 22:39.000
recipient's salary.

22:39.000 --> 22:41.000
This type

22:41.000 --> 22:43.000
accounts for

22:43.000 --> 22:45.000
20% of the collectors.

22:45.000 --> 22:47.000
The third type

22:47.000 --> 22:49.000
is selfish

22:49.000 --> 22:51.000
to the recipient's

22:51.000 --> 22:53.000
salary.

22:53.000 --> 22:55.000
The last type

22:55.000 --> 22:57.000
is other.

22:57.000 --> 22:59.000
The last type

22:59.000 --> 23:01.000
is other.

23:01.000 --> 23:03.000
The last type

23:03.000 --> 23:05.000
is other.

23:05.000 --> 23:07.000
When we analyze

23:07.000 --> 23:09.000
sensitive type and

23:09.000 --> 23:11.000
insensitive type,

23:11.000 --> 23:13.000
we find that

23:13.000 --> 23:15.000
the result

23:15.000 --> 23:17.000
is

23:17.000 --> 23:19.000
consistent

23:19.000 --> 23:21.000
with our

23:21.000 --> 23:23.000
theoretical prediction.

23:27.000 --> 23:29.000
The main conclusion

23:29.000 --> 23:31.000
of our research

23:31.000 --> 23:33.000
is that

23:33.000 --> 23:35.000
our model

23:35.000 --> 23:37.000
considers equality and fairness.

23:37.000 --> 23:39.000
Most collectors

23:39.000 --> 23:41.000
have the same model prediction.

23:41.000 --> 23:43.000
We also found that

23:43.000 --> 23:45.000
collectors can be classified into

23:45.000 --> 23:47.000
three types,

23:47.000 --> 23:49.000
selfish collectors,

23:49.000 --> 23:51.000
sensitive collectors,

23:51.000 --> 23:53.000
and fair collectors.

23:53.000 --> 23:55.000
Sorry,

23:55.000 --> 23:57.000
that's all

23:57.000 --> 23:59.000
for today.

23:59.000 --> 24:01.000
Thank you.

24:01.000 --> 24:03.000
This is my Twitter

24:03.000 --> 24:05.000
and personal website.

24:05.000 --> 24:07.000
You are welcome

24:07.000 --> 24:09.000
to join my Twitter.

24:09.000 --> 24:11.000
Thank you.

24:11.000 --> 24:13.000
Before we start

24:13.000 --> 24:15.000
the Q&A,

24:15.000 --> 24:17.000
please turn on your

24:17.000 --> 24:19.000
microphone.

24:19.000 --> 24:21.000
Let's welcome

24:21.000 --> 24:23.000
PeiXun.

24:25.000 --> 24:27.000
Do you have any questions?

24:27.000 --> 24:29.000
You can ask

24:29.000 --> 24:31.000
directly.

24:31.000 --> 24:33.000
You can also

24:33.000 --> 24:35.000
type your questions

24:35.000 --> 24:37.000
in the chat box.

24:37.000 --> 24:39.000
Do you have any questions?

24:39.000 --> 24:41.000
You can ask

24:41.000 --> 24:43.000
directly.

24:43.000 --> 24:45.000
Hi,

24:45.000 --> 24:47.000
I'm Jeffrey from

24:47.000 --> 24:49.000
Indiana University Bloomington.

24:49.000 --> 24:51.000
I'm also a fifth-year professor.

24:51.000 --> 24:53.000
Thank you, PeiXun.

24:53.000 --> 24:55.000
Sorry,

24:55.000 --> 24:57.000
it's PeiXun.

24:57.000 --> 24:59.000
I think your topic is

24:59.000 --> 25:01.000
very interesting.

25:01.000 --> 25:03.000
After listening to your

25:03.000 --> 25:05.000
experimental design,

25:05.000 --> 25:07.000
I have some questions.

25:07.000 --> 25:09.000
First of all,

25:09.000 --> 25:11.000
you didn't talk much about

25:11.000 --> 25:13.000
decision-makers and recipients.

25:13.000 --> 25:15.000
In your experimental design,

25:15.000 --> 25:17.000
did you design

25:17.000 --> 25:19.000
decision-makers to give money

25:19.000 --> 25:21.000
to the recipients?

25:21.000 --> 25:23.000
Or did you just say

25:23.000 --> 25:25.000
there will be a part like this?

25:25.000 --> 25:27.000
There is no right answer.

25:27.000 --> 25:29.000
Sorry.

25:29.000 --> 25:31.000
There is no right answer.

25:31.000 --> 25:33.000
He can make such a decision.

25:33.000 --> 25:35.000
We will ask him

25:35.000 --> 25:37.000
if the recipient's salary

25:37.000 --> 25:39.000
is $1

25:39.000 --> 25:41.000
or $2.

25:41.000 --> 25:43.000
If the recipient

25:43.000 --> 25:45.000
completes

25:45.000 --> 25:47.000
several tasks,

25:47.000 --> 25:49.000
how much money

25:49.000 --> 25:51.000
does he want to give to the recipient?

25:51.000 --> 25:53.000
He can choose not to give.

25:53.000 --> 25:55.000
We didn't give him any reason.

25:55.000 --> 25:57.000
How much money he can get

25:57.000 --> 25:59.000
is determined

25:59.000 --> 26:01.000
by his decisions.

26:01.000 --> 26:03.000
If he decides to give more money

26:03.000 --> 26:05.000
to the recipient,

26:05.000 --> 26:07.000
he will get less money in the end.

26:07.000 --> 26:09.000
In fact,

26:09.000 --> 26:11.000
if he is a

26:11.000 --> 26:13.000
direct recipient,

26:13.000 --> 26:15.000
there is no reason

26:15.000 --> 26:17.000
to give money to the recipient.

26:17.000 --> 26:19.000
The more money he gives,

26:19.000 --> 26:21.000
the less money he will get.

26:21.000 --> 26:23.000
I'm a little confused.

26:23.000 --> 26:25.000
I'm not convinced.

26:25.000 --> 26:27.000
It depends on

26:27.000 --> 26:29.000
how you get the income.

26:29.000 --> 26:31.000
Generally speaking,

26:31.000 --> 26:33.000
rich people

26:33.000 --> 26:35.000
are generous.

26:35.000 --> 26:37.000
However,

26:37.000 --> 26:39.000
I don't know.

26:39.000 --> 26:41.000
I don't think

26:41.000 --> 26:43.000
he is generous enough.

26:43.000 --> 26:45.000
Maybe there are many reasons

26:45.000 --> 26:47.000
why he doesn't give money.

26:47.000 --> 26:49.000
I don't think

26:49.000 --> 26:51.000
it has anything to do

26:51.000 --> 26:53.000
with strategic thinking.

26:53.000 --> 26:55.000
I don't think

26:55.000 --> 26:57.000
it has anything to do

26:57.000 --> 26:59.000
with strategic thinking.

26:59.000 --> 27:01.000
I don't think

27:01.000 --> 27:03.000
it has anything to do

27:03.000 --> 27:05.000
with strategic thinking.

27:05.000 --> 27:07.000
For example,

27:07.000 --> 27:09.000
if you win a lottery

27:09.000 --> 27:11.000
and win 10 million,

27:11.000 --> 27:13.000
you can give

27:13.000 --> 27:15.000
10 yuan to the recipient.

27:15.000 --> 27:17.000
How generous do you think

27:17.000 --> 27:19.000
he is?

27:19.000 --> 27:21.000
How generous

27:21.000 --> 27:23.000
do you think

27:23.000 --> 27:25.000
he is?

27:25.000 --> 27:27.000
For example,

27:27.000 --> 27:29.000
you can see that

27:29.000 --> 27:31.000
the one who earns more

27:31.000 --> 27:33.000
is more likely to give the money.

27:33.000 --> 27:35.000
The one who earns less

27:35.000 --> 27:37.000
is more likely to give the money.

27:37.000 --> 27:39.000
Do you think

27:39.000 --> 27:41.000
he is more generous?

27:41.000 --> 27:43.000
I think

27:43.000 --> 27:45.000
he is more generous

27:45.000 --> 27:47.000
because he is rich.

27:47.000 --> 27:49.000
I don't know.

27:49.000 --> 27:51.000
I don't know.

27:51.000 --> 27:53.000
I don't know.

27:53.000 --> 27:55.000
I don't know.

27:55.000 --> 27:57.000
I don't know.

27:57.000 --> 27:59.000
I don't know.

27:59.000 --> 28:01.000
Thank you for your question.

28:01.000 --> 28:03.000
Thank you for your question.

28:03.000 --> 28:05.000
First of all,

28:05.000 --> 28:07.000
I'm not saying

28:07.000 --> 28:09.000
the rich are more generous.

28:09.000 --> 28:11.000
It's not our...

28:15.000 --> 28:17.000
First of all,

28:17.000 --> 28:19.000
the definition of

28:19.000 --> 28:21.000
the rich is...

28:21.000 --> 28:23.000
I think

28:23.000 --> 28:25.000
the definition of the rich

28:25.000 --> 28:27.000
is...

28:27.000 --> 28:29.000
The definition of the rich

28:29.000 --> 28:31.000
is...

28:31.000 --> 28:33.000
The definition of the rich

28:33.000 --> 28:35.000
is...

28:35.000 --> 28:37.000
The definition of the rich

28:37.000 --> 28:39.000
is...

28:39.000 --> 28:41.000
This definition is simple.

28:41.000 --> 28:43.000
It's not about

28:43.000 --> 28:45.000
being generous or anything.

28:45.000 --> 28:47.000
Of course,

28:47.000 --> 28:49.000
this is a matter of discussion.

28:49.000 --> 28:51.000
The definition of the rich

28:51.000 --> 28:53.000
is very simple.

28:53.000 --> 28:55.000
It's about

28:57.000 --> 28:59.000
considering

28:59.000 --> 29:01.000
other people's benefits.

29:03.000 --> 29:05.000
The first thing is...

29:07.000 --> 29:09.000
The first thing is...

29:09.000 --> 29:11.000
Let me think about

29:11.000 --> 29:13.000
how to answer this.

29:13.000 --> 29:15.000
The definition of the rich

29:15.000 --> 29:17.000
is not...

29:17.000 --> 29:19.000
It's very simple.

29:19.000 --> 29:21.000
It's about

29:21.000 --> 29:23.000
considering other people's benefits.

29:23.000 --> 29:25.000
I'm not saying

29:25.000 --> 29:27.000
the rich are universal.

29:27.000 --> 29:29.000
For example,

29:29.000 --> 29:31.000
everyone has the same rights.

29:31.000 --> 29:33.000
We're not saying

29:33.000 --> 29:35.000
everyone has the same rights.

29:35.000 --> 29:37.000
In this experiment,

29:37.000 --> 29:39.000
their income

29:39.000 --> 29:41.000
comes from their tasks.

29:41.000 --> 29:43.000
Their income

29:43.000 --> 29:45.000
comes from their hard work.

29:45.000 --> 29:47.000
The income is divided by the recipient.

29:47.000 --> 29:49.000
Another thing is...

29:49.000 --> 29:51.000
Our model

29:51.000 --> 29:53.000
predicts

29:53.000 --> 29:55.000
that the more

29:55.000 --> 29:57.000
the recipient...

29:57.000 --> 29:59.000
I'm sorry.

29:59.000 --> 30:01.000
The more the decision-maker's income,

30:01.000 --> 30:03.000
the more money the recipient should get.

30:03.000 --> 30:05.000
But in our result,

30:05.000 --> 30:07.000
we didn't find

30:07.000 --> 30:09.000
that the decision-maker's income

30:09.000 --> 30:11.000
had a significant effect.

30:11.000 --> 30:13.000
We found that

30:13.000 --> 30:15.000
the income of the recipient

30:15.000 --> 30:17.000
and the hard work of the recipient

30:17.000 --> 30:19.000
had a significant effect

30:19.000 --> 30:21.000
on the decision-maker's income.

30:21.000 --> 30:23.000
on the decision-maker's income.

30:23.000 --> 30:25.000
I'm sorry.

30:25.000 --> 30:27.000
How did the decision-maker

30:27.000 --> 30:29.000
see the hard work

30:29.000 --> 30:31.000
of the recipient?

30:31.000 --> 30:33.000
How did you present

30:33.000 --> 30:35.000
the hard work?

30:35.000 --> 30:37.000
We gave him information.

30:37.000 --> 30:39.000
Actually, we asked this question.

30:39.000 --> 30:41.000
Let me show you the timeline.

30:41.000 --> 30:43.000
Let's say

30:43.000 --> 30:45.000
the recipient

30:45.000 --> 30:47.000
earned $1.

30:47.000 --> 30:49.000
The recipient earned $1.

30:49.000 --> 30:51.000
The recipient earned $1.

30:51.000 --> 30:53.000
The recipient earned $1.

30:53.000 --> 30:55.000
How much do you want to give

30:55.000 --> 30:57.000
to the recipient?

30:57.000 --> 30:59.000
As I said,

30:59.000 --> 31:01.000
they have to answer 22 questions.

31:01.000 --> 31:03.000
We will ask

31:03.000 --> 31:05.000
the combination of income and salary.

31:05.000 --> 31:07.000
$0 to $10

31:07.000 --> 31:09.000
is determined

31:09.000 --> 31:11.000
by the recipient's income.

31:11.000 --> 31:13.000
The reason $10 is shown here

31:13.000 --> 31:15.000
is because

31:15.000 --> 31:17.000
the recipient

31:17.000 --> 31:19.000
earned $10

31:19.000 --> 31:21.000
from the real effort task.

31:21.000 --> 31:23.000
If the recipient

31:23.000 --> 31:25.000
earned $5,

31:25.000 --> 31:27.000
$5 is shown.

31:27.000 --> 31:29.000
If the recipient earned nothing,

31:29.000 --> 31:31.000
$0 is shown.

31:31.000 --> 31:33.000
$0 is shown.

31:35.000 --> 31:37.000
Thank you.

31:37.000 --> 31:39.000
You're welcome.

31:39.000 --> 31:41.000
Thank you for your question.

31:43.000 --> 31:45.000
Before we move on

31:45.000 --> 31:47.000
to other questions,

31:47.000 --> 31:49.000
let me read the question

31:49.000 --> 31:51.000
from the chat room.

31:51.000 --> 31:53.000
Sorry,

31:53.000 --> 31:55.000
I don't know

31:55.000 --> 31:57.000
what your Chinese name is.

31:57.000 --> 31:59.000
It's Jeffrey Wang.

31:59.000 --> 32:01.000
He asked a question.

32:01.000 --> 32:03.000
He said the experiment design

32:03.000 --> 32:05.000
was given to the decision-makers.

32:05.000 --> 32:07.000
I just asked him.

32:07.000 --> 32:09.000
OK.

32:09.000 --> 32:11.000
Thank you.

32:11.000 --> 32:13.000
OK.

32:13.000 --> 32:15.000
I have some other questions.

32:15.000 --> 32:17.000
Let's see if there are any questions

32:17.000 --> 32:19.000
to be asked.

32:19.000 --> 32:21.000
Let me advertise

32:21.000 --> 32:23.000
some activities for Terra.

32:23.000 --> 32:25.000
Terra is now

32:25.000 --> 32:27.000
issuing a participation certificate.

32:27.000 --> 32:29.000
The certificate can be

32:29.000 --> 32:31.000
submitted to your mobile app.

32:31.000 --> 32:33.000
We call it

32:33.000 --> 32:35.000
Terra Badge.

32:35.000 --> 32:37.000
I've posted the certificate

32:37.000 --> 32:39.000
in the chat room.

32:39.000 --> 32:41.000
When you receive the certificate,

32:41.000 --> 32:43.000
you need to use

32:43.000 --> 32:45.000
a different code.

32:45.000 --> 32:47.000
The code is also in the chat room.

32:47.000 --> 32:49.000
Today's code is

32:49.000 --> 32:51.000
orcaatiger425.

32:51.000 --> 32:53.000
Terra also

32:53.000 --> 32:55.000
has a Slack channel.

32:55.000 --> 32:57.000
You can join it

32:57.000 --> 32:59.000
and get to know

32:59.000 --> 33:01.000
other friends

33:01.000 --> 33:03.000
and activities

33:03.000 --> 33:05.000
related to

33:05.000 --> 33:07.000
academia.

33:07.000 --> 33:09.000
You can also

33:09.000 --> 33:11.000
join the Slack space

33:11.000 --> 33:13.000
of Project Terra.

33:13.000 --> 33:15.000
The link is in the chat room.

33:15.000 --> 33:17.000
Lastly,

33:17.000 --> 33:19.000
if you like today's talk

33:19.000 --> 33:21.000
or if there's anything

33:21.000 --> 33:23.000
we can improve,

33:23.000 --> 33:25.000
there's a link for

33:25.000 --> 33:27.000
audience feedback.

33:27.000 --> 33:29.000
You can fill it in.

33:29.000 --> 33:31.000
We'll do our best

33:31.000 --> 33:33.000
to make it better.

33:33.000 --> 33:35.000
Now,

33:35.000 --> 33:37.000
do you have

33:37.000 --> 33:39.000
any questions?

33:39.000 --> 33:41.000
Hello.

33:41.000 --> 33:43.000
Can you hear me?

33:43.000 --> 33:45.000
Yes.

33:45.000 --> 33:47.000
I'm a student

33:47.000 --> 33:49.000
in a math class.

33:49.000 --> 33:51.000
My background is not in humanities,

33:51.000 --> 33:53.000
so I don't understand much.

33:53.000 --> 33:55.000
I'd like to ask

33:55.000 --> 33:57.000
if I'm interested

33:57.000 --> 33:59.000
in the math model.

33:59.000 --> 34:01.000
I'd like to know

34:01.000 --> 34:03.000
the meaning of the parameters.

34:03.000 --> 34:05.000
Okay.

34:07.000 --> 34:09.000
I'd like to know

34:09.000 --> 34:11.000
if MJ and EJ

34:11.000 --> 34:13.000
refer to the

34:13.000 --> 34:15.000
recipient's income.

34:15.000 --> 34:17.000
MJ is

34:17.000 --> 34:19.000
the salary of the recipient.

34:19.000 --> 34:21.000
EJ is

34:21.000 --> 34:23.000
his tasks.

34:23.000 --> 34:25.000
These two are his income.

34:27.000 --> 34:29.000
I'd like to know

34:29.000 --> 34:31.000
why the recipient

34:31.000 --> 34:33.000
uses this way of expression

34:33.000 --> 34:35.000
but the recipient doesn't

34:35.000 --> 34:37.000
use this way of setting.

34:37.000 --> 34:39.000
He only uses his income

34:39.000 --> 34:41.000
as input,

34:41.000 --> 34:43.000
not his efforts.

34:43.000 --> 34:45.000
Okay.

34:49.000 --> 34:51.000
Let me think.

34:53.000 --> 34:55.000
The main problem is

34:55.000 --> 34:57.000
sometimes people...

34:57.000 --> 34:59.000
Oh, sorry. You go first.

34:59.000 --> 35:01.000
Sorry, you go first.

35:01.000 --> 35:03.000
You go first.

35:03.000 --> 35:05.000
Okay.

35:05.000 --> 35:07.000
I think

35:07.000 --> 35:09.000
when I set it,

35:09.000 --> 35:11.000
I'd consider the difference

35:11.000 --> 35:13.000
in the degree of efforts of two people.

35:13.000 --> 35:15.000
Water usually flows down.

35:15.000 --> 35:17.000
So when I find

35:17.000 --> 35:19.000
the difference between

35:19.000 --> 35:21.000
their efforts and mine,

35:21.000 --> 35:23.000
I might not

35:23.000 --> 35:25.000
give him that much.

35:25.000 --> 35:27.000
At that time,

35:27.000 --> 35:29.000
my data function

35:29.000 --> 35:31.000
might be the difference of efforts.

35:31.000 --> 35:33.000
If I consider this,

35:33.000 --> 35:35.000
I don't know if your model

35:35.000 --> 35:37.000
or your prediction

35:37.000 --> 35:39.000
will be different.

35:39.000 --> 35:41.000
I think it's a good question.

35:41.000 --> 35:43.000
Actually,

35:43.000 --> 35:45.000
I thought about it when I analyzed it.

35:47.000 --> 35:49.000
Of course,

35:49.000 --> 35:51.000
you can modify the model.

35:51.000 --> 35:53.000
I think

35:53.000 --> 35:55.000
the decision-maker's income

35:55.000 --> 35:57.000
doesn't matter

35:57.000 --> 35:59.000
whether you set it as

35:59.000 --> 36:01.000
mi, ei or yi.

36:01.000 --> 36:03.000
It won't make any difference.

36:03.000 --> 36:05.000
But there is a place you can modify.

36:05.000 --> 36:07.000
For example,

36:07.000 --> 36:09.000
we assume that

36:09.000 --> 36:11.000
the relationship of the line

36:11.000 --> 36:13.000
is theta0 plus beta

36:13.000 --> 36:15.000
times the degree of efforts

36:15.000 --> 36:17.000
of the recipient.

36:17.000 --> 36:19.000
According to your theory,

36:19.000 --> 36:21.000
the function of theta

36:21.000 --> 36:23.000
is actually the model.

36:23.000 --> 36:25.000
We assume that

36:25.000 --> 36:27.000
it is a theta0,

36:27.000 --> 36:29.000
which is the baseline,

36:29.000 --> 36:31.000
plus beta times

36:31.000 --> 36:33.000
ez minus ei,

36:33.000 --> 36:35.000
which is the degree of difference

36:35.000 --> 36:37.000
between the two efforts.

36:37.000 --> 36:39.000
I think it's possible.

36:39.000 --> 36:41.000
It's a very good question.

36:41.000 --> 36:43.000
I remember that

36:43.000 --> 36:45.000
we didn't find

36:45.000 --> 36:47.000
such an effect in our analysis.

36:47.000 --> 36:49.000
I remember that

36:49.000 --> 36:51.000
we didn't find

36:51.000 --> 36:53.000
such an effect in our analysis.

36:53.000 --> 36:55.000
I remember that

36:55.000 --> 36:57.000
we didn't find

36:57.000 --> 36:59.000
such an effect in our analysis.

36:59.000 --> 37:01.000
I remember that

37:01.000 --> 37:03.000
we didn't find

37:03.000 --> 37:05.000
such an effect in our analysis.

37:05.000 --> 37:07.000
I see.

37:07.000 --> 37:09.000
So the decision-maker's income

37:09.000 --> 37:11.000
doesn't matter

37:11.000 --> 37:13.000
whether you set it as mi, ei

37:13.000 --> 37:15.000
or yi.

37:15.000 --> 37:17.000
You mean here?

37:17.000 --> 37:19.000
No, I don't mean that.

37:19.000 --> 37:21.000
You set it as mi, ei

37:21.000 --> 37:23.000
or yi

37:23.000 --> 37:25.000
in the income.

37:25.000 --> 37:27.000
You set it as mi, ei or yi

37:27.000 --> 37:29.000
in the income.

37:29.000 --> 37:31.000
You're right.

37:31.000 --> 37:33.000
But what you say is

37:33.000 --> 37:35.000
mainly about the C-harm.

37:35.000 --> 37:37.000
You can see

37:37.000 --> 37:39.000
how much money

37:39.000 --> 37:41.000
the decision-maker

37:41.000 --> 37:43.000
will give to the recipient.

37:43.000 --> 37:45.000
For example,

37:45.000 --> 37:47.000
if you set mi as mi and ei,

37:47.000 --> 37:49.000
the effect of dividing

37:49.000 --> 37:51.000
mi and ei is the same.

37:51.000 --> 37:53.000
The effect of dividing mi and ei is the same.

37:53.000 --> 37:55.000
The higher the income of the decision-maker,

37:55.000 --> 37:57.000
the more money he will give.

37:57.000 --> 37:59.000
the more money he will give.

37:59.000 --> 38:01.000
So there's no point

38:01.000 --> 38:03.000
in separating them.

38:03.000 --> 38:05.000
But what you said is about the C-harm.

38:05.000 --> 38:07.000
But what you said is about the C-harm.

38:07.000 --> 38:09.000
I see.

38:09.000 --> 38:11.000
Thank you.

38:13.000 --> 38:15.000
Thank you.

38:15.000 --> 38:17.000
Thank you.

38:17.000 --> 38:19.000
Okay.

38:19.000 --> 38:21.000
Do you have any other questions?

38:21.000 --> 38:23.000
Okay.

38:23.000 --> 38:25.000
If you have no other questions,

38:25.000 --> 38:27.000
If you have no other questions,

38:27.000 --> 38:29.000
I'll turn off the recording.

38:29.000 --> 38:31.000
I'll turn off the recording.

