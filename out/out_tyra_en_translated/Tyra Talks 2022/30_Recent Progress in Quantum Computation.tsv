start	end	text
0	3000	Thank you, Yen-Yung.
3000	9000	We have talked about how to join Project Terra.
9000	14000	Today, we are going to start the lecture.
14000	17000	Before we start, let me introduce the speaker.
17000	23000	Today's speaker is Professor Hsieh Ming-Hsiu from Sydney University of Science and Technology.
23000	28000	Professor Hsieh is also a senior at NTU.
28000	34000	In 2008, he got his Ph.D. from the University of Sydney.
34000	39000	He has worked in Japan and Cambridge.
39000	51000	His research direction from Ph.D. to now is related to quantum computer and quantum information.
51000	56000	His more detailed direction is related to quantum information.
56000	68000	From the beginning of error correction to focus on quantum machine learning and different projects.
68000	75000	Next, Professor Hsieh will introduce his research.
75000	81000	By the way, I would like to promote that NTU will have a team this year.
81000	88000	I hope we can work on quantum computing together.
88000	94000	If you are interested, I believe Professor Hsieh can introduce the team later.
94000	97000	There are electrical engineering and physics.
97000	100000	I think this is a big project.
101000	106000	I hope Professor Hsieh can introduce the team later.
106000	111000	I believe this project is good for Taiwan's research.
111000	114000	Professor Hsieh, you can start.
114000	118000	Hello everyone, my name is Hsieh.
118000	121000	Thank you, Hsieh Ming-Hsiu.
121000	126000	I am very happy to share with you today.
126000	135000	The Minister of Science and Technology has a SOC in NTU.
135000	137000	System on Chip.
137000	140000	They have a front page.
140000	146000	They will find some front page topics every year.
146000	150000	This year they chose quantum computation.
150000	153000	They invited me to give a speech.
153000	156000	Ted just mentioned that.
156000	163000	Now Taiwan is starting to pay attention to quantum computation.
163000	168000	Hardware or software development.
168000	172000	The Ministry of Science and Technology has started to invest.
172000	175000	Of course, there are many in Taiwan.
175000	178000	But there are very few overseas.
178000	182000	This year, there are about 7,000 to 7,500 million.
182000	191000	Now they have set up a center for quantum technology in NTU.
191000	193000	It's a five-year plan.
193000	195000	There are 50 million in a year.
195000	197000	The Ministry of Science and Technology has 25 million.
197000	200000	The Ministry of Education has 25 million.
200000	205000	I am also in charge of the International Advisory Board.
206000	211000	After Taiwan started to invest in this place.
211000	213000	There are many opportunities.
213000	216000	If you happen to be in the field.
216000	220000	In fact, there are many people in this field.
220000	223000	Because quantum computing is a very cross-field.
223000	225000	You can also find it in my talk later.
225000	227000	It also has a lot to do with CS.
227000	230000	It also needs to do these things now.
230000	234000	For example, quantum control needs to be micro-wave controlled.
234000	237000	So traditional micro-wave people can also come in.
237000	242000	Then look at what your process is.
242000	244000	Semiconductor talents can also come in.
244000	246000	Superconductor talents can also come in.
246000	249000	These classic physics people can also come in.
249000	255000	So no matter what field you are.
255000	258000	Engineering, science, or electronics.
258000	262000	As long as you are interested in quantum computing.
262000	264000	You can come in very quickly.
264000	269000	In fact, I also studied a master's degree in Taiwan University.
269000	270000	It's a telecommunications company.
270000	273000	Then I went to Nanyang University to study for a doctorate.
273000	276000	I just switched to quantum error correction.
276000	279000	Then slowly do some information computation.
279000	283000	Then I did a lot of other things.
283000	288000	So what is it like to cross-field with students from science and engineering colleges?
293000	296000	Some background introduction.
296000	297000	Then there's my talk.
297000	303000	Because I mainly prepared for the SOC workshop.
303000	304000	It's only 40 minutes.
304000	308000	So I probably only have 40 minutes of material.
308000	311000	Then you can use Q&A later.
311000	315000	See what kind of questions you have about this direction.
315000	317000	You can bring it up.
317000	320000	Of course, in the middle of my report later.
321000	325000	You can also bring up any questions you have.
325000	328000	Let me remind you.
328000	329000	Sorry.
329000	332000	Because we have a lot of participants today.
332000	333000	So we have a prediction.
333000	337000	The microphone is off when everyone enters.
337000	339000	So if you want to ask a question.
339000	342000	Please click on the microphone icon above.
342000	347000	This way you won't feel ignored.
347000	351000	We usually welcome questions at any time.
351000	354000	But I'm afraid there's a lot of responses today.
354000	355000	So when you ask a question.
355000	358000	Please click on the icon.
358000	361000	Okay, let's get started.
361000	366000	Okay, I'll continue my report.
366000	370000	This is my title page.
370000	374000	I mainly want to introduce some.
374000	376000	Because we know that machine learning.
376000	379000	Now it's a very popular topic.
379000	382000	Quantum Computation is another popular topic.
382000	385000	Quantum Computation.
385000	389000	Basically, everyone has a vague idea about it.
389000	391000	That is, it can be used to do some.
391000	394000	High-performance operations.
394000	396000	So, uh.
396000	397000	So, uh.
397000	399000	About three or five years ago.
399000	400000	In fact, earlier.
400000	402000	Someone started to notice that.
402000	404000	How to use quantum computers.
404000	407000	To do some machine learning tasks.
407000	410000	But it was said that the early development was slower.
410000	412000	Because everyone said.
412000	414000	Especially in academia.
414000	416000	People will think that.
416000	420000	Your effect on your quantum mechanics.
420000	423000	Then your quantum computer.
423000	425000	The power of this computation.
425000	427000	Your definitions are not very clear yet.
427000	429000	To do this quantum machine learning.
429000	432000	It seems to be a bit, uh.
432000	434000	A little bit of this.
434000	437000	Uh, it's a little unrealistic.
437000	439000	So, uh.
439000	441000	So, uh.
441000	443000	Before 2010.
443000	445000	That is, in this quantum machine learning topic.
445000	447000	In fact, very few people do it.
447000	449000	Then, uh.
449000	451000	The main results are also relatively limited.
451000	453000	So, uh.
453000	455000	Until the last three or five years.
455000	457000	That is, everyone has been from this.
457000	459000	Uh, this technology.
459000	461000	In this news.
461000	463000	For general public news.
463000	465000	You also find that.
465000	467000	Uh, all the big companies have invested.
467000	469000	Google, IBM.
469000	471000	Microsoft, Intel.
471000	474000	Especially like Intel, IBM, Google.
474000	476000	They all have more than 50 qubits of machine.
476000	478000	That is to say, their, this.
478000	480000	Highway development can already be 50 qubits.
480000	482000	In fact, you can already do a little bit.
482000	483000	A little something like this.
483000	485000	So, uh.
485000	488000	In the academic world, everyone started to go back and say.
488000	491000	In fact, quantum computers may still be used.
491000	492000	To do some.
492000	494000	Uh, the operation of this big data.
494000	496000	So that's why this quantum machine learning.
496000	498000	In the last few years.
498000	500000	Very, uh.
500000	501000	Very hot.
501000	503000	So, uh.
503000	505000	Uh, so in 2017.
505000	507000	In Nature.
507000	509000	Uh, this journal, it has.
509000	511000	Two reviewed articles.
511000	513000	That is, two of the same.
513000	515000	Uh, the same, uh.
515000	517000	The same, uh, the same value.
517000	519000	And then it just happened to be the first two.
519000	521000	Uh, it's all in the review of this quantum machine.
521000	523000	Uh, uh, uh.
523000	525000	What about me?
525000	527000	This slide is from.
527000	529000	One of the review articles.
529000	531000	It was taken out, it was by Monta, this person.
531000	533000	Uh, it's written like this.
533000	535000	He actually said.
535000	537000	He did some summary, which means he took it to.
537000	539000	Before 2017, that is to say.
539000	541000	Uh, which one.
541000	543000	You can do it with quantum computation.
543000	545000	And then?
545000	547000	What kind of acceleration does it have?
547000	549000	Uh, on the far left.
549000	551000	He's actually just.
551000	553000	It's just, uh, it's classification.
553000	555000	What are the problems like this?
555000	557000	For example, he has that dangerous inference.
557000	559000	Uh.
559000	561000	Uh, let's say perceptron perceptron.
561000	563000	It's like classification.
563000	565000	Uh, let's say.
565000	567000	The left and right, or if it's linear.
567000	569000	That's what you're going to do.
569000	571000	Uh, and then.
571000	573000	Uh.
573000	575000	The third is fitting.
575000	577000	It's similar to what we used to have in curve fitting.
577000	579000	You'll see your curve is probably.
579000	581000	If it's a very complicated curve.
581000	583000	For example, you can use linearity or sine wave to.
583000	585000	To fit this curve.
585000	587000	Uh, then.
587000	589000	Uh, the fourth is the Boltzmann machine.
589000	591000	We're probably just saying this is more statistical.
591000	593000	Uh.
593000	595000	That's classical Boltzmann machine.
595000	597000	Of course, there's a corresponding quantum Boltzmann machine.
597000	599000	Uh, that Boltzmann machine is also this.
599000	601000	Uh, uh.
601000	603000	Uh, similar to neural network.
603000	605000	It's actually a very similar concept.
605000	607000	And then there's PCA.
607000	609000	Uh, Principle Component Analysis.
609000	611000	Uh, this is basically.
611000	613000	PCA is also a machine learning.
613000	615000	Uh, one.
615000	617000	Uh, a very weird study.
617000	619000	A problem like this.
619000	621000	You have a covariance matrix.
621000	623000	Then you have to put it in a right angle.
623000	625000	After that, you just need to focus on.
625000	627000	A relatively large value.
627000	629000	So they're called, uh.
629000	631000	Principle component.
631000	633000	The biggest one, the biggest one, the biggest one.
633000	635000	It means that it affects your distribution.
635000	637000	Is relatively large.
637000	639000	So you cut off some of the smaller ones.
639000	641000	Uh, the value.
641000	643000	Uh, this is what classical PCA is doing.
643000	645000	Uh, uh, and then there's SVM.
645000	647000	This is actually called a vector.
647000	649000	Uh, supporting vector.
649000	651000	Basically, it's also part of this.
651000	653000	Uh, classification problem.
653000	655000	Um, the second column?
655000	657000	The second row on the left is to say that this is the acceleration on the value.
657000	659000	Uh, uh, uh.
659000	661000	Uh, it means, uh,
661000	663000	for example, uh,
663000	665000	Perceptron, it has this, uh,
665000	667000	Square root, it's called the acceleration of square root n.
667000	669000	It's your sample size.
669000	671000	If you have a point, you're going to do the classification.
671000	673000	Then use the value, it will have one.
673000	675000	Probably, uh,
675000	677000	n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n-n
677000	679000	So there's this acceleration.
679000	681000	Some of the questions it has a value acceleration.
681000	683000	Log n is the value of the value of the value.
683000	685000	And then, uh,
685000	687000	You see some signals in this place.
687000	689000	In fact, this signal is
689000	691000	It's actually these accelerations.
691000	693000	There are some conditions.
693000	695000	It has some assumptions, so it's not
695000	697000	If you're going to
697000	699000	Uh, from this classical
699000	701000	To the classical
701000	703000	Its acceleration will not
703000	705000	To the value of the value
705000	707000	It's just that, uh, my classical
707000	709000	Data, I have to map it first.
709000	711000	Map to a quantum state
711000	713000	Quantum state is your quantum
713000	715000	A type that the computer can handle
715000	717000	Is, uh, your input
717000	719000	To a quantum machine
719000	721000	You must have one, just say this
721000	723000	An input that the machine can accept
723000	725000	That's it, so you're from classical
725000	727000	Data to map to this quantum data
727000	729000	Where is this place
729000	731000	In fact, there is a, uh
731000	733000	There is a trade-off
733000	735000	It's basically impossible to do very well now
735000	737000	Because you can imagine
737000	739000	The classical data, I must have every data
739000	741000	I have to read it once, so you can
741000	743000	Map it over, right?
743000	745000	Uh, so
745000	747000	In this place, you must be
747000	749000	With your data size
749000	751000	There is already a proportional relationship
751000	753000	What about these accelerations
753000	755000	It's usually just looking at
755000	757000	I already have a preprocessing
757000	759000	Map it to the quantum state
759000	761000	And then it can
761000	763000	What about the calculation of these quantities
763000	765000	It has this index acceleration
765000	767000	This is what this signal means
767000	769000	That's it
769000	771000	But if there is no signal, this is
771000	773000	Hey, uh, it's
773000	775000	Overall, I can have an index
775000	777000	The acceleration of this square root n
777000	779000	Okay, what about this a
779000	781000	It's actually a call
781000	783000	This algorithm
783000	785000	Then I will introduce it later
785000	787000	It's actually a global
787000	789000	Uh, a more advanced version of global search
789000	791000	Then I will introduce
791000	793000	Global search
793000	795000	Then this place means y
795000	797000	That is to say, in these methods
797000	799000	It actually uses this
799000	801000	Amplitude amplification or global
801000	803000	Search like this
803000	805000	Then the other one is hhl
805000	807000	It's a quantum
807000	809000	Very famous algorithm now
809000	811000	I'll introduce it later. It's actually used to solve the linear equation.
811000	813000	If you say
813000	815000	Quantum computation
815000	817000	Quantum algorithm to solve the linear equation
817000	819000	It will have an index acceleration
819000	821000	Then we know that machine learning
821000	823000	Uh, a lot of problems
823000	825000	You can map it to the solution
825000	827000	So you can do this
827000	829000	You can just use this hhl algorithm
829000	831000	To solve
831000	833000	I'm not familiar with this piece
833000	835000	So I didn't go to
835000	837000	Go to
837000	839000	I don't explain much
839000	841000	That's what I just said
841000	843000	You have to put classical data
843000	845000	Map to quantum state
845000	847000	Basically you need an encoding step
847000	849000	Then this is what qrem is doing
849000	851000	N means
851000	853000	This method, for example, the first
853000	855000	Bayesian inference, it doesn't use
855000	857000	This qrem place
857000	859000	Because qrem is actually a
859000	861000	It's a more complicated operation
861000	863000	So you can jump as much as you can
863000	865000	You can try not to use it
865000	867000	Don't use it
867000	869000	Ok
869000	871000	Is there a problem?
871000	873000	No problem, I'll go down
873000	875000	Introduce this
875000	877000	From here
877000	879000	I will introduce some quantum algorithms
879000	881000	Why
881000	883000	Why is there an index acceleration?
883000	885000	Basically because there are these
885000	887000	These are more basic
887000	889000	This quantum algorithm
889000	891000	As your
891000	893000	This
893000	895000	From
895000	897000	This
897000	899000	The more complicated algorithm
899000	901000	Then do more complicated things
901000	903000	Then I introduced these
903000	905000	These are more fundamental
905000	907000	There are a few
907000	909000	Basically you are going to read these textbooks
909000	911000	There are already introductions in the textbooks
911000	913000	I basically know here
913000	915000	Everyone is not doing this
915000	917000	Quantum information or computation
917000	919000	So I basically only talk about the results
919000	921000	Everyone can basically have an impression
921000	923000	The first one is Fourier transform
923000	925000	Why is quantum computation
925000	927000	It's so useful because
927000	929000	Quantum
929000	931000	You can let you go
931000	933000	Do
933000	935000	Do the action of superposition
935000	937000	Your state
937000	939000	Your state is
939000	941000	For example, we are 0 1, right?
941000	943000	Quantum state is 0 1
943000	945000	Superposition
945000	947000	What about this superposition
947000	949000	That is, you can use parallel operations
949000	951000	You can
951000	953000	You can do a 0 operation
953000	955000	Do a 1 operation
955000	957000	Then correspond to it
957000	959000	Its superposition will correspond to
959000	961000	Your operation will superposition
961000	963000	Quantum mechanics is a linear
963000	965000	Linear
965000	967000	What about this place
967000	969000	So why
969000	971000	Quantum computer
971000	973000	Why is it so effective
973000	975000	Because of this quantum Fourier transform
975000	977000	If we talk about science students
977000	979000	Everyone knows what Fourier transform is doing
979000	981000	That is, he wants to put a state
981000	983000	Plus this
983000	985000	Plus this face
985000	987000	This is called Fourier coefficient
987000	989000	This omega n xk
989000	991000	If you say there are n bits
991000	993000	Then your capital n
993000	995000	Is the square root of 2n
995000	997000	This is what you do to n bits
997000	999000	A Fourier transform
999000	1001000	Then my notation is actually
1001000	1003000	Quantum notation
1003000	1005000	Basically this x
1005000	1007000	There is a
1007000	1009000	This place
1009000	1011000	This place
1011000	1013000	It's actually a quantum state
1013000	1015000	You can think of it as a vector
1015000	1017000	This is a
1017000	1019000	That is to say there are 2n
1019000	1021000	One
1021000	1023000	A vector element that is a
1023000	1025000	This technical detail
1025000	1027000	It doesn't matter if you don't need to know
1027000	1029000	Then
1029000	1031000	I can't introduce this Fourier transform
1031000	1033000	The introduction is too detailed, I just want to tell you
1033000	1035000	What is the difference between it and classical
1035000	1037000	Our science students
1037000	1039000	Everyone knows that classical Fourier transform
1039000	1041000	How many operations do you need
1041000	1043000	If you say you have n
1043000	1045000	n bits
1045000	1047000	The operation you need is n
1047000	1049000	Multiplied by the value of 2
1049000	1051000	n square
1051000	1053000	So it is
1053000	1055000	It's not just it's polynomial
1055000	1057000	In number of bits
1057000	1059000	It's exponential in number of bits
1059000	1061000	This is what classical Fourier transform needs
1061000	1063000	It's n times 2n
1063000	1065000	So many operations
1065000	1067000	You need so many operations
1067000	1069000	But in quantum Fourier transform
1069000	1071000	You only need
1071000	1073000	n square
1073000	1075000	That is to say, n is your number of bits
1075000	1077000	So you have a place
1077000	1079000	Uh, uh, the acceleration of the index
1079000	1081000	Uh, exponential speed
1081000	1083000	Uh, exponential speed
1083000	1085000	That
1085000	1087000	Uh, the reason for this
1087000	1089000	Of course, it just happens to be this operation
1089000	1091000	Can be divided into this bit
1091000	1093000	Single qubit operation and this two qubit operation
1093000	1095000	Then you add it up
1095000	1097000	That is to say, its operation
1097000	1099000	You can decompose it into this elementary quantum gate
1099000	1101000	Then it just needs n
1101000	1103000	n square and so many
1103000	1105000	Yeah, if you're interested in this
1105000	1107000	You can go to Wikipedia
1107000	1109000	It's also very simple
1109000	1111000	If you have a background in physics
1111000	1113000	It's easy to understand
1113000	1115000	Uh, so this place has an index
1115000	1117000	So your algorithm if you need
1117000	1119000	If you use quantum Fourier transform
1119000	1121000	You just have a big acceleration
1121000	1123000	And then this
1123000	1125000	Uh, and then you
1125000	1127000	This, uh, quantum Fourier transform
1127000	1129000	It will be used in some
1129000	1131000	More complicated this is called phase estimation
1131000	1133000	In this algorithm
1133000	1135000	I will introduce phase in a moment
1135000	1137000	I will introduce phase estimation in the next slide
1137000	1139000	Because phase estimation is a very important algorithm
1139000	1141000	Okay, what is phase estimation doing?
1141000	1143000	What is phase estimation doing?
1143000	1145000	It is to estimate your eigenvalue
1145000	1147000	That is to say, suppose I have a unitary
1147000	1149000	U, and then
1149000	1151000	I have one, uh, this vector
1151000	1153000	This is the eigenvector of U
1153000	1155000	Psi, right?
1155000	1157000	Then we know that your U
1157000	1159000	Multiply by your eigenvector
1159000	1161000	You will take your eigenvalue
1161000	1163000	It will be equal to the eigenvalue multiplied by your eigenvector
1163000	1165000	Right?
1165000	1167000	Then this phase estimation is to estimate
1167000	1169000	Because of your eigenvalue
1169000	1171000	Because it is unitary, it is only related to this theta
1171000	1173000	Right? So we're going to estimate this theta
1173000	1175000	Right? So we're going to estimate this theta
1175000	1177000	Uh, what about this?
1177000	1179000	What about this estimation?
1179000	1181000	That is to say, this place is also
1181000	1183000	That is, uh, this take-home message is like this
1183000	1185000	Uh, there is a quantum algorithm
1185000	1187000	Uh, there is a quantum algorithm
1187000	1189000	It can estimate your eigenvalue
1189000	1191000	Can estimate your eigenvalue
1191000	1193000	It's very efficient
1193000	1195000	It's very efficient
1195000	1197000	How about this?
1197000	1199000	How about this?
1199000	1201000	It can use all 1 over
1201000	1203000	It can use all 1 over
1203000	1205000	It can use all 1 over
1205000	1209000	If you say, if your requirement is
1209000	1211000	If you say, if your requirement is
1211000	1213000	It just needs all 1 over
1213000	1215000	to calculate this eigenvalue
1215000	1217000	Then we know
1217000	1219000	In fact, many later algorithms
1219000	1221000	In fact, many later algorithms
1221000	1223000	The most important part of it is
1223000	1225000	I have a matrix
1225000	1227000	Because quantum mechanics
1227000	1229000	You can use this matrix to express
1229000	1231000	You can use this matrix to express
1231000	1233000	A matrix
1233000	1235000	What do you need to focus on?
1235000	1237000	Just the place where its eigenvalue is large
1237000	1239000	Just the place where its eigenvalue is large
1239000	1241000	So we can use some
1241000	1243000	Fastest method
1243000	1245000	Make some changes
1245000	1247000	Then let it just estimate this
1247000	1249000	Bigger eigenvalue
1249000	1251000	Then you can
1251000	1253000	Smaller eigenvalue
1253000	1255000	Truncate it
1255000	1257000	Then this truncate
1257000	1259000	It basically doesn't affect
1259000	1261000	Because if you look at its distance
1261000	1263000	For example, trace distance
1263000	1265000	It's basically just comparing its eigenvalue
1265000	1267000	So if you truncate a smaller eigenvalue
1267000	1269000	So if you truncate a smaller eigenvalue
1269000	1271000	And then
1271000	1273000	Like this Fastest method
1273000	1275000	It uses, for example, I just mentioned
1275000	1277000	Hhl is the solution
1277000	1279000	Linear equation algorithm
1279000	1281000	It should be the most famous
1281000	1283000	It should be the most famous
1283000	1285000	It can be used to do
1285000	1287000	The value factor is highly efficient
1289000	1291000	It can be used to do
1291000	1293000	The value factor is highly efficient
1293000	1295000	It can
1295000	1297000	It can
1297000	1299000	It's so effective
1299000	1301000	That's it
1301000	1303000	That's it
1303000	1305000	That's it
1305000	1307000	That's it
1309000	1311000	Here
1311000	1313000	Here
1313000	1315000	Here
1315000	1317000	This
1317000	1319000	This
1319000	1321000	This
1321000	1323000	This
1323000	1325000	This
1325000	1327000	This place is in classical
1327000	1329000	Actually there is no way to avoid
1329000	1331000	We know that we used to be on
1331000	1333000	It should be high school
1333000	1335000	Is the Gauss elimination method
1335000	1337000	I have n variables n equations
1337000	1339000	I just delete one variable first
1339000	1341000	Then slowly delete all the variables
1341000	1343000	Until the last variable and then push it back
1343000	1345000	This is the Gauss elimination method
1345000	1347000	Then, but in quantum
1347000	1349000	If you say this x is a quantum state
1349000	1351000	It already has superposition to help you
1351000	1353000	Then v is also a quantum state
1353000	1355000	To do this
1355000	1357000	You only need to log n
1357000	1359000	n is the size of your matrix
1359000	1361000	The matrix of this a
1361000	1363000	Is n by n
1363000	1365000	Then you only need to log n so much
1365000	1367000	Then we know that the Gauss elimination method
1367000	1369000	You probably need n squared
1369000	1371000	Steps like this
1371000	1373000	Then this
1373000	1375000	This place is actually in major
1375000	1377000	This matrix a is
1377000	1379000	How regular
1379000	1381000	How should it be said
1381000	1383000	The biggest eigenvalue and the smallest eigenvalue
1383000	1385000	You don't want it to be too different
1385000	1387000	So this kappa is a bit
1387000	1389000	This linear equation
1389000	1391000	In the end
1391000	1393000	His
1393000	1395000	What is his behavior
1395000	1397000	So this is actually a technical condition
1397000	1399000	It doesn't matter if you don't care first
1399000	1401000	Everyone basically only compares this log n
1401000	1403000	Then
1403000	1405000	In the best classic algorithm
1405000	1407000	In fact, it is n
1407000	1409000	If your matrix is n by n
1409000	1411000	You don't need to do so many operations
1411000	1413000	Basically with the Gauss elimination method
1413000	1415000	A little better than the Gauss elimination method
1415000	1417000	This is actually a teacher at MIT
1417000	1419000	Aaron Harrow
1419000	1421000	They are all at MIT
1421000	1423000	They proposed it in 2009
1423000	1425000	You see, it's less than 10 years
1425000	1427000	This breakthrough within 10 years
1427000	1429000	This article until now
1429000	1431000	It seems that I have almost 600 citations
1431000	1433000	It's because of the quantum algorithm
1433000	1435000	It's all up
1435000	1437000	Then this machine learning also got up
1437000	1439000	A lot of this step
1439000	1441000	This matrix
1441000	1443000	Inversion step, so now citation
1443000	1445000	This paper is very high
1445000	1447000	So it's the same
1447000	1449000	It's like this
1449000	1451000	Linear equation
1451000	1453000	The quantum algorithm has an index acceleration
1453000	1455000	That's it
1455000	1457000	Okay, then
1457000	1459000	The other one is that in addition to HHL
1459000	1461000	The other one is most commonly used in machine learning
1461000	1463000	Is global search
1463000	1465000	What is global search?
1465000	1467000	Then this data space
1467000	1469000	You don't need to have some structure
1469000	1471000	It's an unsorted search
1471000	1473000	Then use this global search
1473000	1475000	What about this quantum algorithm
1475000	1477000	It will have the acceleration of square root n
1477000	1479000	What about this square root n
1479000	1481000	n is your
1481000	1483000	This size of the database
1483000	1485000	Sorry
1485000	1487000	Then basically global search
1487000	1489000	What are you doing?
1489000	1491000	If my omega is an n-bit sequence
1491000	1493000	I wrote it into this vector form
1493000	1495000	It's like this
1499000	1501000	Then this
1501000	1503000	Every omega is a 0 1 bit
1503000	1505000	Every omega i
1505000	1507000	Is a 0 1 bit
1507000	1509000	Then I just want from this
1509000	1511000	All of this bit sequence
1511000	1513000	This ps below me is all of the bit sequence
1513000	1515000	The superposition
1515000	1517000	I want to identify from the inside
1517000	1519000	A particular
1519000	1521000	This omega
1521000	1523000	From a uniform superposition
1523000	1525000	I want to identify this omega
1525000	1527000	This is what global search is doing
1527000	1529000	That global is basically
1529000	1531000	Very simple
1531000	1533000	It uses two very elementary
1533000	1535000	This operation
1535000	1537000	This is basically
1537000	1539000	These two are reflection
1539000	1541000	Is to reverse
1541000	1543000	I have a picture below
1543000	1545000	That is
1545000	1547000	It is a
1547000	1549000	Geometry picture like this
1549000	1551000	I use this picture to show
1551000	1553000	It's called omega
1553000	1555000	This y-axis
1555000	1557000	This omega is what I want to estimate
1557000	1559000	What about this s
1559000	1561000	Is all of the state
1561000	1563000	Uniform superposition
1563000	1565000	Because omega is also in this s
1565000	1567000	So
1567000	1569000	Sorry
1569000	1571000	There is a call
1573000	1575000	Because omega is also in this s
1575000	1577000	An element
1577000	1579000	This omega is positive
1579000	1581000	But you can
1581000	1583000	It's positive with this omega
1583000	1585000	This
1595000	1597000	OK
1597000	1599000	That
1599000	1601000	What about the two operations just now
1601000	1603000	One is US, one is UW
1603000	1605000	That is, there are two operations in the previous slide
1605000	1607000	What are these two operations doing
1607000	1609000	That is, this UW
1609000	1611000	This operation is for this s
1611000	1613000	This one
1613000	1615000	This plane
1615000	1617000	It is s
1617000	1619000	You use this u
1619000	1621000	Omega this operation is used in s
1621000	1623000	It's actually s, this s, this place
1623000	1625000	Do a flip for s prime
1625000	1627000	So when you do the first step, it will flip here
1627000	1629000	The following gray line
1629000	1631000	Can everyone see it?
1631000	1633000	Should be right
1633000	1635000	I can see it
1635000	1637000	Because it's too quiet
1637000	1639000	I don't know if I can see it like this
1639000	1641000	OK
1641000	1643000	Do the first flip
1643000	1645000	It will become the gray line below
1645000	1647000	Right then
1647000	1649000	It's the second flip
1649000	1651000	Is to flip to s
1651000	1653000	So it's now this gray line
1653000	1655000	Will run to the top here
1655000	1657000	If you assume that at first
1657000	1659000	s and s prime
1659000	1661000	s and s prime
1661000	1663000	Is the angle of theta over 2
1663000	1665000	Then you do the first flip
1665000	1667000	You will be here below
1667000	1669000	Here is also theta over 2
1669000	1671000	Then you flip to s now
1671000	1673000	It will go up here
1673000	1675000	There is a theta angle, right?
1675000	1677000	That is to say, you did a round
1677000	1679000	That is to say, u omega and us
1679000	1681000	After doing a round
1681000	1683000	You actually put s this angle
1683000	1685000	s this s this vector
1685000	1687000	Up to the direction of omega
1687000	1689000	Then you just need it now
1689000	1691000	Your goal now
1691000	1693000	Is to let you repeat this step
1693000	1695000	Let the last
1695000	1697000	It can be close to omega
1697000	1699000	Right, you just let it keep flipping
1699000	1701000	To omega flip to where you want
1701000	1703000	Then you can search
1703000	1705000	Search to the value you want
1705000	1707000	This global
1707000	1709000	In fact, it is very interesting that he is not doing quantum information
1709000	1711000	He only has one
1711000	1713000	One to two pieces of quantum paper
1713000	1715000	He basically proposed this global search
1715000	1717000	Classical
1717000	1719000	He is a classic CS person
1719000	1721000	Then he mentioned this very interesting result
1721000	1723000	Then he is very famous now
1723000	1725000	This is a very fundamental
1725000	1727000	A quantum algorithm
1729000	1731000	Because this is a search algorithm
1731000	1733000	And it is your data space
1733000	1735000	No need to have
1735000	1737000	No need to have assumptions
1737000	1739000	So it is an unsorted search
1739000	1741000	So it is very useful
1741000	1743000	It is in many algorithms
1743000	1745000	This is a global search
1745000	1747000	Ok
1747000	1749000	Uh
1749000	1751000	Uh
1751000	1753000	I need to introduce one
1753000	1755000	Because the progress of quantum machine learning
1755000	1757000	Very fast
1757000	1759000	Then the progress of 2017
1759000	1761000	This review article
1761000	1763000	In fact, it has been a bit
1763000	1765000	There have been a lot of results in the past year
1765000	1767000	There is a very interesting result
1767000	1769000	Is the recommendation system
1769000	1771000	Uh
1771000	1773000	Recommendation system I think everyone knows
1773000	1775000	You do online shopping
1775000	1777000	Or you watch netflix
1777000	1779000	You watch netflix
1779000	1781000	After you choose
1781000	1783000	You can finally say
1783000	1785000	Give him a star
1785000	1787000	See if you like five stars
1787000	1789000	Or three stars
1789000	1791000	Do you like it
1791000	1793000	He has an algorithm
1793000	1795000	Will recommend you a similar movie
1795000	1797000	Online shopping is the same
1797000	1799000	What did you buy
1799000	1801000	Will recommend you something like
1801000	1803000	So this is the recommendation system
1803000	1805000	Uh system is doing
1805000	1807000	That uh
1807000	1809000	Recommendation system
1809000	1811000	You use
1811000	1813000	Describe it in math
1813000	1815000	Describe what it is
1815000	1817000	I have n users
1817000	1819000	This n
1819000	1821000	N
1821000	1823000	Every user
1823000	1825000	He has m things
1825000	1827000	You can choose, right
1827000	1829000	I have a movie, I have a database
1829000	1831000	Then every user can choose
1831000	1833000	All the movies in this database
1833000	1835000	Uh
1835000	1837000	You can put this thing
1837000	1839000	Think of it as a big evidence
1839000	1841000	This evidence is really big because
1841000	1843000	For example, I have a million movies
1843000	1845000	So my m is
1845000	1847000	1 million
1847000	1849000	If I say 1 million users
1849000	1851000	Then my evidence is 1 million times
1851000	1853000	Such a big size
1853000	1855000	Of course, usually
1855000	1857000	Most of the entries are empty
1857000	1859000	Because you can't say 1 million movies
1859000	1861000	All finished
1861000	1863000	So you can only watch
1863000	1865000	For example, 10,000 movies
1865000	1867000	So this rank is very important
1867000	1869000	Most of this matrix
1869000	1871000	Uh, the rank is very low
1871000	1873000	What about the recommendation system
1873000	1875000	What is this algorithm basically doing
1875000	1877000	He just wants to reconstruct
1877000	1879000	This matrix
1879000	1881000	I already chose some elements
1881000	1883000	Then I want to be on you
1883000	1885000	I want to reconstruct other places
1885000	1887000	This is what the recommendation system does
1887000	1889000	Then this place is very interesting
1889000	1891000	In 2017
1891000	1893000	What about us
1893000	1895000	Uh, in this quantum information
1895000	1897000	Quantum machine learning
1897000	1899000	Uh, where
1899000	1901000	They sent a big message
1901000	1903000	They have a algorithm now
1903000	1905000	Just need
1905000	1907000	Log m n so much
1907000	1909000	This is very impressive
1909000	1911000	Why
1911000	1913000	Because before this algorithm
1913000	1915000	All classic
1915000	1917000	Do recommendation system
1917000	1919000	Algorithm
1919000	1921000	All need at least
1921000	1923000	Polynomial
1923000	1925000	In m and n
1925000	1927000	So you can see this quantum algorithm
1927000	1929000	There is an increase in the number of points
1929000	1931000	Right, this recommendation system
1931000	1933000	Is a very important algorithm
1933000	1935000	Because all your online shopping
1935000	1937000	As long as it is related to online
1937000	1939000	Maybe this
1939000	1941000	Uh, this
1941000	1943000	What are they advertising
1943000	1945000	Also have this recommendation system
1945000	1947000	Build in
1947000	1949000	That is to say, everyone felt at that time
1949000	1951000	Quantum algorithm is very useful
1951000	1953000	Uh, but
1953000	1955000	This year in 2018
1955000	1957000	There is a student in Texas
1957000	1959000	It seems to be
1959000	1961000	Scott Aaronson
1961000	1963000	It's not a student
1963000	1965000	It's like a specialty
1965000	1967000	This kind of student
1967000	1969000	He should be only 18 years old
1969000	1971000	Uh
1971000	1973000	He just
1973000	1975000	He read this quantum algorithm paper
1975000	1977000	Found it
1977000	1979000	What about this quantum algorithm paper
1979000	1981000	In fact, this quantum algorithm paper
1981000	1983000	It's actually not entirely
1983000	1985000	Classic recommendation system
1985000	1987000	With classic recommendation system
1987000	1989000	Basically not doing the same thing
1989000	1991000	It's actually just estimating this
1991000	1993000	Uh, this is inside
1993000	1995000	The most likely
1995000	1997000	Entry
1997000	1999000	This low probability entry
1999000	2001000	Basically, you don't need to reconstruct
2001000	2003000	What about their quantum algorithm
2003000	2005000	Do it like this
2005000	2007000	Quantum algorithm
2007000	2009000	Can go to
2009000	2011000	These high probability values
2011000	2013000	Then the second
2013000	2015000	You don't need to reconstruct the whole
2015000	2017000	All elements
2017000	2019000	So this will have this index
2019000	2021000	That's it
2021000	2023000	This 18-year-old
2023000	2025000	He actually
2025000	2027000	What about this quantum idea
2027000	2029000	He found this
2029000	2031000	Sampling can be done
2031000	2033000	That is, in 2018, this is about a few months ago
2033000	2035000	Oh, no, it's July
2035000	2037000	Uh
2037000	2039000	He also uses a
2039000	2041000	Classic algorithm can do exactly the same
2041000	2043000	Uh, this
2043000	2045000	Efficiency
2045000	2047000	So in the recommendation system
2047000	2049000	This quantum algorithm is completely
2049000	2051000	No acceleration, it's the same as the classic algorithm
2051000	2053000	It's exactly the same
2053000	2055000	But this is another interesting place
2055000	2057000	That is, in this quantum information and quantum computation
2057000	2059000	What about
2059000	2061000	No matter what field you are in, you can come in
2061000	2063000	You have classical knowledge
2063000	2065000	You can take your classical knowledge
2065000	2067000	To improve your
2067000	2069000	Classical algorithm
2069000	2071000	Uh, and then you
2071000	2073000	You're starting on both sides
2073000	2075000	It's kind of like a competition
2075000	2077000	Keep pushing the algorithm on both sides
2077000	2079000	The efficiency keeps going up
2079000	2081000	This is a very interesting result
2081000	2083000	So good
2083000	2085000	Uh, and then
2085000	2087000	Uh, because I've been doing some of this lately
2087000	2089000	This aspect of work
2089000	2091000	So I took this opportunity
2091000	2093000	Uh, this opportunity
2093000	2095000	I also told you
2095000	2097000	I'm wearing
2101000	2103000	The place
2103000	2105000	What kind of questions did this place have just now
2105000	2107000	Next is that I want to introduce
2107000	2109000	My my
2109000	2111000	Some of the results on this quantum machine learning
2111000	2113000	Is there a problem
2113000	2115000	It seems like
2115000	2117000	Some people want to ask questions
2117000	2119000	But that one
2119000	2121000	There's a jump
2121000	2123000	Oh, really? I didn't notice
2123000	2125000	Good good
2125000	2127000	Now we're going to
2127000	2129000	Because it's another one
2129000	2131000	It's more like my topic
2131000	2133000	We can pause a little bit
2133000	2135000	Let everyone ask
2135000	2137000	These algorithms
2141000	2143000	Hello
2143000	2145000	My question is
2145000	2147000	That is
2147000	2149000	Maybe in the short term
2149000	2151000	We might just
2151000	2153000	Rely on dozens of qubits
2153000	2155000	Or I don't know
2155000	2157000	Something like 100 qubits
2157000	2159000	For this kind of really
2159000	2161000	Strong is really
2161000	2163000	These
2163000	2165000	For example, like recommendation system
2165000	2167000	This kind of thing
2167000	2169000	Is there a way to do this
2169000	2171000	Or a few hundred qubits
2171000	2173000	Test
2173000	2175000	Uh
2175000	2177000	Uh
2177000	2179000	This may not be
2179000	2181000	But your question is very good
2181000	2183000	Because my next report is basically
2183000	2185000	Focus on this question
2185000	2187000	So that's it
2187000	2189000	We
2189000	2191000	What I do with my students
2191000	2193000	Now this
2193000	2195000	Hardware development
2195000	2197000	Can do this
2197000	2199000	That is to say, we need this
2199000	2201000	The circuit size is actually very small
2201000	2203000	I'll give you one later
2203000	2205000	This is a more specific number
2205000	2207000	Good good
2207000	2209000	Thank you
2209000	2211000	Because of these
2211000	2213000	Most of the algorithms
2213000	2215000	All
2215000	2217000	I need a bigger scale
2217000	2219000	This quantum computer can do
2219000	2221000	A negative quantum computer
2221000	2223000	That is to say
2223000	2225000	Uh he can't
2225000	2227000	Just a small device
2227000	2229000	You can do these things
2229000	2231000	Right
2231000	2233000	I also want to ask you
2233000	2235000	Is the circuit size talking about the circuit depth?
2235000	2237000	You can do a few gates
2237000	2239000	This thing
2239000	2241000	There are still others
2241000	2243000	Or qubit size
2243000	2245000	Or circuit depth
2245000	2247000	I'll give you one later
2247000	2249000	My example is
2249000	2251000	You will see later
2251000	2253000	How many qubits do you need
2253000	2255000	How many gates do you need
2259000	2261000	Any other questions
2261000	2263000	Can you hear that
2263000	2265000	Can
2265000	2267000	Let me introduce myself
2267000	2269000	Sorry
2269000	2271000	I am that Guo Enrui
2271000	2273000	Now in
2273000	2275000	University of Maryland
2275000	2277000	Hello
2277000	2279000	Just like me
2279000	2281000	As far as I know
2281000	2283000	For example, like Shor's algorithm
2283000	2285000	It has exponential acceleration
2285000	2287000	Then like Gravel's search
2287000	2289000	At least
2289000	2291000	Even if
2291000	2293000	At least you can reach the square root of n
2293000	2295000	I want to ask
2295000	2297000	Now everyone is mainly doing quantum algorithms
2297000	2299000	Is studying quantum algorithms
2299000	2301000	In different
2301000	2303000	Different aspects
2303000	2305000	Expansion of different problems
2305000	2307000	That is, now everyone is mainly studying
2307000	2309000	In this
2309000	2311000	Part of the algorithm
2311000	2313000	Right algorithm
2313000	2315000	Actually from
2315000	2317000	From this 1980s
2317000	2319000	That one
2319000	2321000	Everyone started to think
2321000	2323000	Should it be possible to have a quantum computer
2323000	2325000	Do some
2325000	2327000	That's the one
2327000	2329000	He mentioned that maybe we can
2329000	2331000	Can have a quantum device to do the operation
2331000	2333000	From that time on
2333000	2335000	Quantum algorithm is a very, very
2335000	2337000	Important topic
2337000	2339000	Its influence is basically
2339000	2341000	Very comprehensive
2341000	2343000	For example, we know
2343000	2345000	In this crypto
2345000	2347000	In the early days, we were very focused on
2347000	2349000	This cryptography
2349000	2351000	Cryptography basically has some
2351000	2353000	Assumption of computation
2353000	2355000	Right, that is, I want to assume
2355000	2357000	What kind of problem
2357000	2359000	Not good
2359000	2361000	Then
2361000	2363000	That
2363000	2365000	This is the assumption of computation
2365000	2367000	Many places may
2367000	2369000	It means that if you say there is a good one
2369000	2371000	You have a good quantum algorithm
2371000	2373000	Your assumption of computation is not true
2373000	2375000	Not established
2375000	2377000	The most classic is this rsa
2377000	2379000	Because it assumes this
2379000	2381000	Prime number is
2381000	2383000	It's not easy to be affected
2383000	2385000	It's not easy to find its
2385000	2387000	The value factor
2387000	2389000	That's why it's so famous
2389000	2391000	Another reason
2391000	2393000	So
2393000	2395000	You design a good quantum algorithm
2395000	2397000	You may be able to break a lot
2397000	2399000	A lot of things
2399000	2401000	This is why the quantum algorithm is very important
2401000	2403000	A reason
2403000	2405000	Thank you
2407000	2409000	Thank you
2409000	2411000	That
2411000	2413000	I'll continue
2413000	2415000	We'll have one last time
2415000	2417000	Q&A, you can ask any questions
2417000	2419000	Okay, that's it
2419000	2421000	University of Sydney
2421000	2423000	I'm from the University of Sydney
2423000	2425000	I happen to be with
2425000	2427000	The last author
2427000	2429000	Tao Dacheng
2429000	2431000	In this classical machine learning
2431000	2433000	Is a very famous person
2433000	2435000	This group
2435000	2437000	He is also interested in this machine learning
2437000	2439000	Quantum machine learning is very interesting
2439000	2441000	So I took his students
2441000	2443000	The first author is his student
2443000	2445000	From the University of Sydney
2445000	2447000	Our paper
2447000	2449000	Just put it on the archive a few days ago
2449000	2451000	Just put it on the archive in September
2451000	2453000	We basically
2453000	2455000	The idea is basically
2455000	2457000	Combine some classical
2457000	2459000	Classical machine learning
2459000	2461000	Then combine some
2461000	2463000	Quantum algorithm
2463000	2465000	The idea
2465000	2467000	Then our
2467000	2469000	Our target is that we can
2469000	2471000	Now this hardware
2471000	2473000	We can do this
2473000	2475000	Proposal like this
2475000	2477000	Okay, that's the first one
2477000	2479000	Let me do a little background introduction
2479000	2481000	I just introduced global search
2481000	2483000	Right global search
2483000	2485000	It can be used basically
2485000	2487000	Machine learning to implement
2487000	2489000	How to implement
2489000	2491000	Basically global search has to do one thing
2491000	2493000	What is it
2493000	2495000	I want to find a state
2495000	2497000	This size, this file, this state
2497000	2499000	Let it get closer
2499000	2501000	Omega is the direction I want
2501000	2503000	I want him to get closer to Omega
2503000	2505000	The better
2505000	2507000	I just want to
2507000	2509000	Basically, this is me
2509000	2511000	I am doing this place
2511000	2513000	What are you doing
2513000	2515000	I want to
2515000	2517000	The closer he gets to Omega, the better
2517000	2519000	I want him
2519000	2521000	The less part perpendicular to Omega
2521000	2523000	Right, this is basically
2523000	2525000	What is this equation doing
2525000	2527000	Then we just said
2527000	2529000	That's why we can
2529000	2531000	We can come and learn this
2531000	2533000	This state
2533000	2535000	We just use
2535000	2537000	We use circuit to learn him
2537000	2539000	Because just that global
2539000	2541000	The algorithm is that I have two operations
2541000	2543000	I'm just going to flip it, right
2543000	2545000	Keep doing it and then flip it to a certain state
2545000	2547000	Just say about n
2547000	2549000	This is probably this
2549000	2551000	This step
2551000	2553000	I can flip it to the state I want
2553000	2555000	Right
2555000	2557000	Then this variational
2557000	2559000	This idea is basically
2559000	2561000	Anyway, I'm going to find a state
2561000	2563000	The closer to this
2563000	2565000	Omega is better, right?
2565000	2567000	I just want to
2567000	2569000	I can use machine learning to recognize this state
2569000	2571000	Right, this is also this
2571000	2573000	Biomonte
2573000	2575000	This nature just wrote that nature
2575000	2577000	Review article
2577000	2579000	A thought from their group
2579000	2581000	This is also a paper in May
2581000	2583000	This development is very fast
2583000	2585000	Then he went to recognize this
2585000	2587000	Recognize this circuit
2587000	2589000	How does he recognize it?
2589000	2591000	Hey, I designed two operations
2591000	2593000	One of them is this P Omega
2593000	2595000	This operation is what I just had in that slide
2595000	2597000	And then another PS is just
2597000	2599000	That is to say, there are two at the beginning
2599000	2601000	This flip operation
2601000	2603000	One is Omega and the other is S
2603000	2605000	It's not exactly
2605000	2607000	Do P Omega and PS
2607000	2609000	He can do P Omega again
2609000	2611000	Give him an alpha angle
2611000	2613000	Give him a beta angle
2613000	2615000	What about the angle of this beta
2615000	2617000	This can be used
2617000	2619000	This can be used
2619000	2621000	Optimization
2621000	2623000	To update your alpha and beta
2623000	2625000	Right
2625000	2627000	We are doing
2627000	2629000	We can all use some mms
2629000	2631000	Right to do this
2631000	2633000	To do update
2633000	2635000	Then he is to update this alpha and beta
2635000	2637000	Angle to let you next round
2637000	2639000	Your alpha and beta angles are different
2639000	2641000	Right then
2641000	2643000	Then they thought
2643000	2645000	Hey, if I use this machine
2645000	2647000	What about the method of learning
2647000	2649000	What is the comparison with Global Search
2649000	2651000	Very interesting, look at the next picture
2651000	2653000	This is their numerical experiment
2653000	2655000	They just used four cubits
2655000	2657000	I used four cubits here
2657000	2659000	And then
2659000	2661000	And then this blue dot
2661000	2663000	It's called Global Search
2663000	2665000	Global Search means
2665000	2667000	We just said you need to flip, right
2667000	2669000	You need this
2669000	2671000	Omega and S do a round like this
2671000	2673000	Right
2673000	2675000	Is this a round
2675000	2677000	One round is two steps, right
2677000	2679000	And then you need to do a few rounds
2679000	2681000	This Global Search probably tells you
2681000	2683000	Probably four rounds
2683000	2685000	Is eight operations
2685000	2687000	In fact, it is
2687000	2689000	2n plus 1, so it should be nine
2689000	2691000	Right
2691000	2693000	Nine words
2693000	2695000	Look at this
2695000	2697000	Horizontal
2697000	2699000	X-axis, you will find
2703000	2705000	Several times
2705000	2707000	This point is done nine times
2707000	2709000	Global Search
2709000	2711000	It can get the biggest
2711000	2713000	The biggest chance you see is 1
2713000	2715000	You see you are in this Oracle P
2715000	2717000	This is equal to 9
2717000	2719000	Your chance is 1, right
2719000	2721000	This means that Global Search
2721000	2723000	At this point, you can get the biggest chance
2723000	2725000	And then at other points
2725000	2727000	You see this probability is fluctuating
2727000	2729000	Right, it's actually like this
2729000	2731000	Up and down, right, it's only in
2731000	2733000	The probability is the biggest
2733000	2735000	And then in this part of the red cross
2735000	2737000	They got it by machine learning
2737000	2739000	Very interesting
2739000	2741000	At nine
2741000	2743000	It can be the same as Global Search
2743000	2745000	The result
2745000	2747000	But in other places
2747000	2749000	It can be better than Global Search
2749000	2751000	It's more robust
2751000	2753000	That is to say
2753000	2755000	You won't be because of yours
2755000	2757000	For example, I was supposed to be in the nine
2757000	2759000	I'm going to stop
2759000	2761000	But I stopped early
2761000	2763000	For example, I stopped at 8
2763000	2765000	I'm this up and down
2765000	2767000	I did it eight times
2767000	2769000	What if you use Variation
2769000	2771000	It will be better, right
2771000	2773000	It will be robust to your system design
2773000	2775000	What about this idea
2775000	2777000	That is to say, we
2777000	2779000	We want to say that this idea
2779000	2781000	Perception
2781000	2783000	Perception is basically
2783000	2785000	That is to say, you are also going to search
2785000	2787000	So basically you can put this
2787000	2789000	Global Search Embedded in your Perception
2789000	2791000	You just have to identify you
2791000	2793000	Mislabeled this value
2793000	2795000	Right, that is to say
2795000	2797000	That is to say, we
2797000	2799000	Use this idea
2799000	2801000	This is our step
2801000	2803000	Give me a database
2803000	2805000	This database
2805000	2807000	I have so many points
2807000	2809000	i from 1 to n
2809000	2811000	I have so many points
2811000	2813000	Then every point of mine
2813000	2815000	Xi is my input
2815000	2817000	Every point of mine
2817000	2819000	I have m so many features
2819000	2821000	I just want to go from feature
2821000	2823000	To identify my
2823000	2825000	To identify my label
2825000	2827000	This yi is my label
2827000	2829000	It may be positive or negative
2829000	2831000	Then, of course, the first step
2831000	2833000	I want to put this classical data
2833000	2835000	Encode to a quantum state
2835000	2837000	What about this quantum state
2837000	2839000	What do we do
2839000	2841000	That is to say, I put this feature
2841000	2843000	I put this
2843000	2845000	Xi
2845000	2847000	Encode to my input
2847000	2849000	Then
2849000	2851000	Then I have another
2851000	2853000	This index register
2853000	2855000	I just came to keep track
2855000	2857000	I have a few
2857000	2859000	A few samples
2859000	2861000	This i this place
2861000	2863000	I just need to say I have n
2863000	2865000	N so many
2865000	2867000	I need to log n
2867000	2869000	Because
2869000	2871000	Because you know
2871000	2873000	Quantum state can be superposition
2873000	2875000	So I just need to log n so many
2875000	2877000	I can keep track
2877000	2879000	This n
2879000	2881000	This n
2881000	2883000	Such a big data
2883000	2885000	Data set
2885000	2887000	The same
2887000	2889000	If I say there are m features
2889000	2891000	This feature register
2891000	2893000	I just need to log n so many
2893000	2895000	That is to say, it actually has a
2895000	2897000	Index compression
2897000	2899000	Of course, this step is actually
2899000	2901000	I'll tell you from the beginning
2901000	2903000	It actually needs a
2903000	2905000	A more non-trivial algorithm
2905000	2907000	But I'll come back to this issue later
2909000	2911000	Uh, and then
2911000	2913000	The first step is to say
2913000	2915000	I'm going to do this UK operation
2915000	2917000	What about this UK operation
2917000	2919000	What am I going to do
2919000	2921000	I want to identify this K
2921000	2923000	Basically, this
2923000	2925000	My state basically means
2925000	2927000	My K
2927000	2929000	Has been mislabeled
2929000	2931000	So I'm going to mislabel
2931000	2933000	This place introduces a
2933000	2935000	Face
2935000	2937000	What about this UK
2937000	2939000	I want to do this thing
2939000	2941000	But what about me
2941000	2943000	I'm going to design a
2943000	2945000	Learning algorithm, I'm going to learn
2945000	2947000	I'm going to
2947000	2949000	Approximate this UK
2949000	2951000	With this
2951000	2953000	With this low depth
2953000	2955000	Quantum circuit
2955000	2957000	Approximate UK and then achieve this
2957000	2959000	Basically this is
2959000	2961000	And then
2961000	2963000	Because this is different from global search
2963000	2965000	Because we have feature register
2965000	2967000	With index register
2967000	2969000	Feature register is for you
2969000	2971000	To recognize your
2971000	2973000	What does your distribution look like
2973000	2975000	Right?
2975000	2977000	There is no feature
2977000	2979000	You just need to do your index register
2979000	2981000	Right?
2981000	2983000	But because
2983000	2985000	We're in front of this
2985000	2987000	Because you're done
2987000	2989000	This
2989000	2991000	Give you this
2991000	2993000	Add a face
2993000	2995000	Because he actually
2995000	2997000	With this index
2997000	2999000	He actually has one
2999000	3001000	These two registers
3001000	3003000	It actually exists
3003000	3005000	You do the operation for K
3005000	3007000	You will affect you
3007000	3009000	Feature this register
3009000	3011000	So we need to do one more step
3011000	3013000	We need to do this step
3013000	3015000	We need to put the feature
3015000	3017000	This register
3017000	3019000	With your index register
3019000	3021000	Disentangle it
3021000	3023000	So you see this is now
3023000	3025000	It means that there is no correlation in the middle
3025000	3027000	There is no correlation
3027000	3029000	That is, we
3029000	3031000	What about our circuit
3031000	3033000	Disentangle
3033000	3035000	This operation
3035000	3037000	These two operations
3037000	3039000	We're going to use the method of machine learning
3039000	3041000	And then
3041000	3043000	Basically, this is my two
3043000	3045000	Basic operation
3045000	3047000	This picture is a demo
3047000	3049000	This ul1 is what I just did
3049000	3051000	And then, no, ul1
3051000	3053000	Plus this control g
3053000	3055000	Basically, this is what I just did
3055000	3057000	And then
3057000	3059000	This ul1
3059000	3061000	In fact, because this is just learning
3061000	3063000	Your feature, so it's just in this feature
3063000	3065000	Register
3065000	3067000	Disentangle can also be used
3067000	3069000	In this feature register
3069000	3071000	And then disentangle
3071000	3073000	This unit
3073000	3075000	This is basically just
3075000	3077000	The second inversion of this global search
3077000	3079000	For this s
3079000	3081000	An inversion of this plane
3081000	3083000	What about this inversion
3083000	3085000	Because it's going to do an inversion of your index
3085000	3087000	So it's just an index register
3087000	3089000	This is a
3089000	3091000	This is a
3091000	3093000	An iteration
3093000	3095000	Then I need a total of about
3095000	3097000	All n so many iterations
3097000	3099000	And then you'll find
3099000	3101000	That's what we're actually doing here
3101000	3103000	This quantum
3103000	3105000	Quantum state
3105000	3107000	This quantum circuit
3107000	3109000	I use this classical
3109000	3111000	Optimization to do
3111000	3113000	Just like this variation of global search
3113000	3115000	It actually has an angle that can be controlled
3115000	3117000	I'm using classical
3117000	3119000	To do optimization
3119000	3121000	That is to say, there are some MMD
3121000	3123000	This is more
3123000	3125000	Anyway, this is some classical optimization
3125000	3127000	Machine learning often does some things
3127000	3129000	Then I update and do it again
3129000	3131000	I've been doing square root n so many times
3131000	3133000	Ah
3133000	3135000	Just like global search
3135000	3137000	Square root n so many times
3137000	3139000	I can adjust it to the angle I want
3139000	3141000	Like this
3141000	3143000	This is our
3143000	3145000	Simple experiment
3145000	3147000	Then we can
3147000	3149000	We use experiments to implement
3149000	3151000	That
3151000	3153000	What about this place
3153000	3155000	I
3155000	3157000	Ah
3157000	3159000	Say
3159000	3161000	Inside this square
3161000	3163000	I told you that every one of me
3163000	3165000	Every one of this
3165000	3167000	How big is this
3167000	3169000	Our circuit
3169000	3171000	It's probably three
3171000	3173000	Three three three
3173000	3175000	Three layers like this
3175000	3177000	Or five layers
3177000	3179000	Because of our
3179000	3181000	Our
3181000	3183000	Feature is probably
3183000	3185000	Just
3185000	3187000	Our example
3187000	3189000	It's actually quite small
3189000	3191000	We'll have a bigger example later
3191000	3193000	Our feature size is probably
3193000	3195000	Four
3195000	3197000	There are four features in each sample
3197000	3199000	Then you will find
3199000	3201000	My success probability
3201000	3203000	Can be up to
3203000	3205000	About 80%
3205000	3207000	Ah
3207000	3209000	The one on the left is this
3209000	3211000	You identify your
3211000	3213000	Mislabel index
3213000	3215000	This probability
3215000	3217000	The error rate is probably
3217000	3219000	It can be up to 80%
3219000	3221000	90% like this
3221000	3223000	Classification
3223000	3225000	This place
3225000	3227000	Then
3227000	3229000	Perception can only do this
3229000	3231000	Linear partition
3231000	3233000	What if we want to
3233000	3235000	How do we do it
3235000	3237000	We just have to combine these
3237000	3239000	Perception
3239000	3241000	I don't know if there is
3241000	3243000	Is there any of this
3243000	3245000	More familiar with machine learning
3245000	3247000	We are
3247000	3249000	There is one called Ensemble Learning
3249000	3251000	Basically the idea is
3251000	3253000	I have a big
3253000	3255000	Ah database
3255000	3257000	I have a bigger sample
3257000	3259000	I don't need to
3259000	3261000	I don't need to use this whole sample
3261000	3263000	What can I do
3263000	3265000	I can
3265000	3267000	I can sample from the inside
3267000	3269000	Some smaller data
3269000	3271000	Then I use these smaller data
3271000	3273000	Set to train
3273000	3275000	My perception
3275000	3277000	Then I use my
3277000	3279000	A few weaker perceptions
3279000	3281000	I can combine it into one
3281000	3283000	Strong perception
3283000	3285000	This idea is called
3285000	3287000	This is the idea of ​​Ensemble Learning
3287000	3289000	Then
3289000	3291000	Doing this is sub-sampling
3291000	3293000	Because I don't need to
3293000	3295000	Such a big database
3295000	3297000	I just sample some smaller data
3297000	3299000	This is the concept of sub-sampling
3299000	3301000	So if I say I can train
3301000	3303000	My capital T
3307000	3309000	Then I can
3309000	3311000	This yt
3311000	3313000	Is the outcome of each vt
3313000	3315000	Then my ct
3315000	3317000	Is my preset threshold
3317000	3319000	Then I can use this thing
3319000	3321000	To do a better
3321000	3323000	This
3323000	3325000	Ah
3325000	3327000	Then we want to know
3327000	3329000	Will this combine be better than
3329000	3331000	My weak
3331000	3333000	Right
3333000	3335000	Then we will do a numerical
3335000	3337000	Conclusion
3337000	3339000	What are the benefits of this
3339000	3341000	It can
3341000	3343000	You can put your bigger
3343000	3345000	Training set to break it down into
3345000	3347000	Smaller training set
3347000	3349000	Then you train this weak classifier
3349000	3351000	Then this just fits
3351000	3353000	This quantum machine learning now
3353000	3355000	Because we mentioned at the beginning
3355000	3357000	What is the bottleneck of quantum machine learning
3357000	3359000	What is it
3359000	3361000	Classical data
3361000	3363000	Quantum state
3363000	3365000	This is almost all
3365000	3367000	Quantum machine learning proposal
3367000	3369000	Then we use
3369000	3371000	The idea of sub-sampling
3371000	3373000	The idea of ensemble learning
3373000	3375000	We can overcome this problem
3375000	3377000	Because you are actually a small data set
3377000	3379000	You are still quite efficient
3379000	3381000	You don't need to say
3381000	3383000	I have a hundred thousand points
3383000	3385000	With me a
3385000	3387000	This complexity
3387000	3389000	It's not just a hundred thousand
3389000	3391000	Divided by one hundred so much
3391000	3393000	It's actually every step
3393000	3395000	After the complexity you need
3395000	3397000	The complexity is multiplied by the index
3397000	3399000	So we use
3399000	3401000	This concept is actually very
3401000	3403000	Very fit for now
3403000	3405000	The development of quantum hardware
3405000	3407000	Good
3407000	3409000	Then
3409000	3411000	Next is a more
3411000	3413000	One of CS
3413000	3415000	We want to characterize our efficiency
3415000	3417000	If we say every data set
3417000	3419000	Every data set
3419000	3421000	Because we want to train some
3421000	3423000	Weak classifier
3423000	3425000	If I say
3425000	3427000	Log square root n so many points
3427000	3429000	Then we
3429000	3431000	Query complexity
3431000	3433000	This capital T means that I have a few
3433000	3435000	I have a total
3435000	3437000	Weak classifier number is
3437000	3439000	This capital T so many
3439000	3441000	Then you will find that because it is actually
3441000	3443000	Still related to this global algorithm
3443000	3445000	So it is with you
3445000	3447000	This square root of the data set
3447000	3449000	So if you say it's log n
3449000	3451000	Square root is probably
3451000	3453000	Multiply by this quantity
3453000	3455000	This m is the size of your feature
3455000	3457000	This is related to me
3457000	3459000	How many steps do you need to do
3459000	3461000	How many steps do you need to do
3461000	3463000	Then this is probably
3463000	3465000	Heuristic is probably
3465000	3467000	Log n so many steps
3467000	3469000	We will demonstrate later
3469000	3471000	If I say I choose log n
3471000	3473000	So many classical optimization
3473000	3475000	Then every data set of mine
3475000	3477000	I'll have one later
3477000	3479000	Performance comparison
3479000	3481000	Run time is probably
3481000	3483000	With log n
3483000	3485000	Multiply this
3485000	3487000	Polynomial relationship
3487000	3489000	So basically it is very efficient
3489000	3491000	it is good
3491000	3493000	it is good
3493000	3495000	This is our
3495000	3497000	A practical example we run
3497000	3499000	A classical database
3499000	3501000	Tens of thousands of points
3501000	3503000	10,000 points
3503000	3505000	Then every point
3505000	3507000	This feature is four
3507000	3509000	Is four features
3509000	3511000	Can be encoded on two qubits
3511000	3513000	Then we sub-sampling
3513000	3515000	We just sample
3515000	3517000	We just make it a sample of four smaller databases
3517000	3519000	Then every database
3519000	3521000	Only eight points
3521000	3523000	So this ratio is very small
3523000	3525000	It's about 8 out of 10,000
3525000	3527000	Then we found that if you use 8 out of 10,000
3527000	3529000	You can follow
3529000	3531000	You can get it directly from this
3531000	3533000	From this 10,000 points
3533000	3535000	To train
3535000	3537000	Your perception
3537000	3539000	Your combined efficiency is basically
3539000	3541000	With you
3541000	3543000	10,000 points to do the same
3543000	3545000	So you look at the last of this table
3545000	3547000	The last column
3547000	3549000	The last column means that I have combined
3549000	3551000	I have four
3551000	3553000	This week
3553000	3555000	This week's perception
3555000	3557000	This is my last combined success probability
3557000	3559000	You will find that it is
3559000	3561000	It can be up to about 80%
3561000	3563000	Of course with
3563000	3565000	It's not so good
3565000	3567000	But
3567000	3569000	Hardware can be implemented now
3569000	3571000	Basically this is already a very
3571000	3573000	Very
3573000	3575000	Very good proposal because it is now
3575000	3577000	Hardware can also be implemented
3577000	3579000	Ah
3579000	3581000	This is my example
3581000	3583000	Ah oh yes
3583000	3585000	This place
3585000	3587000	There is a caveat
3587000	3589000	This is
3589000	3591000	We
3591000	3593000	We can also another one
3593000	3595000	If you say you have a way to let you
3595000	3597000	Ah sample
3597000	3599000	Ah this data set
3599000	3601000	If you say your subsampling
3601000	3603000	You can sample away
3603000	3605000	From this
3605000	3607000	Perception of each week
3607000	3609000	Because perception is basically
3609000	3611000	Classify left or classify right
3611000	3613000	If you say your sample
3613000	3615000	When you do subsampling
3615000	3617000	You can avoid this
3617000	3619000	These points
3619000	3621000	This probability can also be increased to 92%
3623000	3625000	Say this depends on you
3625000	3627000	This method of subsampling
3627000	3629000	At the most
3629000	3631000	Say at the most extreme
3631000	3633000	If you say even if you use uniform sampling
3633000	3635000	Then we can still get 80%
3635000	3637000	If you say your sampling you can adjust
3637000	3639000	You can get 9 times 2
3639000	3641000	9 times 2 is already very impressive
3641000	3643000	I think so
3643000	3645000	I don't know his community
3645000	3647000	Reason
3647000	3649000	I think his community can do very well
3649000	3651000	So at least in the quantum community
3651000	3653000	No one can do it like us
3653000	3655000	So good
3655000	3657000	This place is just that
3657000	3659000	Ah
3659000	3661000	The question asked by the agent is
3661000	3663000	How many operations do we need
3663000	3665000	In the numerical experiment we just did
3665000	3667000	We actually use that regated
3667000	3669000	Cloud platform they have a library
3669000	3671000	Then we implement
3671000	3673000	We all use his standard library
3673000	3675000	If they want they can take our code
3675000	3677000	Can compile to their
3677000	3679000	This circuit can be run like this
3679000	3681000	Then
3681000	3683000	We say that in encoding
3683000	3685000	We want to put classical map to quantum state
3685000	3687000	If you use subsampling you only need to use
3687000	3689000	29 operations
3689000	3691000	Probably use 29 operations
3691000	3693000	Then this is all from him
3693000	3695000	That's what I just said
3695000	3697000	You use their library
3697000	3699000	You can
3699000	3701000	After you input the database
3701000	3703000	He will tell you your
3703000	3705000	How many gates do you need
3705000	3707000	So we
3707000	3709000	Input it about 29
3709000	3711000	On average 29
3711000	3713000	Then the operations behind us
3713000	3715000	Don't we have a few operations
3715000	3717000	That is, you have to flip your face
3717000	3719000	And then you have to disentangle
3719000	3721000	Right then
3721000	3723000	What about the operation we need in total
3723000	3725000	Ah
3725000	3727000	That's what we need
3727000	3729000	About 36
3729000	3731000	Can control your angle
3731000	3733000	Then
3733000	3735000	Then
3735000	3737000	Then
3737000	3739000	Then
3739000	3741000	Then
3741000	3743000	You can
3743000	3745000	So if you say
3745000	3747000	Decomposing
3747000	3749000	We probably need a total of 146
3749000	3751000	Jingle with
3751000	3753000	You can do our experiment
3753000	3755000	Like this
3755000	3757000	Okay, this is probably the details of our experiment
3757000	3759000	Then
3759000	3761000	Well, this one after this
3761000	3763000	The latter is more complicated
3763000	3765000	I won't talk about it first
3765000	3767000	Anyway, when the time comes, everyone has an opinion
3767000	3769000	Maybe I can come back later
3769000	3771000	Give other talk like this
3771000	3773000	Okay, I don't know
3773000	3775000	Do you have any questions
3775000	3777000	Like this
3777000	3779000	Thank you
3779000	3781000	Hello
3781000	3783000	Can you ask a question
3783000	3785000	Related to your talk
3785000	3787000	Okay, no problem
3787000	3789000	Because you mentioned it at the beginning
3789000	3791000	Let me introduce myself first
3791000	3793000	My name is Chen Yuan, I am a fourth grade
3793000	3795000	Then
3795000	3797000	You mentioned that Taiwan is planning
3797000	3799000	Want to
3799000	3801000	It's also related to computation
3801000	3803000	This is more famous
3803000	3805000	Like Microsoft
3805000	3807000	Should be
3807000	3809000	Or google
3809000	3811000	Buy john martini's lab
3811000	3813000	That Taiwan
3813000	3815000	Have decided to use
3815000	3817000	Which one
3817000	3819000	Which direction
3819000	3821000	To do
3821000	3823000	This is a good question
3823000	3825000	Basically
3825000	3827000	Taiwan's tendency is more
3827000	3829000	It should be said that the high-level
3829000	3831000	Government technology minister
3831000	3833000	Their tendency is more
3833000	3835000	Hope to use semiconductor
3835000	3837000	Yes, the semiconductor
3837000	3839000	The big company has not yet invested
3839000	3841000	I can't say that either
3841000	3843000	The big company is now
3843000	3845000	It may have a main force
3845000	3847000	But he didn't give up
3847000	3849000	Other proposals
3849000	3851000	The best semiconductor is Australia
3851000	3853000	Australia
3853000	3855000	University of New South Wales
3855000	3857000	University of New South Wales
3857000	3859000	They are leading the world
3859000	3861000	Semiconductor technology
3861000	3863000	Their Qubit is using
3863000	3865000	Semiconductor process
3865000	3867000	So then
3867000	3869000	Taiwan has TSMC
3869000	3871000	Everyone still hopes to use semiconductor
3871000	3873000	So the government department technology department
3873000	3875000	Preferred approach is also
3875000	3877000	Semiconductor
3877000	3879000	Then Taiwan University
3879000	3881000	Now it is said that the Taiwan University of Physics
3881000	3883000	Then with the Taiwan University of Electrical Engineering
3883000	3885000	Li Junyun
3885000	3887000	With
3887000	3889000	Teacher Chen Shiyuan
3891000	3893000	They are now saying that they have
3893000	3895000	A team is now
3895000	3897000	Follow is to say that they want to
3897000	3899000	Mainly to work with
3899000	3901000	Cooperate with University of New South Wales
3901000	3903000	They want to develop semiconductor technology
3903000	3905000	Then now this Tsinghua University
3905000	3907000	This research center
3913000	3915000	Inside a
3915000	3917000	Project to do superconductivity
3917000	3919000	In fact, the Central Academy of Sciences has a teacher doing superconductivity
3919000	3921000	Qubit
3923000	3925000	So
3925000	3927000	In this Tsinghua University
3927000	3929000	There are people doing superconductivity experiments
3929000	3931000	Of course there is also light and snow
3931000	3933000	There is also a group of people doing light and snow
3933000	3935000	Qubit
3935000	3937000	Light and snow, of course, also need
3937000	3939000	Sorry
3941000	3943000	Sorry
3943000	3945000	I just want to ask if this is in the field
3945000	3947000	Generally speaking, which direction
3947000	3949000	Is the most likely to be done
3949000	3951000	The most robust and then the easiest
3951000	3953000	Large-scale production
3953000	3955000	It's not easy
3955000	3957000	Because these are all super low temperatures
3959000	3961000	Superconductivity now needs
3961000	3963000	Hundreds of
3963000	3965000	Mini-K
3965000	3967000	It's almost absolute zero
3967000	3969000	Then semiconductor
3969000	3971000	Can go to
3971000	3973000	Maybe
3973000	3975000	Around 4K
3975000	3977000	Now
3977000	3979000	Large-scale is almost
3979000	3981000	At present
3981000	3983000	No idea
3985000	3987000	It's not like our computer can be at room temperature
3987000	3989000	These devices all need a
3989000	3991000	A big refrigerator
3991000	3993000	Cool it down to almost zero
3993000	3995000	Absolute zero
3995000	3997000	So
3997000	3999000	At present, everyone's business model
3999000	4001000	It's still more like crowd computing
4001000	4003000	For example, they said
4003000	4005000	Regetti IBM
4005000	4007000	Or Microsoft, they are basically
4007000	4009000	The future trend is that I have one
4009000	4011000	Is cloud
4011000	4013000	Then let you access
4013000	4015000	You can have some more complicated
4015000	4017000	You can upload it to my server
4017000	4019000	Then my quantum computer will help you calculate
4019000	4021000	The current business model is like this
4023000	4025000	Thank you
4029000	4031000	I thought
4031000	4033000	Most of the people in Caltech
4033000	4035000	Are doing theory
4035000	4037000	Caltech is now doing experiments
4037000	4039000	Which direction
4039000	4041000	Caltech itself
4041000	4043000	It doesn't seem to be experimenting with quantum computation
4043000	4045000	We have an institution called IQIM
4045000	4047000	This is quantum information
4047000	4049000	It's called
4049000	4051000	It's called
4051000	4053000	It's all theory
4053000	4055000	The group of people
4055000	4057000	Yes
4061000	4063000	Let me add that I am now a graduate of Dongda
4063000	4065000	What we do here is
4065000	4067000	Just the semiconductor
4067000	4069000	How to make a semiconductor
4069000	4071000	To do Qubit
4071000	4073000	That's true
4073000	4075000	New South Wales
4075000	4077000	Very leading
4077000	4079000	Especially recently
4079000	4081000	From the end of last year
4081000	4083000	About 80 million
4083000	4085000	Establish a new company
4085000	4087000	And their country's funds
4087000	4089000	Invest in this area
4089000	4091000	That 80 million is US dollars
4091000	4093000	That's right
4095000	4097000	So I think
4097000	4099000	All directions are
4099000	4101000	Very difficult
4101000	4103000	I was before
4103000	4105000	When I was in PhD
4105000	4107000	Also use
4107000	4109000	But
4111000	4113000	My personal idea is
4113000	4115000	It's not that easy
4115000	4117000	He's probably
4117000	4119000	A bottleneck
4119000	4121000	I don't know yet
4121000	4123000	How to overcome
4125000	4127000	Yes, but because I do experiments
4127000	4129000	So if
4129000	4131000	If you want to discuss
4131000	4133000	Let's continue to discuss
4135000	4137000	But for now
4137000	4139000	For the content of today's speech
4139000	4141000	Is there a problem
4153000	4155000	Yes, I want to remind you
4155000	4157000	If you want to see that
4157000	4159000	Senior's video
4159000	4161000	I have a link below
4161000	4163000	That link
4163000	4165000	If you want to find
4165000	4167000	If you don't know the problem
4167000	4169000	You can click on the link
4171000	4173000	In the chat
4173000	4175000	There is a link
4175000	4177000	My slides are all online
4177000	4179000	So you can watch it anytime
4179000	4181000	It will always be there
4181000	4183000	I have a question
4183000	4185000	Sorry
4185000	4187000	My question
4187000	4189000	I remember
4189000	4191000	I'm watching now
4191000	4193000	Because I'm not doing theory
4193000	4195000	But I know
4195000	4197000	It seems to be in the algorithm
4197000	4199000	For
4199000	4201000	If you don't know how to deal with it
4201000	4203000	At least there is
4203000	4205000	Grover
4205000	4207000	To do a basic acceleration
4207000	4209000	But we want
4209000	4211000	What you want
4211000	4213000	Things should be
4213000	4215000	Want to do
4215000	4217000	Not just polynomial
4217000	4219000	But exponential acceleration
4219000	4221000	So inside
4221000	4223000	Inside
4223000	4225000	Several algorithms
4229000	4231000	Recommendation now
4231000	4233000	I saw it before
4233000	4235000	Austin
4235000	4237000	This news
4237000	4239000	I think it's super cool
4239000	4241000	Found
4241000	4243000	Is a
4243000	4245000	I thought only quantum computers could do it
4245000	4247000	Found out
4247000	4249000	Can be done
4249000	4251000	What do they call it
4251000	4253000	Quantum Inspired Algorithm
4253000	4255000	I think it's really cool
4255000	4257000	Can I make a comment
4257000	4259000	Yes
4259000	4261000	This is basically
4261000	4263000	If you say now
4263000	4265000	Strict
4265000	4267000	Complexity Theory
4267000	4269000	For those who do theoretical computer science
4269000	4271000	They actually mean
4271000	4273000	You can't define it yet
4273000	4275000	Quantum Computation
4275000	4277000	Really
4277000	4279000	Really
4279000	4281000	Very strict proof of acceleration
4281000	4283000	Even like Grover
4283000	4285000	Even like this Schultz Algorithm
4285000	4287000	We know he has it now
4287000	4289000	He now has this
4289000	4291000	But it's just
4291000	4293000	The best classic algorithm
4293000	4295000	Then we know
4295000	4297000	What about this factoring
4297000	4299000	He is not a
4299000	4301000	He is a question of mouth NP
4301000	4303000	He said
4303000	4305000	Not to say
4305000	4307000	Ah
4307000	4309000	It should be said that the current bottleneck is
4309000	4311000	Classical may
4311000	4313000	I don't know how to do it
4313000	4315000	Maybe one day there will be one
4315000	4317000	There is also a Schultz algorithm
4317000	4319000	This classical algorithm
4319000	4321000	Schultz Algorithm
4321000	4323000	Yes
4323000	4325000	Because of Complexity Class
4325000	4327000	Not to say this
4327000	4329000	Factoring is the most difficult problem
4329000	4331000	He is not a question of NP
4331000	4333000	He is not a question of NP
4333000	4335000	So
4335000	4337000	It's all like this now
4337000	4339000	Although we have some
4339000	4341000	Some
4341000	4343000	Quantum algorithms are better than modern classic algorithms
4343000	4345000	But the point is
4345000	4347000	This is to say
4347000	4349000	There will be a classic algorithm
4349000	4351000	Will improve
4351000	4353000	This is also
4353000	4355000	Very interesting
4355000	4357000	I also encourage if
4357000	4359000	People who do CS
4359000	4361000	You should be able to say
4361000	4363000	Bring your classical knowledge
4363000	4365000	Let's take a look at these quantum algorithms
4365000	4367000	Or
4367000	4369000	Or to discuss
4369000	4371000	You are interested in the corresponding quantum version
4371000	4373000	Maybe it's like this
4373000	4375000	Maybe one day you can also
4375000	4377000	A very good way
4377000	4379000	This way
4379000	4381000	Yes
4381000	4383000	Do you have any questions?
4383000	4385000	Please say
4385000	4387000	I want to ask you again
4387000	4389000	At the beginning of your quantum
4389000	4391000	Principle Component Analysis
4391000	4393000	Is it the HHL algorithm?
4393000	4395000	Or later
4395000	4397000	There are other extensions
4397000	4399000	Uh
4399000	4401000	It is possible
4401000	4403000	It is possible to use this HHL
4403000	4405000	But I didn't go to see it
4405000	4407000	Because I just entered this field
4407000	4409000	Not long
4409000	4411000	Basically this principle component
4411000	4413000	It is definitely a face estimation
4413000	4415000	Because
4415000	4417000	Basically it is to identify these
4417000	4419000	The covariance matrix is relatively large
4419000	4421000	This eigenvalue
4421000	4423000	Maybe you can put HHL
4423000	4425000	Do it like this
4429000	4431000	Hello i have a question
4431000	4433000	Ok you ask
4433000	4435000	Self-introduction
4435000	4437000	Uh
4437000	4439000	Uh
4439000	4441000	I did a high-performance experiment
4441000	4443000	Is
4443000	4445000	I am doing dark matter
4445000	4447000	But because I did
4447000	4449000	Is actually with
4449000	4451000	Very close
4451000	4453000	So when I was in college
4453000	4455000	I went to the teacher's class
4455000	4457000	I almost did it
4457000	4459000	My question is like this
4459000	4461000	Uh actually
4461000	4463000	You just
4463000	4465000	Is what you said
4465000	4467000	In fact, it is more like
4467000	4469000	Is there
4469000	4471000	Strict mathematical logic
4471000	4473000	Machine learning
4473000	4475000	But now the most powerful machine learning is some kind of fossil logic
4475000	4477000	Neural net
4477000	4479000	Now quantum
4479000	4481000	Everyone is in this direction
4481000	4483000	Use quantum to do fossil logic
4483000	4485000	What is the situation now
4485000	4487000	Will be
4487000	4489000	Traditionally more powerful
4489000	4491000	Still don't know
4491000	4493000	Because I don't know
4493000	4495000	I don't know
4495000	4497000	Ok ok
4497000	4499000	Since you ask these questions
4499000	4501000	I will continue to talk about it
4501000	4503000	In fact, one of the results behind me
4503000	4505000	In fact, it may be related to this
4505000	4507000	That is to say
4507000	4509000	Can everyone see it
4513000	4515000	Can see
4515000	4517000	Basically
4517000	4519000	Me and my student
4519000	4521000	The question we want to ask is
4521000	4523000	Because you know that we actually use this
4523000	4525000	Circuit to learn
4525000	4527000	You are actually your quantum circuit
4527000	4529000	You have a parameter in it
4529000	4531000	Then you can use this optimization
4531000	4533000	To adjust your parameters to produce the state you want
4533000	4535000	Right
4535000	4537000	There is a question that you want to know
4537000	4539000	Such a quantum circuit
4539000	4541000	We call parametric quantum circuit
4541000	4543000	His
4543000	4545000	Expressive power because we know
4545000	4547000	Machine learning has two types, one is classification
4547000	4549000	One is generative model
4549000	4551000	That is to say, I want to produce a distribution
4551000	4553000	I want to produce a target distribution
4553000	4555000	Then we want to know this quantum circuit
4555000	4557000	This
4557000	4559000	The ability to generate this
4559000	4561000	Generative ability is how good
4561000	4563000	Right
4563000	4565000	So I did it with my student
4565000	4567000	Did the following thing
4567000	4569000	That is to say
4569000	4571000	If I say I can have
4571000	4573000	L block
4573000	4575000	This every
4575000	4577000	A block of this square
4577000	4579000	Then
4579000	4581000	In fact, the last row
4581000	4583000	Is measurement
4583000	4585000	After measurement, it is a classical data
4585000	4587000	Then I can put this
4587000	4589000	Do some classical optimization
4589000	4591000	Put this
4591000	4593000	Parameter feedback
4593000	4595000	Because this is all adjustable
4595000	4597000	Adjustable parameters
4597000	4599000	Then I want to know
4599000	4601000	This type of circuit
4601000	4603000	I finally generated these classical distributions
4603000	4605000	With
4605000	4607000	We know that the Boltzmann machine
4607000	4609000	Can also be used to generate distributions
4609000	4611000	Right
4611000	4613000	You just mentioned these like this neural network
4613000	4615000	Basically, the Boltzmann machine is like this
4615000	4617000	That is to say, the most general Boltzmann machine
4617000	4619000	The picture in the middle
4619000	4621000	That is to say, it has a visible layer and a hidden layer
4621000	4623000	Then all the layers are fully connected
4623000	4625000	Right
4625000	4627000	Then we all know that this kind of fully connected
4627000	4629000	Very powerful
4629000	4631000	This is basically Deep Neural Network
4631000	4633000	Everything is connected
4633000	4635000	Then
4635000	4637000	Of course there is another type of Boltzmann machine
4637000	4639000	That is to say, it is actually
4639000	4641000	It only has layers and layers
4641000	4643000	Connected
4643000	4645000	Every
4645000	4647000	The middle of your own layer is not
4647000	4649000	This is not connected
4649000	4651000	Right, this is Restricted Boltzmann Machine
4651000	4653000	These two are classic methods
4653000	4655000	Can be generated
4655000	4657000	Then the picture on my left
4657000	4659000	That is to say this is
4659000	4661000	Another version of this picture
4661000	4663000	That is to say
4663000	4665000	The layout of each block of the circuit is different
4665000	4667000	I'm on the left
4667000	4669000	The layout of each circuit on the left
4669000	4671000	The layout of each block of the circuit is the same
4671000	4673000	The quantum circuit on my right
4673000	4675000	It actually has one
4675000	4677000	Its every layout
4677000	4679000	In fact, the first layout is the most powerful
4679000	4681000	Because it
4681000	4683000	The first one
4683000	4685000	The first one is that it has a lot of C-NOT gates
4685000	4687000	Can be connected
4687000	4689000	The second one, it's actually your C-NOT gate
4689000	4691000	It's already less
4691000	4693000	And it's
4693000	4695000	It's every block
4695000	4697000	Then all the way to the end
4697000	4699000	This
4699000	4701000	Connectivity is actually very limited
4701000	4703000	Then we want to know
4703000	4705000	These circuit layouts
4705000	4707000	This
4707000	4709000	From the perspective of deep distribution
4709000	4711000	Which one is the most powerful
4711000	4713000	Just these two quantum
4713000	4715000	With these two classical
4715000	4717000	That's basically it
4717000	4719000	The most powerful is the one on the left
4719000	4721000	This quantum circuit
4721000	4723000	This quantum circuit
4723000	4725000	Each of them is
4725000	4727000	The same layout
4727000	4729000	Then each layout actually has this
4729000	4731000	It has a C-NOT between any two points
4731000	4733000	Can be connected
4733000	4735000	This is the most powerful
4735000	4737000	It's more powerful than the general Boltzmann machine
4737000	4739000	It's more powerful than the general Boltzmann machine
4739000	4741000	In terms of deep
4741000	4743000	Distribution
4743000	4745000	Then this general Boltzmann machine
4745000	4747000	It's better than this quantum circuit
4747000	4749000	The ability to generate
4749000	4751000	Because this quantum circuit
4751000	4753000	In fact, it
4753000	4755000	It's actually more limited
4755000	4757000	It's not all
4757000	4759000	All of this
4759000	4761000	There's a connection between the lines
4761000	4763000	So the general Boltzmann machine
4763000	4765000	Will be stronger than this circuit
4765000	4767000	This circuit will be stronger than this
4767000	4769000	Boltzmann machine
4769000	4771000	This is what we haven't published
4771000	4773000	A result of the paper
4773000	4775000	Maybe next week it will be put on the archive
4775000	4777000	If all goes well
4777000	4779000	This is
4779000	4781000	On the one hand
4781000	4783000	Also answer your question
4783000	4785000	In fact, the quantum circuit
4785000	4787000	It's in the perspective of deep distribution
4787000	4789000	It's actually
4789000	4791000	Classic
4791000	4793000	The classic Boltzmann machine is stronger
4793000	4795000	Of course
4795000	4797000	Maybe there are some specific tasks
4797000	4799000	Now we don't know yet
4799000	4801000	How does the quantum circuit compare with the Boltzmann machine
4801000	4803000	This is still unknown
4805000	4807000	Senior, can I ask you
4807000	4809000	Can you go back to the page above
4809000	4811000	it is good
4811000	4813000	Because like this
4813000	4815000	The two pictures above
4815000	4817000	Does it really correspond to the physical
4817000	4819000	His connection
4819000	4821000	Like this
4821000	4823000	It might be
4823000	4825000	Near snapper coupling
4825000	4827000	So he can only follow the side
4827000	4829000	He's gonna do a bunch of swaps
4829000	4831000	Right, right, right
4831000	4833000	The second picture
4833000	4835000	It's kind of like
4835000	4837000	With this actual
4837000	4839000	Implementation set up
4839000	4841000	Consider
4841000	4843000	Because there are some set ups
4845000	4847000	So in that
4847000	4849000	When you use
4849000	4851000	Every one of them
4851000	4853000	Like he's doing
4853000	4855000	One swap is three CNOT
4855000	4857000	Every CNOT
4857000	4859000	He can now
4859000	4861000	About how much
4861000	4863000	Their
4863000	4865000	Fidelity is not high
4865000	4867000	I heard it's low, but I don't know
4867000	4869000	How much
4869000	4871000	I remember the best is only 80%
4871000	4873000	May be worse
4873000	4875000	Because he's actually every
4875000	4877000	Connectivity between qubits
4877000	4879000	Different
4879000	4881000	A little worse
4883000	4885000	Because I think the above
4885000	4887000	They just got one million
4887000	4889000	One hundred million
4889000	4891000	This investment
4891000	4893000	Very powerful
4893000	4895000	They
4895000	4897000	That's pretty important to them
4897000	4899000	Because they even
4899000	4901000	They do it themselves
4901000	4903000	But I think the above two pictures
4903000	4905000	It's like that
4905000	4907000	Compared with that
4907000	4909000	Really
4909000	4911000	Like that
4911000	4913000	He is
4913000	4915000	A series of times he can
4915000	4917000	As long as he can
4917000	4919000	With five
4919000	4921000	That
4921000	4923000	CNOT
4923000	4925000	So I think
4925000	4927000	At least
4927000	4929000	Why
4929000	4931000	At least for people
4931000	4933000	Ask them
4933000	4935000	The biggest idea is
4935000	4937000	Where is the biggest advantage
4937000	4939000	I think this is what they can do
4939000	4941000	Emphasize
4941000	4943000	Like their CNOT
4943000	4945000	It should be up to 97
4945000	4947000	Around 98
4947000	4949000	It seems like someone has 99
4949000	4951000	But at least I think
4951000	4953000	Their advantage
4953000	4955000	Yeah yeah yeah
4955000	4957000	Mr. Cao
4957000	4959000	Gate is lower but
4959000	4961000	The chance is bigger
4962000	4964000	Any other questions
4976000	4978000	It seems like there's no problem
4982000	4984000	Okay
4984000	4986000	If there's no problem today
4986000	4988000	Thank you for your participation
4988000	4990000	Thank you everyone
4990000	4992000	Thank you
4997000	4999000	My personal website is here
4999000	5001000	If you have any questions
5001000	5003000	You're welcome to contact me
5006000	5008000	We will
5008000	5010000	Put the link of the video
5010000	5012000	And
5012000	5014000	Senior's personal website
5014000	5016000	On our announcement
5016000	5018000	On our website
5018000	5020000	Thank you
5020000	5022000	Our recording will be
5022000	5024000	In a day or two
5024000	5026000	We will put it on
5026000	5028000	Pyra's
5028000	5030000	Pyra's website
5030000	5032000	No problem
5032000	5034000	Yeah
5034000	5036000	Okay, thank you everyone
5036000	5038000	Thank you
5038000	5040000	Bye bye
5040000	5042000	Bye bye
