1
00:00:00,000 --> 00:00:07,000
Hello everyone, welcome to today's Tarot Talk.

2
00:00:07,000 --> 00:00:14,000
We are honored to have Pei-Hsueh to give us a talk.

3
00:00:14,000 --> 00:00:22,000
The topic of today's talk has been shown in his introduction video.

4
00:00:22,000 --> 00:00:32,000
Does the deservingness heuristic explain the effect of effort, luck, and need in a redistribution experiment?

5
00:00:32,000 --> 00:00:42,000
Before we start the talk, let me introduce Pei-Hsueh.

6
00:00:42,000 --> 00:00:47,000
Pei-Hsueh is a PhD student from the Department of Political Science at Stony Brook.

7
00:00:47,000 --> 00:00:54,000
His research focuses on the social preferences of people based on experimental methods and computational social science,

8
00:00:54,000 --> 00:00:58,000
and how these preferences affect people's choices in political issues.

9
00:00:58,000 --> 00:01:06,000
Before we start, let's give a warm welcome to Pei-Hsueh for his wonderful talk.

10
00:01:06,000 --> 00:01:10,000
Oh, the microphone is set to be turned off.

11
00:01:10,000 --> 00:01:14,000
If you want Pei-Hsueh to hear the applause, you have to turn on the microphone first.

12
00:01:14,000 --> 00:01:21,000
Let's give a warm round of applause to Pei-Hsueh for his wonderful talk.

13
00:01:21,000 --> 00:01:24,000
Now it's your turn.

14
00:01:24,000 --> 00:01:31,000
This English is a bit awkward, but what I want to talk about today is

15
00:01:31,000 --> 00:01:38,000
how people's preferences affect people's choices in equality and fairness.

16
00:01:38,000 --> 00:01:48,000
I did this research with my advisor, Ruben Klein.

17
00:01:48,000 --> 00:01:56,000
We're almost done, but I plan to do more analysis.

18
00:01:56,000 --> 00:02:00,000
If you have any comments, please feel free to ask.

19
00:02:00,000 --> 00:02:03,000
Let's get started.

20
00:02:03,000 --> 00:02:08,000
First of all, there's a long-standing problem with social scientists.

21
00:02:08,000 --> 00:02:11,000
Do people care about equality?

22
00:02:11,000 --> 00:02:16,000
Many experiments have shown that people do care about equality.

23
00:02:16,000 --> 00:02:24,000
For example, in a dictatorial election, the dictator gets a sum of money.

24
00:02:25,000 --> 00:02:30,000
Even if there's no reason to give their partner any money,

25
00:02:30,000 --> 00:02:33,000
they'll still choose to give a portion.

26
00:02:33,000 --> 00:02:37,000
Even in a non-dictatorial election,

27
00:02:37,000 --> 00:02:45,000
different experiments have shown that people do care about equality.

28
00:02:45,000 --> 00:02:51,000
Scientists have also found that in a non-industrialized society,

29
00:02:51,000 --> 00:02:56,000
people will punish unfair distributors.

30
00:02:56,000 --> 00:03:00,000
So people do care about equality.

31
00:03:00,000 --> 00:03:07,000
If we use a mathematical model to show that people care about equality,

32
00:03:07,000 --> 00:03:11,000
there are several main models.

33
00:03:11,000 --> 00:03:14,000
The first one is called Inequality Aversion.

34
00:03:14,000 --> 00:03:18,000
It was proposed by Feyer and Schmidt in 1999.

35
00:03:18,000 --> 00:03:22,000
Their concept is very straightforward.

36
00:03:22,000 --> 00:03:29,000
Let's say a person's wealth has not changed.

37
00:03:29,000 --> 00:03:32,000
If he has more money than others,

38
00:03:32,000 --> 00:03:37,000
he might feel embarrassed and unhappy.

39
00:03:37,000 --> 00:03:40,000
If he has less money than others,

40
00:03:40,000 --> 00:03:43,000
he might feel even more unhappy.

41
00:03:43,000 --> 00:03:49,000
If he has the same amount of money as others,

42
00:03:49,000 --> 00:03:54,000
he'll be the happiest if his wealth hasn't changed.

43
00:03:54,000 --> 00:03:58,000
For example, if he has more or less money than others,

44
00:03:58,000 --> 00:04:02,000
he'll be happy because of inequality.

45
00:04:02,000 --> 00:04:06,000
It sounds straightforward and doesn't require a mathematical model.

46
00:04:06,000 --> 00:04:09,000
However, it's better to use a mathematical model

47
00:04:09,000 --> 00:04:14,000
to compare parameters and predictions.

48
00:04:14,000 --> 00:04:23,000
The other model is called Inequality Aversion.

49
00:04:23,000 --> 00:04:27,000
It was proposed by Angiolini and Miller in 2002.

50
00:04:27,000 --> 00:04:30,000
The concept is more straightforward.

51
00:04:30,000 --> 00:04:35,000
People care about their happiness and preferences.

52
00:04:35,000 --> 00:04:37,000
It sounds vague.

53
00:04:37,000 --> 00:04:39,000
People care about their preferences.

54
00:04:39,000 --> 00:04:42,000
It sounds like people don't care about themselves,

55
00:04:42,000 --> 00:04:44,000
but care about others.

56
00:04:44,000 --> 00:04:46,000
It's impossible, right?

57
00:04:46,000 --> 00:04:48,000
Most people are not like this.

58
00:04:48,000 --> 00:04:51,000
The basic idea of this model is that

59
00:04:51,000 --> 00:04:54,000
people will consider others' preferences.

60
00:04:54,000 --> 00:04:57,000
No matter what, people will consider others' preferences.

61
00:04:57,000 --> 00:05:06,000
How can we push people to care about equality?

62
00:05:06,000 --> 00:05:12,000
Let me give you an example.

63
00:05:12,000 --> 00:05:25,000
For most of the wealth,

64
00:05:25,000 --> 00:05:27,000
whether it's money or something else,

65
00:05:27,000 --> 00:05:29,000
of course not everything,

66
00:05:29,000 --> 00:05:35,000
but for most of the wealth,

67
00:05:36,000 --> 00:05:40,000
if you add up all the wealth in one unit,

68
00:05:40,000 --> 00:05:42,000
people will reduce the amount.

69
00:05:42,000 --> 00:05:46,000
For example, if I have $100,000,

70
00:05:46,000 --> 00:05:48,000
and I get $100,000,

71
00:05:48,000 --> 00:05:49,000
I will be happy.

72
00:05:49,000 --> 00:05:51,000
But if I have $1 million,

73
00:05:51,000 --> 00:05:53,000
and I get $100,000,

74
00:05:53,000 --> 00:05:54,000
I will still be happy.

75
00:05:54,000 --> 00:05:59,000
But I won't be as happy as when I only have $100,000.

76
00:05:59,000 --> 00:06:01,000
But if I have $100,000,

77
00:06:01,000 --> 00:06:03,000
and I get $100,000,

78
00:06:03,000 --> 00:06:05,000
and I get $100,000,

79
00:06:05,000 --> 00:06:09,000
I might feel that I don't have a good balance,

80
00:06:09,000 --> 00:06:13,000
but I won't be as happy as when I only have $100,000.

81
00:06:13,000 --> 00:06:16,000
In other words,

82
00:06:16,000 --> 00:06:21,000
if someone is already very rich,

83
00:06:21,000 --> 00:06:23,000
and he gets $100,000,

84
00:06:23,000 --> 00:06:25,000
he might be quite happy.

85
00:06:25,000 --> 00:06:29,000
But if this person cares about another person,

86
00:06:29,000 --> 00:06:31,000
no matter how much he cares,

87
00:06:31,000 --> 00:06:34,000
if this $100,000 is for another poor person,

88
00:06:34,000 --> 00:06:36,000
and this person only has $100,000,

89
00:06:36,000 --> 00:06:38,000
and he cares about this person,

90
00:06:38,000 --> 00:06:41,000
then the other person getting $100,000

91
00:06:41,000 --> 00:06:44,000
might bring him more happiness

92
00:06:44,000 --> 00:06:47,000
than getting $100,000 himself.

93
00:06:47,000 --> 00:06:50,000
So assuming that the editor's use of land

94
00:06:50,000 --> 00:06:53,000
and the user's use of other people's use

95
00:06:53,000 --> 00:06:55,000
these two conditions,

96
00:06:55,000 --> 00:06:58,000
it can also be inferred that

97
00:06:58,000 --> 00:07:01,000
people care about equality.

98
00:07:01,000 --> 00:07:03,000
This model can also explain

99
00:07:03,000 --> 00:07:05,000
why people care about equality.

100
00:07:05,000 --> 00:07:07,000
This model can also explain

101
00:07:07,000 --> 00:07:09,000
why people care about equality.

102
00:07:09,000 --> 00:07:11,000
But the question is,

103
00:07:11,000 --> 00:07:13,000
do people really care about equality?

104
00:07:13,000 --> 00:07:15,000
If we find that,

105
00:07:15,000 --> 00:07:17,000
like the big boss might tell you

106
00:07:17,000 --> 00:07:19,000
that there is no bad luck,

107
00:07:19,000 --> 00:07:21,000
only bad luck,

108
00:07:21,000 --> 00:07:23,000
and then he is not willing to donate his money

109
00:07:23,000 --> 00:07:25,000
or ask for tax, right?

110
00:07:25,000 --> 00:07:27,000
Or some politicians might tell you

111
00:07:27,000 --> 00:07:29,000
that some people refuse to work,

112
00:07:29,000 --> 00:07:31,000
and then use this to blow votes.

113
00:07:31,000 --> 00:07:33,000
Does this mean that people

114
00:07:33,000 --> 00:07:35,000
might not care about equality?

115
00:07:35,000 --> 00:07:37,000
What people care about is actually

116
00:07:37,000 --> 00:07:39,000
the so-called fairness.

117
00:07:39,000 --> 00:07:41,000
Of course, fairness can have many definitions,

118
00:07:41,000 --> 00:07:43,000
but what people care about

119
00:07:43,000 --> 00:07:45,000
is how much effort

120
00:07:45,000 --> 00:07:47,000
people should make

121
00:07:47,000 --> 00:07:49,000
to get paid.

122
00:07:49,000 --> 00:07:51,000
What people care about is how much effort

123
00:07:51,000 --> 00:07:53,000
people should make

124
00:07:53,000 --> 00:07:55,000
to get paid.

125
00:07:55,000 --> 00:07:57,000
If the income in the lab

126
00:07:57,000 --> 00:07:59,000
is determined by effort,

127
00:07:59,000 --> 00:08:01,000
is determined by effort,

128
00:08:01,000 --> 00:08:03,000
is determined by effort,

129
00:08:03,000 --> 00:08:05,000
and not by luck,

130
00:08:05,000 --> 00:08:07,000
and not by luck,

131
00:08:07,000 --> 00:08:09,000
the beneficiary will choose

132
00:08:09,000 --> 00:08:11,000
to redistribute less.

133
00:08:11,000 --> 00:08:13,000
In other words,

134
00:08:13,000 --> 00:08:15,000
the beneficiary will choose

135
00:08:15,000 --> 00:08:17,000
not to redistribute the wealth

136
00:08:17,000 --> 00:08:19,000
according to certain taxes.

137
00:08:19,000 --> 00:08:21,000
In other words,

138
00:08:21,000 --> 00:08:23,000
the beneficiary will choose

139
00:08:23,000 --> 00:08:25,000
not to redistribute the wealth

140
00:08:25,000 --> 00:08:27,000
according to certain taxes.

141
00:08:27,000 --> 00:08:29,000
In other words,

142
00:08:29,000 --> 00:08:31,000
if the income is determined by luck,

143
00:08:31,000 --> 00:08:33,000
the beneficiary will have more social benefits

144
00:08:33,000 --> 00:08:35,000
and higher tax payments.

145
00:08:35,000 --> 00:08:37,000
and higher tax payments.

146
00:08:37,000 --> 00:08:39,000
and higher tax payments.

147
00:08:39,000 --> 00:08:41,000
If the beneficiary

148
00:08:41,000 --> 00:08:43,000
believes that

149
00:08:43,000 --> 00:08:45,000
everyone's income

150
00:08:45,000 --> 00:08:47,000
is determined by effort,

151
00:08:47,000 --> 00:08:49,000
not by luck,

152
00:08:49,000 --> 00:08:51,000
the beneficiary will not

153
00:08:51,000 --> 00:08:53,000
receive social benefits

154
00:08:53,000 --> 00:08:55,000
and lower taxes.

155
00:08:55,000 --> 00:08:57,000
and lower taxes.

156
00:08:57,000 --> 00:08:59,000
Recently,

157
00:08:59,000 --> 00:09:01,000
I've been

158
00:09:01,000 --> 00:09:03,000
talking about

159
00:09:03,000 --> 00:09:05,000
why people

160
00:09:05,000 --> 00:09:07,000
prefer an unequal society.

161
00:09:07,000 --> 00:09:09,000
Why do people think

162
00:09:09,000 --> 00:09:11,000
an unequal society is fair?

163
00:09:17,000 --> 00:09:19,000
Today,

164
00:09:19,000 --> 00:09:21,000
we're going to talk about

165
00:09:21,000 --> 00:09:23,000
whether people

166
00:09:23,000 --> 00:09:25,000
only care about

167
00:09:25,000 --> 00:09:27,000
equality or fairness.

168
00:09:27,000 --> 00:09:29,000
Of course not.

169
00:09:29,000 --> 00:09:31,000
Most people

170
00:09:31,000 --> 00:09:33,000
care about equality and fairness.

171
00:09:33,000 --> 00:09:35,000
Today,

172
00:09:35,000 --> 00:09:37,000
we want to

173
00:09:37,000 --> 00:09:39,000
build a model

174
00:09:39,000 --> 00:09:41,000
that can explain

175
00:09:41,000 --> 00:09:43,000
people's preference for equality

176
00:09:43,000 --> 00:09:45,000
and fairness.

177
00:09:45,000 --> 00:09:47,000
Let's go back

178
00:09:47,000 --> 00:09:49,000
to our model.

179
00:09:49,000 --> 00:09:51,000
Let's assume

180
00:09:51,000 --> 00:09:53,000
that this model

181
00:09:53,000 --> 00:09:55,000
only cares about

182
00:09:55,000 --> 00:09:57,000
other people.

183
00:09:57,000 --> 00:09:59,000
Of course,

184
00:09:59,000 --> 00:10:01,000
the degree of care

185
00:10:01,000 --> 00:10:03,000
may vary

186
00:10:03,000 --> 00:10:05,000
depending on the situation

187
00:10:05,000 --> 00:10:07,000
and individual personality

188
00:10:07,000 --> 00:10:09,000
and experience.

189
00:10:09,000 --> 00:10:11,000
Today,

190
00:10:11,000 --> 00:10:13,000
we want to

191
00:10:13,000 --> 00:10:15,000
put the degree of effort

192
00:10:15,000 --> 00:10:17,000
into the model.

193
00:10:17,000 --> 00:10:19,000
Let's say

194
00:10:19,000 --> 00:10:21,000
there is a decision-maker

195
00:10:21,000 --> 00:10:23,000
and

196
00:10:23,000 --> 00:10:25,000
there are two other people.

197
00:10:25,000 --> 00:10:27,000
Let's say

198
00:10:27,000 --> 00:10:29,000
one person is very hard-working.

199
00:10:29,000 --> 00:10:31,000
The decision-maker

200
00:10:31,000 --> 00:10:33,000
cares about the hard-working person more.

201
00:10:33,000 --> 00:10:35,000
Let's say

202
00:10:35,000 --> 00:10:37,000
the wealth of these two people

203
00:10:37,000 --> 00:10:39,000
is unequal,

204
00:10:39,000 --> 00:10:41,000
and the decision-maker is rich.

205
00:10:41,000 --> 00:10:43,000
Let's say

206
00:10:43,000 --> 00:10:45,000
the wealth of this person is not equal,

207
00:10:45,000 --> 00:10:47,000
but the decision-maker is hard-working.

208
00:10:47,000 --> 00:10:49,000
The decision-maker cares about the recipient,

209
00:10:49,000 --> 00:10:51,000
and decides to

210
00:10:51,000 --> 00:10:53,000
redistribute the wealth

211
00:10:53,000 --> 00:10:55,000
to the recipient.

212
00:10:55,000 --> 00:10:57,000
On the other hand,

213
00:10:57,000 --> 00:10:59,000
if the recipient

214
00:10:59,000 --> 00:11:01,000
is not hard-working,

215
00:11:01,000 --> 00:11:03,000
the decision-maker

216
00:11:03,000 --> 00:11:05,000
will not care about the recipient's preference

217
00:11:05,000 --> 00:11:07,000
and choose to give the recipient

218
00:11:07,000 --> 00:11:09,000
less money.

219
00:11:09,000 --> 00:11:11,000
In order

220
00:11:11,000 --> 00:11:13,000
to verify

221
00:11:13,000 --> 00:11:15,000
the prediction of our model,

222
00:11:15,000 --> 00:11:17,000
we designed

223
00:11:17,000 --> 00:11:19,000
a laboratory experiment.

224
00:11:19,000 --> 00:11:21,000
The experiment

225
00:11:21,000 --> 00:11:23,000
was conducted

226
00:11:23,000 --> 00:11:25,000
by the students

227
00:11:25,000 --> 00:11:27,000
of Stony Brook University.

228
00:11:27,000 --> 00:11:29,000
In this experiment,

229
00:11:29,000 --> 00:11:31,000
we will give them

230
00:11:31,000 --> 00:11:33,000
different amounts of money

231
00:11:33,000 --> 00:11:35,000
based on their decisions.

232
00:11:35,000 --> 00:11:37,000
So,

233
00:11:37,000 --> 00:11:39,000
the money they can get

234
00:11:39,000 --> 00:11:41,000
after the experiment

235
00:11:41,000 --> 00:11:43,000
is determined

236
00:11:43,000 --> 00:11:45,000
by their decisions

237
00:11:45,000 --> 00:11:47,000
in the laboratory.

238
00:11:47,000 --> 00:11:49,000
This is our laboratory,

239
00:11:49,000 --> 00:11:51,000
but this is just a demo.

240
00:11:51,000 --> 00:11:53,000
This is not my experiment.

241
00:11:53,000 --> 00:11:55,000
So, the people here

242
00:11:55,000 --> 00:11:57,000
look a little old.

243
00:11:57,000 --> 00:11:59,000
This is the professor,

244
00:11:59,000 --> 00:12:01,000
and this seems to be

245
00:12:01,000 --> 00:12:03,000
a demo for the website.

246
00:12:03,000 --> 00:12:05,000
This experiment

247
00:12:05,000 --> 00:12:07,000
was not conducted

248
00:12:07,000 --> 00:12:09,000
in the laboratory,

249
00:12:09,000 --> 00:12:11,000
because the recipient

250
00:12:11,000 --> 00:12:13,000
was in the middle of the epidemic.

251
00:12:13,000 --> 00:12:15,000
So, the experiment was conducted

252
00:12:15,000 --> 00:12:17,000
on-site.

253
00:12:17,000 --> 00:12:19,000
This is the design

254
00:12:19,000 --> 00:12:21,000
of our laboratory.

255
00:12:21,000 --> 00:12:23,000
At the beginning

256
00:12:23,000 --> 00:12:25,000
of the experiment,

257
00:12:25,000 --> 00:12:27,000
the recipient

258
00:12:27,000 --> 00:12:29,000
can choose

259
00:12:29,000 --> 00:12:31,000
to give the recipient

260
00:12:31,000 --> 00:12:33,000
less money

261
00:12:33,000 --> 00:12:35,000
based on their decisions.

262
00:12:35,000 --> 00:12:37,000
At the beginning of the experiment,

263
00:12:37,000 --> 00:12:39,000
the recipient

264
00:12:39,000 --> 00:12:41,000
can decide whether

265
00:12:41,000 --> 00:12:43,000
to do this task,

266
00:12:43,000 --> 00:12:45,000
which we call

267
00:12:45,000 --> 00:12:47,000
the counting zero task.

268
00:12:47,000 --> 00:12:49,000
They need to write

269
00:12:49,000 --> 00:12:51,000
the answer here

270
00:12:51,000 --> 00:12:53,000
and send it out.

271
00:12:53,000 --> 00:12:55,000
When they get it right,

272
00:12:55,000 --> 00:12:57,000
they will get a reward.

273
00:12:57,000 --> 00:12:59,000
The reward for getting it right

274
00:12:59,000 --> 00:13:01,000
is random.

275
00:13:01,000 --> 00:13:03,000
Each recipient

276
00:13:03,000 --> 00:13:05,000
can decide

277
00:13:05,000 --> 00:13:07,000
how many tasks

278
00:13:07,000 --> 00:13:09,000
they want to do.

279
00:13:09,000 --> 00:13:11,000
They can choose

280
00:13:11,000 --> 00:13:13,000
not to do it

281
00:13:13,000 --> 00:13:15,000
and skip this step.

282
00:13:15,000 --> 00:13:17,000
They can choose

283
00:13:17,000 --> 00:13:19,000
to do up to 10 tasks.

284
00:13:19,000 --> 00:13:21,000
After this task,

285
00:13:21,000 --> 00:13:23,000
we will randomly

286
00:13:23,000 --> 00:13:25,000
pair the recipients.

287
00:13:25,000 --> 00:13:27,000
One recipient

288
00:13:27,000 --> 00:13:29,000
will be designated as

289
00:13:29,000 --> 00:13:31,000
the decider.

290
00:13:31,000 --> 00:13:33,000
The other recipient

291
00:13:33,000 --> 00:13:35,000
will be designated as the recipient.

292
00:13:35,000 --> 00:13:37,000
The decider can decide

293
00:13:37,000 --> 00:13:39,000
how much money

294
00:13:39,000 --> 00:13:41,000
they want to earn

295
00:13:41,000 --> 00:13:43,000
from this task

296
00:13:43,000 --> 00:13:45,000
to the recipient.

297
00:13:45,000 --> 00:13:47,000
The decider will know

298
00:13:47,000 --> 00:13:49,000
how much the recipient's salary is

299
00:13:49,000 --> 00:13:51,000
and how many tasks they need to complete.

300
00:13:51,000 --> 00:13:53,000
In this experiment,

301
00:13:53,000 --> 00:13:55,000
we use a method

302
00:13:55,000 --> 00:13:57,000
called the strategy method.

303
00:13:57,000 --> 00:13:59,000
This method

304
00:13:59,000 --> 00:14:01,000
allows the recipient

305
00:14:01,000 --> 00:14:03,000
to answer

306
00:14:03,000 --> 00:14:05,000
the combination of

307
00:14:05,000 --> 00:14:07,000
all the salary and tasks.

308
00:14:07,000 --> 00:14:09,000
There are two salaries

309
00:14:09,000 --> 00:14:11,000
and 11 different

310
00:14:11,000 --> 00:14:13,000
effort levels.

311
00:14:13,000 --> 00:14:15,000
There are 22 combinations.

312
00:14:15,000 --> 00:14:17,000
The decider

313
00:14:17,000 --> 00:14:19,000
needs to answer

314
00:14:19,000 --> 00:14:21,000
the question

315
00:14:21,000 --> 00:14:23,000
how much money

316
00:14:23,000 --> 00:14:25,000
they want to earn

317
00:14:25,000 --> 00:14:27,000
from this task

318
00:14:27,000 --> 00:14:29,000
to the recipient.

319
00:14:29,000 --> 00:14:31,000
In the end,

320
00:14:31,000 --> 00:14:33,000
the decider

321
00:14:33,000 --> 00:14:35,000
will know

322
00:14:35,000 --> 00:14:37,000
how much money

323
00:14:37,000 --> 00:14:39,000
the recipient

324
00:14:39,000 --> 00:14:41,000
earns

325
00:14:41,000 --> 00:14:43,000
from this task

326
00:14:43,000 --> 00:14:45,000
to the recipient.

327
00:14:45,000 --> 00:14:47,000
The decider will know

328
00:14:47,000 --> 00:14:49,000
how much money

329
00:14:49,000 --> 00:14:51,000
the recipient

330
00:14:51,000 --> 00:14:53,000
earns

331
00:14:53,000 --> 00:14:55,000
from this task

332
00:14:55,000 --> 00:14:57,000
to the recipient.

333
00:14:57,000 --> 00:14:59,000
In the end,

334
00:14:59,000 --> 00:15:01,000
the decider

335
00:15:01,000 --> 00:15:03,000
will know

336
00:15:03,000 --> 00:15:05,000
how much money

337
00:15:05,000 --> 00:15:07,000
the recipient

338
00:15:07,000 --> 00:15:09,000
earns

339
00:15:09,000 --> 00:15:11,000
from this task

340
00:15:11,000 --> 00:15:13,000
to the recipient.

341
00:15:13,000 --> 00:15:15,000
Our model

342
00:15:15,000 --> 00:15:17,000
is called

343
00:15:17,000 --> 00:15:19,000
the conditional model.

344
00:15:19,000 --> 00:15:21,000
In this model,

345
00:15:21,000 --> 00:15:23,000
this is

346
00:15:23,000 --> 00:15:25,000
the income

347
00:15:25,000 --> 00:15:27,000
of the decider.

348
00:15:27,000 --> 00:15:29,000
T is how much money

349
00:15:29,000 --> 00:15:31,000
the decider wants to give to the recipient.

350
00:15:31,000 --> 00:15:33,000
M is

351
00:15:33,000 --> 00:15:35,000
the salary

352
00:15:35,000 --> 00:15:37,000
of the recipient,

353
00:15:37,000 --> 00:15:39,000
$1 or $2.

354
00:15:39,000 --> 00:15:41,000
E is how many tasks

355
00:15:41,000 --> 00:15:43,000
the decider has completed.

356
00:15:43,000 --> 00:15:45,000
T is how much money

357
00:15:45,000 --> 00:15:47,000
the receiver earns.

358
00:15:47,000 --> 00:15:49,000
Theta is

359
00:15:49,000 --> 00:15:51,000
the weight of the decider.

360
00:15:51,000 --> 00:15:53,000
Theta is the weight of the decider.

361
00:15:53,000 --> 00:15:55,000
Theta is the weight of the decider.

362
00:15:55,000 --> 00:15:57,000
In this case,

363
00:15:57,000 --> 00:15:59,000
Theta is the value of

364
00:15:59,000 --> 00:16:01,000
the function of labor.

365
00:16:01,000 --> 00:16:03,000
Alpha is

366
00:16:03,000 --> 00:16:05,000
the parameter of

367
00:16:05,000 --> 00:16:07,000
the marginal cost of the decision.

368
00:16:07,000 --> 00:16:09,000
Therefore, the alpha determines

369
00:16:09,000 --> 00:16:11,000
the marginal cost of the decision.

370
00:16:11,000 --> 00:16:13,000
The higher the alpha,

371
00:16:13,000 --> 00:16:15,000
the lower the value.

372
00:16:15,000 --> 00:16:17,000
In other words,

373
00:16:17,000 --> 00:16:19,000
the higher the alpha,

374
00:16:19,000 --> 00:16:21,000
the lower the marginal cost of the decision.

375
00:16:21,000 --> 00:16:23,000
We can determine

376
00:16:23,000 --> 00:16:25,000
how much money

377
00:16:25,000 --> 00:16:27,000
the decider

378
00:16:27,000 --> 00:16:29,000
will give

379
00:16:29,000 --> 00:16:31,000
to the recipient

380
00:16:31,000 --> 00:16:33,000
based on

381
00:16:33,000 --> 00:16:35,000
each factor.

382
00:16:35,000 --> 00:16:37,000
We can determine

383
00:16:37,000 --> 00:16:39,000
how much money

384
00:16:39,000 --> 00:16:41,000
the decider

385
00:16:41,000 --> 00:16:43,000
will give

386
00:16:43,000 --> 00:16:45,000
to the recipient

387
00:16:45,000 --> 00:16:47,000
based on

388
00:16:47,000 --> 00:16:49,000
each factor.

389
00:16:49,000 --> 00:16:51,000
based on

390
00:16:51,000 --> 00:16:53,000
each factor.

391
00:16:53,000 --> 00:16:55,000
There are

392
00:16:55,000 --> 00:16:57,000
several predictions

393
00:16:57,000 --> 00:16:59,000
under this model.

394
00:16:59,000 --> 00:17:01,000
The first prediction is that

395
00:17:01,000 --> 00:17:03,000
as long as the weight

396
00:17:03,000 --> 00:17:05,000
is greater than or equal to 0,

397
00:17:05,000 --> 00:17:07,000
the income of the decider

398
00:17:07,000 --> 00:17:09,000
and the degree of labor of the recipient

399
00:17:09,000 --> 00:17:11,000
will remain the same

400
00:17:11,000 --> 00:17:13,000
or less when the income of the receiver

401
00:17:13,000 --> 00:17:15,000
is greater than or equal to 0.

402
00:17:15,000 --> 00:17:17,000
This is called

403
00:17:17,000 --> 00:17:19,000
inequality effect.

404
00:17:19,000 --> 00:17:21,000
The second prediction

405
00:17:21,000 --> 00:17:23,000
is that

406
00:17:23,000 --> 00:17:25,000
as long as the income of the decider

407
00:17:25,000 --> 00:17:27,000
and the degree of labor of the recipient

408
00:17:27,000 --> 00:17:29,000
is greater than or equal to 0,

409
00:17:29,000 --> 00:17:31,000
the income of the decider

410
00:17:31,000 --> 00:17:33,000
will remain the same or less.

411
00:17:33,000 --> 00:17:35,000
In our experiment,

412
00:17:35,000 --> 00:17:37,000
we found that

413
00:17:37,000 --> 00:17:39,000
90% of the receivers

414
00:17:39,000 --> 00:17:41,000
met the first

415
00:17:41,000 --> 00:17:43,000
inequality effect.

416
00:17:43,000 --> 00:17:45,000
inequality effect.

417
00:17:45,000 --> 00:17:47,000
In addition,

418
00:17:47,000 --> 00:17:49,000
we found that

419
00:17:49,000 --> 00:17:51,000
85% of the receivers

420
00:17:51,000 --> 00:17:53,000
met the

421
00:17:53,000 --> 00:17:55,000
deservingness effect.

422
00:17:55,000 --> 00:17:57,000
deservingness effect.

423
00:17:57,000 --> 00:17:59,000
deservingness effect.

424
00:17:59,000 --> 00:18:01,000
In addition,

425
00:18:01,000 --> 00:18:03,000
we ran a random effect regression.

426
00:18:03,000 --> 00:18:05,000
We found that

427
00:18:05,000 --> 00:18:07,000
as long as the

428
00:18:07,000 --> 00:18:09,000
income of the viewers

429
00:18:09,000 --> 00:18:11,000
is higher

430
00:18:11,000 --> 00:18:13,000
than or equal to 0,

431
00:18:13,000 --> 00:18:15,000
the income of the

432
00:18:15,000 --> 00:18:17,000
recipients

433
00:18:17,000 --> 00:18:19,000
is also higher.

434
00:18:19,000 --> 00:18:21,000
If the

435
00:18:21,000 --> 00:18:23,000
income of the

436
00:18:23,000 --> 00:18:25,000
æ™ºlr celebrities

437
00:18:25,000 --> 00:18:27,000
is better than

438
00:18:27,000 --> 00:18:29,000
or equal to 0,

439
00:18:29,000 --> 00:18:31,000
all providers

440
00:18:31,000 --> 00:18:33,000
will offer

441
00:18:33,000 --> 00:18:35,000
a higher

442
00:18:35,000 --> 00:18:37,000
income than

443
00:18:37,000 --> 00:18:39,000
or equal to 0.

444
00:18:39,000 --> 00:18:41,000
The other

445
00:18:41,000 --> 00:18:43,000
prediction is

446
00:18:43,000 --> 00:18:45,000
that

447
00:18:45,000 --> 00:18:47,000
we actually

448
00:18:47,000 --> 00:18:49,000
specify some parameters

449
00:18:49,000 --> 00:18:51,000
for the model.

450
00:18:51,000 --> 00:18:53,000
We actually

451
00:18:53,000 --> 00:18:55,000
assume

452
00:18:55,000 --> 00:18:57,000
the weight

453
00:18:57,000 --> 00:18:59,000
function.

454
00:18:59,000 --> 00:19:01,000
We assume

455
00:19:01,000 --> 00:19:03,000
it is linear.

456
00:19:03,000 --> 00:19:05,000
If the weight

457
00:19:05,000 --> 00:19:07,000
is a baseline weight,

458
00:19:07,000 --> 00:19:09,000
that is,

459
00:19:09,000 --> 00:19:11,000
when the collector

460
00:19:11,000 --> 00:19:13,000
does not work hard at all,

461
00:19:13,000 --> 00:19:15,000
how much is the

462
00:19:15,000 --> 00:19:17,000
basic weight?

463
00:19:17,000 --> 00:19:19,000
How much does the

464
00:19:19,000 --> 00:19:21,000
collector care about

465
00:19:21,000 --> 00:19:23,000
the recipient?

466
00:19:23,000 --> 00:19:25,000
Another parameter is

467
00:19:25,000 --> 00:19:27,000
that once the

468
00:19:27,000 --> 00:19:29,000
recipient increases

469
00:19:29,000 --> 00:19:31,000
how much does the

470
00:19:31,000 --> 00:19:33,000
weight increase?

471
00:19:33,000 --> 00:19:35,000
We use two different

472
00:19:35,000 --> 00:19:37,000
parameters as an

473
00:19:37,000 --> 00:19:39,000
assumption.

474
00:19:39,000 --> 00:19:41,000
The first parameter

475
00:19:41,000 --> 00:19:43,000
is that the

476
00:19:43,000 --> 00:19:45,000
weight baseline is very high.

477
00:19:45,000 --> 00:19:47,000
When the recipient

478
00:19:47,000 --> 00:19:49,000
does not work hard at all,

479
00:19:49,000 --> 00:19:51,000
the collector cares

480
00:19:51,000 --> 00:19:53,000
very much about the

481
00:19:53,000 --> 00:19:55,000
recipient.

482
00:19:55,000 --> 00:19:57,000
The picture on the

483
00:19:57,000 --> 00:19:59,000
left shows

484
00:19:59,000 --> 00:20:01,000
that the

485
00:20:01,000 --> 00:20:03,000
weight baseline

486
00:20:03,000 --> 00:20:05,000
is very low.

487
00:20:05,000 --> 00:20:07,000
When the

488
00:20:07,000 --> 00:20:09,000
recipient does not

489
00:20:09,000 --> 00:20:11,000
work at all,

490
00:20:11,000 --> 00:20:13,000
the collector does

491
00:20:13,000 --> 00:20:15,000
not care about

492
00:20:15,000 --> 00:20:17,000
the recipient.

493
00:20:17,000 --> 00:20:19,000
However,

494
00:20:19,000 --> 00:20:21,000
when the

495
00:20:21,000 --> 00:20:23,000
recipient increases

496
00:20:23,000 --> 00:20:25,000
by one unit,

497
00:20:25,000 --> 00:20:27,000
the weight

498
00:20:27,000 --> 00:20:29,000
is the

499
00:20:29,000 --> 00:20:31,000
income of the

500
00:20:31,000 --> 00:20:33,000
recipient.

501
00:20:33,000 --> 00:20:35,000
The weight

502
00:20:35,000 --> 00:20:37,000
determines

503
00:20:37,000 --> 00:20:39,000
how much

504
00:20:39,000 --> 00:20:41,000
the collector

505
00:20:41,000 --> 00:20:43,000
has to pay

506
00:20:43,000 --> 00:20:45,000
to the recipient.

507
00:20:45,000 --> 00:20:47,000
The higher the

508
00:20:47,000 --> 00:20:49,000
recipient's income,

509
00:20:49,000 --> 00:20:51,000
the higher

510
00:20:51,000 --> 00:20:53,000
the recipient's

511
00:20:53,000 --> 00:20:55,000
salary.

512
00:20:55,000 --> 00:20:57,000
As you can see,

513
00:20:57,000 --> 00:20:59,000
the two lines

514
00:20:59,000 --> 00:21:01,000
on the left

515
00:21:01,000 --> 00:21:03,000
are overlapping.

516
00:21:03,000 --> 00:21:05,000
In other words,

517
00:21:05,000 --> 00:21:07,000
the collector

518
00:21:07,000 --> 00:21:09,000
does not care

519
00:21:09,000 --> 00:21:11,000
about the

520
00:21:11,000 --> 00:21:13,000
recipient's salary,

521
00:21:13,000 --> 00:21:15,000
but the weight

522
00:21:15,000 --> 00:21:17,000
baseline is very high,

523
00:21:17,000 --> 00:21:19,000
so the salary

524
00:21:19,000 --> 00:21:21,000
does not matter.

525
00:21:21,000 --> 00:21:23,000
However,

526
00:21:23,000 --> 00:21:25,000
the higher the

527
00:21:25,000 --> 00:21:27,000
recipient's income,

528
00:21:27,000 --> 00:21:29,000
the less

529
00:21:29,000 --> 00:21:31,000
the collector

530
00:21:31,000 --> 00:21:33,000
has to pay

531
00:21:33,000 --> 00:21:35,000
to the recipient.

532
00:21:35,000 --> 00:21:37,000
On the right,

533
00:21:37,000 --> 00:21:39,000
the two lines

534
00:21:39,000 --> 00:21:41,000
on the left

535
00:21:41,000 --> 00:21:43,000
are overlapping.

536
00:21:43,000 --> 00:21:45,000
The higher the

537
00:21:45,000 --> 00:21:47,000
recipient's income,

538
00:21:47,000 --> 00:21:49,000
the higher

539
00:21:49,000 --> 00:21:51,000
the collector

540
00:21:51,000 --> 00:21:53,000
has to pay

541
00:21:53,000 --> 00:21:55,000
to the recipient.

542
00:21:55,000 --> 00:21:57,000
The higher the

543
00:21:57,000 --> 00:21:59,000
recipient's income,

544
00:21:59,000 --> 00:22:01,000
the more

545
00:22:01,000 --> 00:22:03,000
the collector

546
00:22:03,000 --> 00:22:05,000
has to pay

547
00:22:05,000 --> 00:22:07,000
to the recipient.

548
00:22:07,000 --> 00:22:09,000
According to our

549
00:22:09,000 --> 00:22:11,000
experiment results,

550
00:22:11,000 --> 00:22:13,000
we can divide

551
00:22:13,000 --> 00:22:15,000
these collectors

552
00:22:15,000 --> 00:22:17,000
into four types.

553
00:22:17,000 --> 00:22:19,000
The first type

554
00:22:19,000 --> 00:22:21,000
is sensitive

555
00:22:21,000 --> 00:22:23,000
to the recipient's

556
00:22:23,000 --> 00:22:25,000
salary.

557
00:22:25,000 --> 00:22:27,000
This type

558
00:22:27,000 --> 00:22:29,000
accounts for

559
00:22:29,000 --> 00:22:31,000
25% of the collectors.

560
00:22:31,000 --> 00:22:33,000
The second type

561
00:22:33,000 --> 00:22:35,000
is insensitive

562
00:22:35,000 --> 00:22:37,000
to the

563
00:22:37,000 --> 00:22:39,000
recipient's salary.

564
00:22:39,000 --> 00:22:41,000
This type

565
00:22:41,000 --> 00:22:43,000
accounts for

566
00:22:43,000 --> 00:22:45,000
20% of the collectors.

567
00:22:45,000 --> 00:22:47,000
The third type

568
00:22:47,000 --> 00:22:49,000
is selfish

569
00:22:49,000 --> 00:22:51,000
to the recipient's

570
00:22:51,000 --> 00:22:53,000
salary.

571
00:22:53,000 --> 00:22:55,000
The last type

572
00:22:55,000 --> 00:22:57,000
is other.

573
00:22:57,000 --> 00:22:59,000
The last type

574
00:22:59,000 --> 00:23:01,000
is other.

575
00:23:01,000 --> 00:23:03,000
The last type

576
00:23:03,000 --> 00:23:05,000
is other.

577
00:23:05,000 --> 00:23:07,000
When we analyze

578
00:23:07,000 --> 00:23:09,000
sensitive type and

579
00:23:09,000 --> 00:23:11,000
insensitive type,

580
00:23:11,000 --> 00:23:13,000
we find that

581
00:23:13,000 --> 00:23:15,000
the result

582
00:23:15,000 --> 00:23:17,000
is

583
00:23:17,000 --> 00:23:19,000
consistent

584
00:23:19,000 --> 00:23:21,000
with our

585
00:23:21,000 --> 00:23:23,000
theoretical prediction.

586
00:23:27,000 --> 00:23:29,000
The main conclusion

587
00:23:29,000 --> 00:23:31,000
of our research

588
00:23:31,000 --> 00:23:33,000
is that

589
00:23:33,000 --> 00:23:35,000
our model

590
00:23:35,000 --> 00:23:37,000
considers equality and fairness.

591
00:23:37,000 --> 00:23:39,000
Most collectors

592
00:23:39,000 --> 00:23:41,000
have the same model prediction.

593
00:23:41,000 --> 00:23:43,000
We also found that

594
00:23:43,000 --> 00:23:45,000
collectors can be classified into

595
00:23:45,000 --> 00:23:47,000
three types,

596
00:23:47,000 --> 00:23:49,000
selfish collectors,

597
00:23:49,000 --> 00:23:51,000
sensitive collectors,

598
00:23:51,000 --> 00:23:53,000
and fair collectors.

599
00:23:53,000 --> 00:23:55,000
Sorry,

600
00:23:55,000 --> 00:23:57,000
that's all

601
00:23:57,000 --> 00:23:59,000
for today.

602
00:23:59,000 --> 00:24:01,000
Thank you.

603
00:24:01,000 --> 00:24:03,000
This is my Twitter

604
00:24:03,000 --> 00:24:05,000
and personal website.

605
00:24:05,000 --> 00:24:07,000
You are welcome

606
00:24:07,000 --> 00:24:09,000
to join my Twitter.

607
00:24:09,000 --> 00:24:11,000
Thank you.

608
00:24:11,000 --> 00:24:13,000
Before we start

609
00:24:13,000 --> 00:24:15,000
the Q&A,

610
00:24:15,000 --> 00:24:17,000
please turn on your

611
00:24:17,000 --> 00:24:19,000
microphone.

612
00:24:19,000 --> 00:24:21,000
Let's welcome

613
00:24:21,000 --> 00:24:23,000
PeiXun.

614
00:24:25,000 --> 00:24:27,000
Do you have any questions?

615
00:24:27,000 --> 00:24:29,000
You can ask

616
00:24:29,000 --> 00:24:31,000
directly.

617
00:24:31,000 --> 00:24:33,000
You can also

618
00:24:33,000 --> 00:24:35,000
type your questions

619
00:24:35,000 --> 00:24:37,000
in the chat box.

620
00:24:37,000 --> 00:24:39,000
Do you have any questions?

621
00:24:39,000 --> 00:24:41,000
You can ask

622
00:24:41,000 --> 00:24:43,000
directly.

623
00:24:43,000 --> 00:24:45,000
Hi,

624
00:24:45,000 --> 00:24:47,000
I'm Jeffrey from

625
00:24:47,000 --> 00:24:49,000
Indiana University Bloomington.

626
00:24:49,000 --> 00:24:51,000
I'm also a fifth-year professor.

627
00:24:51,000 --> 00:24:53,000
Thank you, PeiXun.

628
00:24:53,000 --> 00:24:55,000
Sorry,

629
00:24:55,000 --> 00:24:57,000
it's PeiXun.

630
00:24:57,000 --> 00:24:59,000
I think your topic is

631
00:24:59,000 --> 00:25:01,000
very interesting.

632
00:25:01,000 --> 00:25:03,000
After listening to your

633
00:25:03,000 --> 00:25:05,000
experimental design,

634
00:25:05,000 --> 00:25:07,000
I have some questions.

635
00:25:07,000 --> 00:25:09,000
First of all,

636
00:25:09,000 --> 00:25:11,000
you didn't talk much about

637
00:25:11,000 --> 00:25:13,000
decision-makers and recipients.

638
00:25:13,000 --> 00:25:15,000
In your experimental design,

639
00:25:15,000 --> 00:25:17,000
did you design

640
00:25:17,000 --> 00:25:19,000
decision-makers to give money

641
00:25:19,000 --> 00:25:21,000
to the recipients?

642
00:25:21,000 --> 00:25:23,000
Or did you just say

643
00:25:23,000 --> 00:25:25,000
there will be a part like this?

644
00:25:25,000 --> 00:25:27,000
There is no right answer.

645
00:25:27,000 --> 00:25:29,000
Sorry.

646
00:25:29,000 --> 00:25:31,000
There is no right answer.

647
00:25:31,000 --> 00:25:33,000
He can make such a decision.

648
00:25:33,000 --> 00:25:35,000
We will ask him

649
00:25:35,000 --> 00:25:37,000
if the recipient's salary

650
00:25:37,000 --> 00:25:39,000
is $1

651
00:25:39,000 --> 00:25:41,000
or $2.

652
00:25:41,000 --> 00:25:43,000
If the recipient

653
00:25:43,000 --> 00:25:45,000
completes

654
00:25:45,000 --> 00:25:47,000
several tasks,

655
00:25:47,000 --> 00:25:49,000
how much money

656
00:25:49,000 --> 00:25:51,000
does he want to give to the recipient?

657
00:25:51,000 --> 00:25:53,000
He can choose not to give.

658
00:25:53,000 --> 00:25:55,000
We didn't give him any reason.

659
00:25:55,000 --> 00:25:57,000
How much money he can get

660
00:25:57,000 --> 00:25:59,000
is determined

661
00:25:59,000 --> 00:26:01,000
by his decisions.

662
00:26:01,000 --> 00:26:03,000
If he decides to give more money

663
00:26:03,000 --> 00:26:05,000
to the recipient,

664
00:26:05,000 --> 00:26:07,000
he will get less money in the end.

665
00:26:07,000 --> 00:26:09,000
In fact,

666
00:26:09,000 --> 00:26:11,000
if he is a

667
00:26:11,000 --> 00:26:13,000
direct recipient,

668
00:26:13,000 --> 00:26:15,000
there is no reason

669
00:26:15,000 --> 00:26:17,000
to give money to the recipient.

670
00:26:17,000 --> 00:26:19,000
The more money he gives,

671
00:26:19,000 --> 00:26:21,000
the less money he will get.

672
00:26:21,000 --> 00:26:23,000
I'm a little confused.

673
00:26:23,000 --> 00:26:25,000
I'm not convinced.

674
00:26:25,000 --> 00:26:27,000
It depends on

675
00:26:27,000 --> 00:26:29,000
how you get the income.

676
00:26:29,000 --> 00:26:31,000
Generally speaking,

677
00:26:31,000 --> 00:26:33,000
rich people

678
00:26:33,000 --> 00:26:35,000
are generous.

679
00:26:35,000 --> 00:26:37,000
However,

680
00:26:37,000 --> 00:26:39,000
I don't know.

681
00:26:39,000 --> 00:26:41,000
I don't think

682
00:26:41,000 --> 00:26:43,000
he is generous enough.

683
00:26:43,000 --> 00:26:45,000
Maybe there are many reasons

684
00:26:45,000 --> 00:26:47,000
why he doesn't give money.

685
00:26:47,000 --> 00:26:49,000
I don't think

686
00:26:49,000 --> 00:26:51,000
it has anything to do

687
00:26:51,000 --> 00:26:53,000
with strategic thinking.

688
00:26:53,000 --> 00:26:55,000
I don't think

689
00:26:55,000 --> 00:26:57,000
it has anything to do

690
00:26:57,000 --> 00:26:59,000
with strategic thinking.

691
00:26:59,000 --> 00:27:01,000
I don't think

692
00:27:01,000 --> 00:27:03,000
it has anything to do

693
00:27:03,000 --> 00:27:05,000
with strategic thinking.

694
00:27:05,000 --> 00:27:07,000
For example,

695
00:27:07,000 --> 00:27:09,000
if you win a lottery

696
00:27:09,000 --> 00:27:11,000
and win 10 million,

697
00:27:11,000 --> 00:27:13,000
you can give

698
00:27:13,000 --> 00:27:15,000
10 yuan to the recipient.

699
00:27:15,000 --> 00:27:17,000
How generous do you think

700
00:27:17,000 --> 00:27:19,000
he is?

701
00:27:19,000 --> 00:27:21,000
How generous

702
00:27:21,000 --> 00:27:23,000
do you think

703
00:27:23,000 --> 00:27:25,000
he is?

704
00:27:25,000 --> 00:27:27,000
For example,

705
00:27:27,000 --> 00:27:29,000
you can see that

706
00:27:29,000 --> 00:27:31,000
the one who earns more

707
00:27:31,000 --> 00:27:33,000
is more likely to give the money.

708
00:27:33,000 --> 00:27:35,000
The one who earns less

709
00:27:35,000 --> 00:27:37,000
is more likely to give the money.

710
00:27:37,000 --> 00:27:39,000
Do you think

711
00:27:39,000 --> 00:27:41,000
he is more generous?

712
00:27:41,000 --> 00:27:43,000
I think

713
00:27:43,000 --> 00:27:45,000
he is more generous

714
00:27:45,000 --> 00:27:47,000
because he is rich.

715
00:27:47,000 --> 00:27:49,000
I don't know.

716
00:27:49,000 --> 00:27:51,000
I don't know.

717
00:27:51,000 --> 00:27:53,000
I don't know.

718
00:27:53,000 --> 00:27:55,000
I don't know.

719
00:27:55,000 --> 00:27:57,000
I don't know.

720
00:27:57,000 --> 00:27:59,000
I don't know.

721
00:27:59,000 --> 00:28:01,000
Thank you for your question.

722
00:28:01,000 --> 00:28:03,000
Thank you for your question.

723
00:28:03,000 --> 00:28:05,000
First of all,

724
00:28:05,000 --> 00:28:07,000
I'm not saying

725
00:28:07,000 --> 00:28:09,000
the rich are more generous.

726
00:28:09,000 --> 00:28:11,000
It's not our...

727
00:28:15,000 --> 00:28:17,000
First of all,

728
00:28:17,000 --> 00:28:19,000
the definition of

729
00:28:19,000 --> 00:28:21,000
the rich is...

730
00:28:21,000 --> 00:28:23,000
I think

731
00:28:23,000 --> 00:28:25,000
the definition of the rich

732
00:28:25,000 --> 00:28:27,000
is...

733
00:28:27,000 --> 00:28:29,000
The definition of the rich

734
00:28:29,000 --> 00:28:31,000
is...

735
00:28:31,000 --> 00:28:33,000
The definition of the rich

736
00:28:33,000 --> 00:28:35,000
is...

737
00:28:35,000 --> 00:28:37,000
The definition of the rich

738
00:28:37,000 --> 00:28:39,000
is...

739
00:28:39,000 --> 00:28:41,000
This definition is simple.

740
00:28:41,000 --> 00:28:43,000
It's not about

741
00:28:43,000 --> 00:28:45,000
being generous or anything.

742
00:28:45,000 --> 00:28:47,000
Of course,

743
00:28:47,000 --> 00:28:49,000
this is a matter of discussion.

744
00:28:49,000 --> 00:28:51,000
The definition of the rich

745
00:28:51,000 --> 00:28:53,000
is very simple.

746
00:28:53,000 --> 00:28:55,000
It's about

747
00:28:57,000 --> 00:28:59,000
considering

748
00:28:59,000 --> 00:29:01,000
other people's benefits.

749
00:29:03,000 --> 00:29:05,000
The first thing is...

750
00:29:07,000 --> 00:29:09,000
The first thing is...

751
00:29:09,000 --> 00:29:11,000
Let me think about

752
00:29:11,000 --> 00:29:13,000
how to answer this.

753
00:29:13,000 --> 00:29:15,000
The definition of the rich

754
00:29:15,000 --> 00:29:17,000
is not...

755
00:29:17,000 --> 00:29:19,000
It's very simple.

756
00:29:19,000 --> 00:29:21,000
It's about

757
00:29:21,000 --> 00:29:23,000
considering other people's benefits.

758
00:29:23,000 --> 00:29:25,000
I'm not saying

759
00:29:25,000 --> 00:29:27,000
the rich are universal.

760
00:29:27,000 --> 00:29:29,000
For example,

761
00:29:29,000 --> 00:29:31,000
everyone has the same rights.

762
00:29:31,000 --> 00:29:33,000
We're not saying

763
00:29:33,000 --> 00:29:35,000
everyone has the same rights.

764
00:29:35,000 --> 00:29:37,000
In this experiment,

765
00:29:37,000 --> 00:29:39,000
their income

766
00:29:39,000 --> 00:29:41,000
comes from their tasks.

767
00:29:41,000 --> 00:29:43,000
Their income

768
00:29:43,000 --> 00:29:45,000
comes from their hard work.

769
00:29:45,000 --> 00:29:47,000
The income is divided by the recipient.

770
00:29:47,000 --> 00:29:49,000
Another thing is...

771
00:29:49,000 --> 00:29:51,000
Our model

772
00:29:51,000 --> 00:29:53,000
predicts

773
00:29:53,000 --> 00:29:55,000
that the more

774
00:29:55,000 --> 00:29:57,000
the recipient...

775
00:29:57,000 --> 00:29:59,000
I'm sorry.

776
00:29:59,000 --> 00:30:01,000
The more the decision-maker's income,

777
00:30:01,000 --> 00:30:03,000
the more money the recipient should get.

778
00:30:03,000 --> 00:30:05,000
But in our result,

779
00:30:05,000 --> 00:30:07,000
we didn't find

780
00:30:07,000 --> 00:30:09,000
that the decision-maker's income

781
00:30:09,000 --> 00:30:11,000
had a significant effect.

782
00:30:11,000 --> 00:30:13,000
We found that

783
00:30:13,000 --> 00:30:15,000
the income of the recipient

784
00:30:15,000 --> 00:30:17,000
and the hard work of the recipient

785
00:30:17,000 --> 00:30:19,000
had a significant effect

786
00:30:19,000 --> 00:30:21,000
on the decision-maker's income.

787
00:30:21,000 --> 00:30:23,000
on the decision-maker's income.

788
00:30:23,000 --> 00:30:25,000
I'm sorry.

789
00:30:25,000 --> 00:30:27,000
How did the decision-maker

790
00:30:27,000 --> 00:30:29,000
see the hard work

791
00:30:29,000 --> 00:30:31,000
of the recipient?

792
00:30:31,000 --> 00:30:33,000
How did you present

793
00:30:33,000 --> 00:30:35,000
the hard work?

794
00:30:35,000 --> 00:30:37,000
We gave him information.

795
00:30:37,000 --> 00:30:39,000
Actually, we asked this question.

796
00:30:39,000 --> 00:30:41,000
Let me show you the timeline.

797
00:30:41,000 --> 00:30:43,000
Let's say

798
00:30:43,000 --> 00:30:45,000
the recipient

799
00:30:45,000 --> 00:30:47,000
earned $1.

800
00:30:47,000 --> 00:30:49,000
The recipient earned $1.

801
00:30:49,000 --> 00:30:51,000
The recipient earned $1.

802
00:30:51,000 --> 00:30:53,000
The recipient earned $1.

803
00:30:53,000 --> 00:30:55,000
How much do you want to give

804
00:30:55,000 --> 00:30:57,000
to the recipient?

805
00:30:57,000 --> 00:30:59,000
As I said,

806
00:30:59,000 --> 00:31:01,000
they have to answer 22 questions.

807
00:31:01,000 --> 00:31:03,000
We will ask

808
00:31:03,000 --> 00:31:05,000
the combination of income and salary.

809
00:31:05,000 --> 00:31:07,000
$0 to $10

810
00:31:07,000 --> 00:31:09,000
is determined

811
00:31:09,000 --> 00:31:11,000
by the recipient's income.

812
00:31:11,000 --> 00:31:13,000
The reason $10 is shown here

813
00:31:13,000 --> 00:31:15,000
is because

814
00:31:15,000 --> 00:31:17,000
the recipient

815
00:31:17,000 --> 00:31:19,000
earned $10

816
00:31:19,000 --> 00:31:21,000
from the real effort task.

817
00:31:21,000 --> 00:31:23,000
If the recipient

818
00:31:23,000 --> 00:31:25,000
earned $5,

819
00:31:25,000 --> 00:31:27,000
$5 is shown.

820
00:31:27,000 --> 00:31:29,000
If the recipient earned nothing,

821
00:31:29,000 --> 00:31:31,000
$0 is shown.

822
00:31:31,000 --> 00:31:33,000
$0 is shown.

823
00:31:35,000 --> 00:31:37,000
Thank you.

824
00:31:37,000 --> 00:31:39,000
You're welcome.

825
00:31:39,000 --> 00:31:41,000
Thank you for your question.

826
00:31:43,000 --> 00:31:45,000
Before we move on

827
00:31:45,000 --> 00:31:47,000
to other questions,

828
00:31:47,000 --> 00:31:49,000
let me read the question

829
00:31:49,000 --> 00:31:51,000
from the chat room.

830
00:31:51,000 --> 00:31:53,000
Sorry,

831
00:31:53,000 --> 00:31:55,000
I don't know

832
00:31:55,000 --> 00:31:57,000
what your Chinese name is.

833
00:31:57,000 --> 00:31:59,000
It's Jeffrey Wang.

834
00:31:59,000 --> 00:32:01,000
He asked a question.

835
00:32:01,000 --> 00:32:03,000
He said the experiment design

836
00:32:03,000 --> 00:32:05,000
was given to the decision-makers.

837
00:32:05,000 --> 00:32:07,000
I just asked him.

838
00:32:07,000 --> 00:32:09,000
OK.

839
00:32:09,000 --> 00:32:11,000
Thank you.

840
00:32:11,000 --> 00:32:13,000
OK.

841
00:32:13,000 --> 00:32:15,000
I have some other questions.

842
00:32:15,000 --> 00:32:17,000
Let's see if there are any questions

843
00:32:17,000 --> 00:32:19,000
to be asked.

844
00:32:19,000 --> 00:32:21,000
Let me advertise

845
00:32:21,000 --> 00:32:23,000
some activities for Terra.

846
00:32:23,000 --> 00:32:25,000
Terra is now

847
00:32:25,000 --> 00:32:27,000
issuing a participation certificate.

848
00:32:27,000 --> 00:32:29,000
The certificate can be

849
00:32:29,000 --> 00:32:31,000
submitted to your mobile app.

850
00:32:31,000 --> 00:32:33,000
We call it

851
00:32:33,000 --> 00:32:35,000
Terra Badge.

852
00:32:35,000 --> 00:32:37,000
I've posted the certificate

853
00:32:37,000 --> 00:32:39,000
in the chat room.

854
00:32:39,000 --> 00:32:41,000
When you receive the certificate,

855
00:32:41,000 --> 00:32:43,000
you need to use

856
00:32:43,000 --> 00:32:45,000
a different code.

857
00:32:45,000 --> 00:32:47,000
The code is also in the chat room.

858
00:32:47,000 --> 00:32:49,000
Today's code is

859
00:32:49,000 --> 00:32:51,000
orcaatiger425.

860
00:32:51,000 --> 00:32:53,000
Terra also

861
00:32:53,000 --> 00:32:55,000
has a Slack channel.

862
00:32:55,000 --> 00:32:57,000
You can join it

863
00:32:57,000 --> 00:32:59,000
and get to know

864
00:32:59,000 --> 00:33:01,000
other friends

865
00:33:01,000 --> 00:33:03,000
and activities

866
00:33:03,000 --> 00:33:05,000
related to

867
00:33:05,000 --> 00:33:07,000
academia.

868
00:33:07,000 --> 00:33:09,000
You can also

869
00:33:09,000 --> 00:33:11,000
join the Slack space

870
00:33:11,000 --> 00:33:13,000
of Project Terra.

871
00:33:13,000 --> 00:33:15,000
The link is in the chat room.

872
00:33:15,000 --> 00:33:17,000
Lastly,

873
00:33:17,000 --> 00:33:19,000
if you like today's talk

874
00:33:19,000 --> 00:33:21,000
or if there's anything

875
00:33:21,000 --> 00:33:23,000
we can improve,

876
00:33:23,000 --> 00:33:25,000
there's a link for

877
00:33:25,000 --> 00:33:27,000
audience feedback.

878
00:33:27,000 --> 00:33:29,000
You can fill it in.

879
00:33:29,000 --> 00:33:31,000
We'll do our best

880
00:33:31,000 --> 00:33:33,000
to make it better.

881
00:33:33,000 --> 00:33:35,000
Now,

882
00:33:35,000 --> 00:33:37,000
do you have

883
00:33:37,000 --> 00:33:39,000
any questions?

884
00:33:39,000 --> 00:33:41,000
Hello.

885
00:33:41,000 --> 00:33:43,000
Can you hear me?

886
00:33:43,000 --> 00:33:45,000
Yes.

887
00:33:45,000 --> 00:33:47,000
I'm a student

888
00:33:47,000 --> 00:33:49,000
in a math class.

889
00:33:49,000 --> 00:33:51,000
My background is not in humanities,

890
00:33:51,000 --> 00:33:53,000
so I don't understand much.

891
00:33:53,000 --> 00:33:55,000
I'd like to ask

892
00:33:55,000 --> 00:33:57,000
if I'm interested

893
00:33:57,000 --> 00:33:59,000
in the math model.

894
00:33:59,000 --> 00:34:01,000
I'd like to know

895
00:34:01,000 --> 00:34:03,000
the meaning of the parameters.

896
00:34:03,000 --> 00:34:05,000
Okay.

897
00:34:07,000 --> 00:34:09,000
I'd like to know

898
00:34:09,000 --> 00:34:11,000
if MJ and EJ

899
00:34:11,000 --> 00:34:13,000
refer to the

900
00:34:13,000 --> 00:34:15,000
recipient's income.

901
00:34:15,000 --> 00:34:17,000
MJ is

902
00:34:17,000 --> 00:34:19,000
the salary of the recipient.

903
00:34:19,000 --> 00:34:21,000
EJ is

904
00:34:21,000 --> 00:34:23,000
his tasks.

905
00:34:23,000 --> 00:34:25,000
These two are his income.

906
00:34:27,000 --> 00:34:29,000
I'd like to know

907
00:34:29,000 --> 00:34:31,000
why the recipient

908
00:34:31,000 --> 00:34:33,000
uses this way of expression

909
00:34:33,000 --> 00:34:35,000
but the recipient doesn't

910
00:34:35,000 --> 00:34:37,000
use this way of setting.

911
00:34:37,000 --> 00:34:39,000
He only uses his income

912
00:34:39,000 --> 00:34:41,000
as input,

913
00:34:41,000 --> 00:34:43,000
not his efforts.

914
00:34:43,000 --> 00:34:45,000
Okay.

915
00:34:49,000 --> 00:34:51,000
Let me think.

916
00:34:53,000 --> 00:34:55,000
The main problem is

917
00:34:55,000 --> 00:34:57,000
sometimes people...

918
00:34:57,000 --> 00:34:59,000
Oh, sorry. You go first.

919
00:34:59,000 --> 00:35:01,000
Sorry, you go first.

920
00:35:01,000 --> 00:35:03,000
You go first.

921
00:35:03,000 --> 00:35:05,000
Okay.

922
00:35:05,000 --> 00:35:07,000
I think

923
00:35:07,000 --> 00:35:09,000
when I set it,

924
00:35:09,000 --> 00:35:11,000
I'd consider the difference

925
00:35:11,000 --> 00:35:13,000
in the degree of efforts of two people.

926
00:35:13,000 --> 00:35:15,000
Water usually flows down.

927
00:35:15,000 --> 00:35:17,000
So when I find

928
00:35:17,000 --> 00:35:19,000
the difference between

929
00:35:19,000 --> 00:35:21,000
their efforts and mine,

930
00:35:21,000 --> 00:35:23,000
I might not

931
00:35:23,000 --> 00:35:25,000
give him that much.

932
00:35:25,000 --> 00:35:27,000
At that time,

933
00:35:27,000 --> 00:35:29,000
my data function

934
00:35:29,000 --> 00:35:31,000
might be the difference of efforts.

935
00:35:31,000 --> 00:35:33,000
If I consider this,

936
00:35:33,000 --> 00:35:35,000
I don't know if your model

937
00:35:35,000 --> 00:35:37,000
or your prediction

938
00:35:37,000 --> 00:35:39,000
will be different.

939
00:35:39,000 --> 00:35:41,000
I think it's a good question.

940
00:35:41,000 --> 00:35:43,000
Actually,

941
00:35:43,000 --> 00:35:45,000
I thought about it when I analyzed it.

942
00:35:47,000 --> 00:35:49,000
Of course,

943
00:35:49,000 --> 00:35:51,000
you can modify the model.

944
00:35:51,000 --> 00:35:53,000
I think

945
00:35:53,000 --> 00:35:55,000
the decision-maker's income

946
00:35:55,000 --> 00:35:57,000
doesn't matter

947
00:35:57,000 --> 00:35:59,000
whether you set it as

948
00:35:59,000 --> 00:36:01,000
mi, ei or yi.

949
00:36:01,000 --> 00:36:03,000
It won't make any difference.

950
00:36:03,000 --> 00:36:05,000
But there is a place you can modify.

951
00:36:05,000 --> 00:36:07,000
For example,

952
00:36:07,000 --> 00:36:09,000
we assume that

953
00:36:09,000 --> 00:36:11,000
the relationship of the line

954
00:36:11,000 --> 00:36:13,000
is theta0 plus beta

955
00:36:13,000 --> 00:36:15,000
times the degree of efforts

956
00:36:15,000 --> 00:36:17,000
of the recipient.

957
00:36:17,000 --> 00:36:19,000
According to your theory,

958
00:36:19,000 --> 00:36:21,000
the function of theta

959
00:36:21,000 --> 00:36:23,000
is actually the model.

960
00:36:23,000 --> 00:36:25,000
We assume that

961
00:36:25,000 --> 00:36:27,000
it is a theta0,

962
00:36:27,000 --> 00:36:29,000
which is the baseline,

963
00:36:29,000 --> 00:36:31,000
plus beta times

964
00:36:31,000 --> 00:36:33,000
ez minus ei,

965
00:36:33,000 --> 00:36:35,000
which is the degree of difference

966
00:36:35,000 --> 00:36:37,000
between the two efforts.

967
00:36:37,000 --> 00:36:39,000
I think it's possible.

968
00:36:39,000 --> 00:36:41,000
It's a very good question.

969
00:36:41,000 --> 00:36:43,000
I remember that

970
00:36:43,000 --> 00:36:45,000
we didn't find

971
00:36:45,000 --> 00:36:47,000
such an effect in our analysis.

972
00:36:47,000 --> 00:36:49,000
I remember that

973
00:36:49,000 --> 00:36:51,000
we didn't find

974
00:36:51,000 --> 00:36:53,000
such an effect in our analysis.

975
00:36:53,000 --> 00:36:55,000
I remember that

976
00:36:55,000 --> 00:36:57,000
we didn't find

977
00:36:57,000 --> 00:36:59,000
such an effect in our analysis.

978
00:36:59,000 --> 00:37:01,000
I remember that

979
00:37:01,000 --> 00:37:03,000
we didn't find

980
00:37:03,000 --> 00:37:05,000
such an effect in our analysis.

981
00:37:05,000 --> 00:37:07,000
I see.

982
00:37:07,000 --> 00:37:09,000
So the decision-maker's income

983
00:37:09,000 --> 00:37:11,000
doesn't matter

984
00:37:11,000 --> 00:37:13,000
whether you set it as mi, ei

985
00:37:13,000 --> 00:37:15,000
or yi.

986
00:37:15,000 --> 00:37:17,000
You mean here?

987
00:37:17,000 --> 00:37:19,000
No, I don't mean that.

988
00:37:19,000 --> 00:37:21,000
You set it as mi, ei

989
00:37:21,000 --> 00:37:23,000
or yi

990
00:37:23,000 --> 00:37:25,000
in the income.

991
00:37:25,000 --> 00:37:27,000
You set it as mi, ei or yi

992
00:37:27,000 --> 00:37:29,000
in the income.

993
00:37:29,000 --> 00:37:31,000
You're right.

994
00:37:31,000 --> 00:37:33,000
But what you say is

995
00:37:33,000 --> 00:37:35,000
mainly about the C-harm.

996
00:37:35,000 --> 00:37:37,000
You can see

997
00:37:37,000 --> 00:37:39,000
how much money

998
00:37:39,000 --> 00:37:41,000
the decision-maker

999
00:37:41,000 --> 00:37:43,000
will give to the recipient.

1000
00:37:43,000 --> 00:37:45,000
For example,

1001
00:37:45,000 --> 00:37:47,000
if you set mi as mi and ei,

1002
00:37:47,000 --> 00:37:49,000
the effect of dividing

1003
00:37:49,000 --> 00:37:51,000
mi and ei is the same.

1004
00:37:51,000 --> 00:37:53,000
The effect of dividing mi and ei is the same.

1005
00:37:53,000 --> 00:37:55,000
The higher the income of the decision-maker,

1006
00:37:55,000 --> 00:37:57,000
the more money he will give.

1007
00:37:57,000 --> 00:37:59,000
the more money he will give.

1008
00:37:59,000 --> 00:38:01,000
So there's no point

1009
00:38:01,000 --> 00:38:03,000
in separating them.

1010
00:38:03,000 --> 00:38:05,000
But what you said is about the C-harm.

1011
00:38:05,000 --> 00:38:07,000
But what you said is about the C-harm.

1012
00:38:07,000 --> 00:38:09,000
I see.

1013
00:38:09,000 --> 00:38:11,000
Thank you.

1014
00:38:13,000 --> 00:38:15,000
Thank you.

1015
00:38:15,000 --> 00:38:17,000
Thank you.

1016
00:38:17,000 --> 00:38:19,000
Okay.

1017
00:38:19,000 --> 00:38:21,000
Do you have any other questions?

1018
00:38:21,000 --> 00:38:23,000
Okay.

1019
00:38:23,000 --> 00:38:25,000
If you have no other questions,

1020
00:38:25,000 --> 00:38:27,000
If you have no other questions,

1021
00:38:27,000 --> 00:38:29,000
I'll turn off the recording.

1022
00:38:29,000 --> 00:38:31,000
I'll turn off the recording.

