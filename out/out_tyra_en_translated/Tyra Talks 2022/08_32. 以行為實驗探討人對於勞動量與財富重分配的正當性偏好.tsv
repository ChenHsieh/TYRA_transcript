start	end	text
0	7000	Hello everyone, welcome to today's Tarot Talk.
7000	14000	We are honored to have Pei-Hsueh to give us a talk.
14000	22000	The topic of today's talk has been shown in his introduction video.
22000	32000	Does the deservingness heuristic explain the effect of effort, luck, and need in a redistribution experiment?
32000	42000	Before we start the talk, let me introduce Pei-Hsueh.
42000	47000	Pei-Hsueh is a PhD student from the Department of Political Science at Stony Brook.
47000	54000	His research focuses on the social preferences of people based on experimental methods and computational social science,
54000	58000	and how these preferences affect people's choices in political issues.
58000	66000	Before we start, let's give a warm welcome to Pei-Hsueh for his wonderful talk.
66000	70000	Oh, the microphone is set to be turned off.
70000	74000	If you want Pei-Hsueh to hear the applause, you have to turn on the microphone first.
74000	81000	Let's give a warm round of applause to Pei-Hsueh for his wonderful talk.
81000	84000	Now it's your turn.
84000	91000	This English is a bit awkward, but what I want to talk about today is
91000	98000	how people's preferences affect people's choices in equality and fairness.
98000	108000	I did this research with my advisor, Ruben Klein.
108000	116000	We're almost done, but I plan to do more analysis.
116000	120000	If you have any comments, please feel free to ask.
120000	123000	Let's get started.
123000	128000	First of all, there's a long-standing problem with social scientists.
128000	131000	Do people care about equality?
131000	136000	Many experiments have shown that people do care about equality.
136000	144000	For example, in a dictatorial election, the dictator gets a sum of money.
145000	150000	Even if there's no reason to give their partner any money,
150000	153000	they'll still choose to give a portion.
153000	157000	Even in a non-dictatorial election,
157000	165000	different experiments have shown that people do care about equality.
165000	171000	Scientists have also found that in a non-industrialized society,
171000	176000	people will punish unfair distributors.
176000	180000	So people do care about equality.
180000	187000	If we use a mathematical model to show that people care about equality,
187000	191000	there are several main models.
191000	194000	The first one is called Inequality Aversion.
194000	198000	It was proposed by Feyer and Schmidt in 1999.
198000	202000	Their concept is very straightforward.
202000	209000	Let's say a person's wealth has not changed.
209000	212000	If he has more money than others,
212000	217000	he might feel embarrassed and unhappy.
217000	220000	If he has less money than others,
220000	223000	he might feel even more unhappy.
223000	229000	If he has the same amount of money as others,
229000	234000	he'll be the happiest if his wealth hasn't changed.
234000	238000	For example, if he has more or less money than others,
238000	242000	he'll be happy because of inequality.
242000	246000	It sounds straightforward and doesn't require a mathematical model.
246000	249000	However, it's better to use a mathematical model
249000	254000	to compare parameters and predictions.
254000	263000	The other model is called Inequality Aversion.
263000	267000	It was proposed by Angiolini and Miller in 2002.
267000	270000	The concept is more straightforward.
270000	275000	People care about their happiness and preferences.
275000	277000	It sounds vague.
277000	279000	People care about their preferences.
279000	282000	It sounds like people don't care about themselves,
282000	284000	but care about others.
284000	286000	It's impossible, right?
286000	288000	Most people are not like this.
288000	291000	The basic idea of this model is that
291000	294000	people will consider others' preferences.
294000	297000	No matter what, people will consider others' preferences.
297000	306000	How can we push people to care about equality?
306000	312000	Let me give you an example.
312000	325000	For most of the wealth,
325000	327000	whether it's money or something else,
327000	329000	of course not everything,
329000	335000	but for most of the wealth,
336000	340000	if you add up all the wealth in one unit,
340000	342000	people will reduce the amount.
342000	346000	For example, if I have $100,000,
346000	348000	and I get $100,000,
348000	349000	I will be happy.
349000	351000	But if I have $1 million,
351000	353000	and I get $100,000,
353000	354000	I will still be happy.
354000	359000	But I won't be as happy as when I only have $100,000.
359000	361000	But if I have $100,000,
361000	363000	and I get $100,000,
363000	365000	and I get $100,000,
365000	369000	I might feel that I don't have a good balance,
369000	373000	but I won't be as happy as when I only have $100,000.
373000	376000	In other words,
376000	381000	if someone is already very rich,
381000	383000	and he gets $100,000,
383000	385000	he might be quite happy.
385000	389000	But if this person cares about another person,
389000	391000	no matter how much he cares,
391000	394000	if this $100,000 is for another poor person,
394000	396000	and this person only has $100,000,
396000	398000	and he cares about this person,
398000	401000	then the other person getting $100,000
401000	404000	might bring him more happiness
404000	407000	than getting $100,000 himself.
407000	410000	So assuming that the editor's use of land
410000	413000	and the user's use of other people's use
413000	415000	these two conditions,
415000	418000	it can also be inferred that
418000	421000	people care about equality.
421000	423000	This model can also explain
423000	425000	why people care about equality.
425000	427000	This model can also explain
427000	429000	why people care about equality.
429000	431000	But the question is,
431000	433000	do people really care about equality?
433000	435000	If we find that,
435000	437000	like the big boss might tell you
437000	439000	that there is no bad luck,
439000	441000	only bad luck,
441000	443000	and then he is not willing to donate his money
443000	445000	or ask for tax, right?
445000	447000	Or some politicians might tell you
447000	449000	that some people refuse to work,
449000	451000	and then use this to blow votes.
451000	453000	Does this mean that people
453000	455000	might not care about equality?
455000	457000	What people care about is actually
457000	459000	the so-called fairness.
459000	461000	Of course, fairness can have many definitions,
461000	463000	but what people care about
463000	465000	is how much effort
465000	467000	people should make
467000	469000	to get paid.
469000	471000	What people care about is how much effort
471000	473000	people should make
473000	475000	to get paid.
475000	477000	If the income in the lab
477000	479000	is determined by effort,
479000	481000	is determined by effort,
481000	483000	is determined by effort,
483000	485000	and not by luck,
485000	487000	and not by luck,
487000	489000	the beneficiary will choose
489000	491000	to redistribute less.
491000	493000	In other words,
493000	495000	the beneficiary will choose
495000	497000	not to redistribute the wealth
497000	499000	according to certain taxes.
499000	501000	In other words,
501000	503000	the beneficiary will choose
503000	505000	not to redistribute the wealth
505000	507000	according to certain taxes.
507000	509000	In other words,
509000	511000	if the income is determined by luck,
511000	513000	the beneficiary will have more social benefits
513000	515000	and higher tax payments.
515000	517000	and higher tax payments.
517000	519000	and higher tax payments.
519000	521000	If the beneficiary
521000	523000	believes that
523000	525000	everyone's income
525000	527000	is determined by effort,
527000	529000	not by luck,
529000	531000	the beneficiary will not
531000	533000	receive social benefits
533000	535000	and lower taxes.
535000	537000	and lower taxes.
537000	539000	Recently,
539000	541000	I've been
541000	543000	talking about
543000	545000	why people
545000	547000	prefer an unequal society.
547000	549000	Why do people think
549000	551000	an unequal society is fair?
557000	559000	Today,
559000	561000	we're going to talk about
561000	563000	whether people
563000	565000	only care about
565000	567000	equality or fairness.
567000	569000	Of course not.
569000	571000	Most people
571000	573000	care about equality and fairness.
573000	575000	Today,
575000	577000	we want to
577000	579000	build a model
579000	581000	that can explain
581000	583000	people's preference for equality
583000	585000	and fairness.
585000	587000	Let's go back
587000	589000	to our model.
589000	591000	Let's assume
591000	593000	that this model
593000	595000	only cares about
595000	597000	other people.
597000	599000	Of course,
599000	601000	the degree of care
601000	603000	may vary
603000	605000	depending on the situation
605000	607000	and individual personality
607000	609000	and experience.
609000	611000	Today,
611000	613000	we want to
613000	615000	put the degree of effort
615000	617000	into the model.
617000	619000	Let's say
619000	621000	there is a decision-maker
621000	623000	and
623000	625000	there are two other people.
625000	627000	Let's say
627000	629000	one person is very hard-working.
629000	631000	The decision-maker
631000	633000	cares about the hard-working person more.
633000	635000	Let's say
635000	637000	the wealth of these two people
637000	639000	is unequal,
639000	641000	and the decision-maker is rich.
641000	643000	Let's say
643000	645000	the wealth of this person is not equal,
645000	647000	but the decision-maker is hard-working.
647000	649000	The decision-maker cares about the recipient,
649000	651000	and decides to
651000	653000	redistribute the wealth
653000	655000	to the recipient.
655000	657000	On the other hand,
657000	659000	if the recipient
659000	661000	is not hard-working,
661000	663000	the decision-maker
663000	665000	will not care about the recipient's preference
665000	667000	and choose to give the recipient
667000	669000	less money.
669000	671000	In order
671000	673000	to verify
673000	675000	the prediction of our model,
675000	677000	we designed
677000	679000	a laboratory experiment.
679000	681000	The experiment
681000	683000	was conducted
683000	685000	by the students
685000	687000	of Stony Brook University.
687000	689000	In this experiment,
689000	691000	we will give them
691000	693000	different amounts of money
693000	695000	based on their decisions.
695000	697000	So,
697000	699000	the money they can get
699000	701000	after the experiment
701000	703000	is determined
703000	705000	by their decisions
705000	707000	in the laboratory.
707000	709000	This is our laboratory,
709000	711000	but this is just a demo.
711000	713000	This is not my experiment.
713000	715000	So, the people here
715000	717000	look a little old.
717000	719000	This is the professor,
719000	721000	and this seems to be
721000	723000	a demo for the website.
723000	725000	This experiment
725000	727000	was not conducted
727000	729000	in the laboratory,
729000	731000	because the recipient
731000	733000	was in the middle of the epidemic.
733000	735000	So, the experiment was conducted
735000	737000	on-site.
737000	739000	This is the design
739000	741000	of our laboratory.
741000	743000	At the beginning
743000	745000	of the experiment,
745000	747000	the recipient
747000	749000	can choose
749000	751000	to give the recipient
751000	753000	less money
753000	755000	based on their decisions.
755000	757000	At the beginning of the experiment,
757000	759000	the recipient
759000	761000	can decide whether
761000	763000	to do this task,
763000	765000	which we call
765000	767000	the counting zero task.
767000	769000	They need to write
769000	771000	the answer here
771000	773000	and send it out.
773000	775000	When they get it right,
775000	777000	they will get a reward.
777000	779000	The reward for getting it right
779000	781000	is random.
781000	783000	Each recipient
783000	785000	can decide
785000	787000	how many tasks
787000	789000	they want to do.
789000	791000	They can choose
791000	793000	not to do it
793000	795000	and skip this step.
795000	797000	They can choose
797000	799000	to do up to 10 tasks.
799000	801000	After this task,
801000	803000	we will randomly
803000	805000	pair the recipients.
805000	807000	One recipient
807000	809000	will be designated as
809000	811000	the decider.
811000	813000	The other recipient
813000	815000	will be designated as the recipient.
815000	817000	The decider can decide
817000	819000	how much money
819000	821000	they want to earn
821000	823000	from this task
823000	825000	to the recipient.
825000	827000	The decider will know
827000	829000	how much the recipient's salary is
829000	831000	and how many tasks they need to complete.
831000	833000	In this experiment,
833000	835000	we use a method
835000	837000	called the strategy method.
837000	839000	This method
839000	841000	allows the recipient
841000	843000	to answer
843000	845000	the combination of
845000	847000	all the salary and tasks.
847000	849000	There are two salaries
849000	851000	and 11 different
851000	853000	effort levels.
853000	855000	There are 22 combinations.
855000	857000	The decider
857000	859000	needs to answer
859000	861000	the question
861000	863000	how much money
863000	865000	they want to earn
865000	867000	from this task
867000	869000	to the recipient.
869000	871000	In the end,
871000	873000	the decider
873000	875000	will know
875000	877000	how much money
877000	879000	the recipient
879000	881000	earns
881000	883000	from this task
883000	885000	to the recipient.
885000	887000	The decider will know
887000	889000	how much money
889000	891000	the recipient
891000	893000	earns
893000	895000	from this task
895000	897000	to the recipient.
897000	899000	In the end,
899000	901000	the decider
901000	903000	will know
903000	905000	how much money
905000	907000	the recipient
907000	909000	earns
909000	911000	from this task
911000	913000	to the recipient.
913000	915000	Our model
915000	917000	is called
917000	919000	the conditional model.
919000	921000	In this model,
921000	923000	this is
923000	925000	the income
925000	927000	of the decider.
927000	929000	T is how much money
929000	931000	the decider wants to give to the recipient.
931000	933000	M is
933000	935000	the salary
935000	937000	of the recipient,
937000	939000	$1 or $2.
939000	941000	E is how many tasks
941000	943000	the decider has completed.
943000	945000	T is how much money
945000	947000	the receiver earns.
947000	949000	Theta is
949000	951000	the weight of the decider.
951000	953000	Theta is the weight of the decider.
953000	955000	Theta is the weight of the decider.
955000	957000	In this case,
957000	959000	Theta is the value of
959000	961000	the function of labor.
961000	963000	Alpha is
963000	965000	the parameter of
965000	967000	the marginal cost of the decision.
967000	969000	Therefore, the alpha determines
969000	971000	the marginal cost of the decision.
971000	973000	The higher the alpha,
973000	975000	the lower the value.
975000	977000	In other words,
977000	979000	the higher the alpha,
979000	981000	the lower the marginal cost of the decision.
981000	983000	We can determine
983000	985000	how much money
985000	987000	the decider
987000	989000	will give
989000	991000	to the recipient
991000	993000	based on
993000	995000	each factor.
995000	997000	We can determine
997000	999000	how much money
999000	1001000	the decider
1001000	1003000	will give
1003000	1005000	to the recipient
1005000	1007000	based on
1007000	1009000	each factor.
1009000	1011000	based on
1011000	1013000	each factor.
1013000	1015000	There are
1015000	1017000	several predictions
1017000	1019000	under this model.
1019000	1021000	The first prediction is that
1021000	1023000	as long as the weight
1023000	1025000	is greater than or equal to 0,
1025000	1027000	the income of the decider
1027000	1029000	and the degree of labor of the recipient
1029000	1031000	will remain the same
1031000	1033000	or less when the income of the receiver
1033000	1035000	is greater than or equal to 0.
1035000	1037000	This is called
1037000	1039000	inequality effect.
1039000	1041000	The second prediction
1041000	1043000	is that
1043000	1045000	as long as the income of the decider
1045000	1047000	and the degree of labor of the recipient
1047000	1049000	is greater than or equal to 0,
1049000	1051000	the income of the decider
1051000	1053000	will remain the same or less.
1053000	1055000	In our experiment,
1055000	1057000	we found that
1057000	1059000	90% of the receivers
1059000	1061000	met the first
1061000	1063000	inequality effect.
1063000	1065000	inequality effect.
1065000	1067000	In addition,
1067000	1069000	we found that
1069000	1071000	85% of the receivers
1071000	1073000	met the
1073000	1075000	deservingness effect.
1075000	1077000	deservingness effect.
1077000	1079000	deservingness effect.
1079000	1081000	In addition,
1081000	1083000	we ran a random effect regression.
1083000	1085000	We found that
1085000	1087000	as long as the
1087000	1089000	income of the viewers
1089000	1091000	is higher
1091000	1093000	than or equal to 0,
1093000	1095000	the income of the
1095000	1097000	recipients
1097000	1099000	is also higher.
1099000	1101000	If the
1101000	1103000	income of the
1103000	1105000	æ™ºlr celebrities
1105000	1107000	is better than
1107000	1109000	or equal to 0,
1109000	1111000	all providers
1111000	1113000	will offer
1113000	1115000	a higher
1115000	1117000	income than
1117000	1119000	or equal to 0.
1119000	1121000	The other
1121000	1123000	prediction is
1123000	1125000	that
1125000	1127000	we actually
1127000	1129000	specify some parameters
1129000	1131000	for the model.
1131000	1133000	We actually
1133000	1135000	assume
1135000	1137000	the weight
1137000	1139000	function.
1139000	1141000	We assume
1141000	1143000	it is linear.
1143000	1145000	If the weight
1145000	1147000	is a baseline weight,
1147000	1149000	that is,
1149000	1151000	when the collector
1151000	1153000	does not work hard at all,
1153000	1155000	how much is the
1155000	1157000	basic weight?
1157000	1159000	How much does the
1159000	1161000	collector care about
1161000	1163000	the recipient?
1163000	1165000	Another parameter is
1165000	1167000	that once the
1167000	1169000	recipient increases
1169000	1171000	how much does the
1171000	1173000	weight increase?
1173000	1175000	We use two different
1175000	1177000	parameters as an
1177000	1179000	assumption.
1179000	1181000	The first parameter
1181000	1183000	is that the
1183000	1185000	weight baseline is very high.
1185000	1187000	When the recipient
1187000	1189000	does not work hard at all,
1189000	1191000	the collector cares
1191000	1193000	very much about the
1193000	1195000	recipient.
1195000	1197000	The picture on the
1197000	1199000	left shows
1199000	1201000	that the
1201000	1203000	weight baseline
1203000	1205000	is very low.
1205000	1207000	When the
1207000	1209000	recipient does not
1209000	1211000	work at all,
1211000	1213000	the collector does
1213000	1215000	not care about
1215000	1217000	the recipient.
1217000	1219000	However,
1219000	1221000	when the
1221000	1223000	recipient increases
1223000	1225000	by one unit,
1225000	1227000	the weight
1227000	1229000	is the
1229000	1231000	income of the
1231000	1233000	recipient.
1233000	1235000	The weight
1235000	1237000	determines
1237000	1239000	how much
1239000	1241000	the collector
1241000	1243000	has to pay
1243000	1245000	to the recipient.
1245000	1247000	The higher the
1247000	1249000	recipient's income,
1249000	1251000	the higher
1251000	1253000	the recipient's
1253000	1255000	salary.
1255000	1257000	As you can see,
1257000	1259000	the two lines
1259000	1261000	on the left
1261000	1263000	are overlapping.
1263000	1265000	In other words,
1265000	1267000	the collector
1267000	1269000	does not care
1269000	1271000	about the
1271000	1273000	recipient's salary,
1273000	1275000	but the weight
1275000	1277000	baseline is very high,
1277000	1279000	so the salary
1279000	1281000	does not matter.
1281000	1283000	However,
1283000	1285000	the higher the
1285000	1287000	recipient's income,
1287000	1289000	the less
1289000	1291000	the collector
1291000	1293000	has to pay
1293000	1295000	to the recipient.
1295000	1297000	On the right,
1297000	1299000	the two lines
1299000	1301000	on the left
1301000	1303000	are overlapping.
1303000	1305000	The higher the
1305000	1307000	recipient's income,
1307000	1309000	the higher
1309000	1311000	the collector
1311000	1313000	has to pay
1313000	1315000	to the recipient.
1315000	1317000	The higher the
1317000	1319000	recipient's income,
1319000	1321000	the more
1321000	1323000	the collector
1323000	1325000	has to pay
1325000	1327000	to the recipient.
1327000	1329000	According to our
1329000	1331000	experiment results,
1331000	1333000	we can divide
1333000	1335000	these collectors
1335000	1337000	into four types.
1337000	1339000	The first type
1339000	1341000	is sensitive
1341000	1343000	to the recipient's
1343000	1345000	salary.
1345000	1347000	This type
1347000	1349000	accounts for
1349000	1351000	25% of the collectors.
1351000	1353000	The second type
1353000	1355000	is insensitive
1355000	1357000	to the
1357000	1359000	recipient's salary.
1359000	1361000	This type
1361000	1363000	accounts for
1363000	1365000	20% of the collectors.
1365000	1367000	The third type
1367000	1369000	is selfish
1369000	1371000	to the recipient's
1371000	1373000	salary.
1373000	1375000	The last type
1375000	1377000	is other.
1377000	1379000	The last type
1379000	1381000	is other.
1381000	1383000	The last type
1383000	1385000	is other.
1385000	1387000	When we analyze
1387000	1389000	sensitive type and
1389000	1391000	insensitive type,
1391000	1393000	we find that
1393000	1395000	the result
1395000	1397000	is
1397000	1399000	consistent
1399000	1401000	with our
1401000	1403000	theoretical prediction.
1407000	1409000	The main conclusion
1409000	1411000	of our research
1411000	1413000	is that
1413000	1415000	our model
1415000	1417000	considers equality and fairness.
1417000	1419000	Most collectors
1419000	1421000	have the same model prediction.
1421000	1423000	We also found that
1423000	1425000	collectors can be classified into
1425000	1427000	three types,
1427000	1429000	selfish collectors,
1429000	1431000	sensitive collectors,
1431000	1433000	and fair collectors.
1433000	1435000	Sorry,
1435000	1437000	that's all
1437000	1439000	for today.
1439000	1441000	Thank you.
1441000	1443000	This is my Twitter
1443000	1445000	and personal website.
1445000	1447000	You are welcome
1447000	1449000	to join my Twitter.
1449000	1451000	Thank you.
1451000	1453000	Before we start
1453000	1455000	the Q&A,
1455000	1457000	please turn on your
1457000	1459000	microphone.
1459000	1461000	Let's welcome
1461000	1463000	PeiXun.
1465000	1467000	Do you have any questions?
1467000	1469000	You can ask
1469000	1471000	directly.
1471000	1473000	You can also
1473000	1475000	type your questions
1475000	1477000	in the chat box.
1477000	1479000	Do you have any questions?
1479000	1481000	You can ask
1481000	1483000	directly.
1483000	1485000	Hi,
1485000	1487000	I'm Jeffrey from
1487000	1489000	Indiana University Bloomington.
1489000	1491000	I'm also a fifth-year professor.
1491000	1493000	Thank you, PeiXun.
1493000	1495000	Sorry,
1495000	1497000	it's PeiXun.
1497000	1499000	I think your topic is
1499000	1501000	very interesting.
1501000	1503000	After listening to your
1503000	1505000	experimental design,
1505000	1507000	I have some questions.
1507000	1509000	First of all,
1509000	1511000	you didn't talk much about
1511000	1513000	decision-makers and recipients.
1513000	1515000	In your experimental design,
1515000	1517000	did you design
1517000	1519000	decision-makers to give money
1519000	1521000	to the recipients?
1521000	1523000	Or did you just say
1523000	1525000	there will be a part like this?
1525000	1527000	There is no right answer.
1527000	1529000	Sorry.
1529000	1531000	There is no right answer.
1531000	1533000	He can make such a decision.
1533000	1535000	We will ask him
1535000	1537000	if the recipient's salary
1537000	1539000	is $1
1539000	1541000	or $2.
1541000	1543000	If the recipient
1543000	1545000	completes
1545000	1547000	several tasks,
1547000	1549000	how much money
1549000	1551000	does he want to give to the recipient?
1551000	1553000	He can choose not to give.
1553000	1555000	We didn't give him any reason.
1555000	1557000	How much money he can get
1557000	1559000	is determined
1559000	1561000	by his decisions.
1561000	1563000	If he decides to give more money
1563000	1565000	to the recipient,
1565000	1567000	he will get less money in the end.
1567000	1569000	In fact,
1569000	1571000	if he is a
1571000	1573000	direct recipient,
1573000	1575000	there is no reason
1575000	1577000	to give money to the recipient.
1577000	1579000	The more money he gives,
1579000	1581000	the less money he will get.
1581000	1583000	I'm a little confused.
1583000	1585000	I'm not convinced.
1585000	1587000	It depends on
1587000	1589000	how you get the income.
1589000	1591000	Generally speaking,
1591000	1593000	rich people
1593000	1595000	are generous.
1595000	1597000	However,
1597000	1599000	I don't know.
1599000	1601000	I don't think
1601000	1603000	he is generous enough.
1603000	1605000	Maybe there are many reasons
1605000	1607000	why he doesn't give money.
1607000	1609000	I don't think
1609000	1611000	it has anything to do
1611000	1613000	with strategic thinking.
1613000	1615000	I don't think
1615000	1617000	it has anything to do
1617000	1619000	with strategic thinking.
1619000	1621000	I don't think
1621000	1623000	it has anything to do
1623000	1625000	with strategic thinking.
1625000	1627000	For example,
1627000	1629000	if you win a lottery
1629000	1631000	and win 10 million,
1631000	1633000	you can give
1633000	1635000	10 yuan to the recipient.
1635000	1637000	How generous do you think
1637000	1639000	he is?
1639000	1641000	How generous
1641000	1643000	do you think
1643000	1645000	he is?
1645000	1647000	For example,
1647000	1649000	you can see that
1649000	1651000	the one who earns more
1651000	1653000	is more likely to give the money.
1653000	1655000	The one who earns less
1655000	1657000	is more likely to give the money.
1657000	1659000	Do you think
1659000	1661000	he is more generous?
1661000	1663000	I think
1663000	1665000	he is more generous
1665000	1667000	because he is rich.
1667000	1669000	I don't know.
1669000	1671000	I don't know.
1671000	1673000	I don't know.
1673000	1675000	I don't know.
1675000	1677000	I don't know.
1677000	1679000	I don't know.
1679000	1681000	Thank you for your question.
1681000	1683000	Thank you for your question.
1683000	1685000	First of all,
1685000	1687000	I'm not saying
1687000	1689000	the rich are more generous.
1689000	1691000	It's not our...
1695000	1697000	First of all,
1697000	1699000	the definition of
1699000	1701000	the rich is...
1701000	1703000	I think
1703000	1705000	the definition of the rich
1705000	1707000	is...
1707000	1709000	The definition of the rich
1709000	1711000	is...
1711000	1713000	The definition of the rich
1713000	1715000	is...
1715000	1717000	The definition of the rich
1717000	1719000	is...
1719000	1721000	This definition is simple.
1721000	1723000	It's not about
1723000	1725000	being generous or anything.
1725000	1727000	Of course,
1727000	1729000	this is a matter of discussion.
1729000	1731000	The definition of the rich
1731000	1733000	is very simple.
1733000	1735000	It's about
1737000	1739000	considering
1739000	1741000	other people's benefits.
1743000	1745000	The first thing is...
1747000	1749000	The first thing is...
1749000	1751000	Let me think about
1751000	1753000	how to answer this.
1753000	1755000	The definition of the rich
1755000	1757000	is not...
1757000	1759000	It's very simple.
1759000	1761000	It's about
1761000	1763000	considering other people's benefits.
1763000	1765000	I'm not saying
1765000	1767000	the rich are universal.
1767000	1769000	For example,
1769000	1771000	everyone has the same rights.
1771000	1773000	We're not saying
1773000	1775000	everyone has the same rights.
1775000	1777000	In this experiment,
1777000	1779000	their income
1779000	1781000	comes from their tasks.
1781000	1783000	Their income
1783000	1785000	comes from their hard work.
1785000	1787000	The income is divided by the recipient.
1787000	1789000	Another thing is...
1789000	1791000	Our model
1791000	1793000	predicts
1793000	1795000	that the more
1795000	1797000	the recipient...
1797000	1799000	I'm sorry.
1799000	1801000	The more the decision-maker's income,
1801000	1803000	the more money the recipient should get.
1803000	1805000	But in our result,
1805000	1807000	we didn't find
1807000	1809000	that the decision-maker's income
1809000	1811000	had a significant effect.
1811000	1813000	We found that
1813000	1815000	the income of the recipient
1815000	1817000	and the hard work of the recipient
1817000	1819000	had a significant effect
1819000	1821000	on the decision-maker's income.
1821000	1823000	on the decision-maker's income.
1823000	1825000	I'm sorry.
1825000	1827000	How did the decision-maker
1827000	1829000	see the hard work
1829000	1831000	of the recipient?
1831000	1833000	How did you present
1833000	1835000	the hard work?
1835000	1837000	We gave him information.
1837000	1839000	Actually, we asked this question.
1839000	1841000	Let me show you the timeline.
1841000	1843000	Let's say
1843000	1845000	the recipient
1845000	1847000	earned $1.
1847000	1849000	The recipient earned $1.
1849000	1851000	The recipient earned $1.
1851000	1853000	The recipient earned $1.
1853000	1855000	How much do you want to give
1855000	1857000	to the recipient?
1857000	1859000	As I said,
1859000	1861000	they have to answer 22 questions.
1861000	1863000	We will ask
1863000	1865000	the combination of income and salary.
1865000	1867000	$0 to $10
1867000	1869000	is determined
1869000	1871000	by the recipient's income.
1871000	1873000	The reason $10 is shown here
1873000	1875000	is because
1875000	1877000	the recipient
1877000	1879000	earned $10
1879000	1881000	from the real effort task.
1881000	1883000	If the recipient
1883000	1885000	earned $5,
1885000	1887000	$5 is shown.
1887000	1889000	If the recipient earned nothing,
1889000	1891000	$0 is shown.
1891000	1893000	$0 is shown.
1895000	1897000	Thank you.
1897000	1899000	You're welcome.
1899000	1901000	Thank you for your question.
1903000	1905000	Before we move on
1905000	1907000	to other questions,
1907000	1909000	let me read the question
1909000	1911000	from the chat room.
1911000	1913000	Sorry,
1913000	1915000	I don't know
1915000	1917000	what your Chinese name is.
1917000	1919000	It's Jeffrey Wang.
1919000	1921000	He asked a question.
1921000	1923000	He said the experiment design
1923000	1925000	was given to the decision-makers.
1925000	1927000	I just asked him.
1927000	1929000	OK.
1929000	1931000	Thank you.
1931000	1933000	OK.
1933000	1935000	I have some other questions.
1935000	1937000	Let's see if there are any questions
1937000	1939000	to be asked.
1939000	1941000	Let me advertise
1941000	1943000	some activities for Terra.
1943000	1945000	Terra is now
1945000	1947000	issuing a participation certificate.
1947000	1949000	The certificate can be
1949000	1951000	submitted to your mobile app.
1951000	1953000	We call it
1953000	1955000	Terra Badge.
1955000	1957000	I've posted the certificate
1957000	1959000	in the chat room.
1959000	1961000	When you receive the certificate,
1961000	1963000	you need to use
1963000	1965000	a different code.
1965000	1967000	The code is also in the chat room.
1967000	1969000	Today's code is
1969000	1971000	orcaatiger425.
1971000	1973000	Terra also
1973000	1975000	has a Slack channel.
1975000	1977000	You can join it
1977000	1979000	and get to know
1979000	1981000	other friends
1981000	1983000	and activities
1983000	1985000	related to
1985000	1987000	academia.
1987000	1989000	You can also
1989000	1991000	join the Slack space
1991000	1993000	of Project Terra.
1993000	1995000	The link is in the chat room.
1995000	1997000	Lastly,
1997000	1999000	if you like today's talk
1999000	2001000	or if there's anything
2001000	2003000	we can improve,
2003000	2005000	there's a link for
2005000	2007000	audience feedback.
2007000	2009000	You can fill it in.
2009000	2011000	We'll do our best
2011000	2013000	to make it better.
2013000	2015000	Now,
2015000	2017000	do you have
2017000	2019000	any questions?
2019000	2021000	Hello.
2021000	2023000	Can you hear me?
2023000	2025000	Yes.
2025000	2027000	I'm a student
2027000	2029000	in a math class.
2029000	2031000	My background is not in humanities,
2031000	2033000	so I don't understand much.
2033000	2035000	I'd like to ask
2035000	2037000	if I'm interested
2037000	2039000	in the math model.
2039000	2041000	I'd like to know
2041000	2043000	the meaning of the parameters.
2043000	2045000	Okay.
2047000	2049000	I'd like to know
2049000	2051000	if MJ and EJ
2051000	2053000	refer to the
2053000	2055000	recipient's income.
2055000	2057000	MJ is
2057000	2059000	the salary of the recipient.
2059000	2061000	EJ is
2061000	2063000	his tasks.
2063000	2065000	These two are his income.
2067000	2069000	I'd like to know
2069000	2071000	why the recipient
2071000	2073000	uses this way of expression
2073000	2075000	but the recipient doesn't
2075000	2077000	use this way of setting.
2077000	2079000	He only uses his income
2079000	2081000	as input,
2081000	2083000	not his efforts.
2083000	2085000	Okay.
2089000	2091000	Let me think.
2093000	2095000	The main problem is
2095000	2097000	sometimes people...
2097000	2099000	Oh, sorry. You go first.
2099000	2101000	Sorry, you go first.
2101000	2103000	You go first.
2103000	2105000	Okay.
2105000	2107000	I think
2107000	2109000	when I set it,
2109000	2111000	I'd consider the difference
2111000	2113000	in the degree of efforts of two people.
2113000	2115000	Water usually flows down.
2115000	2117000	So when I find
2117000	2119000	the difference between
2119000	2121000	their efforts and mine,
2121000	2123000	I might not
2123000	2125000	give him that much.
2125000	2127000	At that time,
2127000	2129000	my data function
2129000	2131000	might be the difference of efforts.
2131000	2133000	If I consider this,
2133000	2135000	I don't know if your model
2135000	2137000	or your prediction
2137000	2139000	will be different.
2139000	2141000	I think it's a good question.
2141000	2143000	Actually,
2143000	2145000	I thought about it when I analyzed it.
2147000	2149000	Of course,
2149000	2151000	you can modify the model.
2151000	2153000	I think
2153000	2155000	the decision-maker's income
2155000	2157000	doesn't matter
2157000	2159000	whether you set it as
2159000	2161000	mi, ei or yi.
2161000	2163000	It won't make any difference.
2163000	2165000	But there is a place you can modify.
2165000	2167000	For example,
2167000	2169000	we assume that
2169000	2171000	the relationship of the line
2171000	2173000	is theta0 plus beta
2173000	2175000	times the degree of efforts
2175000	2177000	of the recipient.
2177000	2179000	According to your theory,
2179000	2181000	the function of theta
2181000	2183000	is actually the model.
2183000	2185000	We assume that
2185000	2187000	it is a theta0,
2187000	2189000	which is the baseline,
2189000	2191000	plus beta times
2191000	2193000	ez minus ei,
2193000	2195000	which is the degree of difference
2195000	2197000	between the two efforts.
2197000	2199000	I think it's possible.
2199000	2201000	It's a very good question.
2201000	2203000	I remember that
2203000	2205000	we didn't find
2205000	2207000	such an effect in our analysis.
2207000	2209000	I remember that
2209000	2211000	we didn't find
2211000	2213000	such an effect in our analysis.
2213000	2215000	I remember that
2215000	2217000	we didn't find
2217000	2219000	such an effect in our analysis.
2219000	2221000	I remember that
2221000	2223000	we didn't find
2223000	2225000	such an effect in our analysis.
2225000	2227000	I see.
2227000	2229000	So the decision-maker's income
2229000	2231000	doesn't matter
2231000	2233000	whether you set it as mi, ei
2233000	2235000	or yi.
2235000	2237000	You mean here?
2237000	2239000	No, I don't mean that.
2239000	2241000	You set it as mi, ei
2241000	2243000	or yi
2243000	2245000	in the income.
2245000	2247000	You set it as mi, ei or yi
2247000	2249000	in the income.
2249000	2251000	You're right.
2251000	2253000	But what you say is
2253000	2255000	mainly about the C-harm.
2255000	2257000	You can see
2257000	2259000	how much money
2259000	2261000	the decision-maker
2261000	2263000	will give to the recipient.
2263000	2265000	For example,
2265000	2267000	if you set mi as mi and ei,
2267000	2269000	the effect of dividing
2269000	2271000	mi and ei is the same.
2271000	2273000	The effect of dividing mi and ei is the same.
2273000	2275000	The higher the income of the decision-maker,
2275000	2277000	the more money he will give.
2277000	2279000	the more money he will give.
2279000	2281000	So there's no point
2281000	2283000	in separating them.
2283000	2285000	But what you said is about the C-harm.
2285000	2287000	But what you said is about the C-harm.
2287000	2289000	I see.
2289000	2291000	Thank you.
2293000	2295000	Thank you.
2295000	2297000	Thank you.
2297000	2299000	Okay.
2299000	2301000	Do you have any other questions?
2301000	2303000	Okay.
2303000	2305000	If you have no other questions,
2305000	2307000	If you have no other questions,
2307000	2309000	I'll turn off the recording.
2309000	2311000	I'll turn off the recording.
