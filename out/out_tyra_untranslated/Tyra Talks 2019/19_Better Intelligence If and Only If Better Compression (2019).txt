OK,然後那這樣子做了之後的一個最直接的好處,就是平均而言,每一個每一個每一個symbol它所花的容量,就是它的它的呃,它出現的頻率跟我們代表它的頻率,那那這樣子設計起來馬上就可以看出來,我們這樣做,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃
呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃.
就是 Shannon enthropy, 那這個,這個滿經典的結果,我猜可能到滿多人都有知道。
那可是,呃,這邊的討論比較,呃,都是假設在我們已經知道呃 X 的機率是多少? 跟 Y 的機率是多少?
可是,在實際上的應用裏面,呃,其實通常我們都是不知道這件事情的,我們必須要去學這件事情。
OK,所以接下來我們就會主要在討論怎麼去學 PX 跟 PY。
好,那,那,那再來我們就把這個假設再把它放寬鬆一點,假設我們已經不知道 PX 跟 PY 了,
可是我們知道,呃,這個 input 0001 001 代表可能某個人的作息,那這個人的作息他基本上只會在睡覺、吃東西跟跑步中間一直轉。
那假設我們知道這件,呃,這個事情,然後又知道說,呃,00代表睡覺,01代表跑的話,那我們要預測下一個是什麼就就很簡單了,因為我們知道說他現在是在睡覺的狀態,那他下一他下一步會做的事情比較可能是跑,所以說接下來的兩個符號就比較可能會是01。
所以這個是,所以假設說我們知道,呃,那個背後的這個訊號的來源是一個,呃,markov chain的話,那我們就可以直接用數數的方式去去知道說下一步會是會是什麼。
我們就,呃,就比方說我們數到可能十次有六次裡面是跑,十次有兩次裡面是吃飯,那我們就知道說,OK,呃,PY 大概是等於0.6這樣子,那可是這個,呃,還是跟我們實際上遇到的情況還是不太一樣。
因為我們實際上其實我們不知道這串01001代表的是某個人的作息,還是還是DNA,還是一張圖片,那這時候,呃,怎麼辦呢?那再來,那後來就,呃,有人他就想到說,呃,我們其實就,我們就假設他是可以,他是假設他就假設是一個markov chain。
我們就把inputs一直去把它切切開來,那那我們就一直,然後假如說遇到新的沒看過的,呃,chunk的話,我們就把它當做一個新的chunk,那比方說,呃,假如說原始的資料是a,b,r,a,c,a,d,a,b,r,a的話,那他就說,OK,我們看到a,所以a就是,a我沒看過,就是一個chunk,那b也沒看過,也是一個chunk。
那到了這個a的時候,我們就發現說,欸,a其實已經看過了,那我們,所以a的話,那邊就可以加,加e,它的出現的次數就,所以r之後看到出現a的頻率就是,就是e。
然後再來我們就再看下一個,好,a,c,所以a,c這整串是沒看過的東西,所以他就是一個自己的單位,然後a也看過了,可是a,d沒有看過,那彼此累退,然後我們把它chunk起來之後呢,我們就可以一樣把它當做是,呃,個別的,呃,單位,然後去數說,欸,到底b之後出現a的次數是多少,a,c之後出現r的次數是多少。
那這樣子的話,我們就一樣能夠去知道,到底每個符號的機率是多少了。
那,呃,這個其實這件事情,基本上是我們,呃,電腦裡面,就可能有在寫程式的人,我們可能會用,呃,gzip,或是windzip,呃,這些程式來做壓縮,那其實這些,這些windzip,gzip的程式在做壓縮的時候,背後其實是在做這個事情,去預測。
下一個,下一個bit出現的機率是多少,那,然後再根據他做那個,那可是做這個事情有,有幾個缺點,那主要,呃,主要有兩個缺點,其中第一個缺點是,呃。
呃,他他沒有利用這個整串文字,a,b,r,a,c,a裡面所有的資訊,呃,像比方說,假設說我們想回答這個問題,呃,看到br之後出現a的機率是多少。
那這時候,我們沒有chunk到br,所以他可能就會直接用,呃,a出現了幾次來代表,可是我們看到說,至少在這個,這個片段裡面,出現br之後,永遠都會出現a,所以我們知道a的機率是很高的,可是假如說我們用lampelzip來chunk的話,我們其實沒有抓到這個,這個資訊。
那這是第一個問題,那第二個問題是,呃,假設說我們想要知道raa之後,呃,出現b是,b是多少,那我們,我們講,因為raa不在,不在,也不在我們的chunk裡面,所以我們直接拿b出現的次數是多少。
可是其實這邊,其實我們已經,其實已經有b之後出現a的頻率是多少,其實我們是有這個,這個資訊的,那或者是說我們其實有很多不同的prefix都可以拿來用的話,那lampelzip只會按照一個邏輯去說,假如說有,假如說,呃,有a,ac都可以用的話,他就會照一個方式去隨便選一個。
那這時候他有可能選到的不是最適當的那個。所以這是兩個lampelzip主要的缺點。那,呃,所以怎麼去,怎麼去解決lampelzip的這個,這個缺點呢?那再來這些缺點就會在這個context-free weighting的,呃,算法裡面去把它解決了。
那那個lampelzip他的第一個缺點是,呃,有時,有些,有些prefix他,他雖然看到了,可是他沒有去pass他,那怎麼,那就解決這個問題的方法就很簡單,就是我們不要,不要跳過,不要跳過,呃,symbol,我們每一次,每一次都是一個symbol,然後去看他前面的,前面的prefix是多少。
那這樣子的話,我們所有的prefix都會抓到。那像比方說,假設說,我們的,我們已經資料,我們的資料是01,00,110,那我們,我們,那我們去,呃,去記錄到底每一個prefix後面會發生什麼事情的話,我們就ok,所以看到110之後出現0。
所以就看到1,呃,1,呃,0,呃,我們從後面往前看的話,就會看到011,所以我們就記錄說ok,011之後會出現一個0,然後,呃,001之後會出現一個1,所以001之後會出現一個1。
那我們這樣慢慢,慢慢做的話,我們就得到比,呃,更密集的,呃,的歸納了。
所以這樣的話,就解決他的第一個問題。那,那還有第二個問題,就是假如說,有很多很多的prefix都滿足我們要的query的時候,我們應該要挑哪,哪一個?那具體來說,比方說像我們想要,我們假如說,我們已經知道說資料是這樣子的,我們,然後我們想要知道第八個symbol是0的機率是多少。
好,那我們可以說ok,我們可以把這個問題frame成,我們看到0了之後,後面出現0的機率是多少?那這時候,呃,假如說前面是0,那前面是0的符號有,有這個,所以前面是0,後面出現可能會是0,那前面是0,後面也可能會出現是0。
那前面是0,後面也可能會出現是1,這樣,以此類推,我們就會把,我們就會覺得說,噢,出現0之後,又出現0的機率是0.5,可是我們也可以回答這個下一個,第八個是什麼?來,呃,從前面是01的角度來看,那假如說我們從這個角度來看的話,我們就會發現說,ok,假如說前面是01,後面會出現一個0。
那前面是01,後面會出現一個0,所以說,假如說我們看到01的話,那再來會出現0的機率就是百分之百了。那我們,所以我們就變成說,我們到底,我們到底要應該選哪個哪個哪一個pattern去代表我們的機率呢?那,呃,解決這個方式的話,呃,我們其實可以從一些,呃。
自然哲學的,呃,一些原則來看,那其中有一個,呃,蠻常見的principle,就是他就說,還是這是一個哲學,它不是一個定量的描述,不過等一下會看到怎麼把這個定量的哲學就把它,呃,定定量定性的哲學,把它定量的話,那這個哲學的說,假如說,呃,有很多不同的理論都都可能解釋一個現象的話,
那我們沒辦法,我們一定要保存所有的理論,我們不能說,啊,這個理論比較好,那個理論好,只要這個證據都符合的話,那它都是合理的證據。
OK,那接下來我們就會講怎麼去把它,呃,定量化具體化,那,那這邊的話,我們,呃,我們也可以把我們剛剛遇到的問題把它想成說,OK,我們已經有這些記錄這些資料了,那我們到底要用哪一個模型來解釋資料呢?那這邊的話,有很多不同的模型,那第一個模型就是我們完全不管,呃,他的prefix就直接說。
下一個是0或是1的機率,我們就數到底有幾個0跟幾個1,那這個就這個模型比較,呃,複雜一點,他可能會去算一些prefix,那這個模型又更複雜,這個模型又更複雜,所以這邊有五個模型,到底哪一個模型是我們應該用的呢?呃,呃,一個最簡單的,呃,假如說我們,我們接受這個原則,就是我們所有模型都要用的話,那就代表說我們一定要給。
某些模型,我們一定要給這些模型,不同的模型一個權重,那怎麼去給這些模型一個權重呢?呃,呃,就是就是我們接下來要要要探討的,那這邊的話,呃,我們主要是想要說,呃,這個這個這個模型,呃,就就我們把這個權重的把它圖像化,對,比方說,呃,我們想要。
呃,從這邊來看下一個是0還是1,我們就可以把它想成是這個模型跟這個模型的權重,那對於這個模型來講的話,是這個模型跟這個模型的權重,那對於這整個模型來講,他其實是這五個模型的權重,那我們從圖像上可以看到說,呃,這個大模型的。
他其實包含了前面兩個小模型,那我們就問說,有沒有一個辦法去,呃,去去知道說,這個大模型的權重是什麼?假設說我們知道小模型的權重是什麼?那簡單來講就是,呃,我們這邊有五個參數w1 prime,w2 prime,w3 prime,呃,假如說,呃,簡單,呃,廣義來講的話,這個數可能會有,呃。
double exponential,呃,2的2,d次方的數,去要,要去解釋他,我們能不能不要去,呃,計算這麼,這麼多,這麼指,呃,指數數量級的數,去,呃,我們不要計算這麼多的數目,而用比較少的計算,去算出來這個權重,是我們,呃,在實作上會遇到的問題。
那,呃,簡單來講的話,呃,這邊的主要的,呃,主要的一個,呃,insights是說,呃,我們其實可以用這個乘法的概念,來,來做,來做他的那個recursive的計算。
那,主要的概念是因為,假設說,呃,假設說,是p,p的這樣子的話,那這兩,那這兩個相乘,基本上有點像是這4個,呃,子模型在做一個cross product,他就有4個項,那這,這4個項,咦,好像就可以去,去描,描述這4個項。
所以,呃,廣義來講的話,呃,假設說,假設說我們定義,呃,上,比較大的這個模型是等於說,呃,我們單純只看0101的話,然後再加上下面的子模,下面的數的子模型的,然後對他做cross product的話,我們就會有,呃,n平方的項。
那假設說我們去定義這個權重的話,那這似乎是一個,呃,蠻直覺的一個做法,那其實,那其實這個做法是對的,那他對的原因有,有幾個,那其中一個,一個他對的原因是因為,我們這樣定義這些位置之後,呃,就是w1 prime,w2 prime,w3 prime,w4 prime之後,
我們用這個recursive的公式去定義他的時候,就會發現說,其實剛好這些w1 prime,w2 prime,w3 prime,w4 prime,w5 prime,他們加起來都會等於1,那其實這個推導也蠻,呃,蠻直覺的,因為假設說,我們知道下面的數加起來等於1了,那上面比較大的數,他加起來一定會等於1。
所以這是第一個好的性質,那,呃,ok,所以,然後還有,那這個,那這個,這個直覺的recursive的公式,他還有,呃,第二個,第二個直覺去,去證明他是一個好的模式,那他第二,這個好的直覺的,呃,的論述是說,其實假如說我們這樣子做的話,我們其實無形中就把這,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,�
雙重指數,呃,數量級的,呃,的,的數目的數的,呃,呃,把他,把他權重就變成是,呃,所有權重的那個像,像,像成,那比方說,呃,這邊有,有好多好多的數,那這個數我們應該要給他多少權重呢?呃,基本上就是,呃,一減掉這邊,呃,這邊的這邊的位置,
然後乘以一減掉這邊的位置,再乘以這邊的位置,乘以這邊的位置,就是假如說不是leaf的話,就是一減掉,一減掉w,呃,然後假如說到了leaf的話,就是自己,所以,呃,所以基本上我們在做這個事情的話,其實是把這些tree都給,給他一個權重,那這個權重是他們之間的相乘,
那,那假設說,那這邊我們其實有,呃,舉,舉一些實際例子,假設說這,這裡面有,有2的,2d次方的這些數,假設我們這些數都給他一樣的權重的時候,呃,我們就會,我們就會發現說這些w,呃,滿足,滿足這個,呃,這個共識,那比方說最上面是1的話,那在接下來就是1,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那,那
呃,接下來就是5,呃,這邊是1的話,那上面的話就是5,那假如說這樣子做起來的話,我們就會發現說,假如說我們要去specify一個數的時候,呃,的這個描述就需要說這麼多的資訊,這麼多個beats,因為我們有2的2d次方的數目,呃,的不同的東西要去描述他,所以說我們就需要2d次方個beats,來去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去,呃,去
呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,、呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,
呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,uber,uber, necesitabili 的這些w, calculate 的這些w,update 的這些w,使得說,我們能夠用比較小的,比較短的符號去把他描述呢,那其實,是有人,那這個方式就是把這些w都變成2分之一,那都變成2分之1之後,其實,我們在描述一個,數的時候,就變成是有點把這個素描,用用他的。
prefix code 去把他描述出來,像比方說我們描述這個這個形狀,他他就是11000,那那他的長度就是5了,那我們其實不需要,呃,這麼,呃,這麼多個東西的,彼此去去描述他。
那所以,呃,把這些東西都整合在一起,我們就可以發現說,其實,呃,pw是大概是這個這個公司,那這個公司,他就可以很經典的去去描述這麼多的數,然後就給他一個合理的權重,那我們假如說我們又回來看這個公司的話,我們就會發現說,欸,其實我們這樣做之後,我們不同的。
給這些不同的模型的權重,其實是跟他們的複雜度成反比的,假如說這個數越越大的話,那我們給他的權重就會越小,所以簡單來講,其實我們是在套用,呃,一個物理或是科學上蠻常用的一個原理,就是越簡單的的解釋,我們覺得他越可能,所以,呃,這個也蠻蠻符合我們的。
做做科學的直覺的,那,呃,這樣子做之後,其實,呃,我們把所有的東西都把它裝在一起,然後去做一個理論的探討,去做一個,呃,歸納總結的話,就會知道說,呃,那他自己他是一個很好的壓縮,那假如說n很大的時候,他基本上就等同於我們已經,呃,知道說他,他到底是不是一個人的作息?
還是睡覺?還是走路?還是還是吃東西?假如說n夠大的時候,我們去chunk他永遠都會chunk到原本的那個真正的chunk,那他的誤差會很小,那,呃,呃,很多地方,可是有這邊,這邊有個細節,就很多地方的,呃,然後課本裡面他們說是一個optimal的壓縮,呃,這句話他是對的,他他們這邊是指說,呃。
呃,壓縮起來的長度減掉真正的長度,然後除以n的話,大概是1除以log n,所以會趨近於0,對,他們是說,呃,每simple他會趨近,the error會趨近於0,呃,可是其實那policy他不是optimal,呃,嚴格來講,他不是optimal,主要的原因是因為他趨近於0的這個的這個速度不是最快的,那假如說,因為主要有兩個,那兩個缺陷,就是他沒有,呃,趨近於0的這個速度不是最快的,那假如說,因為主要有兩個,那兩個缺陷,就是他沒有,呃,趨近於0的這個速度不是最快的,那假如說,因為主要有兩個,那兩個缺陷,就是他沒有,呃,
因為他,他,他跳過很多很多的東西,而且假如說有很多的解釋的時候,他只調一個他沒有依照他沒有給他沒有參考所有的解釋,依照比較簡單的解釋給他更大的權重,那可是假如說我們這樣做了之後,我們就會發現說,他趨近於0的速度是log n除以n,呃,所以他一樣,每單位的誤差也會趨近於0,不過他趨近於0的速度其實會比那policy快,
那這邊的話,還大概正比於說,呃,原本的source的tree是什麼,所以,呃,所以他,他某種程度是optimal,可是他,嚴格來講,他不是optimal的,主要是因為他的速度,呃,趨近的不夠快,那那總結而言的話,呃,我們可以從一個理論的角度去探討說,其實這個趨近於速度的,呃,趨近於速度的,呃,趨近於速度的,呃,趨近於速度的,呃,趨近於速度的,呃,趨近於速度的,呃,趨近於速度的,呃,趨近於速度的,呃,趨近於速度的,呃,趨近於速度的,呃,趨近於速度的
這個速度,其實也是有個極限的,那這個極限是由,呃,Resultant Bound來給,Resultant Bound他說,呃,壓縮出來的長度減掉,呃,真正的長度永遠永遠會大於二分之,永遠大於二分之k log n,所以,呃,我們從這邊再回去看CTW的,那個,Simple的一個rate就會發現說,欸,其實CTW他趨近於這個理論上最小的
最快最快的速度,就是log n除以n每單位,然後,而且我們也看到說,假設說,呃,k是,呃,這個原本的,呃,是,是,訊號來源的自由度,Degrees of Freedom,我們就看到說,這個訊號越複雜的話,我們其實,我們就算用最強的學習算法,我們還是,這個,它模型越複雜,我們的誤差還是會越大,所以,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,呃,這個,
呃,也符合直覺,那,所以,呃,總共來講就是怎麼從,
呃,更好的智慧,到,更好壓縮其實壓縮這件事情,其實,呃,分三個部分,那,第一個部分,是,呃,資料本身的的複雜度,
那,這個我們知道傳統上就是用Arithmetic Coding去,去解決它,那,其實,還有實務上,其實,呃,還有,其實,更大的問題,是,到底,這是,其實是學習的,呃,的代價,呃,就是,這個,這個,其實是實務上,呃,最大的問題,因為這個問題,它是正比於資料的長度的,
雖然說,呃,我們是可以把它壓在log的長度,而且它不只是正比於log,它其實也直覺上符合,就是它正比於這個問題本身的自由度有多少,這個問題越複雜,本身越複雜,那我們去學習這個問題的代價就要付出的越高,
那,再來剩下的才是,呃,我們,呃,研究人員要,要去把它降到0的,就是我們要想到一個最好的算法,使得這個,我們的,呃,用一個不好的,呃,然後我們從統計的角度來看,用一個不好的estimator,跟真正的estimator,跟最efficient的,最有效率的estimator的差別,我們其實應該要想辦法把這個用到0,
那所以總結來講的話,呃,壓縮其實大概是分這三個部分,我們要把最後這個部分,呃,變成0,然後我們也從上面的看到說,因為,呃,ct,context tree weighting,它,它趨近於這個理論上最,最快最快的速度,所以說它,它是一個非常有效率的estimator,OK,所以這個是,呃,理論的部分,那,呃,理論聽起來,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃,其實,呃
聽起來很完美,很神奇,它到底這個理論有沒有用呢?那這邊,呃,是給一個例子,就是我們想要去壓這個,這個文,這個,這篇這篇文章,那這個文章其實是,呃,林肯的那個address,他就是英文的for school seven years ago,那他原本,這個,這個,這篇演講大概是,呃,有,呃,大概1k多的容量,那假如說我們把它用,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃
用一些比較常見的壓縮程式去壓它,然後就會發現,它大概可以壓到這個大小,不過假如說我們用ctw的話,理論上更有效率的,更有更好的學習,算法,智慧,去壓的話,就會發現,欸,它其實壓的比較小,所以這代表說,欸,其實我們,呃,預測的能力,呃,做的越好的話,我們其實總共能夠壓的程度就會越小,所以
這個,這個理論是,應該,應該是沒有錯的,OK,所以這大概就把,呃,這個等式從更好的智慧,會得到更好的壓縮,呃,去把它講,證明完了,那再來我,我猜比較,比較神奇的,比較,比較有趣的是,呃,怎麼從更好的壓縮程式去得到智慧,假設說我們,我們每每台電腦上面都有winzip,我們
怎麼去用winzip這個.exe去做機器學習,那或是說我們有一個更好的,呃,的壓縮程式,我們怎麼用機器去做更好的機器學習,那我,我想要這邊去證明這件事情,呃,的角度是從,呃,我們是從假設說智慧,我們把它剛剛是說是預測未來,假設說我們把智慧的定義稍微改成是從
我們知道裡面去找,呃,pattern的話,呃,或是更精確,或是更具體來講,就是假設說我們把智慧當成是做非監督式學習,unsupervised learning的能力的話,呃,我們能不能去證明這件事情,更好的壓縮程式就能夠做更好的unsupervised learning,那呃,我猜就這邊,我們能夠做unsupervised learning的話,我們就當然能夠做很多
有智慧的東西能做的事情,像,呃,那個condense method的研究,或是一些天文的研究,或或更多,對,那更具體來講,我們在做unsupervised learning的時候,其實我們是想要去回答說,呃,假設說我有ABC三個東西,到底A跟B比較接近,或是A跟C比較接近,或是B跟C比較接近,呃,或者簡單來講就是,呃,其實unsupervised learning的
智慧其實是在找一個距離,是在量測兩個東西它有多接近,或是,或是多不一樣,那OK,所以,呃,這邊主要是一個簡單的,呃,架構去說,我們到底怎麼用一個windzip.exe去算出兩個任意東西的,呃,的距離,那我們這邊,呃,其實這個unsupervised learning的
算法其實蠻廣義的,因為我們假設說,任何東西都可以變成010101,那,那,假設說,呃,x是一個0101的string的話,然後我們kx就是,呃,用這個壓縮程式去壓完之後的長度的話,那所以說,那假如說把x跟y併在一起,然後去壓,就是這個,這個長度,那我們知道說,當這個
壓縮程式蠻好的時候,呃,kxy會大概大概等於kyx,那這邊是,呃,沿用條件機率的那個notation,假設說我們已經給定x之後,呃,y
還需要多少,多,多額外的長度去,去重建y,那這個,這個,這個,這個定義就是kxy減掉kx,呃,所以嚴格,所以換句話說,呃,ky given x,就是那些在y裡面,可是不在x裡面的資訊,
所以有了這個條件機率的定義之後,我們就可以知道說,ok,對一個好的壓縮程式來講,給定x,再去壓x,我們其實只需要0的長度而已,那,那剛剛這個,呃,ky given x是總共的長度,那假如說我們想要問,
呃,per單位symbol或是perbit,呃,呃,呃,在y裡面,可是不在x裡面的資訊的話,呃,就直接再把這個ky given x,x為y就好了,這個就是per單位的資訊,那所以這個,這個已經有點像是,呃,我們用一個智慧的程式去,去,去量測,到底在y裡面,可是不在x裡面的資訊,那,那這個,這些,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,
呃,x裡面沒有的資訊,就可以算成是,呃,y跟x之間有多不同,也就是他們之間的距離是多少,這個是,這有點像是一個蠻不錯的,
呃,general purpose的做answers by learning的方式,或是定義任意兩個東西之間的距離是多少,那可是,呃,這個,這個他已經抓到我們的intuition了,可是他,他有些缺點就是他不是對稱的,那我們假設稍微做一點,做一點改變的話,呃,去做他的max和max的話,就會發現說,其實這個東西,他就是對稱的,
而且當,呃,y大於x的時候,他會,他會跑到這邊來,或是x大於y的話,他會跑到這邊來,所以,呃,這個東西,我們把它當,把它定義為任意兩個東西的,呃,距離的話,他就是,他就是對稱的,那剛好假如說我們這樣定義的話,欸,就會反著說,欸,其實這個長度的定義,其實太滿足三角不等式,所以3號好像,呃,這個就是,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃
然後去算出任意兩個東西之間的距離,所以這個好像,欸,我們好像這樣子推導完之後,就發現說,這個方法可以讓我們給,給一個,可以壓縮程式,我們也不知道拍照程式裡面是做什麼,只是假如說他做的夠好的話,他3號就可以幫我們量測,呃,任意兩個東西之間的距離是多少,那ok,那到底這個,這個想法,呃,這個,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,
理論上很好,那實務上到底有沒有,有沒有用處呢,那我們,呃,我今天的話,我是主要給兩個例子,來去說明,呃,他其實是有某種程度上的用處的,那第一個例子的話,我們是想要去解決,呃,比較生物學上面的問題,那精確來說,是想要解決,呃,生物上的一個,呃,
哺乳類動物裡面,呃,他們,呃,所關注的一個問題,那,呃,其實這個,這個debate,呃,算是生物學大概在1990年,2000年初的時候,一個蠻,蠻常見的debate,就是他們debate說,到底我們人類,呃,比較接近豬,還是跟老鼠比較,比較接近,那,呃,有,有,有些人,有些人說,呃,人類跟,呃,老鼠比較接近,那,呃,有,有些人,有些人說,呃,人類跟,呃,老鼠比較接近,那,呃,有些人,有些人說,呃,人類跟,呃,老鼠比較接近,那,呃,有些人,有些人說,呃,人類
跟,呃,就是豬或是馬比較接近,可是,呃,用另外的資料的話,用另外的生物學的處理的方式,就會發現,就會說,欸,其實人跟老鼠比較接近,那我們能不能用一個壓縮程式去幫忙回答這個問題,呃,簡單來講,就是我們假如說給定,OK,呃,猴子是這個DNA的話,GTTAT這個,這個字串,
然後豬這個字串,然後我們對每一個,每一個字串去跑壓縮,去算出這些字串之間,在這個字串,不在這個字串的資訊有多少的話,我們然後去做一個分類的話,我們能,能不能回答這個問題,那,我們在做這個事情,呃,呃,有,有,呃,其實是跟傳統生物學蠻不一樣的,呃,主要的主要原因是因為傳統生物學其實利用了很多,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃,呃
呃,生物學的資訊,像比方說,呃,通常上我們其實會,會,他們會對這些原始的資料去做alignment,說
其實,其實,大概我們其實這個,這個部分要對到這邊來,GT跟GT,我們其實應該要這樣對起來,而且,呃,比方說,可能三個,三個,呃,RNA,其實會,呃,組成一個condom
那第三個,第三個condom裡面的第三個,呃,可能它的變異,變異,它變異的頻率比較高,所以我們把它忽略掉,對,有點像會做一些傳統機器學習說的feature engineering
去,去得到一個結果,那我們這邊就想說,我們能不能,呃,跟現在比較新的machine learning一樣,我們完全不做任何的feature engineering,就直接去一個壓縮程式,就對raw data去操作,然後就得到,呃,一個也蠻正確的結果呢,呃,呃,然後看起來其實是,是可以的,那左邊這個圖是用單純我們拿一個winzip或是gzip的程式
然後我們就得到說,欸,動物學的分類根據DNA大概應該是長這個樣子,呃,我們可以看到說,欸,其實它基本上都蠻,都已經,已經做得蠻不錯的,像馬跟donkey其實是蠻類似的,老,老鼠其實也都是一個,然後人類跟猩猩其實是一樣的,那不過它還是遇到一些,一些問題,像比方說,呃,這些,呃,這個飛猴其實它是個猴子
所以其實猴子應該要在這邊的,然後而且它也說,呃,其實人類,呃,其實,呃,豬跟,呃,老鼠比較接近,後面,外面才是人類,那這個是gzip的做法,那我們知道說gzip它其實是在做,呃,lampel zip,所以它在預測,呃,未來下一個bit的時候,它其實準確率不像ctw這麼高
那假如說我們用這個比較好的壓縮程式,或是比較好的預測未來能力的,的智慧的話,我們就會發現說,欸,它做出來的成果就比較好了,那它做法,做法都有分類到,而且它說,欸,其實人類跟豬是比較接近的,那,那這個,那這個結果其實也跟,呃,傳統生物學用他們的方,用他們的方式去做的結果其實蠻吻合的
呃,所以說這個是第一個例子說,欸,其實我們是可以用單純壓縮的方式去,去得到,欸,跟傳統生物學比較多feature engineering的方式做結果,呃,然後make sense
那我們除了可以做,呃,哺乳類的的分類,我們其實也可以做對那個sars病毒的的一些,呃,DNA做一些研究,那sars這個病毒我猜很多人知道它就是大概,呃,10年前一個很大的一個疾病,那它的致死致死率非常非常的高,而且到今天,呃,2019年,好,我們還還是沒有。
得到一個疫苗,那我們到底是為什麼呢?為什麼它會,呃,有這麼高的致死率,然後而且又沒有疫苗呢?我們能不能用壓縮的角度來去嘗試回答這個問題?
那我們會用壓縮程式回答這個問題,主要的概念也是把它拿來跟其他類似的,呃,家族的virus去做一個比對,做一個分類研究,然後我們這邊有就是把它拿來跟感冒或是豬或是老鼠的病毒去做一個研究,然後我們這邊的研究也是完全沒有任何的資料資料欲處理。
然後直接去用也沒有任何的feature engineering,那些一些生物的heuristic就直接去拿一個壓縮程式去壓,那其實傳統上,生物學他們其實會做蠻多的箝制跟heuristic,像比方說,呃,生物學家他們在,呃,算,呃,做DNA的研究,他們通常會從一個資料庫裡面去說,呃,A變異到R的機率是多少,A變異到D的這個氨基酸。
然後去大概做一個矩陣一個表格去說,哎,到底這個這個這個痔瘡跟這個痔瘡有有多相近,跟照他們可能會變異的頻率來做來做研究。
那假如說我們單純只從壓這個CTW壓縮的程式,然後根據這個公式去做他們的相似度去做的話,你就會發現說,哎,其實,呃,大概的。
Final logic,final journey,大概長這個樣子,我們可以看到說,最主要就是,哎,sars他其實是跟其他我們說已知的coronavirus是非常非常非常不同的,他自己單純是在一個家族。
然後不只這樣子的話,其實coronavirus大概有一個group1的家族,有一個group2的家族,那group2就是老老鼠跟牛的家族,然後貓跟狗的病毒是在一起,這個是我們單純純粹只用一個廣義的壓縮程式去得到結果。
那這個結果他其實跟,呃,這篇science對應該是蠻有,呃,蠻正,蠻有權威性的一個研究,做出他的研究非常符合,就是說coronavirus大概分成這這幾個group,那然後sars這個病毒還是真的是非常獨一無二的。
那這也有點去解釋說為什麼sars的致死率這麼高,然後而且他沒有疫苗,因為他實在跟其他的病毒都太不一樣。
呃,ok,所以呃,簡單來講,我猜這邊就大概有點把這個兩邊的等式去把它證明完了,我們其實不只傳統上知道說有智慧就有壓縮,其實有更好的壓縮會得到更好的智慧。
那壓縮這個事情的話,呃,其實這個觀點其實也已經蠻多人把它拿反過來應用說,那到底我們在怎麼去衡量IQ這個智慧這個問題,其實我們可以用壓縮的壓縮的方法去秒,假如說這個智慧他壓的更好,他就是更好的智慧。
所以這有點像是可能比我們傳統做的IQ測驗台一個更科學化兩側智慧的方式。那呃,具體來講的話,我們知道說人類其實在壓英文的時候,其實大概是one bit per character。
這有點像是一個我們做人工智能的一個一個目標。那然後再來我我猜呃,第二個第二個重要的訊息就是說,其實我們科學裡面呃,用的這個最根本的原則就是越簡單的解釋,我們應該越重視他這個這個定性的哲學,他其實是非常非常正確的,他其實是可以轉化成一個非常定量的方式。
去呃,去去衡量這些不同的假設,那他不只對不只不只對自然科學家有幫助,他其實在人工智能裡面也是非常非常有用的一個一個原則。
那所以呃,今天的演講的話就呃,大概到這邊。那呃,最後的話,我主要是覺得說呃,就是演講呃,或是或是客戶的分享,通常一定要有一個互動方式比較好。所以這邊是一個一個一個很簡單的一個小quiz。
就是假如說我們去這個網頁去看這個這個壓縮程式的話,呃,然後這個壓縮程式,他其實開發者是是一個他他不是一個亂來的人,他是一個認真的人,那他就是說他這個壓縮程式。
他對任何的資料,他用他eventually會把他壓到0。
這個這個是對的嗎?這個這個這有有這個可能嗎?所以呃,這個是一個小的quiz,呃,給大家有興趣,可以回去想想。
好,大概到這邊,對謝謝大家。
OK,我們謝謝傅明的演講,那呃,請問在在場的觀眾有沒有一些問題想要討論。
哈嘍哈嘍。
呃,我有兩個問題,第一個是,呃,就是後面這個實驗這一些這個on supervised learning的部分。
呃,你們有跟前面其他做這個叫做什麼?呃,computational biology之類的研究比較嗎?就是因為我對computational biology也不熟,所以有點好奇了。
然後,呃,第二個問題是說,呃,on supervised learning這一些分類。
現在有有另外一些類似的應用,就是呃,比如說跟跟跟分類DNA有點像嗎?就是分類這個過去人類歷史上這麼多文獻,然後去去做on supervised learning。
然後比如說,呃,這個所有用英文寫的小說,這個只要收集得到的呢,去on supervised learning,這是是有人在做的。
然後,但是在在做這一些literature learning的時候,他們做的人通常都是都是有一些literature的background,所以他們會覺得說,為了達到某些目的,他們也跟這個biology的人一樣要先處理一下資料。
比如說,呃,假如我想要分類出這個某一類文學作品的的group,然後用on supervised learning做,那直接用raw data送進去learning可能得不到好的結果,因為每一個作者都有自己的習慣。
然後每一個作者寫出來的都會自己變成一個group,所以他們做預處理就是為了要把這些作者的要他們有一些方法,比如說有點像是砍掉outlier這種方法,就是frequency特別高或特別低的把它砍掉這樣子。
也是我聽別人講的,我自己沒有這方面的研究,那我好奇的是,如果用這種壓縮的approach的話,你覺得結果會是怎麼樣?這是我第二個問題。
嗯,是,對,這是非常非常好的問題,抱歉,我剛剛那個實驗的一些細節,我講的比較比較快,那我們這邊跟傳統主要的literature的比較,我們主要是從傳統的話,他們有一些定量的方式去衡量說到底哪個算法比較好。
那主要的問題是,我們這邊的算法是,其實他沒辦法去跟去算出跟傳統方法有一樣的metrics,因為他是直接去計算出兩個東西之間的距離是多少了。
那所以我們這邊的主要的衡量的方式是從這些畫出來的phylogeny的圖,然後去跟他做定向定性的比較。
那比方說,具體來講的話,我們這邊其實在做這個哺乳類演化的研究的時候,我們主要的方式是用他,剛剛忘了提到,是用他mitochondrial的基因。
那回答比方說,剛剛你的第二個問題說,有很多不同的做法,那也的確說,除了mitochondrial這個基因是,他是用他的原因是,因為比較他是比較早就已經被完全簡易出來的資料,所以說他的文獻的數目比較多。
那後來,當大家技術比較成熟的時候,或者成本比較低的時候,也有很多用不同串的基因,或是轉移出來的胺基酸的序列去做排序。
那我們這邊的話,主要就是從最傳統的mitochondrial的基因去做分類,然後做跟文獻上面做的研究去比較。
那我們這邊跟研究的比較的話,我們主要就是,我們沒有特別去說哪一篇的研究比較好。
那我是知道說,比方說像哺乳類動物的研究,其實有些人,後面比較新的研究可能會跟前面的研究比較不一樣。
那不過我們這邊的話,是至少說,至少跟這個單純只用mitochondrial DNA去做分類的研究,得到的結果是蠻,是蠻類似的。
那我們一樣說其實SARS的virus,它其實有好幾段,我們這邊主要是用它的S-protein的那一段去做比較。
那S-protein它主要是那個SARS或是coronavirus的殼,那我們知道說病毒的殼,其實包含了很多很多的那些,有點像針的訊號。
所以說病毒他們要去target某一個新的物體的話,它就會變化,變異它的殼。
那它的殼就更容易穿透另外一種細胞的細胞膜。
那我們所以我們主要是跟我們一樣,我們之所以選擇這個S-virus,也是主要是跟這個,這個以往的文獻,比方說這篇SARS的文章,去用它一樣的資料去選擇。
那我們知道說,我是知道說後來,我們其實SARS的序列其實也不止簡易S-protein。
它其實連它比較有重複的protein的部分也簡易了,那我猜應該也可以用。
然後所以這是回答你第一個問題,我們怎麼去比較,我們主要就是從它pull出來的phenology的圖去做一個定性的比較。
然後你剛剛問出第二個問題是,比方說在其他領域做一些資料域處理會有什麼差別嗎?
那我是知道說其實用壓縮的方式去做非建築物學習,也有在其他領域蠻多的應用。
像比方說我知道說有人把它應用在很多很多的文本,然後想要知道說這個文本是哪個作者的。
比方說我們有很多的小說,可能這個小說是托爾夫斯基,然後有這些小說是莎士比亞寫的。
那我們能不能只從這個文本當中去描述這文本的相似性,就是說這一群都是莎士比亞寫的,這一群都是虎克寫的。
那我就知道說其實是能夠用壓縮的方式,然後不做任何的資料域處理。
比方說像刪除punctuation,或是大小寫去把它統一,或是把空白特殊符號給拿掉。
就是在完全不做這些事情的時候,去對文本做一個蠻不錯的分類。
那除了文本之外,我猜其實基本上任何可以變成,其實它應該蠻general purpose,就是它只要能變成0101,它就能壓縮。
那我是知道特別說不只文本,像比方說像音樂,像比方說我們有這些MIDI MIDI檔。
那我們去做一個分類之後就會發現說,其實這一群MIDI檔好像都是巴哈的風格,那這群MIDI檔都是貝多芬的風格,也可以去做這個分類。
那我是知道說在科學上,比方說像天文裡面做light curve的分類。
就是我們有從天文望遠鏡知道說,這群光它跟時間的變化大概是有這個變化。
就看它可能是個週期,或是它是一個慢慢地可以越上來的變化。
然後有這群天文物體,它可能是這個圖,它的light curve是長這樣子。
我們就對這些序列去做個分類,就會發現說,可能這群天文物體都是超新星,因為它大概都長這個樣子,然後這群物體是白矮星。
所以我是知道說其實是可以在其他領域做一些應用的。
請問還有其他觀眾想要提出問題嗎?
副明,我這邊有一個問題。
嗯,是。
就是,我想問一下,CTW這個壓縮的方法,在很多地方都成功,那它有沒有什麼情況下是它沒有表現那麼好的,那有沒有知道為什麼?
是,有。
那我猜CTW,它有一個問題就是,它這個演算法,它必須事先知道我的數的長度是多少。
像比方說,我們剛剛的例子都是假設它是3,所以它理論上,假如說我們在算這個數字的時候,它是3的長度。
像比方說,我們剛剛的例子都是假設它是3,所以它理論上,假如說我們在算那些prefix,只算到3的話,那假設說我們這個資料,它的pattern是一定要看4或是5個symbol以前的話,
那我們只限制3的話,就知道說邏輯上它就永遠沒辦法學到這個pattern了。
那這個問題有一個具體的體現,就是在圖片的資料裡面,我們知道說圖片的話,在我的左邊的pixel應該都是一樣的顏色,那不只是在我的左邊,在我的上下也應該要是一樣的顏色。
可是假如說我們把圖片去把它變成row by row的去把它變成一個序列的話,就會發現說這個上下的pixel要是一樣的顏色的這個規則,只是要看比方說寬度是600x400的話,那它就要看400個pixel,400個符號以前的東西,才能去學到這個規則。
那我猜這個就是它主要的原因,那我們之所以沒辦法把這個set depth,數的depth去把它拉長的話,是因為它這個建造一個數,它的記憶體的需求量其實是大概指數型成長的。
所以我們知道說指數型成長的東西,它很難去get到某一個大小以上,所以我猜這是它最大的一個問題。
但是如果我們就拿一個random number generator來去放到這個裡面,就是一串數據來放在這個裡面,那它最後會有得到任何的結果嗎?
假如說它真的是完全random的話,它壓出來的kx就會等於自己了。那我們這邊主要是在探究條件機率,就發現說其實它壓出來的話,基本上也會是等於它自己。
它有的資訊是世界上所有其他的字串都沒有的。那所以我猜得到的結果就是它是一個很特別的東西了。
好,了解。
對,直覺上。
前面還有人有其他問題嗎?
我可以再問一個嗎?
可以,可以。
剛剛講ctw不能壓圖片的問題,其實我們有很多圖片壓縮演算法,它就是專門看上下左右來壓縮的。
我ctw是不是也可以設計成我看比如說上面跟左邊之類的,然後來建這些樹,然後就可以。
我猜應該也是可以的。比方說像因為我們這邊的樹是假設它只有兩個branch,我猜樹其實也可以有更多的branch。
我是知道有蠻多的論文是在做這方面的改進。
然後我猜另外一個有些人的改進是說,因為ctw它主要是很密集很密集的去看每個prefix,那有些論文是說我們可不可以,因為知道知道有些特性可以去跳過一些heuristic,跳過一些symbol。
那我猜這方面是可以的。
那我猜這方面是可以的。
喂!
喂喂,你們剛剛有說話嗎?還是剛剛沒有在說話?
我怕我的網路斷掉。
我怕我的網路斷掉。
那想請問一下,還有其他問題嗎?
那想請問一下,還有其他問題嗎?
那各位觀眾如果還沒有加入Taiwan會員,請利用chat裡面的連結加入。
那我們今天再次感謝富明為我們帶來精彩的演講。
我們今天再次感謝富明為我們帶來精彩的演講。
謝謝大家。
有問題的話可以私下再討論。
對富明的網站也在chat裡面有連結。
謝謝大家,歡迎大家下次再來。
謝謝大家,歡迎大家下次再來。
拜拜。
拜拜。
