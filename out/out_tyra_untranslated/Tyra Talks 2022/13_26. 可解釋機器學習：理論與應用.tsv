start	end	text
0	2000	這樣能看到嗎?
2000	4000	可以
4000	6000	OK
6000	8000	大家好
8000	10000	我是莊育南
10000	12000	可以叫我Allen
12000	14000	我現在是
14000	16000	Rice University的PhD學生
16000	18000	我讀的是
18000	20000	Computer Science
20000	22000	我現在是二年級
22000	24000	我在做的研究就是
24000	26000	像是剛才那個
26000	28000	主持人提到的
28000	30000	Predictable Machine Learning
30000	32000	或者是這種Explainable
32000	34000	Artificial Intelligence
34000	36000	目的就是在
36000	38000	介紹
38000	40000	這個Machine Learning Model
40000	42000	為什麼會做出這種Prediction
42000	44000	你要去給他一個解釋
44000	46000	讓人家去
46000	48000	信服你說
48000	50000	這個Prediction Result
50000	52000	不是亂來的
52000	54000	OK
54000	56000	講一下我的
56000	58000	自我介紹
58000	60000	我以前在台灣念碩班
60000	62000	我以前做的是
62000	64000	推薦系統
64000	66000	做推薦系統相關
66000	68000	等下Q&A如果有推薦系統相關的問題
68000	70000	也可以問我
70000	72000	但我會盡量回答的出來
72000	74000	OK
74000	76000	我以前是
76000	78000	大學的時候念數學
78000	80000	我大學在四年級
80000	82000	畢業以前我是不會寫Code
82000	84000	我連一行Code都看不懂
84000	86000	所以
86000	88000	在這邊鼓勵
88000	90000	還想學Code的人
90000	92000	不要放棄
92000	94000	都有機會
94000	96000	努力練練練
96000	98000	應該是大家都有機會
98000	100000	可以去做這件事
100000	102000	OK
102000	104000	那今天的話主要是
104000	106000	OK
106000	108000	那個
108000	110000	我只是想要講說
110000	112000	這開頭太勵志了
112000	114000	反正
114000	116000	今天我不是什麼大師
116000	118000	我只是一個
118000	120000	普通的二年級的PhD學生
120000	122000	所以你如果要打斷我的話
122000	124000	你就直接開啟你的麥克風
124000	126000	然後把我打斷都沒關係
126000	128000	OK
128000	130000	那我就開始吧
130000	132000	OK
132000	134000	今天的Outline會比較是
134000	136000	著重在四個點
136000	138000	第一個就是我來介紹一下
138000	140000	什麼是XAI
140000	142000	什麼是可解釋機器學習
142000	144000	然後再來就是
144000	146000	接下來就是比較偏向
146000	148000	我的研究的部分
148000	150000	在可解釋機器學習裡面
150000	152000	主要會有兩種
152000	154000	一種是模型的本身就有可解釋性
154000	156000	舉個例子來說
156000	158000	大家可能有聽過二分法術
158000	160000	就是Decision Tree
160000	162000	這種Decision Tree的話
162000	164000	它在做Prediction
164000	166000	我們可以看到它的決策過程
166000	168000	那這種就是
168000	170000	它本身就是具有可解釋性的
170000	172000	Machine Learning Model
172000	174000	那另外一種是像是
174000	176000	你們比較常聽到的神經網路
176000	178000	然後像是
178000	180000	Deep Neural Network
180000	182000	就是那種Fully Connected
182000	184000	那種就是Black Box Model
184000	186000	那它本身就不具有可解釋性
186000	188000	因為你不知道它裡面的那些Learning Weight
188000	190000	就是那些學習的那些Weight
190000	192000	到底怎麼分布
192000	194000	或是怎麼去讓這個Model
194000	196000	學習的這個動作
196000	198000	所以我們就需要一個
198000	200000	事後的解釋
200000	202000	就是我在模型學Prediction Model
202000	204000	學完之後
204000	206000	我再額外去Learn一個
206000	208000	我們叫Explainer
208000	210000	就是專門用來解釋的模型
210000	212000	去解釋那個Prediction Model
212000	214000	然後這種的學習方式叫做Post-Hoc
214000	216000	就是After Prediction Process
216000	218000	OK
218000	220000	然後再來就是我來講完這個之後
220000	222000	我來講一下
222000	224000	這個XAI目前有什麼應用
224000	226000	大家如果不是Computer Science
226000	228000	背景的人可能也不要
228000	230000	就是會覺得
230000	232000	我到底學的有什麼用
232000	234000	我講一下現在有什麼Open Source Package
234000	236000	就是
236000	238000	可以做這個
238000	240000	Explainable Model
240000	242000	那當然現在就是大家都用Python
242000	244000	所以我就是講了兩個Python的
244000	246000	Open Source Package
246000	248000	然後跟大家說其實
248000	250000	這個東西不是想像中的遙不可及
250000	252000	就是每個人在每個人領域
252000	254000	當你有Prediction Model
254000	256000	你想要做Explainable
256000	258000	那人人都可以有機會
258000	260000	可以踏進這個領域
260000	262000	OK好那接下來就來
262000	264000	講一下什麼是
264000	266000	什麼是XAI
266000	268000	OK那就是
268000	270000	人工智慧在
270000	272000	在我們日常生活中
272000	274000	已經有很多很多應用了
274000	276000	像是這最近很
276000	278000	前陣子比較流行的
278000	280000	就是DeepMind不是
280000	282000	就是去玩這個
282000	284000	然後他可以下贏那個
284000	286000	人類的Expert
286000	288000	那前陣子
288000	290000	還有DeepMind
290000	292000	有給了一個叫Alpha Tensor
292000	294000	跟Alpha那個叫
294000	296000	算蛋白質的那個東西
296000	298000	那其實上他們都是
298000	300000	就是那種機器人
300000	302000	強化學
302000	304000	強化式學習的
304000	306000	這種應用在日常生活
306000	308000	領域當中
308000	310000	像是一些Medical Diagnosis
310000	312000	就是例如說我給你一個X光片
312000	314000	然後你希望這個
314000	316000	機器人的Model判斷你有沒有腦瘤
316000	318000	那他就會說
318000	320000	有或沒有類似這種
320000	322000	或是甚至說你給今天一個
322000	324000	機器人的Model說你今天有
324000	326000	咳嗽有什麼症狀
326000	328000	然後他幫你判斷你到底得了什麼病
328000	330000	類似這種的Medical Diagnosis
330000	332000	或是像是
332000	334000	Tesla的Autopilot
334000	336000	就是這種
336000	338000	自動駕駛
338000	340000	他不是會Detect
340000	342000	就是有什麼物體
342000	344000	然後他會給你一個最佳路徑
344000	346000	讓你這個車子去開
346000	348000	再來就是Voice Recognition
348000	350000	像是Alexa
350000	352000	不知道大家有沒有用過智慧音響
352000	354000	你只要對這個音響講的時候
354000	356000	他就會偵測你的
356000	358000	語音然後把這個東西
358000	360000	轉換成
360000	362000	文字
362000	364000	然後透過這個文字
364000	366000	Online Server去Request
366000	368000	或是像最近Meta有個很紅的
368000	370000	Speech to Speech
370000	372000	就是把台語轉換成英文
372000	374000	那個也很神
374000	376000	所以就類似這種AI的
376000	378000	應用在日常生活中越來越常見
378000	380000	OK
380000	382000	那但是
382000	384000	就是我們其實很難知道
384000	386000	就是這些模型就為什麼會去
386000	388000	做出這些判斷像是
388000	390000	假設你去看一個醫生
390000	392000	然後今天醫生推了一個
392000	394000	然後你把
394000	396000	你的病狀全部輸入這個
396000	398000	模型他就說
398000	400000	你今天只是感冒你信嗎
400000	402000	為什麼就問你為什麼會做出這個
402000	404000	判斷或者是
404000	406000	今天Tesla給你一個
406000	408000	最佳的
408000	410000	自動駕駛
410000	412000	路線規劃
412000	414000	他給你給出來之後
414000	416000	你真的按照他這樣開你真的覺得
416000	418000	他不會出事嗎
418000	420000	這個決策過程中
420000	422000	如果能夠提供
422000	424000	說今天是因為
424000	426000	這個模型學到什麼什麼特徵
426000	428000	或是他感知到了
428000	430000	例如說有一個行人走過去他停下來
430000	432000	或是他覺得他detect
432000	434000	前方是綠燈
434000	436000	這種感知的
436000	438000	這種為什麼的這種過程
438000	440000	如果提供給使用者的話
440000	442000	那他其實上可以增加
442000	444000	人們對這個模型
444000	446000	對prediction model這種機器學習
446000	448000	模型的這種可信度
448000	450000	對吧
450000	452000	所以整個這個xai
452000	454000	他其實就是在做一件事
454000	456000	他希望提供給使用者一個
456000	458000	合理的原因
458000	460000	例如說因為這個模型
460000	462000	學了什麼特徵
462000	464000	或是這個模型
464000	466000	因為看到了什麼或是這個模型
466000	468000	因為學習了某兩三個特徵
468000	470000	而導致我做出這個prediction
470000	472000	的結果
472000	474000	那xai就是在
474000	476000	把這個過程給完善
476000	478000	像是左邊這個
478000	480000	假設這個autopilot我剛才講過
480000	482000	就是我們xai要做什麼
482000	484000	其實就是想要提供
484000	486000	就是為什麼這個autopilot
486000	488000	會給你的
488000	490000	autopilot做的路徑是安全的
490000	492000	所以你就要跟使用者講說
492000	494000	因為我偵測到模型
494000	496000	我的模型偵測到人
496000	498000	我的模型偵測到前方
498000	500000	可能50米內沒有車子
500000	502000	那再來就是例如說
502000	504000	像是右邊這個圖
504000	508000	這個醫療的診斷
508000	510000	醫療診斷它其實就是要
510000	512000	例如說你要說服這個醫生
512000	514000	或是說服這個病人說
514000	516000	因為我在你
516000	518000	腦的X光片裡面
518000	520000	看到某一塊是腦瘤
520000	522000	我把它框起來
522000	524000	所以我判斷你的腦瘤
524000	526000	這個動作其實上
526000	528000	看起來就是沒有這麼的
528000	530000	在整個prediction的過程中
530000	532000	看起來沒有這麼的
532000	534000	這麼有需求
534000	536000	但是如果你提供給使用者
536000	538000	這個步驟的話
538000	540000	就是為什麼這個步驟的話
540000	542000	使用者其實會對於你的決策模型
542000	544000	感到就是非常的
544000	546000	例如說他可以放下心來去
546000	548000	相信這個決策
548000	550000	當然還有很多我沒有提到的應用
550000	552000	像是例如做股市交易
552000	554000	或是什麼的
554000	556000	你今天股市說你就買50萬的Meta股票
556000	558000	類似這樣
558000	560000	Meta現在已經跌到
560000	562000	骨折了
562000	564000	他叫你賣掉
564000	566000	你說為什麼他不會跌超過
566000	568000	低於100塊
568000	570000	這模型如果預測錯了
570000	572000	他叫你趕快賣掉
572000	574000	他說因為股票可能跌超過100塊
574000	576000	類似這種東西
576000	578000	你總要說服買賣的trader
578000	580000	說因為我看到
580000	582000	他的財報全部都不達標
582000	584000	什麼什麼貴的
584000	586000	這種檢測到特徵
586000	588000	你才能說服使用者說
588000	590000	這模型是可信的
590000	592000	所以
592000	594000	這種東西其實
594000	596000	牽扯到幾個點
596000	598000	第一個是我們剛才講到你要說服使用者
598000	600000	然後再來就是你要提供安全
600000	602000	還有一個很重要的東西就是
602000	604000	就是一些法規的東西
604000	606000	像是GDPR
606000	608000	像是Facebook在前陣子
608000	610000	就是Zack Scruber
610000	612000	被叫去歐洲議會罵
612000	614000	或是Google CEO
614000	616000	被叫去美國的
616000	618000	眾議院被叫去罵
618000	620000	其實就是在說
620000	622000	你們使用的
622000	624000	這些privacy data
624000	626000	或是你們使用的個人資料
626000	628000	這些資料的使用
628000	630000	你沒有提供給使用者
630000	632000	一個合理或是讓他
632000	634000	覺得安全幸福的這種
634000	636000	使用方法的準則
636000	638000	你不能就是無條件無上限
638000	640000	去使用這些東西
640000	642000	這些feature當作訓練模型的一個依據
642000	644000	所以如果你今天能透過
644000	646000	XAI的這些方法
646000	648000	提供給這些使用者
648000	650000	我今天是因為用了你
650000	652000	這些feature
652000	654000	然後導致我提供給你
654000	656000	這些個人化的一些
656000	658000	不管是推薦結果或是預測結果
658000	660000	那其實使用者他在
660000	662000	某種程度上他會比較安心說
662000	664000	原來你是使用過這些東西
664000	666000	縱使他是隱私資料
666000	668000	但是總比就是完全關起門來
668000	670000	然後做事然後就跟你講結果
670000	672000	好得非常多
672000	674000	這是XAI為什麼
674000	676000	我不敢說這一兩年
676000	678000	或是這三四年
678000	680000	越來越多人關注的一個
680000	682000	原因
682000	684000	好
684000	686000	再來就是
686000	688000	剛才那些都是一些
688000	690000	比較
690000	692000	high level層面
692000	694000	那我們現在就講一些
694000	696000	比較
696000	698000	科技層面的東西
698000	700000	反正就是如果我們現在在做這種
700000	702000	image classification
702000	704000	就是我們這個圖片分類
704000	706000	我們在判斷這個模型好不好
706000	708000	就是我們單純做實驗我們要
708000	710000	train一個我們要訓練一個
710000	712000	圖片分類的模型
712000	714000	這個圖片分類模型我們總不能
714000	716000	說
716000	718000	當然可以說就是prediction
718000	720000	就是例如說我們去看它說
720000	722000	預測準不準確就例如說看它
722000	724000	accuracy好不好啊
724000	726000	我有沒有成功的預測到屬於這樣圖的
726000	728000	當然是可以這樣做
728000	730000	你可以其實可以想一件事就是
730000	732000	今天這張圖是一隻青蛙
732000	734000	那
734000	736000	今天這張圖是一隻青蛙那今天這個模型
736000	738000	是真的學到青蛙的特徵嗎
738000	740000	也就是說換句話說就是
740000	742000	他真的學到青蛙這個頭嗎
742000	744000	就是這個image classification
744000	746000	這個模型真的學到這個青蛙
746000	748000	沒有人知道他有可能是學
748000	750000	假設後面有一個
750000	752000	一個水塘然後上面有一個荷葉
752000	754000	然後他就覺得
754000	756000	大部分的青蛙圖片都是因為青蛙
756000	758000	在水塘跟荷葉上面
758000	760000	所以他就反而去抓那個水塘
760000	762000	跟荷葉的特徵
762000	764000	那這件事情不是我們樂見的為什麼
764000	766000	那當今天有一個像是這種圖片的
766000	768000	來那學到水塘跟青蛙
768000	770000	的這種的圖片
770000	772000	分類的這種的model
772000	774000	是不是就會predict錯誤
774000	776000	所以這個東西其實是我們不樂見
776000	778000	所以透過這種XAI的方法
778000	780000	就是我們去看說到底哪一個
780000	782000	哪一塊哪一塊重要
782000	784000	那最後如果我們得到我們最後的解釋是
784000	786000	因為
786000	788000	他判斷這張圖是
788000	790000	青蛙的原因是因為
790000	792000	他有這個很具代表性的
792000	794000	這個頭那我們就知道
794000	796000	這個模型其實是
796000	798000	學往對的方向而不是
798000	800000	在抓一些背景的漏洞
800000	802000	或是一些shortcut
802000	804000	就是一些捷徑
804000	806000	捷徑的feature來讓我們的
806000	808000	image classification的accuracy上升
808000	810000	那或是像是在
810000	812000	medical diagnosis就例如說
812000	814000	我們就假設
814000	816000	假設今天有一個檢測
816000	818000	你是不是得covid的classifier
818000	820000	那大家都知道
820000	822000	covid就是很大
822000	824000	很大的機率就會例如
824000	826000	發燒喉嚨痛那今天假設
826000	828000	你只是感冒
828000	830000	那你只要輸入發燒跟
830000	832000	喉嚨痛這兩個症狀
832000	834000	進去他就說你的covid
834000	836000	那你就是慌了就為什麼
836000	838000	所以這時候如果我們要去
838000	840000	判斷說今天這個
840000	842000	的這個
842000	844000	是不是真的學到我們要
844000	846000	學的特徵就例如說我們
846000	848000	也提供了一些肺部X光照片
848000	850000	啊什麼的
850000	852000	他就會去說因為他做出這個covid診斷
852000	854000	並不單單只是
854000	856000	透過你有
856000	858000	喉嚨痛跟發燒這兩個特徵
858000	860000	而得出你有的covid這個結論
860000	862000	所以我們今天
862000	864000	如果在
864000	866000	提供這種prediction result給
866000	868000	使用者的時候我們附帶上
868000	870000	我們因為detect到
870000	872000	你有什麼什麼特徵
872000	874000	使用者在相信
874000	876000	這一種prediction model
876000	878000	就會更加的幸福
878000	880000	好
880000	882000	那我喝個水
884000	886000	所以
886000	888000	總結來說就是AI跟XAI
888000	890000	他其實上他是一個相輔相成
890000	892000	的一個循環的
892000	894000	循環的一個過程
894000	896000	如果今天沒有prediction model
896000	898000	那我們今天如果沒有AI的預測
898000	900000	那我們根本就不需要去解釋
900000	902000	這個東西所以XAI就不會
902000	904000	不會出現
904000	906000	所以我們今天XAI其實
906000	908000	主要是透過
908000	910000	我們另外訓練一個
910000	912000	模型或是模型這個prediction模型
912000	914000	本身就具有可解釋的特性
914000	916000	然後來達到這個
916000	918000	提供解釋的這個動作
918000	920000	所以你可以看
920000	922000	這邊有一張柴犬
922000	924000	然後這邊有一個prediction model
924000	926000	他是一隻柴犬
926000	928000	那為什麼
928000	930000	有可能是這個柴犬趴在地上
930000	932000	因為大部分的狗都趴在地上
932000	934000	我學到這邊比較
934000	936000	偏旁的這些特徵
936000	938000	導致他判斷他是柴犬
938000	940000	也有可能或是因為他
940000	942000	只看到他是橘色的毛色
942000	944000	就說他是柴犬
944000	946000	類似這種我們不知道
946000	948000	因為他是一個黑盒子的模型
948000	950000	就是我們所謂的black box model
950000	952000	所以我們需要另外一個
952000	954000	XAI的這種model
954000	956000	去把這個黑盒打開
956000	958000	把這個black box給打開
958000	960000	然後我們就看到說為什麼他給出
960000	962000	這個圖片是一隻柴犬
962000	964000	然後因為他看到柴犬細細的眼睛
964000	966000	跟柴犬特有的耳朵
966000	968000	那因為這個特徵的
968000	970000	給予
970000	972000	的學習所以我們
972000	974000	讓這個圖片被判斷為柴犬
974000	976000	那這時候這個prediction model
976000	978000	就非常的可信了
978000	980000	因為我們知道他學習到
980000	982000	正確的方向
982000	984000	而不是他去偷懶
984000	986000	去偷懶
986000	988000	學習別的特徵造成這個prediction的結果
988000	990000	所以這個模型
990000	992000	prediction model會是比較一個可信的
992000	994000	一個程度
994000	996000	ok
996000	998000	那剛才那個就是
998000	1000000	一個XAI的
1000000	1002000	大概念的一個解釋
1002000	1004000	那我們現在來
1004000	1006000	比較深入一點
1006000	1008000	因為我不確定這邊是不是全部都是
1008000	1010000	CS background的人
1010000	1012000	的同學或是老師
1012000	1014000	或是一些博士
1014000	1016000	碩士生所以我今天就是
1016000	1018000	會給一個比較high level
1018000	1020000	的一個
1020000	1022000	talk那我們現在就是
1022000	1024000	深入了解就是
1024000	1026000	這種XAI有什麼
1026000	1028000	現行有什麼
1028000	1030000	sota technique
1030000	1032000	就是可以去執行這個
1032000	1034000	explanation的動作
1034000	1036000	ok
1036000	1038000	那在講這個之前我們先
1038000	1040000	可以大概看一下
1040000	1042000	幾個不同的scope
1042000	1044000	那這邊有分成global跟local
1044000	1046000	我其實就大概講一下
1046000	1048000	這個global其實就是
1048000	1050000	我們在看模型
1050000	1052000	假設今天這個image classifier
1052000	1054000	這個classifier
1054000	1056000	in total
1056000	1058000	就是這個模型在乎什麼特徵
1058000	1060000	也就是說我們今天如果都
1060000	1062000	都例如在train一個
1062000	1064000	假設狗的品種分類
1064000	1066000	的這種圖片分類
1066000	1068000	的model
1068000	1070000	那我們可以就是
1070000	1072000	想要知道這個模型到底在乎什麼
1072000	1074000	例如說狗的很多不同品種
1074000	1076000	耳朵都不一樣所以耳朵就是
1076000	1078000	這個模型在乎的一個特徵
1078000	1080000	這就是global的
1080000	1082000	scope那什麼是local呢
1082000	1084000	local比較偏向是
1084000	1086000	我們去解釋每一張
1086000	1088000	圖片他在乎什麼
1088000	1090000	像是我舉個例子
1090000	1092000	像剛剛那張柴犬
1092000	1094000	我們要做global的話我們就是
1094000	1096000	必須要去看上面的classification
1096000	1098000	model他在乎的
1098000	1100000	在所有的狗裡面他在乎的什麼特徵
1100000	1102000	但是如果在local scope下面的話
1102000	1104000	我們就可以我們就必須一張一張
1104000	1106000	圖片去看說
1106000	1108000	這張圖片他是因為
1108000	1110000	柴犬的臉那下一張圖片
1110000	1112000	可能因為什麼邊境牧羊犬
1112000	1114000	的尾巴或是邊境牧羊犬的
1114000	1116000	耳朵這種所以在
1116000	1118000	local scope底下每一張圖片
1118000	1120000	都會具有每一張圖片的解釋
1120000	1122000	那這其實有各有各的好處就是
1122000	1124000	我們當今天如果要去宏觀的看說
1124000	1126000	這個模型可不可信那我們當然就
1126000	1128000	使用global就可以了嘛
1128000	1130000	我們就要去提供給使用者說
1130000	1132000	in total來說我們這個
1132000	1134000	autopilot for example就是
1134000	1136000	in total來說我們這個假設autopilot
1136000	1138000	這個模型因為他
1138000	1140000	會去
1140000	1142000	detect紅綠燈
1142000	1144000	有沒有亮或是有沒有行人或是路上
1144000	1146000	有沒有障礙物這種in total的
1146000	1148000	方式我們去說服使用者那我們當然
1148000	1150000	就使用global那local的話
1150000	1152000	就是我們說在現在這個
1152000	1154000	場景裡面就這一秒
1154000	1156000	那現在這一秒我們這個
1156000	1158000	sensor detect到的這個街景
1158000	1160000	那因為他有什麼什麼
1160000	1162000	下一秒又可以提供到什麼
1162000	1164000	所以當我們今天要去
1164000	1166000	去分析每一秒的
1166000	1168000	這種街景圖的時候
1168000	1170000	那我們就必須使用local
1170000	1172000	的模型那local模型就會
1172000	1174000	提供比較
1174000	1176000	算是比較客製化的一個
1176000	1178000	解釋的結果給使用者
1178000	1180000	ok那這個是
1180000	1182000	global跟local那再來旁邊兩個
1182000	1184000	有不同的manners就是一個是intrinsic
1184000	1186000	一個是prototype那intrinsic
1186000	1188000	就是剛才有提到說模型本身
1188000	1190000	就具有可解釋性那什麼
1190000	1192000	要具有可解釋性本身就具有可解釋性
1192000	1194000	像這個決策數
1194000	1196000	decision tree就是模型最後說
1196000	1198000	喔我今天要預測
1198000	1200000	他的結果是c
1200000	1202000	那因為我們就可以從這個c
1202000	1204000	往後走是假設他因為
1204000	1206000	這個走到這個走到這個例如
1206000	1208000	這樣子所以我們就可以知道模型在做
1208000	1210000	決策的時候他是因為經過了
1210000	1212000	什麼條件然後來達到這個
1212000	1214000	最後prediction的結果
1214000	1216000	那這個模型本身
1216000	1218000	就是具有可解釋性因為
1218000	1220000	我們可以從模型的決策過程中
1220000	1222000	知道他做了
1222000	1224000	截取了什麼特徵或是做了什麼
1224000	1226000	做了什麼condition的選擇
1226000	1228000	ok這個就是intrinsic
1228000	1230000	model或是像右邊這個就是
1230000	1232000	如果大家有稍微聽
1232000	1234000	過一些machine learning的講座
1234000	1236000	或是課程的話應該就會知道
1236000	1238000	BERT這個東西那BERT這個東西
1238000	1240000	他裡面字跟字在模型訓練裡面
1240000	1242000	字會有一個
1242000	1244000	東西叫做tension score
1244000	1246000	所謂的tension score就是例如說
1246000	1248000	我今天把一個句子
1248000	1250000	丟到這個BERT model裡面
1250000	1252000	那我每一個字對於模型最後
1252000	1254000	預測出假設我要做semantic
1254000	1256000	prediction我預測這句話是
1256000	1258000	正向或是負向
1258000	1260000	的情緒那
1260000	1262000	我每一個字對於這個正向或負向
1262000	1264000	情緒的貢獻程度
1264000	1266000	都有加有減就是例如說
1266000	1268000	I love this morning那love對於
1268000	1270000	他最後判斷出是正向
1270000	1272000	絕對有很大的幫助因為他love這個字
1272000	1274000	本身就是一個正向的
1274000	1276000	動詞那
1276000	1278000	這個
1278000	1280000	很大貢獻這個分數
1280000	1282000	我們可以理解為
1282000	1284000	model的tension weight
1284000	1286000	就是這邊比較high level
1286000	1288000	所以我們可以透過這些字跟字之間
1288000	1290000	不同的重要的這種對於
1290000	1292000	model prediction不同的重要程度
1292000	1294000	來去判斷說
1294000	1296000	今天這句話
1296000	1298000	到底為什麼
1298000	1300000	會做出他是
1300000	1302000	正向的這個prediction result
1302000	1304000	透過
1304000	1306000	去拿裡面的
1306000	1308000	important score來最後
1308000	1310000	做prediction的結果
1310000	1312000	那這個我剛才講的這兩個模型
1312000	1314000	都是intrinsic
1314000	1316000	就是模型本身是具有prediction模型
1316000	1318000	模型本身是具有可理解之心的
1318000	1320000	這個範疇那再來就是post hoc
1320000	1322000	那post hoc我覺得
1322000	1324000	我等一下可以講一下為什麼我比較focus
1324000	1326000	在這個點上
1326000	1328000	不是前面不好是因為我覺得
1328000	1330000	現在目前大部分就是
1330000	1332000	machine learning model都是用一些比較黑盒子的
1332000	1334000	模型像是我可能就隨便
1334000	1336000	建一個深度學習的
1336000	1338000	網路
1338000	1340000	就開始疊一些積木啊
1340000	1342000	疊一些什麼全連接層
1342000	1344000	什麼的convolution什麼的疊疊疊
1344000	1346000	那這些
1346000	1348000	這些prediction model縱使他有很好的
1348000	1350000	結果但是他本身
1350000	1352000	因為我不知道他裡面幹了什麼事
1352000	1354000	就是例如說我不知道他
1354000	1356000	他去學了什麼feature什麼
1356000	1358000	所以這種比較
1358000	1360000	黑盒的學習就需要
1360000	1362000	這種post hoc的解釋
1362000	1364000	就是我需要在模型訓練
1364000	1366000	預測模型訓練完後再提供
1366000	1368000	一個解釋專門用來解釋的
1368000	1370000	模型來去解釋
1370000	1372000	前面那個預測的模型
1372000	1374000	所以這種模型
1374000	1376000	在日常生活的這種
1376000	1378000	scenario下是
1378000	1380000	比較常遇到的就舉個例子
1380000	1382000	像是例如說我們做推薦系統
1382000	1384000	我們做推薦系統可能會用一些
1384000	1386000	神經網路的
1386000	1388000	預測模型來做訓練
1388000	1390000	那這時候
1390000	1392000	他神經網路的這個推薦系統
1392000	1394000	本身就不具有可解釋性
1394000	1396000	對吧所以我們就需要一個
1396000	1398000	事後的解釋性模型
1398000	1400000	來去解釋那個推薦的
1400000	1402000	預測模型
1402000	1404000	那這邊一樣有提供
1404000	1406000	這個事後解釋模型一樣有兩個
1406000	1408000	scope一個就是global一個是local
1408000	1410000	那global的話就一樣就是
1410000	1412000	我去看說這個神經網路
1412000	1414000	預測模型呢他到底
1414000	1416000	這個模型本身注重了什麼
1416000	1418000	大方向的feature
1418000	1420000	那local的話我就去說
1420000	1422000	這個神經網路對於每一個
1422000	1424000	使用者來說因為
1424000	1426000	我看到使用者A
1426000	1428000	他的年齡過
1428000	1430000	18歲然後例如說
1430000	1432000	他是男性然後例如說
1432000	1434000	他是例如說他居住在
1434000	1436000	亞洲那居住在
1436000	1438000	亞洲呢他會說中文
1438000	1440000	他會說台語所以
1440000	1442000	所以我推那個高五人的歌給他
1442000	1444000	對就是每一個人都有不同的
1444000	1446000	特徵所以我根據每一個人去提供
1446000	1448000	不同的模型解釋這個就是在
1448000	1450000	local scope在做的事情
1450000	1452000	ok好那
1452000	1454000	所以in conclusion就是
1454000	1456000	今天推
1456000	1458000	比較主流的分成這兩種
1458000	1460000	intrinsic跟post hoc
1460000	1462000	對然後
1462000	1464000	我今天會比較著重在討論
1464000	1466000	這個post hoc的explanation
1466000	1468000	然後再更進一步
1468000	1470000	就是我會比較著重在local的部分
1470000	1472000	ok因為local比較是
1472000	1474000	屬於更日常生活中
1474000	1476000	會碰到的像我剛剛舉的
1476000	1478000	推薦系統的場景
1478000	1480000	ok
1480000	1482000	好那
1482000	1484000	稍微來講一下
1484000	1486000	intrinsic local
1486000	1488000	那就是這邊有一個
1488000	1490000	這邊有一個圖片
1490000	1492000	然後這是一篇文章
1492000	1494000	那例如說
1494000	1496000	我今天說欸這篇文章很重要
1496000	1498000	為什麼例如說
1498000	1500000	因為他detect到這個什麼ent23
1500000	1502000	或是他detect到
1502000	1504000	什麼natural
1504000	1506000	tenacity
1506000	1508000	這種字眼所以
1508000	1510000	透過這種
1510000	1512000	我說哪一個字哪一個字比較重要的
1512000	1514000	這個過程
1514000	1516000	我提供給這個使用者
1516000	1518000	使用者說欸我今天為什麼
1518000	1520000	拿這篇文章做出以下的預測
1520000	1522000	是因為我在
1522000	1524000	我的模型吃了這個ent23
1524000	1526000	今天我模型吃了這種
1526000	1528000	比較具有情緒的字眼
1528000	1530000	這種的過程
1530000	1532000	然後讓使用者更加信服
1532000	1534000	說我的這種文字
1534000	1536000	預測模型預測出來的
1536000	1538000	結果是可以信服的
1538000	1540000	ok這就是intrinsic
1540000	1542000	本身的
1542000	1544000	intrinsic本身要提供的
1544000	1546000	的東西
1546000	1548000	那再來就是
1548000	1550000	intrinsic global
1550000	1552000	我剛說global主要是
1552000	1554000	在模型本身注重什麼
1554000	1556000	所以例如說我們今天設計一個
1556000	1558000	可解釋的
1558000	1560000	CNN就是
1560000	1562000	是屬於image classification
1562000	1564000	的其中一個比較具有代表性的
1564000	1566000	的模型
1566000	1568000	那我今天就是要去說
1568000	1570000	為什麼這個CNN會成功預測出
1570000	1572000	這個貓是
1572000	1574000	這個貓的圖片是一隻貓
1574000	1576000	那是因為它detect到
1576000	1578000	這個貓的頭
1578000	1580000	那如果我們從CNN裡面
1580000	1582000	把裡面抽取出一層
1582000	1584000	例如說其中一層model weight
1584000	1586000	那這個model weight我把它蓋在圖片上
1586000	1588000	我們是不是可以去看說
1588000	1590000	因為這個模型去
1590000	1592000	對其中某些feature做convolution
1592000	1594000	得到了某些
1594000	1596000	特定的feature
1596000	1598000	是不是這個CNN的模型去做出
1598000	1600000	它是貓的prediction的這個結果
1600000	1602000	那從模型中
1602000	1604000	抽出這個mask
1604000	1606000	或是抽出這個feature map
1606000	1608000	然後來讓這個
1608000	1610000	最後的結果是
1610000	1612000	可以讓使用者來看說
1612000	1614000	其實這個
1614000	1616000	最後的預測結果
1616000	1618000	是可以信服的
1618000	1620000	這個過程它其實就是
1620000	1622000	我們所謂的intrinsic的過程
1622000	1624000	那所謂intrinsic就是
1624000	1626000	模型本身就具有
1626000	1628000	可以製造解釋的這個功能
1628000	1630000	或是模型本身就具有
1630000	1632000	可以提煉出某些component
1632000	1634000	然後進而提供使用者解釋的
1634000	1636000	這個
1636000	1638000	過程我們就叫做
1638000	1640000	intrinsic的過程
1640000	1642000	ok
1642000	1644000	那另外一個就是
1644000	1646000	我們今天會比較著重的post hoc
1646000	1648000	post hoc的explanation
1648000	1650000	那post hoc explanation
1650000	1652000	它主要是分成兩個步驟
1652000	1654000	前面就會是我們的
1654000	1656000	black box model
1656000	1658000	所謂的black box model就是我們的prediction model
1658000	1660000	我們會有data
1660000	1662000	我們會有model去訓練
1662000	1664000	那接下來我們會有一個explainer
1664000	1666000	就是我們的所謂的拿來解釋
1666000	1668000	這個prediction model
1668000	1670000	的machine learning model
1670000	1672000	ok
1672000	1674000	那再來我們會需要一些metric
1674000	1676000	就是需要一些評估準則
1676000	1678000	來去看說
1678000	1680000	這個explainer到底學得好不好
1680000	1682000	並不是說我今天給你一個
1682000	1684000	explainer你就要五條天去相信它
1684000	1686000	所以我們當然可以透過
1686000	1688000	很多很多不同種的
1688000	1690000	metric來去判斷它好不好
1690000	1692000	那其中最有名的就是
1692000	1694000	屬於這個sharp e-value
1694000	1696000	如果這邊有經濟學背景的
1696000	1698000	或是曾經有學過經濟學的
1698000	1700000	人呢
1700000	1702000	可能會對這個字滿熟悉
1702000	1704000	它就是同一個東西
1704000	1706000	其實就只是要去算說
1706000	1708000	每一個feature
1708000	1710000	對於這個prediction model
1710000	1712000	的貢獻程度是多少
1712000	1714000	我們藉由這個
1714000	1716000	game theory這個理論裡面的
1716000	1718000	sharp e-value
1718000	1720000	來製造我們的
1720000	1722000	所謂的正確答案然後去做evaluation
1722000	1724000	對
1724000	1726000	那這個explainer有很多
1726000	1728000	很多方法
1728000	1730000	像是sharp啊像是line啊
1730000	1732000	這種
1732000	1734000	那這邊
1734000	1736000	列出來這些方法其實
1736000	1738000	有一個很大的缺點就是
1738000	1740000	如果我們要做剛剛我們所說的
1740000	1742000	local explanation的話
1742000	1744000	那我們每張圖都必須要提供一個解釋
1744000	1746000	那以現在目前的
1746000	1748000	比較
1748000	1750000	常見的方法
1750000	1752000	我不能說全部因為現在有
1752000	1754000	另外一種方法就是
1754000	1756000	現在比較常見的方法呢
1756000	1758000	就是我們一張圖
1758000	1760000	例如說我們要給一張圖片解釋
1760000	1762000	我們就必須要有一個explainer
1762000	1764000	也就是說我們今天有十萬張圖片
1764000	1766000	因為我們要給每一張圖片一個
1766000	1768000	客製化的解釋結果
1768000	1770000	所以我們就必須要有十萬
1770000	1772000	那會導致什麼導致這個過程非常非常
1772000	1774000	慢所以
1774000	1776000	這些
1776000	1778000	方法就有一些缺點
1778000	1780000	當然就有些人說你今天有
1780000	1782000	sharp e就是經濟學
1782000	1784000	game theory這麼好的一個
1784000	1786000	一個
1786000	1788000	準則你為什麼不拿來用
1788000	1790000	那我等一下會解釋一下為什麼我們
1790000	1792000	不能直接拿sharp e來做
1792000	1794000	最後模型解釋的一個結論
1794000	1796000	ok
1796000	1798000	好那我們就來講一下
1798000	1800000	比較在網
1800000	1802000	裡面挖一點那什麼是
1802000	1804000	sharp e value呢其實sharp e value其實
1804000	1806000	就是我們把game theory的一個
1806000	1808000	東西借過來然後來
1808000	1810000	來就是給模型
1810000	1812000	的prediction做解釋
1812000	1814000	好那我們
1814000	1816000	要製造sharp e value必須要做一件事
1816000	1818000	就是我們需要
1818000	1820000	知道每一個feature我們的目的就是
1820000	1822000	知道每一個feature的
1822000	1824000	important score就是我們需要知道每一個feature
1824000	1826000	的重要程度的分數
1826000	1828000	那我們要知道每一個feature
1828000	1830000	的重要分數
1830000	1832000	必須要做一些什麼事
1832000	1834000	其實很簡單就是我們要把
1834000	1836000	每一個假設我們這邊有
1836000	1838000	user0有四個feature
1838000	1840000	5個12345
1840000	1842000	我們要判斷這個user0
1842000	1844000	是不是他可以
1844000	1846000	還清這個房貸
1846000	1848000	ok那我們就必須要知道說
1848000	1850000	每一個feature對於
1850000	1852000	能不能還清房貸的重要程度
1852000	1854000	我們要做一件事就是例如說
1854000	1856000	我們必須要
1856000	1858000	這個job加這個
1858000	1860000	marital status
1860000	1862000	婚姻狀況的兩個feature
1862000	1864000	的共同貢獻
1864000	1866000	對於這個房貸
1866000	1868000	能不能還清房貸這個prediction
1868000	1870000	的重要程度
1870000	1872000	所以我們必須要去
1872000	1874000	go through
1874000	1876000	all the combination
1876000	1878000	feature的combination
1878000	1880000	也就是說我們必須要說
1880000	1882000	我們必須要去走例如age加job
1882000	1884000	加marital加education
1884000	1886000	或是age加job加marital
1886000	1888000	或是age加job
1888000	1890000	這種所有的feature的組合我們才可以知道
1890000	1892000	在shopping eval裡面我們才可以知道
1892000	1894000	每一個
1894000	1896000	每一個單一feature對於最後prediction
1896000	1898000	結果的重要程度
1898000	1900000	所以這會導致說什麼
1900000	1902000	這會導致說如果我這邊只有
1902000	1904000	5個feature那倒還好
1904000	1906000	因為這個組合量就是2的5次方
1906000	1908000	要不要要不要
1908000	1910000	所以是2的5次方
1910000	1912000	那今天如果我在
1912000	1914000	大公司裡面做推薦系統
1914000	1916000	我有2000個feature
1916000	1918000	那就會是2的2000次方
1918000	1920000	這個一個user就需要
1920000	1922000	2的2000次方的計算的
1922000	1924000	次數
1924000	1926000	那還得了我今天有11個user
1926000	1928000	那撐下去
1928000	1930000	那就是算不完了
1930000	1932000	整個運算過程算不完了
1932000	1934000	所以今天這個shopping eval
1934000	1936000	為什麼不能用
1936000	1938000	它其實本身是一個NP-hard problem
1938000	1940000	就是它的複雜度太高了
1940000	1942000	而且我們不能用
1942000	1944000	低複雜度的方法去
1944000	1946000	去驗證它
1946000	1948000	我們算出來的東西是不是對
1948000	1950000	這個東西就NP-hard
1950000	1952000	所以我們在
1952000	1954000	我們在直接拿shopping eval
1954000	1956000	這個詞來當作
1956000	1958000	最後解釋的important score的話
1958000	1960000	其實可以但是
1960000	1962000	是不現實的
1962000	1964000	因為它太
1964000	1966000	太吃時間
1966000	1968000	它需要太大的計算的複雜度
1968000	1970000	所以導致我們
1970000	1972000	沒辦法使用這個狀況
1972000	1974000	ok
1974000	1976000	剛才漏講一下後面這個
1976000	1978000	其實後面這個東西它就是在算說
1978000	1980000	如果我今天把
1980000	1982000	例如說我今天只在乎job跟merito
1982000	1984000	我今天把另外三個拿掉後
1984000	1986000	那我的model會
1986000	1988000	的prediction會做出什麼結果
1988000	1990000	我model prediction
1990000	1992000	會發生什麼變化
1992000	1994000	如果我今天把這個balance
1994000	1996000	把job把merito拿掉
1996000	1998000	我只把age跟education
1998000	2000000	考進去我的prediction model
2000000	2002000	然後我今天它原本是
2002000	2004000	不能還房貸的預測結果
2004000	2006000	突然變成說你可以還得起房貸
2006000	2008000	那這說明什麼
2008000	2010000	這三個feature很重要
2010000	2012000	這三個feature是導致它預測正確的
2012000	2014000	一個key feature
2014000	2016000	ok
2016000	2018000	所以透過這種不斷的
2018000	2020000	這種iterate的過程
2020000	2022000	去找到說每一個feature的
2022000	2024000	重要的分數
2024000	2026000	這個就是shopping value
2026000	2028000	ok
2028000	2030000	懂經濟學的人應該
2030000	2032000	看到這個東西會覺得蠻
2032000	2034000	蠻親切的
2034000	2036000	ok
2036000	2038000	好那
2038000	2040000	我shopping value不能用的話
2040000	2042000	那怎麼辦
2042000	2044000	這就是我在做研究
2044000	2046000	就是我想要把這個
2046000	2048000	explanation的這個過程給加速
2048000	2050000	我想要讓
2050000	2052000	因為每一個feature
2052000	2054000	每一個
2054000	2056000	我們在做local explanation
2056000	2058000	每一個使用者都必須要
2058000	2060000	給一個客製化的
2060000	2062000	explanation的結果
2062000	2064000	那我剛才提到每一個客製化的explanation的結果
2064000	2066000	都必須要有一個獨立的
2066000	2068000	explanator
2068000	2070000	就是一個解釋prediction model的模型
2070000	2072000	那這個過程本身
2072000	2074000	就是一個非常耗
2074000	2076000	耗時間的一個過程
2076000	2078000	像是你如果
2078000	2080000	給一個推薦結果然後你叫使用者
2080000	2082000	說不好意思等我10秒
2082000	2084000	我給你一個推薦我給你一個解釋
2084000	2086000	使用者一定就不能接受
2086000	2088000	如果你一個網頁叫你等10秒
2088000	2090000	你一定就左上角插他就按掉了
2090000	2092000	類似這樣
2092000	2094000	所以這個加速結果是很重要
2094000	2096000	那在這邊這個work
2096000	2098000	這是我的去年投稿上的一個work
2098000	2100000	這個work
2100000	2102000	這個work就是
2102000	2104000	我們目的是在加速這個
2104000	2106000	shopping value的
2106000	2108000	這個運算過程
2108000	2110000	然後把這個過程的結果
2110000	2112000	提供給使用者當作
2112000	2114000	解釋的結果
2114000	2116000	ok那我們做了什麼像我剛才在講說
2116000	2118000	shopping value他其實
2118000	2120000	因為我要go through all the combination
2120000	2122000	of the feature set
2122000	2124000	所以我們今天假設有五個feature
2124000	2126000	那就是2的五次方
2126000	2128000	那是不是每一個feature的組合
2128000	2130000	對模型來說很重要呢
2130000	2132000	不見得為什麼就是有些feature
2132000	2134000	他的交互作用其實對模型來說
2134000	2136000	是不重要的
2136000	2138000	那這個資訊可不可以在模型訓練完
2138000	2140000	之後就得到呢
2140000	2142000	其實是可以的就是我們其實可以把
2142000	2144000	模型的訓練的權重
2144000	2146000	給拿出來去看到說
2146000	2148000	feature1跟feature2
2148000	2150000	黑色代表很重要那feature
2150000	2152000	x1跟x2他很重要
2152000	2154000	x1跟x3
2154000	2156000	突然之間沒有跟x2那麼重要
2156000	2158000	x1跟x5來說
2158000	2160000	其實完全不重要
2160000	2162000	所以這個時候我們就可以把x1x5
2162000	2164000	這個組合從算shopping value
2164000	2166000	的這個組合給拿掉
2166000	2168000	那拿掉之後
2168000	2170000	例如說我們原本要2的五次方可能要32次
2170000	2172000	可能透過
2172000	2174000	這種拿掉的過程例如說
2174000	2176000	我們把這個灰色以下的
2176000	2178000	色階的色塊的
2178000	2180000	組合全部拿掉那我們
2180000	2182000	裡面所需要算的次數就變成是
2182000	2184000	可能2 2 4 6 8
2184000	2186000	還有14次
2186000	2188000	所以其實是減了一半的過程
2188000	2190000	對吧這個過程其實是
2190000	2192000	可以加速這個
2192000	2194000	shopping value的計算
2194000	2196000	然後把這個東西變成一個
2196000	2198000	使用者可以接受的
2198000	2200000	一個等待時間
2200000	2202000	那最後提供給使用者
2202000	2204000	比較快速的這個解釋的
2204000	2206000	解釋的
2206000	2208000	這個結果
2208000	2210000	所以這個數學其實就是在表達說
2210000	2212000	我們今天在shopping value
2212000	2214000	裡面我們需要
2214000	2216000	去經歷所有的
2216000	2218000	所有的feature的組合
2218000	2220000	那我們今天把它變成是
2220000	2222000	我們透過模型的權重
2222000	2224000	去看到feature跟feature之間的
2224000	2226000	interaction的重要性
2226000	2228000	然後把不重要拿掉
2228000	2230000	最後我們去算
2230000	2232000	approximate的shopping value
2232000	2234000	把這個shopping value當作
2234000	2236000	當作是
2236000	2238000	explanation的important score
2238000	2240000	提供給使用者
2240000	2242000	就是這個work的精神
2242000	2244000	ok
2244000	2246000	所以你可以看到說
2246000	2248000	我們這個work其實有一個
2248000	2250000	application的scenario
2250000	2252000	就是我們今天在train的一個
2252000	2254000	prediction model
2254000	2256000	我們去預測說
2256000	2258000	他的income是高還是低
2258000	2260000	這個東西,我們其實可以用我們的模型
2260000	2262000	去解釋這個訓練好的
2262000	2264000	這個prediction model
2264000	2266000	然後進而去提供說
2266000	2268000	為什麼他可能是因為你的
2268000	2270000	高學歷,例如說你是
2270000	2272000	稻乾工作人,類似那種
2272000	2274000	然後是造成說
2274000	2276000	我判斷你是高工資
2276000	2278000	的這種結果
2278000	2280000	所以我們這個模型其實是
2280000	2282000	可以用一個比較快速的
2282000	2284000	快速的這個explanation的方式
2284000	2286000	然後在
2286000	2288000	提供給使用者的這個情境
2288000	2290000	上面做使用
2290000	2292000	那右下角這個圖其實可以看到
2292000	2294000	左邊這個藍色的這個bar
2294000	2296000	其實是
2296000	2298000	ground truth
2298000	2300000	也就是說我們用經濟學
2300000	2302000	跟theory的那個公式去算出來
2302000	2304000	說屬於這個
2304000	2306000	屬於這個feature來說
2306000	2308000	真正他應該要有
2308000	2310000	那可以看到其實
2310000	2312000	我們就算把那些不重要的
2312000	2314000	feature跟interaction給拿掉
2314000	2316000	我們的shopping value在
2316000	2318000	我們的這個model在預測出來
2318000	2320000	這個
2320000	2322000	importance的這個score
2322000	2324000	他其實跟shopping value是非常靠近
2324000	2326000	完全一模一樣
2326000	2328000	畢竟我們把一些combination拿掉了
2328000	2330000	所以這個東西其實是
2330000	2332000	更快速但是我們同樣不失去
2332000	2334000	那個shopping value的那個
2334000	2336000	準度所以我們在提供給
2336000	2338000	每個feature的重要性
2338000	2340000	給user的時候我們可以說
2340000	2342000	學歷這一塊的feature
2342000	2344000	對於我們的
2344000	2346000	模型來說預測你是
2346000	2348000	高薪水低薪水的
2348000	2350000	這個
2350000	2352000	模型來說是很重要的
2352000	2354000	所以我們會把這個例如分數
2354000	2356000	或是把這個重要程度的
2356000	2358000	ranking的結果提供給
2358000	2360000	使用者作為他
2360000	2362000	想不想相信這個
2362000	2364000	這個預測模型的一個參考
2364000	2366000	ok
2366000	2368000	好
2368000	2370000	那下一個工作
2370000	2372000	那我加速一下
2372000	2374000	下一個工作的話是
2374000	2376000	因為我們前面是
2376000	2378000	加速那個shopping value
2378000	2380000	的計算過程
2380000	2382000	對吧
2382000	2384000	這個工作是我們
2384000	2386000	與其要去加速那個過程
2386000	2388000	那倒不如說我們去訓練一個模型
2388000	2390000	去把shopping value
2390000	2392000	計算的那個distribution
2392000	2394000	計算那個distribution給學起來
2394000	2396000	那這個東西其實
2396000	2398000	可以更快地去
2398000	2400000	給預測結果
2400000	2402000	為什麼呢是因為
2402000	2404000	我們這個傳統的
2404000	2406000	一般的這種DNA模型
2406000	2408000	我們剛才是每一個
2408000	2410000	每一個
2410000	2412000	user我們要給他一個
2412000	2414000	客製化的結果我們就必須要有一個
2414000	2416000	explanation model
2416000	2418000	所以我們今天要預測十萬個user
2418000	2420000	我們就要給十萬個model
2420000	2422000	那在這邊我們只要給一個user
2422000	2424000	我們只要給一個model去
2424000	2426000	預測所有user的
2426000	2428000	explanation結果就可以了
2428000	2430000	那這個過程其實上也是加速
2430000	2432000	加速我們給
2432000	2434000	explanation過程的這個進程
2434000	2436000	ok那為什麼是因為
2436000	2438000	一般的這種
2438000	2440000	深度學習這種網路
2440000	2442000	我們只要給例如說我們可以
2442000	2444000	一次把十萬個user當作
2444000	2446000	data送進去那個
2446000	2448000	預測網路然後這個預測網路
2448000	2450000	他就可以突出屬於這十萬個
2450000	2452000	不同的結果
2452000	2454000	ok所以我們再也不需要
2454000	2456000	我們再也不需要一個使用者
2456000	2458000	對應一個獨立的
2458000	2460000	explanator
2460000	2462000	我們現在只需要一個explanator
2462000	2464000	去對應所有user
2464000	2466000	這個訓練過程
2466000	2468000	其實是我另外一個work
2468000	2470000	那這篇也在投稿當中
2470000	2472000	那他主要就是說
2472000	2474000	我們是不是可以透過一些
2474000	2476000	sorry
2476000	2478000	透過一些正樣本跟負樣本的這種
2478000	2480000	學習什麼是正樣本
2480000	2482000	例如說
2482000	2484000	我們今天把某些一張圖片
2484000	2486000	把某些重要的
2486000	2488000	特徵給遮掉
2488000	2490000	如果我們遮掉這個特徵呢
2490000	2492000	是屬於不重要的特徵
2492000	2494000	比如說我們遮掉狗的眼睛
2494000	2496000	那因為他有另外一隻眼睛
2496000	2498000	所以可能一隻眼睛相對來說不重要
2498000	2500000	那模型在判斷
2500000	2502000	這張圖是不是一隻狗的時候
2502000	2504000	他還是做出一樣的判斷
2504000	2506000	這個就是所謂的正樣本
2506000	2508000	那這個正樣本其實可以幫助
2508000	2510000	幫助我們去解釋
2510000	2512000	例如說我們說
2512000	2514000	我們希望一個人判斷說
2514000	2516000	這張圖片
2516000	2518000	希望一個模型判斷說他是不是一隻狗
2518000	2520000	那我們給出的解釋
2520000	2522000	因為狗
2522000	2524000	狗在乎這些特徵
2524000	2526000	所以我模型做出
2526000	2528000	這個prediction的結果
2528000	2530000	那這些東西是正向
2530000	2532000	那我們可以給一些負向
2532000	2534000	例如說我們這邊給一個場景錄
2534000	2536000	場景錄就不是狗
2536000	2538000	所以模型就會知道說
2538000	2540000	今天遇到這個場景錄的時候
2540000	2542000	他就是屬於這個狗的
2542000	2544000	negative sample
2544000	2546000	就是所謂的負面樣
2546000	2548000	負面教材
2548000	2550000	負面教材跟負面教材的時候
2550000	2552000	我就可以去學
2552000	2554000	就是我希望我的模型
2554000	2556000	給出的解釋
2556000	2558000	要越靠近正面教材
2558000	2560000	然後離負面教材越來越遠
2560000	2562000	這就是我要做的事情
2562000	2564000	所以我透過我的這個
2564000	2566000	設計的這個模型
2566000	2568000	那這個過程
2568000	2570000	其實有一個名詞叫contrastive learning
2570000	2572000	就是中文叫
2572000	2574000	對比學習吧
2574000	2576000	應該是這個對比學習的過程
2576000	2578000	然後讓模型知道什麼是對
2578000	2580000	什麼是錯
2580000	2582000	然後我希望我對的跟錯的
2582000	2584000	之間的距離越來越大
2584000	2586000	然後讓模型往正確方向去訓練
2586000	2588000	那為什麼後面還有一個
2588000	2590000	fine tuning的過程
2590000	2592000	這邊的fine tuning的過程是
2592000	2594000	為什麼我剛剛提到shoppy value
2594000	2596000	是因為shoppy value有很強大的
2596000	2598000	經濟學game theory的一些
2598000	2600000	理論上的support
2600000	2602000	所以如果我們能
2602000	2604000	如果我們能製造出
2604000	2606000	這些
2606000	2608000	這些important score
2608000	2610000	他也具有shoppy value
2610000	2612000	這些特性的話
2612000	2614000	那
2614000	2616000	在提供給
2616000	2618000	我不敢說提供給使用者用
2618000	2620000	但至少在提供給一些
2620000	2622000	例如machine learning engineer
2622000	2624000	或是一些高層
2624000	2626000	他們要去看說去evaluate說
2626000	2628000	這些evaluation的
2628000	2630000	這些explanation的結果是不是可信
2630000	2632000	那這個過程其實上
2632000	2634000	一是去讓人家更幸福這個過程
2634000	2636000	二是我透過這個fine tuning的過程
2636000	2638000	我希望我的
2638000	2640000	我的這些東西不要離shoppy value
2640000	2642000	太遠
2642000	2644000	那這個不要離shoppy value太遠的
2644000	2646000	這個特性他somehow可以去
2646000	2648000	因為這邊我完全沒有使用任何的
2648000	2650000	一些shoppy value
2650000	2652000	作為label去訓練
2652000	2654000	那後面我給少量的label
2654000	2656000	我是可以把他拉回shoppy value的正軌
2656000	2658000	讓整個
2658000	2660000	整個大模型在訓練的時候
2660000	2662000	不要離我們剛才所謂的shoppy value
2662000	2664000	給explanation這個framework太遠
2664000	2666000	讓我們可以
2666000	2668000	拿這個結果去
2668000	2670000	讓別人相信說
2670000	2672000	我們不是隨便去製造一個
2672000	2674000	important score的預測
2674000	2676000	我們是基於shoppy value的
2676000	2678000	這個大框架去做
2678000	2680000	prediction
2680000	2682000	所以這個模型其實有
2682000	2684000	幾個優點
2684000	2686000	第一個是他可以很快
2686000	2688000	因為他只要一個模型就可以應付所有的user
2688000	2690000	不像是以前一個模型
2690000	2692000	只能應付一個user
2692000	2694000	然後再來就是他提供了正樣本跟負樣本的選取
2694000	2696000	然後讓整個訓練的
2696000	2698000	過程中我們不需要
2698000	2700000	太大量的樣本
2700000	2702000	不需要太大量的label
2702000	2704000	然後也可以去訓練出對於所有user的
2704000	2706000	這個訓練結果
2706000	2708000	好那這個我就
2708000	2710000	就不講了我就跳過
2710000	2712000	可以看到右邊這個
2712000	2714000	右邊這個兩張圖就是
2714000	2716000	我們訓練的結果
2716000	2718000	這個訓練的結果就是可以看到說
2718000	2720000	這是一隻鳥嘛
2720000	2722000	那這個鳥的中間被框住
2722000	2724000	紅色就代表重要,藍色就代表不重要
2724000	2726000	所以我們就知道
2726000	2728000	這個image classification model
2728000	2730000	他其實上是在
2730000	2732000	為什麼會預測他是鳥
2732000	2734000	那就是因為他抓到了這個鳥的特徵
2734000	2736000	所以我們就可以去相信
2736000	2738000	這個image classification model是做對的
2738000	2740000	然後也可以去看說
2740000	2742000	我們這個explainer是trend好的
2742000	2744000	是trend對的
2744000	2746000	下面這個狗
2746000	2748000	最重要的這個特徵是臉
2748000	2750000	狗的這個臉
2750000	2752000	那我們就把我們的這個
2752000	2754000	explainer訓練結果
2754000	2756000	他其實就是說這個臉很重要
2756000	2758000	那我們把這個臉highlight起來
2758000	2760000	那就是最後的explanation
2760000	2762000	結果
2764000	2766000	好那有幾個
2766000	2768000	有幾個application
2768000	2770000	就是例如說
2770000	2772000	其實我前面大致上都提過
2772000	2774000	XAI在recommended system上面
2774000	2776000	的重要性
2776000	2778000	是因為可以針對幾個不同的
2778000	2780000	人
2780000	2782000	幾個不同的使用情境來說
2782000	2784000	例如說我對於客人來說
2784000	2786000	我可以讓他知道我拿什麼
2786000	2788000	怎麼逼選給你訓練
2788000	2790000	那為什麼你會做出這個結果
2790000	2792000	讓大家覺得
2792000	2794000	這個decision是對
2794000	2796000	然後更安心
2796000	2798000	對於這個recommended system更安心
2798000	2800000	那對於商人來說
2800000	2802000	那他可以透過這個
2802000	2804000	例如說我投放廣告
2804000	2806000	那你總要跟廣告商說
2806000	2808000	因為我
2808000	2810000	截取了使用者
2810000	2812000	使用者的A feature B feature
2812000	2814000	C feature D feature
2814000	2816000	然後導致
2816000	2818000	我推薦這個使用者廣告
2818000	2820000	所以商人可以透過這種
2820000	2822000	顯示的分析去做出
2822000	2824000	更好的例如說
2824000	2826000	因為我發現可能這個市場
2826000	2828000	某些使用者會在乎什麼
2828000	2830000	商品的特徵
2830000	2832000	我可以去對這些商品的特徵做調整
2832000	2834000	然後他也可以更信服
2834000	2836000	這個廣告的投放
2836000	2838000	那對於我們
2838000	2840000	這種工程師來說
2840000	2842000	提供這種
2842000	2844000	explanation的結果
2844000	2846000	他其實可以幫我們去
2846000	2848000	debug這個system
2848000	2850000	也可以讓我們去改善這個system
2850000	2852000	例如說我們如果抓錯了feature的話
2852000	2854000	我們要怎麼去調整
2854000	2856000	那再來就是health care
2856000	2858000	那就是例如說
2858000	2860000	我們對於病人來說
2860000	2862000	我們可以去說服病人說
2862000	2864000	相信這個診斷結果是沒錯
2864000	2866000	那對於醫生來說
2866000	2868000	可以去協助他做正確的結論
2868000	2870000	就是醫生憑什麼相信
2870000	2872000	這個prediction model
2872000	2874000	但你總要說服他說
2874000	2876000	因為我detect到什麼症狀
2876000	2878000	這個東西其實是
2878000	2880000	可以協助醫生
2880000	2882000	但我必須要說
2882000	2884000	他不能取代醫生的重要程度
2884000	2886000	所有的AI model
2886000	2888000	他都是在協助為本質
2888000	2890000	這個outlier
2890000	2892000	我覺得就先跳過
2892000	2894000	OK
2894000	2896000	好那最後的話
2896000	2898000	我來講幾個常見的
2898000	2900000	這個open source package
2900000	2902000	這些都是免費完全免費
2902000	2904000	不用付任何錢
2904000	2906000	第一個就是Captain
2906000	2908000	Captain其實是Facebook開發的一個
2908000	2910000	可解釋性的
2910000	2912000	開源的套件
2912000	2914000	那這個套件
2914000	2916000	使用的方式很簡單
2916000	2918000	就我們要使用這種
2918000	2920000	套件的時候
2920000	2922000	首先我們必須要有一個
2922000	2924000	我們要解釋的模型
2924000	2926000	如果沒有解釋模型
2926000	2928000	那這個XI就沒用了
2928000	2930000	所以我們
2930000	2932000	這個code的話就是
2932000	2934000	首先我們要有一個
2934000	2936000	我們要去解釋的目標模型
2936000	2938000	那這個integrated gradient
2938000	2940000	這個東西
2940000	2942000	其實就是
2942000	2944000	其中一個我們剛才說
2944000	2946000	一直在說的explainer
2946000	2948000	就是解釋模型的
2948000	2950000	機器學系模型
2950000	2952000	有點local
2952000	2954000	所以我們把這個
2954000	2956000	image classifier丟進這個
2956000	2958000	的explainer裡面
2958000	2960000	那他的
2960000	2962000	這個explainer的
2962000	2964000	output結果就會是
2964000	2966000	我去highlight一張圖片哪裡重要
2966000	2968000	所以右邊這個結果就是
2968000	2970000	這個東西 output出來的結果
2970000	2972000	這是一隻天鵝
2972000	2974000	那他predict是不是一隻天鵝
2974000	2976000	是因為他
2976000	2978000	看了
2978000	2980000	這些重要的feature
2980000	2982000	所以我們就可以透過這個輪廓
2982000	2984000	他真的是detect到這個天鵝
2984000	2986000	而不是detect到旁邊這些水
2986000	2988000	這就是過程
2988000	2990000	那不然就是
2990000	2992000	例如說我們有一些文字的模型
2992000	2994000	就是文字的classifier的結果
2994000	2996000	那是因為為什麼他會說
2996000	2998000	這句話是一個
2998000	3000000	positive的attitude
3000000	3002000	那是因為沒有
3002000	3004000	例如fantastic的字
3004000	3006000	那為什麼他是positive
3006000	3008000	因為他有best
3008000	3010000	那為什麼他不好
3010000	3012000	例如說negative
3012000	3014000	類似這種showcase
3014000	3016000	所以這整個package的
3016000	3018000	使用方式非常簡單
3018000	3020000	所以只要幾行
3020000	3022000	一行兩行三行
3022000	3024000	只要三行就可以搞定
3024000	3026000	explanation的process
3026000	3028000	那當然就是這裡面還有一些
3028000	3030000	模型調教的一些
3030000	3032000	trick或是一些經驗談
3032000	3034000	那這個就是要靠
3034000	3036000	大家平常常去用的一些
3036000	3038000	technique才會知道的一些
3038000	3040000	makeup
3040000	3042000	那另外一個package叫shop
3042000	3044000	那他shop他也是
3044000	3046000	用同樣的concept在
3046000	3048000	設計這個package
3048000	3050000	一樣他就是例如說
3050000	3052000	我需要有一個prediction model在這裡
3052000	3054000	然後把他寫成一個function
3054000	3056000	那我已經有一個prediction model在這裡
3056000	3058000	我已經有一個explanator在這裡
3058000	3060000	那這個explanator的目的
3060000	3062000	就是要去製造說
3062000	3064000	每一個圖片
3064000	3066000	或是每一個
3066000	3068000	每一句話或是每一個user
3068000	3070000	他重要的feature或重要的字
3070000	3072000	或是重要的pixel是什麼
3072000	3074000	那就是透過這個
3074000	3076000	這個explanator
3076000	3078000	來最後做一個製造
3078000	3080000	所以他output出來的shop event
3080000	3082000	就是屬於每一個pixel
3082000	3084000	重要的important score
3084000	3086000	ok所以整個過程就會比較像是這樣
3086000	3088000	我已經有一個黑盒模型
3088000	3090000	然後最後把他打開
3090000	3092000	然後知道每一個feature的重要程度是什麼
3092000	3094000	ok
3094000	3096000	所以整個feature會比較像是這樣
3096000	3098000	就例如說這是一艘船
3098000	3100000	所以他就會說
3100000	3102000	highlight船的這個
3102000	3104000	部件在這裡
3104000	3106000	那就讓人家知道說
3106000	3108000	這個船很重要
3108000	3110000	就可以了解到這個船的特徵
3110000	3112000	ok
3112000	3114000	那後面這些圖是
3114000	3116000	說今天錯誤的樣本
3116000	3118000	會發生什麼事
3118000	3120000	其實後面就不用管
3120000	3122000	因為我們只在
3122000	3124000	XAI這個模型
3124000	3126000	其實我們不能去導正
3126000	3128000	直接去導正
3128000	3130000	prediction model不好的結果
3130000	3132000	我們就是基於prediction model
3132000	3134000	給出來的結果做最忠實
3134000	3136000	最真誠的
3136000	3138000	這個解釋的結果
3138000	3140000	所以今天不能說
3140000	3142000	為什麼這個
3142000	3144000	他在fountain這個level下
3144000	3146000	他的explanation的結果好像很爛
3146000	3148000	那廢話因為他prediction
3148000	3150000	本來就是錯
3150000	3152000	因為你本來決策就錯了
3152000	3154000	所以我基於你做錯的決策
3154000	3156000	去做錯誤的explanation
3156000	3158000	其實是
3158000	3160000	XAI是不能管
3160000	3162000	我們只是基於你給我的
3162000	3164000	prediction model最忠實的決定
3164000	3166000	所以在看這個speed
3166000	3168000	這個boat
3168000	3170000	這個level才是比較
3170000	3172000	合理的
3172000	3174000	好
3174000	3176000	可能多拖了五分鐘
3176000	3178000	那今天演講就
3178000	3180000	大概到這裡
3180000	3182000	謝謝大家
3186000	3188000	有問題都可以提問
3190000	3192000	我想確認一下
3192000	3194000	因為現在沒有什麼設定上的問題
3194000	3196000	所以說
3196000	3198000	有問題的人
3198000	3200000	可以直接問一下
3216000	3218000	你好
3218000	3220000	我在聊天室有一個問題
3220000	3222000	好想要問
3222000	3224000	因為我主要是做tree-based model
3224000	3226000	就是提升資源區那種
3226000	3228000	然後因為
3228000	3230000	就是在很多那種數據競賽
3230000	3232000	或是實用目的可能比較少
3232000	3234000	那數據競賽
3234000	3236000	最後都會結合很多
3236000	3238000	模型然後塞在一起
3238000	3240000	那這種也可以用XAI這種方式嗎
3242000	3244000	如果是post hoc的話
3244000	3246000	你是用什麼
3246000	3248000	XGBoost或是LightGBM
3248000	3250000	例如說我最近就有研究
3250000	3252000	就是XGBoost加LightGBM
3252000	3254000	然後結果再把它拼進去
3254000	3256000	OK
3256000	3258000	我必須要說XGBoost那些
3258000	3260000	縱使它是樹模型
3260000	3262000	但是它中間有很多不同的layer
3262000	3264000	它其實是不能解釋的
3264000	3266000	所以一個可解
3266000	3268000	如果我們要說一個模型本身具有可解釋性
3268000	3270000	那它每一個component都要是清楚的
3270000	3272000	就縱使中間有一個
3272000	3274000	是不清楚的
3274000	3276000	那它整個就是不清楚的
3276000	3278000	XGBoost或是LightGBM
3278000	3280000	都必須要用post hoc的方式
3280000	3282000	所以用那些package
3282000	3284000	應該是可以去幫你做
3284000	3286000	解釋上面的動作
3288000	3290000	那如果我今天的模型的預測
3290000	3292000	是基於這四種模型的
3292000	3294000	預測結果去做平均的話
3294000	3296000	那我有辦法
3296000	3298000	一樣用你剛說的工具去回推
3298000	3300000	就是它綜合的
3300000	3302000	這樣子的性能
3302000	3304000	然後它的feature對哪一些
3304000	3306000	feature是哪一些重要的嗎
3306000	3308000	你四個模型
3308000	3310000	都是不同的模型嗎
3310000	3312000	就是其實我是用
3312000	3314000	不同的模型
3314000	3316000	然後去訓練
3316000	3318000	然後去預測出結果
3318000	3320000	只是說我最終的預測是這四個模型的輸出
3320000	3322000	再去做平均
3322000	3324000	ok 就單純的insampling這樣
3324000	3326000	ok
3326000	3328000	那我覺得是不太
3328000	3330000	不太能對四個模型的綜合
3330000	3332000	做評估
3332000	3334000	四個模型各做一個評估
3334000	3336000	對對
3336000	3338000	例如說我explanation score
3338000	3340000	把它加起來平均
3340000	3342000	對對對
3342000	3344000	ok 謝謝
3344000	3346000	ok 謝謝
3346000	3348000	謝謝你的問題
3348000	3350000	然後推薦系統中
3350000	3352000	遇到全新的商品
3352000	3354000	可能啊可能
3354000	3356000	例如說我就舉個最簡單的例子
3356000	3358000	就為什麼一些
3358000	3360000	假設Netflix它一進去
3360000	3362000	它叫你按說你喜歡什麼歌
3362000	3364000	你喜歡什麼藝人
3364000	3366000	你看過什麼影片
3366000	3368000	這些東西就是在建立你的個人的profile
3368000	3370000	那透過這些相同的profile
3370000	3372000	我可以去找到跟你
3372000	3374000	習性比較像的使用者
3374000	3376000	那我就把這些使用者的先前的習慣
3376000	3378000	當作你最一開始的推薦結果
3378000	3380000	那你當然之後
3380000	3382000	拿到這些推薦結果你會繼續再做你自己的使用嘛
3382000	3384000	那再把你自己的使用的
3384000	3386000	這些東西加上別人的
3386000	3388000	一些以前的習慣
3388000	3390000	如何來弄起來
3390000	3392000	然後做最後的
3392000	3394000	例如說你使用半年之後
3394000	3396000	你會發現結果越來越準
3396000	3398000	就是因為它推薦系統模型
3398000	3400000	你在經歷的過程
3400000	3402000	ok 希望有回答到你的問題
3404000	3406000	推薦的入門
3406000	3408000	ok 有
3408000	3410000	就是你可以上網搜
3410000	3412000	XAI什麼block
3412000	3414000	有一個寫的蠻好的
3414000	3416000	算是一本書吧
3416000	3418000	五個章節還是十個章節
3418000	3420000	然後我覺得他寫的蠻好的
3420000	3422000	然後包括現在這個XAI很多人在做
3422000	3424000	所以你去youtube上面
3424000	3426000	打例如introduction to
3426000	3428000	XAI可能也會跳出一些
3428000	3430000	不錯的影片
3430000	3432000	但我蠻推薦我剛才說的那個block
3432000	3434000	ok
3434000	3436000	特徵
3436000	3438000	ok 在這邊
3438000	3440000	其實都
3440000	3442000	ok
3442000	3444000	在我剛才講的
3444000	3446000	這個假設
3446000	3448000	它其實都假設每個特徵
3448000	3450000	是獨立的
3450000	3452000	那也有人在
3452000	3454000	研究說
3454000	3456000	那今天特徵相異的時候
3456000	3458000	我應該要怎麼去
3458000	3460000	去分析
3460000	3462000	那這些人就是比較著重在研究
3462000	3464000	feature interaction的這種XAI的方式
3464000	3466000	有有有
3466000	3468000	有些人在做這些方式
3468000	3470000	我記得上海
3470000	3472000	上海交通大學的
3472000	3474000	張全時老師的實驗室
3474000	3476000	有一些papers在研究
3476000	3478000	這方面的東西
3478000	3480000	如果你有興趣可以去看一下
3480000	3482000	我相信問出這麼specific的問題的人應該
3482000	3484000	對這方面應該是稍微有研究
3484000	3486000	或是其實是個expert
3486000	3488000	所以這樣講應該
3488000	3490000	希望能回答到你的問題
3490000	3492000	ok
3492000	3494000	ok
3494000	3496000	這種
3496000	3498000	這種全新商品的推薦模型
3498000	3500000	叫做
3500000	3502000	co-start
3502000	3504000	recommendation
3504000	3506000	ok
3506000	3508000	很多很多
3508000	3510000	很多很多方式
3510000	3512000	我宣傳一下
3512000	3514000	我之前發了一篇paper
3514000	3516000	叫做TPR
3516000	3518000	textual Preference
3518000	3520000	Ranking
3520000	3522000	這個東西它可以透過
3522000	3524000	不同的使用者特徵
3524000	3526000	然後來達到這種
3526000	3528000	你剛才所謂的這種
3528000	3530000	co-start recommendation
3530000	3532000	的推薦效果
3532000	3534000	對
3534000	3536000	這是我之前自己發的
3536000	3538000	跟我們商量一下
3538000	3540000	或者是你可以去找
3540000	3542000	找有一個
3542000	3544000	UCSD有一個老師叫Julia McCauley
3544000	3546000	他們實驗室也有很多人在做
3546000	3548000	這種co-start recommendation
3548000	3550000	然後你打這個關鍵字
3550000	3552000	應該可以跳出很多
3552000	3554000	ok
3554000	3556000	希望回答到你的問題
3560000	3562000	喂
3562000	3564000	不好意思我想要問一個問題
3564000	3566000	就是你剛剛在訓練
3566000	3568000	那個狗狗的圖
3568000	3570000	就是positive跟negative
3570000	3572000	的那個狗狗的圖
3572000	3574000	跟對比下面
3574000	3576000	下面一張
3576000	3578000	那個錯誤的圖
3578000	3580000	然後你把狗狗的那個眼睛拔掉了
3580000	3582000	那我在想說
3582000	3584000	你在那個錯誤的答案
3584000	3586000	是否就是
3586000	3588000	也許把那隻狗狗的眼睛貼上去
3588000	3590000	會不會是
3590000	3592000	會不會讓訓練的效果比較好
3592000	3594000	喔
3594000	3596000	我這邊只是給一個
3596000	3598000	例子
3598000	3600000	但其實像我真的在訓練的時候
3600000	3602000	例如說我是
3602000	3604000	假設他是一個32x32的圖片
3604000	3606000	那我就是給一個random mask
3606000	3608000	那這個random mask套上去的時候
3608000	3610000	他其實並不會
3610000	3612000	例如說並不會單純聚在這
3612000	3614000	他可能是散佈在一張圖片的不同地方
3614000	3616000	那例如說散佈在不同圖片的地方
3616000	3618000	我怎麼確認這個
3618000	3620000	被遮住過後的
3620000	3622000	這種
3622000	3624000	example是positive
3624000	3626000	我就把他丟到原本的prediction model去看說
3626000	3628000	他prediction的
3628000	3630000	例如說predict出來他是狗的分數
3630000	3632000	是不是掉很多
3632000	3634000	如果他幾乎沒掉
3634000	3636000	那我就可以肯定說我遮住的這些feature
3636000	3638000	所以我覺得你剛才說的是對的
3638000	3640000	例如說我今天把這個東西
3640000	3642000	遮到其他地方是有可能
3642000	3644000	幫助模型訓練是有可能
3644000	3646000	所以我在
3646000	3648000	這個是比較technical detail
3648000	3650000	我在訓練的時候其實是真的有做這件事情
3650000	3652000	所以像
3652000	3654000	像這種方式直接遮的話
3654000	3656000	那他有可能遮出來的東西
3656000	3658000	是例如說
3658000	3660000	我就直接遮掉很重要的東西是有可能
3660000	3662000	我覺得你的問題
3662000	3664000	是一個很好的問題
3664000	3666000	ok謝謝
3666000	3668000	好謝謝
3668000	3670000	喂你好
3670000	3672000	哈囉
3672000	3674000	欸sorry我的那個
3674000	3676000	鏡頭壞掉我把你打開
3676000	3678000	沒關係沒關係我們很free
3678000	3680000	對
3680000	3682000	那Alan跟我想問一下那個
3682000	3684000	第12頁那個sharp t value那邊
3684000	3686000	ok
3686000	3688000	就是我想確認我的那個觀念沒有錯
3688000	3690000	ok
3690000	3692000	我可以理解他是
3692000	3694000	我可以理解那difference那邊
3694000	3696000	是我把某些feature拿掉以後
3696000	3698000	然後他的difference如果很大的話
3698000	3700000	就代表那個feature是重要的
3700000	3702000	是這樣理解嗎
3702000	3704000	對沒錯
3704000	3706000	因為他改變了原本的prediction的pattern
3706000	3708000	那下一頁
3708000	3710000	那個第13頁就是
3710000	3712000	在您的paper中
3712000	3714000	就是有建立各個feature之間的
3714000	3716000	的關係
3716000	3718000	那個矩陣
3718000	3720000	我可以把它理解成是那種
3720000	3722000	的這種關係嗎
3722000	3724000	可以
3724000	3726000	那為什麼
3726000	3728000	為什麼他們相關係數
3728000	3730000	的高或低會跟
3730000	3732000	就是我預測出來他這個feature
3732000	3734000	對結果的重要性是有關的
3734000	3736000	之間的分別
3736000	3738000	跟
3738000	3740000	你還有抱歉打斷你
3740000	3742000	我太興奮了
3742000	3744000	ok好
3744000	3746000	那這個東西為什麼跟那個prediction有關
3746000	3748000	是因為我這個矩陣
3748000	3750000	你可以把它理解成一種相關係數
3750000	3752000	但他不是完全是correlation
3752000	3754000	他是基於我們model
3754000	3756000	predict出來的中間那些prediction
3756000	3758000	predict出來的那個model weight
3758000	3760000	我們把那個model weight抽出來
3760000	3762000	去建立這個
3762000	3764000	feature跟feature之間關係的
3764000	3766000	這種權重的
3766000	3768000	的矩陣
3768000	3770000	的這種權重圖
3770000	3772000	所以他這裡面每一個圖片的
3772000	3774000	每一個色塊的這種interaction
3774000	3776000	的高或低都是
3776000	3778000	based on
3778000	3780000	model prediction的結果
3780000	3782000	所以我們才可以拿這個東西
3782000	3784000	當作去減少這種
3784000	3786000	計算combination
3786000	3788000	的依據
3788000	3790000	他不單單只是
3790000	3792000	對
3792000	3794000	就是例如說我可能
3794000	3796000	換了另外一個input
3796000	3798000	然後可能兩個feature同時的可能增加
3798000	3800000	或減少或是之間的關聯
3800000	3802000	然後是based在這個weight的變化
3802000	3804000	去建那個matrix
3804000	3806000	例如說你可以
3806000	3808000	把
3808000	3810000	今天這個model裡面某一個
3810000	3812000	乘的weight拿出來
3812000	3814000	那我們就可以去透過這個weight
3814000	3816000	可以去看說今天這個x1跟這個x2
3816000	3818000	他是不是
3818000	3820000	例如說他interaction是不是比較大
3820000	3822000	或是今天x1跟x3是不是比較大
3822000	3824000	類似這樣
3826000	3828000	可以想像是變化的
3828000	3830000	一起變化的那種感覺嗎
3830000	3832000	就一起變
3832000	3834000	就例如說
3834000	3836000	今天這個x1加x3
3836000	3838000	變化嗎
3838000	3840000	應該說今天這個x1x3對於
3840000	3842000	模型的訓練
3842000	3844000	這邊先不牽扯到變化
3844000	3846000	我們單純看說
3846000	3848000	就單純把這個模型扒開
3848000	3850000	然後強制拿裡面的某一個
3850000	3852000	訓練的權重出來
3852000	3854000	拿出這個訓練的權重出來
3854000	3856000	我們可以去提取
3856000	3858000	就是說這個x1跟x3
3858000	3860000	加在一起的時候
3860000	3862000	對於這個prediction來說到底多重要
3862000	3864000	類似這樣
3864000	3866000	例如說他的x1加x3
3866000	3868000	那條weight很粗
3868000	3870000	例如說可能是0.8
3870000	3872000	那我們就把這個0.8
3872000	3874000	貼在這,類似這樣
3874000	3876000	先沒有牽扯到後面
3878000	3880000	那還有一個小問題想問
3880000	3882000	沒事沒事
3882000	3884000	因為我們現在可能討論說
3884000	3886000	單一某一個feature對結果的重要程度
3886000	3888000	到底是高還低
3888000	3890000	那會不會說有一種狀況是說
3890000	3892000	獨立看A他其實影響不大
3892000	3894000	獨立看B他影響也不大
3894000	3896000	但是然而只有A跟B
3896000	3898000	同時存在的時候
3898000	3900000	他對結果是影響很大的
3900000	3902000	那這種情況我要怎麼去解釋他
3902000	3904000	ok
3904000	3906000	我覺得這個問題很好
3906000	3908000	我覺得這個問題跟前面
3908000	3910000	就是
3910000	3912000	有一個人提到
3912000	3914000	就是今天這個feature
3914000	3916000	不是相依的這種情況
3916000	3918000	產生你剛才那個問題
3918000	3920000	就是今天就是A B不是獨立的
3920000	3922000	那A B加在一起
3922000	3924000	如果A B不是獨立
3924000	3926000	那A不重要B不重要
3926000	3928000	那A加B有可能重要
3928000	3930000	有可能產生這種情形
3930000	3932000	那這種方式要怎麼去判斷
3932000	3934000	例如說我們去設計這個
3934000	3936000	explanation model的時候
3936000	3938000	我們本來就要去考慮
3938000	3940000	就不能把這種feature獨立的這種
3940000	3942000	假設加在我們explanation model的設計
3942000	3944000	所以像是我今天這兩個word
3944000	3946000	就是不管是剛才那個
3946000	3948000	九宮格或是這個word
3948000	3950000	我們都假設每一個feature是獨立的
3950000	3952000	所以我們在算的時候我們都把每一個feature當作
3952000	3954000	算是一個
3954000	3956000	特徵來算
3956000	3958000	但如果你今天要考慮那種feature不同組合的話
3958000	3960000	那你考慮的層面可能
3960000	3962000	例如說你並不單只是
3962000	3964000	考慮A B C D
3964000	3966000	你可能還要考慮A B C D
3966000	3968000	A加B A加C A加D
3968000	3970000	當作A加D等於
3970000	3972000	假設等於F類似這種
3972000	3974000	就是你把A加B
3974000	3976000	當作一個新的feature來分析
3976000	3978000	這是我目前想要比較naive的
3978000	3980000	解法
3980000	3982000	我記得就是我剛才講的交通大學
3982000	3984000	上海交通大學的那個張宣世老師
3984000	3986000	他們的實驗室
3986000	3988000	有人在研究這種
3988000	3990000	這種feature interaction
3990000	3992000	組合的
3992000	3994000	貢獻度
3994000	3996000	對於最後prediction
3996000	3998000	就是explanation的performance的影響
3998000	4000000	是有人在做這個
4000000	4002000	但我的研究就比較
4002000	4004000	focus在基於
4004000	4006000	這種獨立feature的
4006000	4008000	對對對
4008000	4010000	了解
4010000	4012000	謝謝Alan
4012000	4014000	感謝你的問題
4024000	4026000	討論區裡面有一個人有問說
4026000	4028000	random mask影像的時候
4028000	4030000	mask size要怎麼選擇
4030000	4032000	mask size
4032000	4034000	你是說整張mask的
4034000	4036000	大小嗎
4036000	4038000	還是
4038000	4040000	比如說像狗的眼睛
4040000	4042000	那一個
4042000	4044000	比如說你今天
4044000	4046000	你今天mask如果是
4046000	4048000	比眼睛還要小的話
4048000	4050000	這個mask是有效的
4050000	4052000	ok
4052000	4054000	我們在做這種東西的時候
4054000	4056000	我們最小單位是一個pixel
4056000	4058000	對所以
4058000	4060000	我們應該不可能比pixel還要小
4060000	4062000	對所以例如說
4062000	4064000	如果mask size你再問說
4064000	4066000	要mask幾個的話
4066000	4068000	這種東西就是
4068000	4070000	例如說我們在random的時候完全不去
4070000	4072000	我在這個works裡面完全沒有去限制
4072000	4074000	但我知道有人是會去限制說
4074000	4076000	我不希望我遮掉的東西太多
4076000	4078000	就是為什麼不希望
4078000	4080000	遮掉的東西太多是因為
4080000	4082000	如果遮太多的話其實會反而導致
4082000	4084000	他那個random出來的結果不這麼positive
4084000	4086000	類似這樣
4086000	4088000	或是根據他的目的
4088000	4090000	而有不同的那個
4090000	4092000	選擇類似這樣
4092000	4094000	或是他可能只要挑幾個很重要的
4094000	4096000	feature出來
4096000	4098000	那我今天如果mask掉了
4098000	4100000	例如說很少部分
4100000	4102000	那其實就不是我們樂見的結果
4102000	4104000	類似這樣
4104000	4106000	但如果你是說mask的那個
4106000	4108000	那個dimension的話
4108000	4110000	那他必須要跟原本的image的dimension是一樣
4110000	4112000	不然我蓋不上去
4112000	4114000	對
4114000	4116000	OK
4116000	4118000	希望有回答到你的問題
4120000	4122000	這是安德嗎
4124000	4126000	OK
4126000	4128000	好 謝謝
4128000	4130000	感覺這個有點人為決定
4130000	4132000	因為我會覺得說像比如說你今天mask size
4132000	4134000	如果今天你的mask size
4134000	4136000	是
4136000	4138000	感覺上是有一種所謂的feature
4138000	4140000	的最小單位的這種概念
4140000	4142000	就是說因為你今天眼睛
4142000	4144000	feature的最小單位
4144000	4146000	你一定會超過一個pixel
4146000	4148000	因為你的
4148000	4150000	情況下面
4150000	4152000	我不知道 我只是覺得說
4152000	4154000	這個問題我也不知道怎麼回答
4154000	4156000	或者是說這個問題
4156000	4158000	有時候感覺上你這個size的選擇
4158000	4160000	是不是其實蠻主觀的
4160000	4162000	這個選擇
4162000	4164000	其實有人在做
4164000	4166000	就是例如說
4166000	4168000	也不是有人在做
4168000	4170000	其中一個方法就是例如說
4170000	4172000	他知道他指這些component
4172000	4174000	這些object都是比較大
4174000	4176000	例如眼睛是比較大
4176000	4178000	那他就例如說他的mask他就不是random
4178000	4180000	每一個pixel 他是random
4180000	4182000	每4x4 pixel
4182000	4184000	random每6x6 pixel
4184000	4186000	那透過這個方式去遮
4186000	4188000	也有人是這樣做
4188000	4190000	但我覺得這個東西就是
4190000	4192000	因為我們都已經不知道模型
4192000	4194000	他在乎的pixel可能是眼睛的某
4194000	4196000	右上加左下
4196000	4198000	類似這種
4198000	4200000	如果給這麼強的假設
4200000	4202000	我覺得模型訓練會比較不好
4202000	4204000	就是這種
4204000	4206000	explanation model會比較不好
4206000	4208000	當然你說的是
4208000	4210000	exactly right
4210000	4212000	真的是這樣子
4212000	4214000	其實有人詬病說
4214000	4216000	這種做出來的解釋
4216000	4218000	他其實就是大概
4218000	4220000	遮一下遮一下
4220000	4222000	例如說大概秀出
4222000	4224000	其實這邊眼睛也沒遮到
4224000	4226000	其實有可能就是
4226000	4228000	因為這種mask
4228000	4230000	一開始還沒有做好的原因
4230000	4232000	導致我最後給出來的東西
4232000	4234000	不這麼comprehensive
4236000	4238000	這是有可能
4238000	4240000	我覺得這個問題超好的
4240000	4242000	好
4252000	4254000	感覺上
4254000	4256000	聊天室裡面沒有新的問題
4258000	4260000	那我們再謝謝一次
4260000	4262000	裕能今天的演講
4264000	4266000	謝謝大家
4270000	4272000	謝謝
4272000	4274000	感謝大家的捧場
4274000	4276000	今天有幾個人
4276000	4278000	我比較好奇
4278000	4280000	因為我看不到
4280000	4282000	今天我這邊看到是有
4282000	4284000	最高有到32
4284000	4286000	才這麼多
4286000	4288000	我發現好多人
4288000	4290000	是怎樣間領域的關係
4292000	4294000	可是沒有人幫我的facebook按讚
4294000	4296000	開玩笑
4296000	4298000	必須說我之前
4298000	4300000	給過一次我本身是物理專業的
4300000	4302000	total五個人
4304000	4306000	可能這個東西
4306000	4308000	大家比較好奇
4308000	4310000	然後不知道怎麼入門吧
4310000	4312000	我也不知道
4312000	4314000	不過蠻開心有這個機會
4314000	4316000	分享一下所學
4316000	4318000	不然都在吃社會資源
4318000	4320000	總要貢獻一下
4320000	4322000	吃社會資源
4322000	4324000	對
4324000	4326000	你是高虹安嗎
4326000	4328000	沒有
4328000	4330000	我不是台大的
4330000	4332000	沒事
4332000	4334000	我全身而退
4334000	4336000	OK
4336000	4338000	感謝
4338000	4340000	突然在外查
4340000	4342000	補問一個問題
4342000	4344000	比如說你前一張投影片裡面
4346000	4348000	這個嗎
4348000	4350000	那個舉證的
4350000	4352000	這個嗎
4352000	4354000	對對對
4354000	4356000	舉證對角
4356000	4358000	所有這種x1 x1 x2 x2
4358000	4360000	你自動忽略是這個意思嗎
4360000	4362000	因為他們在這個
4362000	4364000	自己跟自己的關係
4364000	4366000	就是不討論
4366000	4368000	我們就不討論
4368000	4370000	因為可能在這個問題定義
4370000	4372000	定義裡面就沒有意義的
4372000	4374000	感覺
4374000	4376000	因為shopping value
4376000	4378000	自己跟自己還是自己
4378000	4380000	自己跟自己的組合還是自己
4380000	4382000	所以我們就不討論
4382000	4384000	那搶單兒自己跟自己
4384000	4386000	前提是高度相關
4386000	4388000	對
4390000	4392000	其實我不知道怎麼接觸分享
4392000	4394000	呵呵
4396000	4398000	那大概我想一下
4398000	4400000	那我可能先stop recording
4400000	4402000	OK
