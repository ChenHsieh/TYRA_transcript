start	end	text
0	25000	好,那很欢迎大家来到今天的Taiwan Talk,今天是1月8号,我们今天很高兴呢,到张翰,他目前呢是在台湾大学服务,他的PhD是在美国University of Massachusetts Amherst,然后PhD后来呢他在Boston进了一家生技产业,
25000	45000	然后后来呢又到了Boston University,就是BU的医学院,就是School of Medicine,做博士后研究,然后在2021年的时候呢他回到台湾大学,在医材影像所担任助理教授,今天呢我们很高兴有他来演讲。
45000	61000	那Taiwan的小小的传统就是呢,我们会请大家把麦克风打开,然后拍手,欢迎我们的讲者,那现在欢迎我们的讲者张翰老师。
65000	69000	好的,那可以share screen了。
75000	77000	有看到吗?
95000	97000	有看到吗?
97000	99000	可以看到了。
99000	104000	好,那也谢谢您介绍。大家好,我是张翰。
105000	122000	我刚刚所说,我是2016年博士班毕业的,然后之后是辗转几个单位,就是有一些新创公司要去博士后,然后我是去年2021年的时候回到台大做助理教授。
122000	132000	我记得Tara一开始申请,我记得表格说什么会员有每年一次报告研究之义务,然后我这件事情一直记在心里。
132000	142000	我回台湾之后基本上就很少上线,因为都起不来,所以想说还是要贡献一下。
142000	162000	今天也没全省,不过就是来跟大家聊聊。主要目的还是跟大家分享一下,另外也是如果大家对回台湾的环境有什么问题的话,我们等一下可以再讨论一下。
163000	189000	我自己实验室是做号称AI影像的分析,我本来做博士后的时候是做肌肉骨骼影像,主要治疗标的是退化性关节炎,另一方面就是回台湾之后又找一些其他的合作项目,包括脑科学跟线外影像等等。
189000	205000	等一下我有时间的话会稍微讲一下。AI这件事情可能是大家听演讲,每两周就会听到一次,所以介绍性的话我就很快带过去了。
205000	223000	大家知道AI在过去的五年、十年大概发展很快,很多影像辨识、分类、政策等等的表现超过人的标准,大概是最近几年发生的事情。
224000	242000	如果你看脑科学的话,比较旧的Letter,他们都会说图像辨识、人脑、电脑这些超过,可是假设你们再看一下Letter,可能五年内的话他们就会改口。
242000	250000	所以这个改变是最近几年的事情,可能大家都在自己的领域会感受到了。
250000	270000	在艺术影像上,影像上的modality是一个非常非常缓慢的过程。大家可能听过Hobart有一个Gordon Center,Gordon Center是影像中心合作PathGen的发明人,就是Gordon跟Charles。
271000	285000	他们做的是PathGen的前身,基本上原理其实是差不多的。他们的影像解析度的lower boundary是跟现在是一样的。
285000	300000	所以在艺术影像上的modality是一个非常非常慢的过程,通常都是几十年。通常都是几十年的adaptation要implement,然后被定创大规模使用。
300000	317000	所以有时候这一个领域或者AI有兴趣,不是因为,是不得不然,是因为发展太慢了。所以看到新的工具就赶快想利用看看,看到什么改变机会。
317000	333000	基本上Machine Learning或Deep Learning这些东西,它的功能就是找一个function,好比说我们可以做一个function去看图像,可以做分类。假设自然影像的话,分类就是分车子,分猫狗之类的。
333000	351000	马上就有人想说,我可不可以做诊断,所以分类在艺术影像的对应就是诊断嘛,好比说一把子hammer,就是calculation。像detection,大家做生物实验的话就发现鼠细胞这件事情是非常古老的labor。
351000	366000	那detection的话,假设自驾车的话,就是数行人,数别车等等。基本上一样的idea就是做一个function,然后放进slide,就会数细胞等等,这是侦测。
366000	389000	最后就是切割,切割就是在艺术影像上面也是使用关节的各部分,或者把器官切割出来。基本上就是你有想做的function,你有一个input,你有一个output。Machine Learning就是用自我最佳化的方式去把这个function找出来。
390000	414000	我自己做的题目是ulcerocytes,就是退化性关节炎。这是很老的疾病,就是老人的疾病。大家可能听到第一波的AMS,可能听到比较有名的,好比说糖尿病DMD影像,好比说oncology,就是癌症的肿瘤分割政策,还有cardiovascular。
414000	439000	这些当然就是因为这些疾病在美国是大课,假设我们数致死率的话,最前面是oncology和cardiovascular,就是因为需求而产生的这些情绪。我自己做ulcerocytes,它是退化性关节炎,它没有什么直接致死的passway。
440000	458000	如果你看美国第一名造成失能,disability的话,第一名就是ulcerocytes,就是一切关节炎。那里面大概三分之二是ulcerocytes,其他是免疫性关节炎。
459000	473000	第二名通常是背痛,我们国外很常会背痛,所以背痛也大大造成是很常见的。所以基本上这些肌肉骨骼病病人在造成disability在美国是非常非常常见的。
474000	484000	如果我们看开刀又花钱的话,通常数一数二的就是第一个是spine infusion,就是治疗脊椎,第二个就是replacement。
485000	499000	所以基本上这些肌肉骨骼的疾病,它如果不开刀的话就造成很多的失能。假设它开刀的话就会有很大的花费。
500000	510000	我们大家知道台湾是老化社会,这是第一个特性。第二个是台湾什么事情美国发生,台湾五年后就会发生。
511000	526000	这些事情对社会成本和社会健保的负担是会一增加一增加的增加。数数字就知道了,不需要看什么forecast,数数字就会负不起。
526000	540000	比方台湾的话,像这个new placement在美国也算是蛮贵的手术,成功率很高。台湾也算是一个一定要付但是非常昂贵的手术。
541000	551000	所以怎么用一些方式可以让病人不需要进入到社会分解的pathway其实是蛮重要的。
552000	564000	基本上,如果我们想说什么样的原因会造成病人需要换吸管,基本上就是疼痛难当,就是疼痛无法忍受就会换吸管。
565000	583000	大部分人都会忍受自己失能,就好比说这个关节炎有好几个symptom,可能一开始会活动能力受限,要conquer stiffness,最后可能会疼痛。
583000	595000	那其实stiffness或是活动能力受限很多人可以忍受,大家在美国大家知道吧,可能坐这个代步车就可以走一走去了。
595000	618000	那疼痛是无法忍受的。所以病人为什么最后会做开刀的决定,通常都是有一个pain spike,就是在要开刀之前,如果我们回顾他每年的function是怎么发生的话,都会有一年或两年前是突然疼痛增加了。
618000	633000	疼痛一增加,然后无法忍受,就跑去开刀了。如果疼痛是慢慢增加的话,大家知道耐痛是可以慢慢习惯的,反而还可以撑比较久。如果有一个pain spike的话,通常就跑去开刀了。
633000	650000	那会造成什么原因造成疼痛增加呢?如果我们直接画那个coalition是一个OA的定程,它叫KO grid,是一个OA的rheographic,就是X光上面的平分的话,那基本上跟疼痛是没有什么关系的。
650000	667000	很多人,好比说大家听到的那个NBA球员,有人没有软骨,然后还是可以打球,然后有人是膝盖正常,但很疼痛,所以基本上传统上的rheographic的关节点的平分跟疼痛没有关系。
667000	689000	所以先假设我们要研究这个OA是什么造成疼痛的话,传统上我们就是找到那边的人,可能像美国的话就OA付钱做trial,然后把这些MRI都拍下来,然后接下来要找医生去做grading,大家都很熟悉嘛。
689000	701000	基本上OA这些影像判读是需要有经验的医生,专门要看膝盖和关节炎的,所以成本很贵。
719000	740000	主持人员:"不好意思,不好意思,我小孩在重庆诶,不好意思。我请他妈来救援一下。
741000	758000	所以基本上你得到的资料都是非常少,也不一定一致,因为这些grading工作很慢,所以大部分都是很多routing center协作完成的,所以每个人的标准不太一样。
759000	771000	所以有时候统计分析的话,基本上过去效果都很差。所以这时候我们就讲到,有时候是需求决定了为什么要导入这些新的技术,是不得不然的。
771000	787000	所以我们既然用传统方式的grading很困难,又很花时间,那我们就导入AI,直接就最后一步,我们就完全放弃了这些所谓的影像特征的grading,
787000	801000	我们直接把你看到这些OA病变的MRI输到先进网络里面去,然后我们用的outcome就是我们想要看的地方。
802000	816000	好比说以这个project而言的话,它的outcome就是疼痛,病人有多少疼痛,再加上疼痛有没有突然增加。
816000	831000	所以这就是一个不再无法得知,或者本来就对什么样的病变造成疼痛不太清楚的状况下,使用这个方式。
831000	845000	好处大家知道吗,就是它很一致,它是同一个模型去看一些东西,所以特别适合这种clinical trial,就假设你有一个很一致的cohort,全部资料都是用同样的方式拍的话,
845000	851000	它标准的非常一致,它是自动的,所以只有你训练成功,它就没有成本。
851000	863000	那这个处理这个图像的基本上deep learning的网络就是convolutional neural network,大家很常听过,就是全机神经网络。
863000	877000	那它原理大概述下来,就是用一层一层的filter,大家做影像处理可能用filter,filter就是抓不同的特征,影像特征这样子。
877000	883000	那因为我不知道什么样的特征是比较重要的,我就让它自己optimize,自我更新。
883000	893000	那它就会照着这些方式层层更新,然后把这些影像处理成一些对结果的特征这样子。
893000	903000	那通常这个trivial neural network有一个重大的缺点,就是它很难控制confounder。
904000	914000	我们知道在医学上面,所有的事情都是影像加上confounder。
914000	923000	比如说这个人会不会,可能会恶化,是他的影像加上体重,加上他的生活情况,加上这些人有没有受伤等等。
923000	942000	所以我们做这个影像分析的时候,如果你把病人全部资料都丢进去,那虽然你想看到的是病人本身的影像,你想在影像上找到重要的特征。
942000	948000	可是只要neural network有任何读到confounder的方式的话,它就会随着pathway过去。
948000	954000	所以假设你的膝盖比较大,显然是体重比较重。
954000	963000	那neural network看膝盖比较大的事情,如果它可以跟outcome有关联的话,它就看膝盖比较大,它就忽略其他一些因素。
963000	969000	所以基本上deep learning的模型对confounder有很大的bias。
969000	975000	其中一个最简单的方式就是对冲。
975000	981000	你可以找两个有一样confounder的subject,然后让它互相抵消掉。
981000	987000	比如说我可以选一批评论者,他可能是疼痛增加前或增加后。
987000	990000	他同一个subject,所以他有类似的confounder。
990000	997000	所以如果我把一个网络同时参考两个incidence的话,他们confounder就互相消掉了。
997000	1002000	另一个可能就是所谓的cross lateral,左右膝盖。
1002000	1007000	有些人在刚恶化的时候,一个膝盖会先恶化。
1007000	1014000	那通常久了之后另一个膝盖也会带长,也会追上来。
1014000	1023000	可是是有一些incidence,你是可以抓在一瞬间就这些不平等的现象,那这时候你可以把左右膝盖当对照。
1024000	1027000	这还有好处就是说,其实疼痛是主观因素。
1027000	1032000	我们疼痛,大家可能去看过牙医,然后常常会填笑脸图。
1032000	1036000	就是绿色的笑脸是0,然后黄色的是0。
1036000	1039000	然后医生问你有多痛,然后你讲不出来。
1039000	1044000	然后他问你说,如果是0到10,你有多痛,那你也讲不出来。
1044000	1050000	这很奇怪,但是基本上这是一个主观回报的事情。
1050000	1060000	但是如果同一个病人,他回报两边膝盖的话,他两边都是主观的嘛,他对疼痛的耐受程度是一样的。
1060000	1071000	然后另一个可能是,假如他疼痛并不来自膝盖,就有蛮多病人,他们的症状是来自我们说的neurosystem,那这也是confounder。
1071000	1077000	可是如果他同时回报说两个膝盖中有差异的话,那他回报就是疼痛的差异而不是疼痛。
1077000	1081000	这件事也是一个控制主观因素的方法。
1081000	1093000	所以我本来就有工作是拿这些疼痛跟不痛的膝盖,拿来做这些所谓的contracted learning,就是对照性的学习。
1093000	1101000	然后想要把症状有差异的一边预测出来,显然结果就是比用医生传统判断方式好很多。
1101000	1118000	大概多10%的AUC,AUC就是一个二分类的评分架构,通常本来就是60-70,用的方法可以做到85-90,所以其实本质就有差别。
1119000	1136000	那另一个事情是说,Fockin常听到所谓的可解释性AI,是一个buzzword,所以SAI,Explainable AI,这是一个buzzword,其实你自己看定义什么的话其实很难讲。
1136000	1144000	我们所谓Explainable就是对使用者是Explainable,就是那个使用人觉得是合理的,他就是可解释。
1144000	1149000	所以基本上使用人想要什么,他就是什么。
1149000	1165000	那传统上我们在医学影像上我们认为我们的使用者其实是医生嘛,所以表示我们模型要做出一件决定,那这个决定想要把它传达让医生觉得说它是合理的。
1166000	1185000	那现在大部分的differentiate方式都是用saliency map,热区图,有很多争议,一般人认为热区图上面的热区,指向的地方没有太夸张的话,就表示它可能没有错太离谱,它可解释力没有错太离谱。
1186000	1201000	那基本上这是一个很弱的条件啦,就是如果它热区图很离谱的话就可以排除,但是它如果对的话,也不一定是因为正确原因做正确的事情,它可能就是重叠在那边。
1201000	1204000	基本上这些saliency map只有未知资讯而已。
1204000	1214000	我们今天做的一些工作是想法改进这些热区图的解析度,可以让这些图可以把个别的病变分出来。
1214000	1232000	这时候再回头问医生说,我们指出这些病变的位置有没有跟你图片中想的疼痛的位置是有可能的,可以用这个方法来探讨说是什么样的病变最有可能造成疼痛,然后跟医生的想法有没有合理这样子。
1235000	1246000	不过这种方式有它的很大的限制,第一个它逻辑对不对大家都还在讨论,这种热区图方式能不能解释它是可解释的,大家都还在讨论。
1246000	1261000	第二个,你这个也很难拿来定量,大家如果看热区图的话,它毕竟是interpreter来的,所以它形状其实没什么意义,它就是这些点做一个interpreter,然后就变成区域。
1261000	1276000	可是你仔细看这些形状跟region形状有没有直接对应,其实是有问题的,所以你直接把这些高亮度的地方划成counter来统计,其实是无法定量的。
1276000	1297000	假如不要夸张一点的话,假如看到一些脑肿瘤的热区图,那可能肿瘤在脑的边边,那热区就会一半在脑内一半在脑外在背景上面,所以它定位能力其实是很有问题的,有它的限制。
1297000	1311000	有什么比较直观的方式可以做这些feature的运造工作呢?
1311000	1329000	我大概想一下,如果有看医学影像的话,我们常做的事情就是比较。好比说一病人做疾病在追踪,医生可能看了他的新的影像,他恶化的话,那就是拿前一张影像来比较。
1329000	1345000	假设他是做治疗的话,可能是本身的一张影像变好了,然后他也会拿旧的影像做比较。所以比较影像这件事情是人脑会自动拟合出来,这是人脑很神奇的功能。
1345000	1365000	那其实我们今天对单张影像可以分析得很好,但是影像比较这件事情,其实人脑是有它独特的mechanism,然后我们电脑都还没有办法完全追上。
1366000	1379000	今天假设说好,我今天有一个病人,他会有一个症状突然发生,所以一开始症状是进展比较慢,突然就恶化了。
1380000	1390000	如果你是医生的话,你合理的做法是说,我看一下pain spike前后发生什么事情,所以就会把两张影像调出来。
1391000	1401000	然后医生放左边跟放右边,脑中就自动拟合出两边的差异在哪里。那我们要怎么教电脑做这件事情?
1402000	1417000	传统上的话,像我们刚刚做的所谓的判别式模型,是想法把左边跟右边这两个有疼痛跟没疼痛的影像区分出来。
1417000	1422000	比较合理的方式是做这种比较式模型。
1448000	1451000	大家可能还记得,也差14年了。
1452000	1460000	所以我们看了这两个pair,就马上可以想说哪边是正常,哪边是不正常的老化。
1460000	1464000	就显然左边是不正常老化,这不正常,这正常。
1465000	1470000	所以人脑有神奇的功能,就是它可以看到影像的改变。
1471000	1474000	然后我们脑中第一件事情,我们有两个pre-test。
1474000	1484000	第一个是我们知道它同一个subject,这很重要,因为假设你不跟人说这个跟这个同一个subject的话,
1484000	1486000	没人会拿来一起比较,他以为是两个不同的人。
1486000	1488000	所以你刚刚说的是同一个subject。
1488000	1498000	第二个事情是,你脑中对正常老化有一个概念,你知道说14年人大概会变成什么样子。
1499000	1507000	那我们也可以用同样的方式去观察说,改变前跟改变后的医学影像。
1507000	1512000	到底什么样的变化是正常变化,什么样的变化不是正常变化。
1512000	1519000	这变化可以有很多种,你可以是ten-tone的progression。
1519000	1525000	假设我们用一个统计模型来看说这个人未来开刀几率基于影像的话,
1525000	1532000	那我们也可以问说,假设同一个病人他的toA跟toB突然他的开刀几率变很高了,
1532000	1534000	那是什么样的事情造成他这样的差异。
1534000	1538000	所以是一个比较差异的framework。
1538000	1551000	所以就是,这件事情会成功,就是他从subject,脑中有一个所谓的正常的reference。
1552000	1557000	那我们在做这件事情的时候,脑中是有正常的reference,所以你没有感受到,
1557000	1561000	但人脑是很神奇的,会自动在背后处理。
1561000	1568000	那我们今天要把这个用deep learning来拟合这件事情的话,
1568000	1572000	那最直接的方式就是所谓的商场对抗网络。
1572000	1575000	那大家可能也听过,商场对抗网络就是好比说,
1575000	1584000	很多合成脸、合成video、合成声音等等的模型都是根据这些技术,是同一个family。
1584000	1588000	它逻辑就是说,我今天有一个叫生成器,
1588000	1594000	它生成器是可以拿图片当成它的condition,
1594000	1601000	那生成器图片进去之后,它就会出来一张以input图片当成条件的图片,
1601000	1604000	那它跟它之间有点对应的关系。
1604000	1606000	那它生成条件是什么?
1606000	1614000	它生成条件就是,你拿这个真的reference图片,
1614000	1617000	然后它会拿这个当参考,
1617000	1623000	所以整个结论是说,你的生成器会拿一部图片,
1623000	1629000	生成一张跟reference图片很像的真的图片。
1629000	1634000	所以我们的做法可以说,我们可以把要比较的对比的reference,
1634000	1636000	就是real healthy,
1636000	1641000	好比说,左右膝盖的话,你就是不要不痛的膝盖当成reference,
1641000	1644000	不要痛的膝盖当成real disease,
1644000	1647000	然后我们去合成出一个假的healthy。
1647000	1652000	那这个差值就是比较差异嘛,
1652000	1656000	虚拟同样的造型,它如果不痛的话会长什么样子?
1657000	1659000	那在定义上我们叫counterfactual,
1659000	1661000	就是假设性的一个case,
1661000	1666000	这两个差异就会是我们想要看到的改变部分。
1670000	1673000	另一部分就是所谓的healthy reference,
1673000	1682000	就是我们脑中比较知道什么样的膝盖变化是正常的。
1682000	1687000	那大概讲说我们要比较的这些reference应该不是18岁健康的膝盖,
1687000	1690000	我们的OA是不可,是老化的疾病。
1690000	1694000	所谓老化的疾病就是说,所有人都很老,
1694000	1698000	所以有些影像上面的改变是正常的,我们需要接受,
1698000	1701000	好比说我们看做那个Azheimer影像的话,
1701000	1705000	这个老人的Azheimer影像都会有脑缩,
1705000	1707000	可是脑缩是老人就会缩啊,
1707000	1711000	并不是Azheimer他会缩。
1711000	1713000	所以像Azheimer这个研究的话,
1713000	1717000	很大一部分的目的就是要想要把这些正常老化跟不正常老化,
1717000	1720000	所谓的Azheimer倒转老化来区分出来。
1720000	1722000	那所以我们OA也是一样,
1722000	1727000	我们并不是要把这些膝盖还原到18岁的完美状况,
1727000	1731000	我们也没有这些影像,因为18岁不会去拍MRI。
1731000	1736000	那我们要,我们的reference应该是正常老化,
1736000	1740000	它是50,60,70,就该那样子,老就是会老,
1740000	1746000	但是它要是function或是什么东西还正常的。
1746000	1752000	所以像我们的话,我们就选说假设这些病人都是0致命OA,
1752000	1756000	或是preclinical,就是还没有进入临床上的OA,
1756000	1758000	然后也不会开刀,
1758000	1761000	所以这些人我们就定义成他是所谓的健康老化嘛。
1761000	1768000	那我们要,那我们的reference就是定义到把图拉到这些健康老化的状况上面,
1768000	1772000	而不是一个真空中完全健康完美的膝盖这样子。
1772000	1777000	所以这个好比说治疗前治疗后,
1777000	1780000	然后好比说这个pain spike,control incidence,
1780000	1784000	然后好比说不同的modality,
1784000	1787000	这种比较性概念是一个很强的framework,
1787000	1791000	因为我觉得这个在医学影像上,
1791000	1797000	很多时候我们是凭借这个比较这些事情来处理资讯的,
1797000	1801000	那基本上如果你去看诊的时候,
1801000	1803000	要看医生做什么,
1803000	1805000	其实很常这样发生的,
1805000	1807000	就是左边一张右边一张,
1807000	1813000	然后用比较来决定死因发生,
1813000	1815000	所以这是一个很泛用的framework,
1815000	1819000	并不只是在这个我目前做的topic上面。
1819000	1826000	所以我们先拿一个正常的disease,
1826000	1828000	然后产生说fit healthy,
1828000	1829000	然后产生real healthy,
1829000	1836000	那我们用一个回归器的方式,
1836000	1839000	我们可以训练一个回归器,
1839000	1842000	它就跟一般做回归一样,
1842000	1844000	用图像做回归,
1844000	1848000	那它目的就是要观察这些健康老化的器材,
1848000	1854000	然后要观察说它的年龄长什么样子,
1854000	1855000	因为这些也是健康老化的,
1855000	1861000	所以它的预测出来的年龄就会是它本身的年龄。
1861000	1866000	那做完之后如果拿这些同样的模型,
1866000	1868000	去观察一些不健康老化的病人的话,
1868000	1876000	就发现他的实际年龄会小于他膝盖的appearance age。
1876000	1881000	那这个concept其实大家可能听过脑灵嘛,
1881000	1884000	这个在其他分析中也常用到,
1884000	1887000	或者说大家可能听过脑灵检测等等,
1887000	1891000	就是有些人的脑看起来就是比他实际年龄老,
1891000	1892000	这个脑灵检测,
1892000	1896000	那其实也可以用同样的逻辑来做嘛,
1896000	1900000	就是拿一个不同年龄的模型去看健康老化的老人,
1900000	1903000	然后再回来看一些不同的病人。
1903000	1906000	那有关脑灵这部分的话,
1906000	1907000	台湾也有人做,
1907000	1908000	国外也有人做,
1908000	1913000	那还发现说不同的neurological disorders,
1913000	1914000	还有不同的,
1914000	1916000	就是他的年龄会有不同的shift,
1916000	1918000	他的shift pattern还不一样。
1918000	1923000	那又好比说大家看医生的时候,
1923000	1929000	如果大家去见检的话,
1929000	1930000	常常被问说,
1930000	1932000	你看起来不像年轻人,
1932000	1934000	这件事情也是一样的嘛,
1934000	1938000	就是你的appearance跟你的实际不如预期,
1938000	1941000	不如预期这件事情本身就是一个spell marker,
1941000	1945000	所以病人会跟你说,
1945000	1946000	你看起来不太好,
1946000	1948000	但是你跟五十岁比起来还不错啦,
1948000	1950000	就不会这样子嘛,
1950000	1953000	比起来是跟你的健康年龄该怎么样,
1953000	1956000	如果差异的话就表示有问题嘛,
1956000	1958000	所以这个比较脑灵,
1958000	1961000	比较年龄这些事情是很有意义的。
1961000	1964000	所以我们今天就拿这个,
1964000	1968000	把这个healthy reference这些事情来结合起来,
1968000	1969000	所以我们怎么做,
1969000	1970000	我们有真的disease,
1970000	1972000	然后我们产生一个fake healthy,
1972000	1976000	然后我们这个fake healthy还有一个condition,
1976000	1981000	就是fake healthy经过健康年龄的regression年龄的话,
1981000	1983000	要是本来的年龄,
1983000	1989000	所以我们不会生成出一个很假膝盖的样子,
1989000	1994000	我们生成出一个他该怎么年龄就怎么年龄的膝盖,
1994000	1995000	对,
1995000	2001000	就是一个拟合出他这个年龄上该有的状况的样子,
2001000	2005000	所以他生出来的模型都不会是完美的,
2006000	2007000	如果本来有大脊椎的话,
2007000	2009000	那可能就是小脊椎,
2009000	2012000	本来假设软骨有全掉的话,
2012000	2014000	可能就变掉三层,
2014000	2015000	都不会是完美的膝盖,
2015000	2023000	可是这个差异就是我们认为是造成有疾病的medication的原因。
2025000	2027000	所以基本上,
2029000	2033000	这个GAN技术目前非常有用,
2033000	2037000	当然看到一些生成VDR等等,
2037000	2041000	已经可以很愉快的控制它们,
2041000	2048000	所以目前假设我们有一些MRM想要找各种条件,
2048000	2051000	比如说把疼痛疾病移除掉,
2051000	2053000	都可以很愉快的做到,
2053000	2055000	它就会有类似的anachronism形状,
2055000	2057000	所以同个subject,
2057000	2060000	但是疾病的一些feature就消失掉了。
2064000	2070000	我们对这种GAN生成影像的最大的抗拒就是,
2070000	2072000	它是假的吗?
2072000	2074000	这些影像并不真的存在吗?
2074000	2079000	到底要怎么知道这些影像是可信还是不可信呢?
2079000	2082000	在医学上很困难,
2082000	2084000	因为这些都是假设性的,
2084000	2085000	不能重拍,
2085000	2087000	所以很难说到底是真的还是假的。
2088000	2094000	好比说我今天要预测一个tumor会不会有progression,
2094000	2097000	这很难知道,
2097000	2099000	因为它可能有progression,
2099000	2100000	可能没有progression,
2100000	2101000	如果它有progression,
2101000	2103000	它预测的方式可以有很多种,
2103000	2105000	它可以往某地方转移,
2105000	2107000	或者它的扩散有什么样的扩散,
2107000	2110000	所以progression这件事情很难讲,
2110000	2112000	因为未来是有很多种。
2112000	2116000	但是从病变变回健康这件事情,
2116000	2118000	其实有一个逻辑在那边,
2118000	2119000	它逻辑就是说,
2119000	2122000	健康的状态应该是都很类似的,
2122000	2125000	不健康的状态应该都是很不类似的,
2125000	2128000	大家有听过这个,
2128000	2129000	安娜·卡赖里娜,
2129000	2130000	她的第一句就是说,
2130000	2131000	happy families are all alike,
2131000	2134000	就是所有高薪的家庭都是一样的,
2134000	2135000	都类似的,
2135000	2138000	everyone's happy family is happy in its own way,
2138000	2142000	那这个在科学上有很多,
2142000	2145000	这是principle在科学上有很多应用,
2145000	2146000	主要想法就是说,
2146000	2150000	一个健康的状况应该是都很类似的,
2150000	2153000	所有的条件都很好的健康状况,
2153000	2155000	开始病变之后,
2155000	2158000	它就会往各处发散,
2158000	2160000	就会变得很不一样,
2160000	2164000	所以你要从健康预测不健康是比较困难的,
2164000	2167000	这样不健康反推回来是比较容易的,
2167000	2169000	因为健康状况应该是类似的,
2169000	2172000	其他我不敢讲,
2172000	2174000	但在膝盖影像上面,
2174000	2177000	健康的膝盖骨头,
2177000	2178000	软骨,
2178000	2179000	半夜半,
2179000	2180000	这都是有天生的形状,
2180000	2182000	所以健康状况是,
2182000	2185000	其实我们知道应该长什么样子,
2185000	2189000	所以假如你要画成群剧图的话,
2189000	2192000	你有一坨白色的健康状况,
2192000	2193000	都应该蛮类似的,
2193000	2196000	然后它的病变开始之后就慢慢发散,
2196000	2199000	所以你要从健康推到不健康,
2199000	2201000	其实蛮困难的,
2201000	2204000	因为你很难验证说,
2204000	2206000	你推断对不对,
2206000	2209000	但你从不健康的图片反推回来的话,
2209000	2210000	是比较容易的,
2210000	2212000	然后你有很多工具可以验证,
2212000	2215000	比如说你可以验证软骨的,
2215000	2217000	或是半夜半的形状等等,
2217000	2218000	有没有正常。
2223000	2225000	那最后一点是说,
2225000	2228000	我们刚讲比较性是拿,
2228000	2230000	有疼痛跟没有疼痛比较嘛,
2230000	2233000	那有疼痛跟没有疼痛是一个binary,
2233000	2236000	是我们自己把它变成binary嘛,
2236000	2237000	但其中其实没有可能嘛,
2237000	2239000	因为很多事情变成binary,
2239000	2241000	是我们人为的因素嘛,
2241000	2247000	好比说我今天病人照那个0到10的疼痛表,
2247000	2249000	来回报疼痛,
2249000	2251000	那他可能说左边是3,右边是7,
2251000	2254000	然后下来病人说左边是1,右边是5,
2254000	2255000	那差值都是4,
2255000	2258000	然后我做研究,
2258000	2259000	所以我要强迫变binary,
2259000	2261000	我说这是4,所以是一样的,
2261000	2263000	其实是不一样的嘛,
2263000	2266000	那或是说差是1到8,
2266000	2267000	或是1到6,
2267000	2270000	那差值是一大一小,
2270000	2271000	可是我做binary,
2271000	2272000	我binaryize,
2272000	2274000	那我会觉得是一样的,
2274000	2276000	可是这些一样,
2276000	2278000	其实是人为的解释嘛,
2278000	2280000	大家想要知道就是,
2280000	2282000	这些人为把一些事情做二本法,
2282000	2284000	其实是一个很常用的工具,
2284000	2288000	但是并不代表说二本法的边界有任何特别的地方,
2288000	2290000	这举个例子好了,
2290000	2293000	假如说有常人来台湾的话,
2293000	2294000	很常听说8是量表嘛,
2294000	2296000	8量表就是一个二本法嘛,
2296000	2300000	就是我觉得量表的左边就是不需要看护,
2300000	2301000	右边是需要看护,
2301000	2305000	那这个线的左边右边有什么magical thing happen,
2305000	2306000	它变不一样嘛,
2306000	2307000	其实没有,
2307000	2310000	所以二本法其实是很confusing,
2310000	2314000	我们在医学上知道它有很多人为设出来的边界,
2314000	2316000	或是人为设出来的binary,
2316000	2317000	这举个例子好了,
2317000	2318000	但其实是没有意义的,
2318000	2325000	所以很大的困难就是怎么把这些binary的资讯,
2325000	2327000	再让还原回去交给你的模型,
2327000	2329000	以这个project为例的话,
2329000	2330000	我们就是,
2330000	2332000	我们至少让模型知道说,
2332000	2334000	healthy跟painful,
2334000	2336000	跟healthy跟very unpainful要分开了,
2336000	2338000	所以假如说0到3的话,
2338000	2339000	差异很少,
2339000	2342000	我们让模型知道说这差异很小,
2342000	2344000	假如0到9的话,
2344000	2345000	差异很多,
2345000	2347000	我让它知道说差异很多,
2347000	2350000	幸好这些用模型可以解决,
2350000	2353000	我们是可以把这些差异,
2353000	2358000	encode之后放进去这个模型的生成的部分里面去,
2358000	2362000	所以它就照你想要给它的距离,
2362000	2364000	相比的差异,
2364000	2367000	所以就是数值差比较少会差比较少,
2367000	2369000	差比较多会差比较多,
2369000	2373000	那当然这个传统差异只是小部分,
2373000	2376000	所以这个比较类似像是一个demo一样,
2376000	2378000	证明说我们可以control这些事情,
2378000	2381000	当然是有更多更多的因素在里面,
2381000	2383000	所以可以想办法加进去,
2383000	2388000	所以基本上训练成功之后,
2388000	2392000	你就会得到一个连续的膝盖分布,
2392000	2396000	连续膝盖分布最左边就会是你本来的膝盖,
2396000	2400000	最右边就会是你疼痛的高低的膝盖,
2400000	2406000	那根据它们两个疼痛的scoring的差异,
2406000	2409000	它就会拉近会拉远,
2409000	2412000	所以并不是说左边是不疼痛膝盖,
2412000	2414000	右边是疼痛膝盖,中间都是垃圾,
2414000	2416000	它是一个连续的空间,
2416000	2419000	连续的空间它的分布其实是你可以控制的,
2419000	2424000	那我们之前有做过一些自动切割,
2424000	2427000	自动切割在一平衡上算是一个比较好解决的工作,
2427000	2431000	基本上结论就是说你只要有研究生,
2431000	2434000	只要有处理他就把你用手标完,
2434000	2435000	然后就解决掉,
2435000	2439000	所以其实基本上一平衡上自动切割算是单一,
2439000	2441000	比较困难是在单一data set,
2441000	2444000	多data set你会有差异,
2444000	2446000	但如果是在同一data set,
2446000	2448000	然后有标记的话,
2448000	2449000	你有光处的话,
2449000	2452000	就是我们这种所谓叫做有切割的光处,
2452000	2453000	就是切割的标记嘛,
2453000	2455000	那这种叫做监督式学习,
2456000	2457000	如果是survival的话,
2457000	2459000	基本上还蛮好解决的,
2459000	2461000	所以已经不是新的东西了,
2461000	2466000	那我们做deep learning的话,
2466000	2469000	有一个功能就是帮助这些,
2469000	2471000	本来就可以做的事情,
2471000	2472000	把自动化,
2472000	2474000	就好比说这些膝盖骨头切割,
2474000	2477000	那其实其实不是很难嘛,
2477000	2478000	因为医生都可以切,
2478000	2479000	然后也有些postdoc,
2479000	2481000	或者有些研究生都会切,
2481000	2483000	但是本来手法是非常非常非常慢,
2483000	2485000	那在近期的话,
2485000	2488000	可能用一些统计的statistical shape model,
2488000	2490000	去拟合出这些膝盖形状,
2490000	2492000	总之要花时间嘛,
2492000	2494000	但deep learning有一个很好的好处,
2494000	2495000	就是它可以自动化,
2495000	2497000	所以刚刚说过,
2497000	2498000	只要是survival能力做切割,
2498000	2500000	其实不是很难,
2500000	2502000	所以我们之前做的模型,
2502000	2504000	就是把这些手动标记,
2504000	2505000	转成一个自动切割化模型,
2505000	2507000	那一下子你会分析很多,
2507000	2510000	很多不同的资料,
2511000	2513000	那它科学上有新的东西吗?
2513000	2514000	其实没有,
2514000	2515000	它只是自动化而已,
2515000	2518000	但是生物学上的很多丢零,
2518000	2520000	就是最大的困难嘛,
2520000	2522000	所以它并不是新的功能,
2522000	2524000	但是只是把同样的事情,
2524000	2526000	给别人做一倍,
2526000	2527000	然后我会做一百倍,
2527000	2528000	那就很有价值。
2533000	2536000	那今天在我们这个case上面来说的话,
2536000	2538000	我没想要做自动切割,
2538000	2539000	为什么呢?
2539000	2541000	有一种特别的region是,
2541000	2542000	我们特别想要看的,
2542000	2543000	所谓的Bone Marrow Region,
2543000	2545000	它就是在骨髓内的,
2545000	2547000	在骨头区里面的所有的,
2547000	2551000	所有的hyperintensity,
2551000	2553000	都是被认为是Bone Marrow Region,
2553000	2558000	那它问题是这样子,
2558000	2559000	我们这个医学影像,
2559000	2561000	好多MRI好了,
2561000	2563000	每个MRI是拍的功能不太一样,
2564000	2568000	好比说像这个左边是High Definition,
2568000	2571000	就是它的骨头跟人肉之间的contrast很明显,
2571000	2573000	右边是看发炎的,
2573000	2576000	就是这个Weighted T2 Sequence,
2576000	2578000	所以它看到发炎,
2578000	2581000	可是它的骨头跟人肉之间的边界不是很明显,
2581000	2584000	那你可能想说,
2584000	2586000	我就把左边做自动切割,
2586000	2587000	然后搬到右边来就好了嘛,
2587000	2590000	可问题是即使是同一位病人,
2590000	2592000	它拍的时候也不会是完全align的,
2592000	2596000	像你膝盖的眼花稍微角做Man Spread就不一样了,
2596000	2598000	就已经3D形状就不一样了,
2598000	2600000	所以基本上,
2600000	2602000	这是Clinical是这样拍的,
2602000	2604000	这是Clinical Trial,
2604000	2606000	所以至少是同时拍的,
2606000	2608000	所以是有蛮多病人是重合没错,
2608000	2613000	但是至少一半是Man Spreading,
2613000	2617000	所以拍的3D的剖面不一样了,
2617000	2619000	假设如果是Follow Up的话,
2619000	2621000	基本上看起来就不太一样了,
2621000	2624000	所以基本上,
2624000	2627000	你有一些切割的资讯,
2627000	2629000	想要转到发炎资讯上面去,
2629000	2631000	你要怎么转呢?
2631000	2634000	然后再讲说,
2634000	2636000	为什么不能直接套用?
2636000	2641000	大家知道Neural Network有另一个重大缺点,
2641000	2644000	就是它很难套用在Core Data Stack,
2644000	2646000	最有名的例子是说,
2646000	2650000	Source跟Target其实差不多,
2650000	2653000	但是节度就差很多,
2653000	2656000	所以我们可以用这些Gate方式,
2656000	2658000	把一个模型转到另一个模型上面去,
2658000	2664000	那你就可以在想拍发炎资讯上去,
2664000	2666000	把这些去切割下来,
2666000	2670000	我花了超多时间,
2670000	2672000	所以我就讲快点好了,
2672000	2674000	我们最后结果是说,
2674000	2676000	我们可以拿这些切割模型,
2676000	2680000	在本来这个结病改变的图上面做定量,
2680000	2684000	那我们可以把这些Bomel Region,
2684000	2686000	很敏感的定量下来,
2686000	2688000	那最后我们就做这些统计分析,
2688000	2691000	等于是用后验的统计分析,
2691000	2693000	那根据传统的画Contour方式来比的话,
2693000	2695000	就发现如果计算,
2695000	2699000	还有未来会开到的Out Ratio,
2699000	2701000	就还好蛮多的,
2702000	2704000	所以大概这样子,
2704000	2706000	我之后本来要讲,
2706000	2709000	先这样子好了,
2709000	2711000	那我就直接讲,
2711000	2713000	我conclusion,
2713000	2716000	那我是回台湾之后,
2716000	2722000	大家知道我们做影像都会找合适的Collaborator,
2722000	2724000	我在美国的时候跟台湾的时候就不太一样,
2724000	2726000	那我在台湾的时候就做一个分支,
2726000	2728000	就是做脑影像研究,
2728000	2730000	大家可能知道说,
2730000	2732000	可能听过了,
2732000	2734000	Neural Network的本质就是,
2734000	2736000	一开始学脑会怎么function,
2736000	2738000	它是拟合一个脑细胞,
2738000	2740000	把很多资讯综结在一起,
2740000	2744000	然后用一个Activation Function,
2744000	2746000	然后往前输送这样子,
2746000	2748000	脑装滑,
2748000	2750000	偷别人的Video,
2750000	2755000	每颗脑神经都有几百个接收源,
2755000	2759000	然后再把这些神经连回去,
2759000	2761000	基本上,
2761000	2763000	这些神经集合在这边的Circuit,
2763000	2765000	就变回路,
2765000	2769000	回路就会做各种行为,
2769000	2771000	记忆等等,
2771000	2774000	所谓的脑科学的Holy Grail,
2774000	2776000	就是这个回路结合起来变Conscious,
2776000	2778000	我们是谁,
2778000	2780000	这我就不知道了,
2780000	2782000	因为我们对脑科学的认知,
2783000	2785000	非常非常地开始了,
2785000	2787000	像这些回路结合起来,
2787000	2789000	会变成怎么看东西,
2789000	2791000	这种东西我们都不知道,
2791000	2795000	第一步就是怎么把这些回路区分出来,
2795000	2797000	还是做不到,
2797000	2799000	除非是做电子显微镜,
2799000	2801000	不然解析度是不够的,
2801000	2803000	但你可以做光学显微镜,
2803000	2806000	解析度是不够,
2806000	2808000	但是会快很多,
2808000	2810000	电子显微镜基本上扫人脑,
2811000	2813000	所以有部分的,
2813000	2817000	怎么用AI技术来,
2817000	2820000	回头来分析这些脑科学影像,
2820000	2822000	想法先把这些脑神经,
2822000	2824000	一颗一颗重新连接,
2824000	2826000	重现出来,
2826000	2828000	我们才拿这些东西来,
2828000	2830000	建立一个模型来预测,
2830000	2832000	它的行为被怎么调控,
2832000	2834000	所以基本上就是一个,
2834000	2836000	我想问一下,
2836000	2838000	我问你,
2838000	2840000	左边右边,
2840000	2842000	差别是什么?
2846000	2848000	右边是原图影像,
2848000	2850000	左边是,
2850000	2852000	左边是米禾出的CNET,
2852000	2854000	就是,
2854000	2856000	左边是,
2856000	2858000	脑神经,
2858000	2860000	每颗发散的地方,
2860000	2862000	叫CNET,
2862000	2864000	左边是把CNET强化出来,
2864000	2866000	它有在,
2866000	2868000	它是两个神经的边界,
2868000	2870000	可能是,
2870000	2872000	这些,
2872000	2874000	每一线的终点,
2874000	2876000	是两个神经的边界,
2876000	2878000	它觉得不是同一个神经细胞,
2878000	2880000	有可能是CNET在交换资讯,
2882000	2884000	这结果不是很翻用来子,
2884000	2886000	但是,
2886000	2888000	但是,
2888000	2890000	目的就是说,
2890000	2892000	怎么把,
2892000	2894000	怎么把每颗脑神经,
2894000	2896000	然后,
2896000	2898000	然后把这些看起来是一条线的地方,
2898000	2900000	把它区分出来,
2900000	2902000	哪些是受理脑理,
2902000	2904000	哪些是在地脑理,
2904000	2906000	理论上这些CNET的位置,
2906000	2908000	就会是传递资讯的地方,
2908000	2910000	如果全部都做完,
2910000	2912000	然后很整的话,
2912000	2914000	你就可以利用一些theory,
2914000	2916000	graph theory,
2916000	2918000	去重现出这些会录长什么样子,
2918000	2920000	因为我同事有做这些,
2920000	2922000	Cognitive的测量,
2922000	2924000	并不是很大动物,
2924000	2926000	对,
2926000	2928000	但他们过于,
2928000	2930000	已经被训练说有些行为,
2930000	2932000	他们是要怎样,他们是要怎样,
2932000	2934000	他们做完行为之后,
2934000	2936000	他们再来拍摄这样子,
2936000	2938000	所以他们是可以知道说,
2938000	2940000	哪些会录如果有出现或没有出现的话,
2940000	2942000	可以跟某些方式联系在一起,
2942000	2944000	对,
2944000	2946000	所以,
2946000	2948000	那你可以做一些影像强化等等,
2948000	2950000	等等,
2950000	2952000	影像强化,
2952000	2954000	去噪音,去模糊化等等,
2954000	2956000	显微镜影像是非常常用的课题,
2956000	2958000	就可以拿来做,
2958000	2960000	基本上就是一个,
2960000	2962000	Light Emitting Art,
2962000	2964000	Art Emitting Light的过程,
2964000	2966000	就是当初是,
2966000	2968000	脑科学启发了AI的研究,
2968000	2970000	现在AI又会来做这个脑科学,
2970000	2972000	所以就会慢慢,
2972000	2974000	回旋就会收敛,
2974000	2976000	就会出现Terminated,
2976000	2978000	不过就大概这样子,
2978000	2980000	那我今天就,
2980000	2982000	在讲这边。
2984000	2986000	好,谢谢,
2986000	2988000	那,
2988000	2990000	现在我们就再感谢张涵,
2990000	2992000	谢谢。
2994000	2996000	现场有没有,
2996000	2998000	有问题,然后现在,
2998000	3000000	可以直接开麦克风,直接问。
3002000	3004000	你好,我想请教一下,
3004000	3006000	我对你刚提到那个,
3006000	3008000	不好意思,
3008000	3010000	那个可以先自我介绍一下。
3010000	3012000	对不起,对不起,我叫石清华,
3012000	3014000	我现在在Rochester,
3014000	3016000	New York,
3016000	3018000	那我对你那个脑神经,
3018000	3020000	那部分其实有点感兴趣,
3020000	3022000	因为其实在,
3022000	3024000	大概在一两个月前,
3024000	3026000	有一个,
3026000	3028000	有一个Research,
3028000	3030000	他应该还没有完全Publish,
3030000	3032000	他就是基本上是把,
3032000	3034000	脑神经,
3034000	3036000	有脑细胞养在一些Culture里头,
3036000	3038000	然后用一些Training,
3038000	3040000	然后配上AI的技术之后,
3040000	3042000	可以让他玩Video Game,
3042000	3044000	他可以玩乒乓,
3044000	3046000	这样子,
3046000	3048000	就是比较,
3048000	3050000	对,他是一个人,
3050000	3052000	我看了那篇Paper之后,
3052000	3054000	我觉得,
3054000	3056000	对,就是别人的Research已经到天上,
3056000	3058000	但我还在地上爬这样子。
3058000	3060000	那我只是好奇,
3060000	3062000	我只是好奇,
3062000	3064000	像你在你这边的那个,
3064000	3066000	如果有办法说,
3066000	3068000	用类似的Model,
3068000	3070000	然后对,
3070000	3072000	比如说你可以记住Action Potential,
3072000	3074000	或者是有办法,
3074000	3076000	荧光可以
3076000	3078000	侦测他Action Potential的,
3078000	3080000	然后类似的Pattern,
3080000	3082000	你就可以看到脑回路的反应,
3082000	3084000	或神经回路的反应,
3084000	3086000	针对他的Response这样子。
3086000	3088000	那,
3088000	3090000	而且因为这样东西,
3090000	3092000	呃,
3092000	3094000	我猜想,
3094000	3096000	就是说可以做到,
3096000	3098000	你可以比较,
3098000	3100000	Simplify一些Circuit,
3100000	3102000	然后可以做到比较,
3102000	3104000	呃,
3104000	3106000	比较High Resolution,
3106000	3108000	然后去Detect他的那个,
3108000	3110000	回路的Pattern,
3110000	3112000	那我不晓得说,
3112000	3114000	这个东西会不会在未来,
3114000	3116000	会变得很有趣,
3116000	3118000	或者是会变得是非常,
3118000	3120000	他相对于,
3120000	3122000	佐瑟菲亚的Brain来讲,
3122000	3124000	更小,因为他只有几百几千个细胞而已,
3124000	3126000	所以比那个佐瑟菲亚的Brain更小,
3126000	3128000	所以我觉得他可能,
3128000	3130000	说不定是一个很神奇的东西。
3130000	3132000	是是。
3132000	3134000	好,谢谢。
3134000	3136000	这个我可能要质疑一下我的同事,
3136000	3138000	我先说我们这个是,
3138000	3140000	这个是一个蛮大的Team,
3140000	3142000	然后主要是
3142000	3144000	清化脑科中心,
3144000	3146000	那我这边做影像分析,
3146000	3148000	那我主要一开始做的是,
3148000	3150000	这个,
3150000	3152000	是荧光行为镜影像嘛,
3152000	3154000	所以这是解剖完的染色或影像,
3154000	3156000	但是其实是有另外一个Branch,
3156000	3158000	是做Live Imaging,
3158000	3160000	就是,
3160000	3162000	就是是
3162000	3164000	Real Time的Function影像,
3164000	3166000	所以是有可能的,
3166000	3168000	是有可能的,
3168000	3170000	那我们现在其实,
3170000	3172000	又开始做分析,
3172000	3174000	是做这个,
3174000	3176000	就是这个,
3176000	3178000	类器官的体外培养的神经元,
3178000	3180000	原因也是一样,
3180000	3182000	就是你可以控制好,
3182000	3184000	然后是可以,
3184000	3186000	可以,
3186000	3188000	比较好地做一些,
3188000	3190000	就是这些,
3190000	3192000	你做解构或是
3192000	3194000	方便分析,
3194000	3196000	然后跟这些控制条件做比较,
3196000	3198000	但是这个Brain Cell可以,
3198000	3200000	我先找影片,
3200000	3202000	就是这个,
3202000	3204000	Brain Cell可以Play Pong,
3204000	3206000	可以Play乒乓球,
3206000	3208000	这是我是第一次知道,
3208000	3212000	我刚刚学习在那个Chat里头,
3212000	3214000	就是那个,
3214000	3216000	有看到,
3216000	3218000	我也是刚知道,
3218000	3220000	其实我们在看那个,
3220000	3222000	其实我们在看那个人脑培养的那个,
3222000	3224000	Ocrelinoid的脑细胞的时候,
3224000	3226000	就常常想说,
3226000	3228000	它有没有Conscious,
3228000	3230000	它有有Conscious的话就很可怕,
3230000	3232000	就很可怕,
3234000	3236000	有没有Conscious我是不知道,
3236000	3238000	可是它就可以Play Pong,
3238000	3240000	就已经有点可怕,
3240000	3242000	我一直觉得是我们的,
3242000	3244000	这完全是我自己的猜想,
3244000	3246000	就是我觉得我们的function,
3246000	3248000	或我们的意识其实是一个Module,
3248000	3250000	就是各种Module最后组成出来的效果,
3250000	3252000	所以各自的Module,
3252000	3254000	可能没有所谓的Conscious,
3254000	3256000	但是组合起来就可能有了,
3256000	3258000	是,
3258000	3260000	说得非常好,
3260000	3262000	就是,
3262000	3264000	就是,
3264000	3266000	我们同样的方式是可以分析,
3266000	3268000	人脑的一点点,
3268000	3270000	或者是生物的全体,
3270000	3272000	那像现在的光绘行为,
3272000	3274000	基本上可以做,
3274000	3276000	好比说很小的生物,好比说果蠅,
3276000	3278000	或者大体可能是猪脑,
3278000	3280000	猪脑可能可以做,
3280000	3282000	可能就花好几个礼拜,果蠅脑是一天两天这样子,
3282000	3284000	那假设就是说,
3284000	3286000	它假设是一样的,
3286000	3288000	它假设是全脑的事情,
3288000	3290000	而不是一块块的事情,
3290000	3292000	所以,
3292000	3294000	如果要看,
3294000	3296000	就是从,
3296000	3298000	从脑组织切片,
3298000	3300000	就这个我们有做啦,
3300000	3302000	就是从脑切片区域里面出来,
3302000	3304000	那要看,
3304000	3306000	比较high level的方面,
3306000	3308000	其实是不可能看到的,
3308000	3310000	所以今天会做的这么多AI分析,
3310000	3312000	其实目的就是为了搭配一件事情,
3312000	3314000	就是我们要很快可以处理整个脑的资讯,
3314000	3316000	那在,
3316000	3318000	那在影像技术上面的话,
3318000	3320000	电子显微镜是可以拍到
3320000	3322000	那个神经的那个,
3322000	3324000	每个神经的形状,
3324000	3326000	是可以扫得清楚的,
3326000	3328000	但像这个果蠅脑的话,
3328000	3330000	大家可以去搜那个,
3332000	3334000	Generia吧,
3334000	3336000	就美国有一个很大的这个,
3336000	3338000	这个果蠅脑的电子显微镜计划,
3340000	3342000	它扫了,
3342000	3344000	扫了两年吧,扫了半颗脑,
3344000	3346000	然后可能花了,
3346000	3348000	我不知道多少钱,
3348000	3350000	总之台湾是不可能付得起的钱,
3350000	3352000	然后这种扫描你也,
3352000	3354000	你也没办法跟,
3354000	3356000	好比说果蠅的行为,
3356000	3358000	做那个,
3358000	3360000	做比较吧,
3360000	3362000	因为扫太久了,
3362000	3364000	然后就一颗而已,
3364000	3366000	你也没办法这个,
3366000	3368000	看它行为然后做比较的样子,
3368000	3370000	那轻大他们做的事情就是,
3370000	3372000	然后他们也可以,
3372000	3374000	跟方选做对应,
3374000	3376000	因为他们可以做非常多颗,
3376000	3378000	所以他们知道这个果蠅,
3378000	3380000	他们生前有这种行为,
3380000	3382000	然后有没有记忆啊,
3382000	3384000	有没有等等等等,
3384000	3386000	所以他们有一个platform,
3386000	3388000	是可以把这些脑啊,
3388000	3390000	放大等等,
3390000	3392000	然后自动扫描下来,
3392000	3394000	然后接起来的样子,
3394000	3396000	那插了一块就是,
3396000	3398000	自动影像分析嘛,
3398000	3400000	就把这些影像放在那边了,
3400000	3402000	那第一个它是很小块的影像,
3402000	3404000	就一定你的view有限,
3404000	3406000	自动组合在一起,
3406000	3408000	然后自动,
3408000	3410000	自动,
3410000	3412000	自动接在一起,
3412000	3414000	然后还要知道说,
3414000	3416000	这些fiber长什么样子,
3416000	3418000	什么样的fiber是独立属于哪个神经,
3418000	3420000	然后最后才有可能说,
3420000	3422000	推断说这些东西怎么运作,
3422000	3424000	所以所以说没错就是,
3424000	3426000	就是这目的其实都是为了,
3426000	3428000	那大家觉得说,
3428000	3430000	这唯一solution就是需要,
3430000	3432000	需要用这些方式嘛,
3432000	3434000	那像像国外的话,
3434000	3436000	这些东西都跟,
3436000	3438000	跟Google合作等等,
3438000	3440000	那我们台湾就,
3440000	3442000	就做我们的事情,
3442000	3444000	就我们就想要把,
3444000	3446000	国外的智能智能影像,
3446000	3448000	先用AI可以分析出来,
3448000	3450000	然后那或许之后就可以跟,
3450000	3452000	function这东西搭配起来,
3452000	3454000	那不过这些都是,
3454000	3456000	这些都是生前跟生后的影像,
3456000	3458000	生前的行为跟,
3458000	3460000	跟结果后的影像对应啊,
3460000	3462000	那像你刚刚说这个live,
3462000	3464000	这个这个,
3464000	3466000	然后他做某些方面,
3466000	3468000	或者说play pump的话,
3468000	3470000	我觉得,
3470000	3472000	可能蛮有可能的,
3472000	3474000	如果成功的话,
3474000	3476000	可能对人类会很shock,
3476000	3478000	我觉得有可能,
3478000	3480000	有可能会很shock,
3480000	3482000	我真的不知道这件事情,
3482000	3484000	我常常看一些稀奇古怪的,
3484000	3486000	那个research news,
3486000	3488000	所以刚好看到这个,
3488000	3490000	oh my god,
3490000	3492000	我赶快同时讲,谢谢,
3496000	3498000	那,
3498000	3500000	这个,
3502000	3504000	我现在,
3504000	3506000	那我们今天就先到这边,
3506000	3508000	反正就是这个,
3508000	3510000	我现在要关这个,
3510000	3512000	感谢最后,
3512000	3514000	最后感谢那个张翰,
3514000	3516000	谢谢,
3516000	3518000	OK
