WEBVTT

00:00.000 --> 00:02.000
這樣能看到嗎?

00:02.000 --> 00:04.000
可以

00:04.000 --> 00:06.000
OK

00:06.000 --> 00:08.000
大家好

00:08.000 --> 00:10.000
我是莊育南

00:10.000 --> 00:12.000
可以叫我Allen

00:12.000 --> 00:14.000
我現在是

00:14.000 --> 00:16.000
Rice University的PhD學生

00:16.000 --> 00:18.000
我讀的是

00:18.000 --> 00:20.000
Computer Science

00:20.000 --> 00:22.000
我現在是二年級

00:22.000 --> 00:24.000
我在做的研究就是

00:24.000 --> 00:26.000
像是剛才那個

00:26.000 --> 00:28.000
主持人提到的

00:28.000 --> 00:30.000
Predictable Machine Learning

00:30.000 --> 00:32.000
或者是這種Explainable

00:32.000 --> 00:34.000
Artificial Intelligence

00:34.000 --> 00:36.000
目的就是在

00:36.000 --> 00:38.000
介紹

00:38.000 --> 00:40.000
這個Machine Learning Model

00:40.000 --> 00:42.000
為什麼會做出這種Prediction

00:42.000 --> 00:44.000
你要去給他一個解釋

00:44.000 --> 00:46.000
讓人家去

00:46.000 --> 00:48.000
信服你說

00:48.000 --> 00:50.000
這個Prediction Result

00:50.000 --> 00:52.000
不是亂來的

00:52.000 --> 00:54.000
OK

00:54.000 --> 00:56.000
講一下我的

00:56.000 --> 00:58.000
自我介紹

00:58.000 --> 01:00.000
我以前在台灣念碩班

01:00.000 --> 01:02.000
我以前做的是

01:02.000 --> 01:04.000
推薦系統

01:04.000 --> 01:06.000
做推薦系統相關

01:06.000 --> 01:08.000
等下Q&A如果有推薦系統相關的問題

01:08.000 --> 01:10.000
也可以問我

01:10.000 --> 01:12.000
但我會盡量回答的出來

01:12.000 --> 01:14.000
OK

01:14.000 --> 01:16.000
我以前是

01:16.000 --> 01:18.000
大學的時候念數學

01:18.000 --> 01:20.000
我大學在四年級

01:20.000 --> 01:22.000
畢業以前我是不會寫Code

01:22.000 --> 01:24.000
我連一行Code都看不懂

01:24.000 --> 01:26.000
所以

01:26.000 --> 01:28.000
在這邊鼓勵

01:28.000 --> 01:30.000
還想學Code的人

01:30.000 --> 01:32.000
不要放棄

01:32.000 --> 01:34.000
都有機會

01:34.000 --> 01:36.000
努力練練練

01:36.000 --> 01:38.000
應該是大家都有機會

01:38.000 --> 01:40.000
可以去做這件事

01:40.000 --> 01:42.000
OK

01:42.000 --> 01:44.000
那今天的話主要是

01:44.000 --> 01:46.000
OK

01:46.000 --> 01:48.000
那個

01:48.000 --> 01:50.000
我只是想要講說

01:50.000 --> 01:52.000
這開頭太勵志了

01:52.000 --> 01:54.000
反正

01:54.000 --> 01:56.000
今天我不是什麼大師

01:56.000 --> 01:58.000
我只是一個

01:58.000 --> 02:00.000
普通的二年級的PhD學生

02:00.000 --> 02:02.000
所以你如果要打斷我的話

02:02.000 --> 02:04.000
你就直接開啟你的麥克風

02:04.000 --> 02:06.000
然後把我打斷都沒關係

02:06.000 --> 02:08.000
OK

02:08.000 --> 02:10.000
那我就開始吧

02:10.000 --> 02:12.000
OK

02:12.000 --> 02:14.000
今天的Outline會比較是

02:14.000 --> 02:16.000
著重在四個點

02:16.000 --> 02:18.000
第一個就是我來介紹一下

02:18.000 --> 02:20.000
什麼是XAI

02:20.000 --> 02:22.000
什麼是可解釋機器學習

02:22.000 --> 02:24.000
然後再來就是

02:24.000 --> 02:26.000
接下來就是比較偏向

02:26.000 --> 02:28.000
我的研究的部分

02:28.000 --> 02:30.000
在可解釋機器學習裡面

02:30.000 --> 02:32.000
主要會有兩種

02:32.000 --> 02:34.000
一種是模型的本身就有可解釋性

02:34.000 --> 02:36.000
舉個例子來說

02:36.000 --> 02:38.000
大家可能有聽過二分法術

02:38.000 --> 02:40.000
就是Decision Tree

02:40.000 --> 02:42.000
這種Decision Tree的話

02:42.000 --> 02:44.000
它在做Prediction

02:44.000 --> 02:46.000
我們可以看到它的決策過程

02:46.000 --> 02:48.000
那這種就是

02:48.000 --> 02:50.000
它本身就是具有可解釋性的

02:50.000 --> 02:52.000
Machine Learning Model

02:52.000 --> 02:54.000
那另外一種是像是

02:54.000 --> 02:56.000
你們比較常聽到的神經網路

02:56.000 --> 02:58.000
然後像是

02:58.000 --> 03:00.000
Deep Neural Network

03:00.000 --> 03:02.000
就是那種Fully Connected

03:02.000 --> 03:04.000
那種就是Black Box Model

03:04.000 --> 03:06.000
那它本身就不具有可解釋性

03:06.000 --> 03:08.000
因為你不知道它裡面的那些Learning Weight

03:08.000 --> 03:10.000
就是那些學習的那些Weight

03:10.000 --> 03:12.000
到底怎麼分布

03:12.000 --> 03:14.000
或是怎麼去讓這個Model

03:14.000 --> 03:16.000
學習的這個動作

03:16.000 --> 03:18.000
所以我們就需要一個

03:18.000 --> 03:20.000
事後的解釋

03:20.000 --> 03:22.000
就是我在模型學Prediction Model

03:22.000 --> 03:24.000
學完之後

03:24.000 --> 03:26.000
我再額外去Learn一個

03:26.000 --> 03:28.000
我們叫Explainer

03:28.000 --> 03:30.000
就是專門用來解釋的模型

03:30.000 --> 03:32.000
去解釋那個Prediction Model

03:32.000 --> 03:34.000
然後這種的學習方式叫做Post-Hoc

03:34.000 --> 03:36.000
就是After Prediction Process

03:36.000 --> 03:38.000
OK

03:38.000 --> 03:40.000
然後再來就是我來講完這個之後

03:40.000 --> 03:42.000
我來講一下

03:42.000 --> 03:44.000
這個XAI目前有什麼應用

03:44.000 --> 03:46.000
大家如果不是Computer Science

03:46.000 --> 03:48.000
背景的人可能也不要

03:48.000 --> 03:50.000
就是會覺得

03:50.000 --> 03:52.000
我到底學的有什麼用

03:52.000 --> 03:54.000
我講一下現在有什麼Open Source Package

03:54.000 --> 03:56.000
就是

03:56.000 --> 03:58.000
可以做這個

03:58.000 --> 04:00.000
Explainable Model

04:00.000 --> 04:02.000
那當然現在就是大家都用Python

04:02.000 --> 04:04.000
所以我就是講了兩個Python的

04:04.000 --> 04:06.000
Open Source Package

04:06.000 --> 04:08.000
然後跟大家說其實

04:08.000 --> 04:10.000
這個東西不是想像中的遙不可及

04:10.000 --> 04:12.000
就是每個人在每個人領域

04:12.000 --> 04:14.000
當你有Prediction Model

04:14.000 --> 04:16.000
你想要做Explainable

04:16.000 --> 04:18.000
那人人都可以有機會

04:18.000 --> 04:20.000
可以踏進這個領域

04:20.000 --> 04:22.000
OK好那接下來就來

04:22.000 --> 04:24.000
講一下什麼是

04:24.000 --> 04:26.000
什麼是XAI

04:26.000 --> 04:28.000
OK那就是

04:28.000 --> 04:30.000
人工智慧在

04:30.000 --> 04:32.000
在我們日常生活中

04:32.000 --> 04:34.000
已經有很多很多應用了

04:34.000 --> 04:36.000
像是這最近很

04:36.000 --> 04:38.000
前陣子比較流行的

04:38.000 --> 04:40.000
就是DeepMind不是

04:40.000 --> 04:42.000
就是去玩這個

04:42.000 --> 04:44.000
然後他可以下贏那個

04:44.000 --> 04:46.000
人類的Expert

04:46.000 --> 04:48.000
那前陣子

04:48.000 --> 04:50.000
還有DeepMind

04:50.000 --> 04:52.000
有給了一個叫Alpha Tensor

04:52.000 --> 04:54.000
跟Alpha那個叫

04:54.000 --> 04:56.000
算蛋白質的那個東西

04:56.000 --> 04:58.000
那其實上他們都是

04:58.000 --> 05:00.000
就是那種機器人

05:00.000 --> 05:02.000
強化學

05:02.000 --> 05:04.000
強化式學習的

05:04.000 --> 05:06.000
這種應用在日常生活

05:06.000 --> 05:08.000
領域當中

05:08.000 --> 05:10.000
像是一些Medical Diagnosis

05:10.000 --> 05:12.000
就是例如說我給你一個X光片

05:12.000 --> 05:14.000
然後你希望這個

05:14.000 --> 05:16.000
機器人的Model判斷你有沒有腦瘤

05:16.000 --> 05:18.000
那他就會說

05:18.000 --> 05:20.000
有或沒有類似這種

05:20.000 --> 05:22.000
或是甚至說你給今天一個

05:22.000 --> 05:24.000
機器人的Model說你今天有

05:24.000 --> 05:26.000
咳嗽有什麼症狀

05:26.000 --> 05:28.000
然後他幫你判斷你到底得了什麼病

05:28.000 --> 05:30.000
類似這種的Medical Diagnosis

05:30.000 --> 05:32.000
或是像是

05:32.000 --> 05:34.000
Tesla的Autopilot

05:34.000 --> 05:36.000
就是這種

05:36.000 --> 05:38.000
自動駕駛

05:38.000 --> 05:40.000
他不是會Detect

05:40.000 --> 05:42.000
就是有什麼物體

05:42.000 --> 05:44.000
然後他會給你一個最佳路徑

05:44.000 --> 05:46.000
讓你這個車子去開

05:46.000 --> 05:48.000
再來就是Voice Recognition

05:48.000 --> 05:50.000
像是Alexa

05:50.000 --> 05:52.000
不知道大家有沒有用過智慧音響

05:52.000 --> 05:54.000
你只要對這個音響講的時候

05:54.000 --> 05:56.000
他就會偵測你的

05:56.000 --> 05:58.000
語音然後把這個東西

05:58.000 --> 06:00.000
轉換成

06:00.000 --> 06:02.000
文字

06:02.000 --> 06:04.000
然後透過這個文字

06:04.000 --> 06:06.000
Online Server去Request

06:06.000 --> 06:08.000
或是像最近Meta有個很紅的

06:08.000 --> 06:10.000
Speech to Speech

06:10.000 --> 06:12.000
就是把台語轉換成英文

06:12.000 --> 06:14.000
那個也很神

06:14.000 --> 06:16.000
所以就類似這種AI的

06:16.000 --> 06:18.000
應用在日常生活中越來越常見

06:18.000 --> 06:20.000
OK

06:20.000 --> 06:22.000
那但是

06:22.000 --> 06:24.000
就是我們其實很難知道

06:24.000 --> 06:26.000
就是這些模型就為什麼會去

06:26.000 --> 06:28.000
做出這些判斷像是

06:28.000 --> 06:30.000
假設你去看一個醫生

06:30.000 --> 06:32.000
然後今天醫生推了一個

06:32.000 --> 06:34.000
然後你把

06:34.000 --> 06:36.000
你的病狀全部輸入這個

06:36.000 --> 06:38.000
模型他就說

06:38.000 --> 06:40.000
你今天只是感冒你信嗎

06:40.000 --> 06:42.000
為什麼就問你為什麼會做出這個

06:42.000 --> 06:44.000
判斷或者是

06:44.000 --> 06:46.000
今天Tesla給你一個

06:46.000 --> 06:48.000
最佳的

06:48.000 --> 06:50.000
自動駕駛

06:50.000 --> 06:52.000
路線規劃

06:52.000 --> 06:54.000
他給你給出來之後

06:54.000 --> 06:56.000
你真的按照他這樣開你真的覺得

06:56.000 --> 06:58.000
他不會出事嗎

06:58.000 --> 07:00.000
這個決策過程中

07:00.000 --> 07:02.000
如果能夠提供

07:02.000 --> 07:04.000
說今天是因為

07:04.000 --> 07:06.000
這個模型學到什麼什麼特徵

07:06.000 --> 07:08.000
或是他感知到了

07:08.000 --> 07:10.000
例如說有一個行人走過去他停下來

07:10.000 --> 07:12.000
或是他覺得他detect

07:12.000 --> 07:14.000
前方是綠燈

07:14.000 --> 07:16.000
這種感知的

07:16.000 --> 07:18.000
這種為什麼的這種過程

07:18.000 --> 07:20.000
如果提供給使用者的話

07:20.000 --> 07:22.000
那他其實上可以增加

07:22.000 --> 07:24.000
人們對這個模型

07:24.000 --> 07:26.000
對prediction model這種機器學習

07:26.000 --> 07:28.000
模型的這種可信度

07:28.000 --> 07:30.000
對吧

07:30.000 --> 07:32.000
所以整個這個xai

07:32.000 --> 07:34.000
他其實就是在做一件事

07:34.000 --> 07:36.000
他希望提供給使用者一個

07:36.000 --> 07:38.000
合理的原因

07:38.000 --> 07:40.000
例如說因為這個模型

07:40.000 --> 07:42.000
學了什麼特徵

07:42.000 --> 07:44.000
或是這個模型

07:44.000 --> 07:46.000
因為看到了什麼或是這個模型

07:46.000 --> 07:48.000
因為學習了某兩三個特徵

07:48.000 --> 07:50.000
而導致我做出這個prediction

07:50.000 --> 07:52.000
的結果

07:52.000 --> 07:54.000
那xai就是在

07:54.000 --> 07:56.000
把這個過程給完善

07:56.000 --> 07:58.000
像是左邊這個

07:58.000 --> 08:00.000
假設這個autopilot我剛才講過

08:00.000 --> 08:02.000
就是我們xai要做什麼

08:02.000 --> 08:04.000
其實就是想要提供

08:04.000 --> 08:06.000
就是為什麼這個autopilot

08:06.000 --> 08:08.000
會給你的

08:08.000 --> 08:10.000
autopilot做的路徑是安全的

08:10.000 --> 08:12.000
所以你就要跟使用者講說

08:12.000 --> 08:14.000
因為我偵測到模型

08:14.000 --> 08:16.000
我的模型偵測到人

08:16.000 --> 08:18.000
我的模型偵測到前方

08:18.000 --> 08:20.000
可能50米內沒有車子

08:20.000 --> 08:22.000
那再來就是例如說

08:22.000 --> 08:24.000
像是右邊這個圖

08:24.000 --> 08:28.000
這個醫療的診斷

08:28.000 --> 08:30.000
醫療診斷它其實就是要

08:30.000 --> 08:32.000
例如說你要說服這個醫生

08:32.000 --> 08:34.000
或是說服這個病人說

08:34.000 --> 08:36.000
因為我在你

08:36.000 --> 08:38.000
腦的X光片裡面

08:38.000 --> 08:40.000
看到某一塊是腦瘤

08:40.000 --> 08:42.000
我把它框起來

08:42.000 --> 08:44.000
所以我判斷你的腦瘤

08:44.000 --> 08:46.000
這個動作其實上

08:46.000 --> 08:48.000
看起來就是沒有這麼的

08:48.000 --> 08:50.000
在整個prediction的過程中

08:50.000 --> 08:52.000
看起來沒有這麼的

08:52.000 --> 08:54.000
這麼有需求

08:54.000 --> 08:56.000
但是如果你提供給使用者

08:56.000 --> 08:58.000
這個步驟的話

08:58.000 --> 09:00.000
就是為什麼這個步驟的話

09:00.000 --> 09:02.000
使用者其實會對於你的決策模型

09:02.000 --> 09:04.000
感到就是非常的

09:04.000 --> 09:06.000
例如說他可以放下心來去

09:06.000 --> 09:08.000
相信這個決策

09:08.000 --> 09:10.000
當然還有很多我沒有提到的應用

09:10.000 --> 09:12.000
像是例如做股市交易

09:12.000 --> 09:14.000
或是什麼的

09:14.000 --> 09:16.000
你今天股市說你就買50萬的Meta股票

09:16.000 --> 09:18.000
類似這樣

09:18.000 --> 09:20.000
Meta現在已經跌到

09:20.000 --> 09:22.000
骨折了

09:22.000 --> 09:24.000
他叫你賣掉

09:24.000 --> 09:26.000
你說為什麼他不會跌超過

09:26.000 --> 09:28.000
低於100塊

09:28.000 --> 09:30.000
這模型如果預測錯了

09:30.000 --> 09:32.000
他叫你趕快賣掉

09:32.000 --> 09:34.000
他說因為股票可能跌超過100塊

09:34.000 --> 09:36.000
類似這種東西

09:36.000 --> 09:38.000
你總要說服買賣的trader

09:38.000 --> 09:40.000
說因為我看到

09:40.000 --> 09:42.000
他的財報全部都不達標

09:42.000 --> 09:44.000
什麼什麼貴的

09:44.000 --> 09:46.000
這種檢測到特徵

09:46.000 --> 09:48.000
你才能說服使用者說

09:48.000 --> 09:50.000
這模型是可信的

09:50.000 --> 09:52.000
所以

09:52.000 --> 09:54.000
這種東西其實

09:54.000 --> 09:56.000
牽扯到幾個點

09:56.000 --> 09:58.000
第一個是我們剛才講到你要說服使用者

09:58.000 --> 10:00.000
然後再來就是你要提供安全

10:00.000 --> 10:02.000
還有一個很重要的東西就是

10:02.000 --> 10:04.000
就是一些法規的東西

10:04.000 --> 10:06.000
像是GDPR

10:06.000 --> 10:08.000
像是Facebook在前陣子

10:08.000 --> 10:10.000
就是Zack Scruber

10:10.000 --> 10:12.000
被叫去歐洲議會罵

10:12.000 --> 10:14.000
或是Google CEO

10:14.000 --> 10:16.000
被叫去美國的

10:16.000 --> 10:18.000
眾議院被叫去罵

10:18.000 --> 10:20.000
其實就是在說

10:20.000 --> 10:22.000
你們使用的

10:22.000 --> 10:24.000
這些privacy data

10:24.000 --> 10:26.000
或是你們使用的個人資料

10:26.000 --> 10:28.000
這些資料的使用

10:28.000 --> 10:30.000
你沒有提供給使用者

10:30.000 --> 10:32.000
一個合理或是讓他

10:32.000 --> 10:34.000
覺得安全幸福的這種

10:34.000 --> 10:36.000
使用方法的準則

10:36.000 --> 10:38.000
你不能就是無條件無上限

10:38.000 --> 10:40.000
去使用這些東西

10:40.000 --> 10:42.000
這些feature當作訓練模型的一個依據

10:42.000 --> 10:44.000
所以如果你今天能透過

10:44.000 --> 10:46.000
XAI的這些方法

10:46.000 --> 10:48.000
提供給這些使用者

10:48.000 --> 10:50.000
我今天是因為用了你

10:50.000 --> 10:52.000
這些feature

10:52.000 --> 10:54.000
然後導致我提供給你

10:54.000 --> 10:56.000
這些個人化的一些

10:56.000 --> 10:58.000
不管是推薦結果或是預測結果

10:58.000 --> 11:00.000
那其實使用者他在

11:00.000 --> 11:02.000
某種程度上他會比較安心說

11:02.000 --> 11:04.000
原來你是使用過這些東西

11:04.000 --> 11:06.000
縱使他是隱私資料

11:06.000 --> 11:08.000
但是總比就是完全關起門來

11:08.000 --> 11:10.000
然後做事然後就跟你講結果

11:10.000 --> 11:12.000
好得非常多

11:12.000 --> 11:14.000
這是XAI為什麼

11:14.000 --> 11:16.000
我不敢說這一兩年

11:16.000 --> 11:18.000
或是這三四年

11:18.000 --> 11:20.000
越來越多人關注的一個

11:20.000 --> 11:22.000
原因

11:22.000 --> 11:24.000
好

11:24.000 --> 11:26.000
再來就是

11:26.000 --> 11:28.000
剛才那些都是一些

11:28.000 --> 11:30.000
比較

11:30.000 --> 11:32.000
high level層面

11:32.000 --> 11:34.000
那我們現在就講一些

11:34.000 --> 11:36.000
比較

11:36.000 --> 11:38.000
科技層面的東西

11:38.000 --> 11:40.000
反正就是如果我們現在在做這種

11:40.000 --> 11:42.000
image classification

11:42.000 --> 11:44.000
就是我們這個圖片分類

11:44.000 --> 11:46.000
我們在判斷這個模型好不好

11:46.000 --> 11:48.000
就是我們單純做實驗我們要

11:48.000 --> 11:50.000
train一個我們要訓練一個

11:50.000 --> 11:52.000
圖片分類的模型

11:52.000 --> 11:54.000
這個圖片分類模型我們總不能

11:54.000 --> 11:56.000
說

11:56.000 --> 11:58.000
當然可以說就是prediction

11:58.000 --> 12:00.000
就是例如說我們去看它說

12:00.000 --> 12:02.000
預測準不準確就例如說看它

12:02.000 --> 12:04.000
accuracy好不好啊

12:04.000 --> 12:06.000
我有沒有成功的預測到屬於這樣圖的

12:06.000 --> 12:08.000
當然是可以這樣做

12:08.000 --> 12:10.000
你可以其實可以想一件事就是

12:10.000 --> 12:12.000
今天這張圖是一隻青蛙

12:12.000 --> 12:14.000
那

12:14.000 --> 12:16.000
今天這張圖是一隻青蛙那今天這個模型

12:16.000 --> 12:18.000
是真的學到青蛙的特徵嗎

12:18.000 --> 12:20.000
也就是說換句話說就是

12:20.000 --> 12:22.000
他真的學到青蛙這個頭嗎

12:22.000 --> 12:24.000
就是這個image classification

12:24.000 --> 12:26.000
這個模型真的學到這個青蛙

12:26.000 --> 12:28.000
沒有人知道他有可能是學

12:28.000 --> 12:30.000
假設後面有一個

12:30.000 --> 12:32.000
一個水塘然後上面有一個荷葉

12:32.000 --> 12:34.000
然後他就覺得

12:34.000 --> 12:36.000
大部分的青蛙圖片都是因為青蛙

12:36.000 --> 12:38.000
在水塘跟荷葉上面

12:38.000 --> 12:40.000
所以他就反而去抓那個水塘

12:40.000 --> 12:42.000
跟荷葉的特徵

12:42.000 --> 12:44.000
那這件事情不是我們樂見的為什麼

12:44.000 --> 12:46.000
那當今天有一個像是這種圖片的

12:46.000 --> 12:48.000
來那學到水塘跟青蛙

12:48.000 --> 12:50.000
的這種的圖片

12:50.000 --> 12:52.000
分類的這種的model

12:52.000 --> 12:54.000
是不是就會predict錯誤

12:54.000 --> 12:56.000
所以這個東西其實是我們不樂見

12:56.000 --> 12:58.000
所以透過這種XAI的方法

12:58.000 --> 13:00.000
就是我們去看說到底哪一個

13:00.000 --> 13:02.000
哪一塊哪一塊重要

13:02.000 --> 13:04.000
那最後如果我們得到我們最後的解釋是

13:04.000 --> 13:06.000
因為

13:06.000 --> 13:08.000
他判斷這張圖是

13:08.000 --> 13:10.000
青蛙的原因是因為

13:10.000 --> 13:12.000
他有這個很具代表性的

13:12.000 --> 13:14.000
這個頭那我們就知道

13:14.000 --> 13:16.000
這個模型其實是

13:16.000 --> 13:18.000
學往對的方向而不是

13:18.000 --> 13:20.000
在抓一些背景的漏洞

13:20.000 --> 13:22.000
或是一些shortcut

13:22.000 --> 13:24.000
就是一些捷徑

13:24.000 --> 13:26.000
捷徑的feature來讓我們的

13:26.000 --> 13:28.000
image classification的accuracy上升

13:28.000 --> 13:30.000
那或是像是在

13:30.000 --> 13:32.000
medical diagnosis就例如說

13:32.000 --> 13:34.000
我們就假設

13:34.000 --> 13:36.000
假設今天有一個檢測

13:36.000 --> 13:38.000
你是不是得covid的classifier

13:38.000 --> 13:40.000
那大家都知道

13:40.000 --> 13:42.000
covid就是很大

13:42.000 --> 13:44.000
很大的機率就會例如

13:44.000 --> 13:46.000
發燒喉嚨痛那今天假設

13:46.000 --> 13:48.000
你只是感冒

13:48.000 --> 13:50.000
那你只要輸入發燒跟

13:50.000 --> 13:52.000
喉嚨痛這兩個症狀

13:52.000 --> 13:54.000
進去他就說你的covid

13:54.000 --> 13:56.000
那你就是慌了就為什麼

13:56.000 --> 13:58.000
所以這時候如果我們要去

13:58.000 --> 14:00.000
判斷說今天這個

14:00.000 --> 14:02.000
的這個

14:02.000 --> 14:04.000
是不是真的學到我們要

14:04.000 --> 14:06.000
學的特徵就例如說我們

14:06.000 --> 14:08.000
也提供了一些肺部X光照片

14:08.000 --> 14:10.000
啊什麼的

14:10.000 --> 14:12.000
他就會去說因為他做出這個covid診斷

14:12.000 --> 14:14.000
並不單單只是

14:14.000 --> 14:16.000
透過你有

14:16.000 --> 14:18.000
喉嚨痛跟發燒這兩個特徵

14:18.000 --> 14:20.000
而得出你有的covid這個結論

14:20.000 --> 14:22.000
所以我們今天

14:22.000 --> 14:24.000
如果在

14:24.000 --> 14:26.000
提供這種prediction result給

14:26.000 --> 14:28.000
使用者的時候我們附帶上

14:28.000 --> 14:30.000
我們因為detect到

14:30.000 --> 14:32.000
你有什麼什麼特徵

14:32.000 --> 14:34.000
使用者在相信

14:34.000 --> 14:36.000
這一種prediction model

14:36.000 --> 14:38.000
就會更加的幸福

14:38.000 --> 14:40.000
好

14:40.000 --> 14:42.000
那我喝個水

14:44.000 --> 14:46.000
所以

14:46.000 --> 14:48.000
總結來說就是AI跟XAI

14:48.000 --> 14:50.000
他其實上他是一個相輔相成

14:50.000 --> 14:52.000
的一個循環的

14:52.000 --> 14:54.000
循環的一個過程

14:54.000 --> 14:56.000
如果今天沒有prediction model

14:56.000 --> 14:58.000
那我們今天如果沒有AI的預測

14:58.000 --> 15:00.000
那我們根本就不需要去解釋

15:00.000 --> 15:02.000
這個東西所以XAI就不會

15:02.000 --> 15:04.000
不會出現

15:04.000 --> 15:06.000
所以我們今天XAI其實

15:06.000 --> 15:08.000
主要是透過

15:08.000 --> 15:10.000
我們另外訓練一個

15:10.000 --> 15:12.000
模型或是模型這個prediction模型

15:12.000 --> 15:14.000
本身就具有可解釋的特性

15:14.000 --> 15:16.000
然後來達到這個

15:16.000 --> 15:18.000
提供解釋的這個動作

15:18.000 --> 15:20.000
所以你可以看

15:20.000 --> 15:22.000
這邊有一張柴犬

15:22.000 --> 15:24.000
然後這邊有一個prediction model

15:24.000 --> 15:26.000
他是一隻柴犬

15:26.000 --> 15:28.000
那為什麼

15:28.000 --> 15:30.000
有可能是這個柴犬趴在地上

15:30.000 --> 15:32.000
因為大部分的狗都趴在地上

15:32.000 --> 15:34.000
我學到這邊比較

15:34.000 --> 15:36.000
偏旁的這些特徵

15:36.000 --> 15:38.000
導致他判斷他是柴犬

15:38.000 --> 15:40.000
也有可能或是因為他

15:40.000 --> 15:42.000
只看到他是橘色的毛色

15:42.000 --> 15:44.000
就說他是柴犬

15:44.000 --> 15:46.000
類似這種我們不知道

15:46.000 --> 15:48.000
因為他是一個黑盒子的模型

15:48.000 --> 15:50.000
就是我們所謂的black box model

15:50.000 --> 15:52.000
所以我們需要另外一個

15:52.000 --> 15:54.000
XAI的這種model

15:54.000 --> 15:56.000
去把這個黑盒打開

15:56.000 --> 15:58.000
把這個black box給打開

15:58.000 --> 16:00.000
然後我們就看到說為什麼他給出

16:00.000 --> 16:02.000
這個圖片是一隻柴犬

16:02.000 --> 16:04.000
然後因為他看到柴犬細細的眼睛

16:04.000 --> 16:06.000
跟柴犬特有的耳朵

16:06.000 --> 16:08.000
那因為這個特徵的

16:08.000 --> 16:10.000
給予

16:10.000 --> 16:12.000
的學習所以我們

16:12.000 --> 16:14.000
讓這個圖片被判斷為柴犬

16:14.000 --> 16:16.000
那這時候這個prediction model

16:16.000 --> 16:18.000
就非常的可信了

16:18.000 --> 16:20.000
因為我們知道他學習到

16:20.000 --> 16:22.000
正確的方向

16:22.000 --> 16:24.000
而不是他去偷懶

16:24.000 --> 16:26.000
去偷懶

16:26.000 --> 16:28.000
學習別的特徵造成這個prediction的結果

16:28.000 --> 16:30.000
所以這個模型

16:30.000 --> 16:32.000
prediction model會是比較一個可信的

16:32.000 --> 16:34.000
一個程度

16:34.000 --> 16:36.000
ok

16:36.000 --> 16:38.000
那剛才那個就是

16:38.000 --> 16:40.000
一個XAI的

16:40.000 --> 16:42.000
大概念的一個解釋

16:42.000 --> 16:44.000
那我們現在來

16:44.000 --> 16:46.000
比較深入一點

16:46.000 --> 16:48.000
因為我不確定這邊是不是全部都是

16:48.000 --> 16:50.000
CS background的人

16:50.000 --> 16:52.000
的同學或是老師

16:52.000 --> 16:54.000
或是一些博士

16:54.000 --> 16:56.000
碩士生所以我今天就是

16:56.000 --> 16:58.000
會給一個比較high level

16:58.000 --> 17:00.000
的一個

17:00.000 --> 17:02.000
talk那我們現在就是

17:02.000 --> 17:04.000
深入了解就是

17:04.000 --> 17:06.000
這種XAI有什麼

17:06.000 --> 17:08.000
現行有什麼

17:08.000 --> 17:10.000
sota technique

17:10.000 --> 17:12.000
就是可以去執行這個

17:12.000 --> 17:14.000
explanation的動作

17:14.000 --> 17:16.000
ok

17:16.000 --> 17:18.000
那在講這個之前我們先

17:18.000 --> 17:20.000
可以大概看一下

17:20.000 --> 17:22.000
幾個不同的scope

17:22.000 --> 17:24.000
那這邊有分成global跟local

17:24.000 --> 17:26.000
我其實就大概講一下

17:26.000 --> 17:28.000
這個global其實就是

17:28.000 --> 17:30.000
我們在看模型

17:30.000 --> 17:32.000
假設今天這個image classifier

17:32.000 --> 17:34.000
這個classifier

17:34.000 --> 17:36.000
in total

17:36.000 --> 17:38.000
就是這個模型在乎什麼特徵

17:38.000 --> 17:40.000
也就是說我們今天如果都

17:40.000 --> 17:42.000
都例如在train一個

17:42.000 --> 17:44.000
假設狗的品種分類

17:44.000 --> 17:46.000
的這種圖片分類

17:46.000 --> 17:48.000
的model

17:48.000 --> 17:50.000
那我們可以就是

17:50.000 --> 17:52.000
想要知道這個模型到底在乎什麼

17:52.000 --> 17:54.000
例如說狗的很多不同品種

17:54.000 --> 17:56.000
耳朵都不一樣所以耳朵就是

17:56.000 --> 17:58.000
這個模型在乎的一個特徵

17:58.000 --> 18:00.000
這就是global的

18:00.000 --> 18:02.000
scope那什麼是local呢

18:02.000 --> 18:04.000
local比較偏向是

18:04.000 --> 18:06.000
我們去解釋每一張

18:06.000 --> 18:08.000
圖片他在乎什麼

18:08.000 --> 18:10.000
像是我舉個例子

18:10.000 --> 18:12.000
像剛剛那張柴犬

18:12.000 --> 18:14.000
我們要做global的話我們就是

18:14.000 --> 18:16.000
必須要去看上面的classification

18:16.000 --> 18:18.000
model他在乎的

18:18.000 --> 18:20.000
在所有的狗裡面他在乎的什麼特徵

18:20.000 --> 18:22.000
但是如果在local scope下面的話

18:22.000 --> 18:24.000
我們就可以我們就必須一張一張

18:24.000 --> 18:26.000
圖片去看說

18:26.000 --> 18:28.000
這張圖片他是因為

18:28.000 --> 18:30.000
柴犬的臉那下一張圖片

18:30.000 --> 18:32.000
可能因為什麼邊境牧羊犬

18:32.000 --> 18:34.000
的尾巴或是邊境牧羊犬的

18:34.000 --> 18:36.000
耳朵這種所以在

18:36.000 --> 18:38.000
local scope底下每一張圖片

18:38.000 --> 18:40.000
都會具有每一張圖片的解釋

18:40.000 --> 18:42.000
那這其實有各有各的好處就是

18:42.000 --> 18:44.000
我們當今天如果要去宏觀的看說

18:44.000 --> 18:46.000
這個模型可不可信那我們當然就

18:46.000 --> 18:48.000
使用global就可以了嘛

18:48.000 --> 18:50.000
我們就要去提供給使用者說

18:50.000 --> 18:52.000
in total來說我們這個

18:52.000 --> 18:54.000
autopilot for example就是

18:54.000 --> 18:56.000
in total來說我們這個假設autopilot

18:56.000 --> 18:58.000
這個模型因為他

18:58.000 --> 19:00.000
會去

19:00.000 --> 19:02.000
detect紅綠燈

19:02.000 --> 19:04.000
有沒有亮或是有沒有行人或是路上

19:04.000 --> 19:06.000
有沒有障礙物這種in total的

19:06.000 --> 19:08.000
方式我們去說服使用者那我們當然

19:08.000 --> 19:10.000
就使用global那local的話

19:10.000 --> 19:12.000
就是我們說在現在這個

19:12.000 --> 19:14.000
場景裡面就這一秒

19:14.000 --> 19:16.000
那現在這一秒我們這個

19:16.000 --> 19:18.000
sensor detect到的這個街景

19:18.000 --> 19:20.000
那因為他有什麼什麼

19:20.000 --> 19:22.000
下一秒又可以提供到什麼

19:22.000 --> 19:24.000
所以當我們今天要去

19:24.000 --> 19:26.000
去分析每一秒的

19:26.000 --> 19:28.000
這種街景圖的時候

19:28.000 --> 19:30.000
那我們就必須使用local

19:30.000 --> 19:32.000
的模型那local模型就會

19:32.000 --> 19:34.000
提供比較

19:34.000 --> 19:36.000
算是比較客製化的一個

19:36.000 --> 19:38.000
解釋的結果給使用者

19:38.000 --> 19:40.000
ok那這個是

19:40.000 --> 19:42.000
global跟local那再來旁邊兩個

19:42.000 --> 19:44.000
有不同的manners就是一個是intrinsic

19:44.000 --> 19:46.000
一個是prototype那intrinsic

19:46.000 --> 19:48.000
就是剛才有提到說模型本身

19:48.000 --> 19:50.000
就具有可解釋性那什麼

19:50.000 --> 19:52.000
要具有可解釋性本身就具有可解釋性

19:52.000 --> 19:54.000
像這個決策數

19:54.000 --> 19:56.000
decision tree就是模型最後說

19:56.000 --> 19:58.000
喔我今天要預測

19:58.000 --> 20:00.000
他的結果是c

20:00.000 --> 20:02.000
那因為我們就可以從這個c

20:02.000 --> 20:04.000
往後走是假設他因為

20:04.000 --> 20:06.000
這個走到這個走到這個例如

20:06.000 --> 20:08.000
這樣子所以我們就可以知道模型在做

20:08.000 --> 20:10.000
決策的時候他是因為經過了

20:10.000 --> 20:12.000
什麼條件然後來達到這個

20:12.000 --> 20:14.000
最後prediction的結果

20:14.000 --> 20:16.000
那這個模型本身

20:16.000 --> 20:18.000
就是具有可解釋性因為

20:18.000 --> 20:20.000
我們可以從模型的決策過程中

20:20.000 --> 20:22.000
知道他做了

20:22.000 --> 20:24.000
截取了什麼特徵或是做了什麼

20:24.000 --> 20:26.000
做了什麼condition的選擇

20:26.000 --> 20:28.000
ok這個就是intrinsic

20:28.000 --> 20:30.000
model或是像右邊這個就是

20:30.000 --> 20:32.000
如果大家有稍微聽

20:32.000 --> 20:34.000
過一些machine learning的講座

20:34.000 --> 20:36.000
或是課程的話應該就會知道

20:36.000 --> 20:38.000
BERT這個東西那BERT這個東西

20:38.000 --> 20:40.000
他裡面字跟字在模型訓練裡面

20:40.000 --> 20:42.000
字會有一個

20:42.000 --> 20:44.000
東西叫做tension score

20:44.000 --> 20:46.000
所謂的tension score就是例如說

20:46.000 --> 20:48.000
我今天把一個句子

20:48.000 --> 20:50.000
丟到這個BERT model裡面

20:50.000 --> 20:52.000
那我每一個字對於模型最後

20:52.000 --> 20:54.000
預測出假設我要做semantic

20:54.000 --> 20:56.000
prediction我預測這句話是

20:56.000 --> 20:58.000
正向或是負向

20:58.000 --> 21:00.000
的情緒那

21:00.000 --> 21:02.000
我每一個字對於這個正向或負向

21:02.000 --> 21:04.000
情緒的貢獻程度

21:04.000 --> 21:06.000
都有加有減就是例如說

21:06.000 --> 21:08.000
I love this morning那love對於

21:08.000 --> 21:10.000
他最後判斷出是正向

21:10.000 --> 21:12.000
絕對有很大的幫助因為他love這個字

21:12.000 --> 21:14.000
本身就是一個正向的

21:14.000 --> 21:16.000
動詞那

21:16.000 --> 21:18.000
這個

21:18.000 --> 21:20.000
很大貢獻這個分數

21:20.000 --> 21:22.000
我們可以理解為

21:22.000 --> 21:24.000
model的tension weight

21:24.000 --> 21:26.000
就是這邊比較high level

21:26.000 --> 21:28.000
所以我們可以透過這些字跟字之間

21:28.000 --> 21:30.000
不同的重要的這種對於

21:30.000 --> 21:32.000
model prediction不同的重要程度

21:32.000 --> 21:34.000
來去判斷說

21:34.000 --> 21:36.000
今天這句話

21:36.000 --> 21:38.000
到底為什麼

21:38.000 --> 21:40.000
會做出他是

21:40.000 --> 21:42.000
正向的這個prediction result

21:42.000 --> 21:44.000
透過

21:44.000 --> 21:46.000
去拿裡面的

21:46.000 --> 21:48.000
important score來最後

21:48.000 --> 21:50.000
做prediction的結果

21:50.000 --> 21:52.000
那這個我剛才講的這兩個模型

21:52.000 --> 21:54.000
都是intrinsic

21:54.000 --> 21:56.000
就是模型本身是具有prediction模型

21:56.000 --> 21:58.000
模型本身是具有可理解之心的

21:58.000 --> 22:00.000
這個範疇那再來就是post hoc

22:00.000 --> 22:02.000
那post hoc我覺得

22:02.000 --> 22:04.000
我等一下可以講一下為什麼我比較focus

22:04.000 --> 22:06.000
在這個點上

22:06.000 --> 22:08.000
不是前面不好是因為我覺得

22:08.000 --> 22:10.000
現在目前大部分就是

22:10.000 --> 22:12.000
machine learning model都是用一些比較黑盒子的

22:12.000 --> 22:14.000
模型像是我可能就隨便

22:14.000 --> 22:16.000
建一個深度學習的

22:16.000 --> 22:18.000
網路

22:18.000 --> 22:20.000
就開始疊一些積木啊

22:20.000 --> 22:22.000
疊一些什麼全連接層

22:22.000 --> 22:24.000
什麼的convolution什麼的疊疊疊

22:24.000 --> 22:26.000
那這些

22:26.000 --> 22:28.000
這些prediction model縱使他有很好的

22:28.000 --> 22:30.000
結果但是他本身

22:30.000 --> 22:32.000
因為我不知道他裡面幹了什麼事

22:32.000 --> 22:34.000
就是例如說我不知道他

22:34.000 --> 22:36.000
他去學了什麼feature什麼

22:36.000 --> 22:38.000
所以這種比較

22:38.000 --> 22:40.000
黑盒的學習就需要

22:40.000 --> 22:42.000
這種post hoc的解釋

22:42.000 --> 22:44.000
就是我需要在模型訓練

22:44.000 --> 22:46.000
預測模型訓練完後再提供

22:46.000 --> 22:48.000
一個解釋專門用來解釋的

22:48.000 --> 22:50.000
模型來去解釋

22:50.000 --> 22:52.000
前面那個預測的模型

22:52.000 --> 22:54.000
所以這種模型

22:54.000 --> 22:56.000
在日常生活的這種

22:56.000 --> 22:58.000
scenario下是

22:58.000 --> 23:00.000
比較常遇到的就舉個例子

23:00.000 --> 23:02.000
像是例如說我們做推薦系統

23:02.000 --> 23:04.000
我們做推薦系統可能會用一些

23:04.000 --> 23:06.000
神經網路的

23:06.000 --> 23:08.000
預測模型來做訓練

23:08.000 --> 23:10.000
那這時候

23:10.000 --> 23:12.000
他神經網路的這個推薦系統

23:12.000 --> 23:14.000
本身就不具有可解釋性

23:14.000 --> 23:16.000
對吧所以我們就需要一個

23:16.000 --> 23:18.000
事後的解釋性模型

23:18.000 --> 23:20.000
來去解釋那個推薦的

23:20.000 --> 23:22.000
預測模型

23:22.000 --> 23:24.000
那這邊一樣有提供

23:24.000 --> 23:26.000
這個事後解釋模型一樣有兩個

23:26.000 --> 23:28.000
scope一個就是global一個是local

23:28.000 --> 23:30.000
那global的話就一樣就是

23:30.000 --> 23:32.000
我去看說這個神經網路

23:32.000 --> 23:34.000
預測模型呢他到底

23:34.000 --> 23:36.000
這個模型本身注重了什麼

23:36.000 --> 23:38.000
大方向的feature

23:38.000 --> 23:40.000
那local的話我就去說

23:40.000 --> 23:42.000
這個神經網路對於每一個

23:42.000 --> 23:44.000
使用者來說因為

23:44.000 --> 23:46.000
我看到使用者A

23:46.000 --> 23:48.000
他的年齡過

23:48.000 --> 23:50.000
18歲然後例如說

23:50.000 --> 23:52.000
他是男性然後例如說

23:52.000 --> 23:54.000
他是例如說他居住在

23:54.000 --> 23:56.000
亞洲那居住在

23:56.000 --> 23:58.000
亞洲呢他會說中文

23:58.000 --> 24:00.000
他會說台語所以

24:00.000 --> 24:02.000
所以我推那個高五人的歌給他

24:02.000 --> 24:04.000
對就是每一個人都有不同的

24:04.000 --> 24:06.000
特徵所以我根據每一個人去提供

24:06.000 --> 24:08.000
不同的模型解釋這個就是在

24:08.000 --> 24:10.000
local scope在做的事情

24:10.000 --> 24:12.000
ok好那

24:12.000 --> 24:14.000
所以in conclusion就是

24:14.000 --> 24:16.000
今天推

24:16.000 --> 24:18.000
比較主流的分成這兩種

24:18.000 --> 24:20.000
intrinsic跟post hoc

24:20.000 --> 24:22.000
對然後

24:22.000 --> 24:24.000
我今天會比較著重在討論

24:24.000 --> 24:26.000
這個post hoc的explanation

24:26.000 --> 24:28.000
然後再更進一步

24:28.000 --> 24:30.000
就是我會比較著重在local的部分

24:30.000 --> 24:32.000
ok因為local比較是

24:32.000 --> 24:34.000
屬於更日常生活中

24:34.000 --> 24:36.000
會碰到的像我剛剛舉的

24:36.000 --> 24:38.000
推薦系統的場景

24:38.000 --> 24:40.000
ok

24:40.000 --> 24:42.000
好那

24:42.000 --> 24:44.000
稍微來講一下

24:44.000 --> 24:46.000
intrinsic local

24:46.000 --> 24:48.000
那就是這邊有一個

24:48.000 --> 24:50.000
這邊有一個圖片

24:50.000 --> 24:52.000
然後這是一篇文章

24:52.000 --> 24:54.000
那例如說

24:54.000 --> 24:56.000
我今天說欸這篇文章很重要

24:56.000 --> 24:58.000
為什麼例如說

24:58.000 --> 25:00.000
因為他detect到這個什麼ent23

25:00.000 --> 25:02.000
或是他detect到

25:02.000 --> 25:04.000
什麼natural

25:04.000 --> 25:06.000
tenacity

25:06.000 --> 25:08.000
這種字眼所以

25:08.000 --> 25:10.000
透過這種

25:10.000 --> 25:12.000
我說哪一個字哪一個字比較重要的

25:12.000 --> 25:14.000
這個過程

25:14.000 --> 25:16.000
我提供給這個使用者

25:16.000 --> 25:18.000
使用者說欸我今天為什麼

25:18.000 --> 25:20.000
拿這篇文章做出以下的預測

25:20.000 --> 25:22.000
是因為我在

25:22.000 --> 25:24.000
我的模型吃了這個ent23

25:24.000 --> 25:26.000
今天我模型吃了這種

25:26.000 --> 25:28.000
比較具有情緒的字眼

25:28.000 --> 25:30.000
這種的過程

25:30.000 --> 25:32.000
然後讓使用者更加信服

25:32.000 --> 25:34.000
說我的這種文字

25:34.000 --> 25:36.000
預測模型預測出來的

25:36.000 --> 25:38.000
結果是可以信服的

25:38.000 --> 25:40.000
ok這就是intrinsic

25:40.000 --> 25:42.000
本身的

25:42.000 --> 25:44.000
intrinsic本身要提供的

25:44.000 --> 25:46.000
的東西

25:46.000 --> 25:48.000
那再來就是

25:48.000 --> 25:50.000
intrinsic global

25:50.000 --> 25:52.000
我剛說global主要是

25:52.000 --> 25:54.000
在模型本身注重什麼

25:54.000 --> 25:56.000
所以例如說我們今天設計一個

25:56.000 --> 25:58.000
可解釋的

25:58.000 --> 26:00.000
CNN就是

26:00.000 --> 26:02.000
是屬於image classification

26:02.000 --> 26:04.000
的其中一個比較具有代表性的

26:04.000 --> 26:06.000
的模型

26:06.000 --> 26:08.000
那我今天就是要去說

26:08.000 --> 26:10.000
為什麼這個CNN會成功預測出

26:10.000 --> 26:12.000
這個貓是

26:12.000 --> 26:14.000
這個貓的圖片是一隻貓

26:14.000 --> 26:16.000
那是因為它detect到

26:16.000 --> 26:18.000
這個貓的頭

26:18.000 --> 26:20.000
那如果我們從CNN裡面

26:20.000 --> 26:22.000
把裡面抽取出一層

26:22.000 --> 26:24.000
例如說其中一層model weight

26:24.000 --> 26:26.000
那這個model weight我把它蓋在圖片上

26:26.000 --> 26:28.000
我們是不是可以去看說

26:28.000 --> 26:30.000
因為這個模型去

26:30.000 --> 26:32.000
對其中某些feature做convolution

26:32.000 --> 26:34.000
得到了某些

26:34.000 --> 26:36.000
特定的feature

26:36.000 --> 26:38.000
是不是這個CNN的模型去做出

26:38.000 --> 26:40.000
它是貓的prediction的這個結果

26:40.000 --> 26:42.000
那從模型中

26:42.000 --> 26:44.000
抽出這個mask

26:44.000 --> 26:46.000
或是抽出這個feature map

26:46.000 --> 26:48.000
然後來讓這個

26:48.000 --> 26:50.000
最後的結果是

26:50.000 --> 26:52.000
可以讓使用者來看說

26:52.000 --> 26:54.000
其實這個

26:54.000 --> 26:56.000
最後的預測結果

26:56.000 --> 26:58.000
是可以信服的

26:58.000 --> 27:00.000
這個過程它其實就是

27:00.000 --> 27:02.000
我們所謂的intrinsic的過程

27:02.000 --> 27:04.000
那所謂intrinsic就是

27:04.000 --> 27:06.000
模型本身就具有

27:06.000 --> 27:08.000
可以製造解釋的這個功能

27:08.000 --> 27:10.000
或是模型本身就具有

27:10.000 --> 27:12.000
可以提煉出某些component

27:12.000 --> 27:14.000
然後進而提供使用者解釋的

27:14.000 --> 27:16.000
這個

27:16.000 --> 27:18.000
過程我們就叫做

27:18.000 --> 27:20.000
intrinsic的過程

27:20.000 --> 27:22.000
ok

27:22.000 --> 27:24.000
那另外一個就是

27:24.000 --> 27:26.000
我們今天會比較著重的post hoc

27:26.000 --> 27:28.000
post hoc的explanation

27:28.000 --> 27:30.000
那post hoc explanation

27:30.000 --> 27:32.000
它主要是分成兩個步驟

27:32.000 --> 27:34.000
前面就會是我們的

27:34.000 --> 27:36.000
black box model

27:36.000 --> 27:38.000
所謂的black box model就是我們的prediction model

27:38.000 --> 27:40.000
我們會有data

27:40.000 --> 27:42.000
我們會有model去訓練

27:42.000 --> 27:44.000
那接下來我們會有一個explainer

27:44.000 --> 27:46.000
就是我們的所謂的拿來解釋

27:46.000 --> 27:48.000
這個prediction model

27:48.000 --> 27:50.000
的machine learning model

27:50.000 --> 27:52.000
ok

27:52.000 --> 27:54.000
那再來我們會需要一些metric

27:54.000 --> 27:56.000
就是需要一些評估準則

27:56.000 --> 27:58.000
來去看說

27:58.000 --> 28:00.000
這個explainer到底學得好不好

28:00.000 --> 28:02.000
並不是說我今天給你一個

28:02.000 --> 28:04.000
explainer你就要五條天去相信它

28:04.000 --> 28:06.000
所以我們當然可以透過

28:06.000 --> 28:08.000
很多很多不同種的

28:08.000 --> 28:10.000
metric來去判斷它好不好

28:10.000 --> 28:12.000
那其中最有名的就是

28:12.000 --> 28:14.000
屬於這個sharp e-value

28:14.000 --> 28:16.000
如果這邊有經濟學背景的

28:16.000 --> 28:18.000
或是曾經有學過經濟學的

28:18.000 --> 28:20.000
人呢

28:20.000 --> 28:22.000
可能會對這個字滿熟悉

28:22.000 --> 28:24.000
它就是同一個東西

28:24.000 --> 28:26.000
其實就只是要去算說

28:26.000 --> 28:28.000
每一個feature

28:28.000 --> 28:30.000
對於這個prediction model

28:30.000 --> 28:32.000
的貢獻程度是多少

28:32.000 --> 28:34.000
我們藉由這個

28:34.000 --> 28:36.000
game theory這個理論裡面的

28:36.000 --> 28:38.000
sharp e-value

28:38.000 --> 28:40.000
來製造我們的

28:40.000 --> 28:42.000
所謂的正確答案然後去做evaluation

28:42.000 --> 28:44.000
對

28:44.000 --> 28:46.000
那這個explainer有很多

28:46.000 --> 28:48.000
很多方法

28:48.000 --> 28:50.000
像是sharp啊像是line啊

28:50.000 --> 28:52.000
這種

28:52.000 --> 28:54.000
那這邊

28:54.000 --> 28:56.000
列出來這些方法其實

28:56.000 --> 28:58.000
有一個很大的缺點就是

28:58.000 --> 29:00.000
如果我們要做剛剛我們所說的

29:00.000 --> 29:02.000
local explanation的話

29:02.000 --> 29:04.000
那我們每張圖都必須要提供一個解釋

29:04.000 --> 29:06.000
那以現在目前的

29:06.000 --> 29:08.000
比較

29:08.000 --> 29:10.000
常見的方法

29:10.000 --> 29:12.000
我不能說全部因為現在有

29:12.000 --> 29:14.000
另外一種方法就是

29:14.000 --> 29:16.000
現在比較常見的方法呢

29:16.000 --> 29:18.000
就是我們一張圖

29:18.000 --> 29:20.000
例如說我們要給一張圖片解釋

29:20.000 --> 29:22.000
我們就必須要有一個explainer

29:22.000 --> 29:24.000
也就是說我們今天有十萬張圖片

29:24.000 --> 29:26.000
因為我們要給每一張圖片一個

29:26.000 --> 29:28.000
客製化的解釋結果

29:28.000 --> 29:30.000
所以我們就必須要有十萬

29:30.000 --> 29:32.000
那會導致什麼導致這個過程非常非常

29:32.000 --> 29:34.000
慢所以

29:34.000 --> 29:36.000
這些

29:36.000 --> 29:38.000
方法就有一些缺點

29:38.000 --> 29:40.000
當然就有些人說你今天有

29:40.000 --> 29:42.000
sharp e就是經濟學

29:42.000 --> 29:44.000
game theory這麼好的一個

29:44.000 --> 29:46.000
一個

29:46.000 --> 29:48.000
準則你為什麼不拿來用

29:48.000 --> 29:50.000
那我等一下會解釋一下為什麼我們

29:50.000 --> 29:52.000
不能直接拿sharp e來做

29:52.000 --> 29:54.000
最後模型解釋的一個結論

29:54.000 --> 29:56.000
ok

29:56.000 --> 29:58.000
好那我們就來講一下

29:58.000 --> 30:00.000
比較在網

30:00.000 --> 30:02.000
裡面挖一點那什麼是

30:02.000 --> 30:04.000
sharp e value呢其實sharp e value其實

30:04.000 --> 30:06.000
就是我們把game theory的一個

30:06.000 --> 30:08.000
東西借過來然後來

30:08.000 --> 30:10.000
來就是給模型

30:10.000 --> 30:12.000
的prediction做解釋

30:12.000 --> 30:14.000
好那我們

30:14.000 --> 30:16.000
要製造sharp e value必須要做一件事

30:16.000 --> 30:18.000
就是我們需要

30:18.000 --> 30:20.000
知道每一個feature我們的目的就是

30:20.000 --> 30:22.000
知道每一個feature的

30:22.000 --> 30:24.000
important score就是我們需要知道每一個feature

30:24.000 --> 30:26.000
的重要程度的分數

30:26.000 --> 30:28.000
那我們要知道每一個feature

30:28.000 --> 30:30.000
的重要分數

30:30.000 --> 30:32.000
必須要做一些什麼事

30:32.000 --> 30:34.000
其實很簡單就是我們要把

30:34.000 --> 30:36.000
每一個假設我們這邊有

30:36.000 --> 30:38.000
user0有四個feature

30:38.000 --> 30:40.000
5個12345

30:40.000 --> 30:42.000
我們要判斷這個user0

30:42.000 --> 30:44.000
是不是他可以

30:44.000 --> 30:46.000
還清這個房貸

30:46.000 --> 30:48.000
ok那我們就必須要知道說

30:48.000 --> 30:50.000
每一個feature對於

30:50.000 --> 30:52.000
能不能還清房貸的重要程度

30:52.000 --> 30:54.000
我們要做一件事就是例如說

30:54.000 --> 30:56.000
我們必須要

30:56.000 --> 30:58.000
這個job加這個

30:58.000 --> 31:00.000
marital status

31:00.000 --> 31:02.000
婚姻狀況的兩個feature

31:02.000 --> 31:04.000
的共同貢獻

31:04.000 --> 31:06.000
對於這個房貸

31:06.000 --> 31:08.000
能不能還清房貸這個prediction

31:08.000 --> 31:10.000
的重要程度

31:10.000 --> 31:12.000
所以我們必須要去

31:12.000 --> 31:14.000
go through

31:14.000 --> 31:16.000
all the combination

31:16.000 --> 31:18.000
feature的combination

31:18.000 --> 31:20.000
也就是說我們必須要說

31:20.000 --> 31:22.000
我們必須要去走例如age加job

31:22.000 --> 31:24.000
加marital加education

31:24.000 --> 31:26.000
或是age加job加marital

31:26.000 --> 31:28.000
或是age加job

31:28.000 --> 31:30.000
這種所有的feature的組合我們才可以知道

31:30.000 --> 31:32.000
在shopping eval裡面我們才可以知道

31:32.000 --> 31:34.000
每一個

31:34.000 --> 31:36.000
每一個單一feature對於最後prediction

31:36.000 --> 31:38.000
結果的重要程度

31:38.000 --> 31:40.000
所以這會導致說什麼

31:40.000 --> 31:42.000
這會導致說如果我這邊只有

31:42.000 --> 31:44.000
5個feature那倒還好

31:44.000 --> 31:46.000
因為這個組合量就是2的5次方

31:46.000 --> 31:48.000
要不要要不要

31:48.000 --> 31:50.000
所以是2的5次方

31:50.000 --> 31:52.000
那今天如果我在

31:52.000 --> 31:54.000
大公司裡面做推薦系統

31:54.000 --> 31:56.000
我有2000個feature

31:56.000 --> 31:58.000
那就會是2的2000次方

31:58.000 --> 32:00.000
這個一個user就需要

32:00.000 --> 32:02.000
2的2000次方的計算的

32:02.000 --> 32:04.000
次數

32:04.000 --> 32:06.000
那還得了我今天有11個user

32:06.000 --> 32:08.000
那撐下去

32:08.000 --> 32:10.000
那就是算不完了

32:10.000 --> 32:12.000
整個運算過程算不完了

32:12.000 --> 32:14.000
所以今天這個shopping eval

32:14.000 --> 32:16.000
為什麼不能用

32:16.000 --> 32:18.000
它其實本身是一個NP-hard problem

32:18.000 --> 32:20.000
就是它的複雜度太高了

32:20.000 --> 32:22.000
而且我們不能用

32:22.000 --> 32:24.000
低複雜度的方法去

32:24.000 --> 32:26.000
去驗證它

32:26.000 --> 32:28.000
我們算出來的東西是不是對

32:28.000 --> 32:30.000
這個東西就NP-hard

32:30.000 --> 32:32.000
所以我們在

32:32.000 --> 32:34.000
我們在直接拿shopping eval

32:34.000 --> 32:36.000
這個詞來當作

32:36.000 --> 32:38.000
最後解釋的important score的話

32:38.000 --> 32:40.000
其實可以但是

32:40.000 --> 32:42.000
是不現實的

32:42.000 --> 32:44.000
因為它太

32:44.000 --> 32:46.000
太吃時間

32:46.000 --> 32:48.000
它需要太大的計算的複雜度

32:48.000 --> 32:50.000
所以導致我們

32:50.000 --> 32:52.000
沒辦法使用這個狀況

32:52.000 --> 32:54.000
ok

32:54.000 --> 32:56.000
剛才漏講一下後面這個

32:56.000 --> 32:58.000
其實後面這個東西它就是在算說

32:58.000 --> 33:00.000
如果我今天把

33:00.000 --> 33:02.000
例如說我今天只在乎job跟merito

33:02.000 --> 33:04.000
我今天把另外三個拿掉後

33:04.000 --> 33:06.000
那我的model會

33:06.000 --> 33:08.000
的prediction會做出什麼結果

33:08.000 --> 33:10.000
我model prediction

33:10.000 --> 33:12.000
會發生什麼變化

33:12.000 --> 33:14.000
如果我今天把這個balance

33:14.000 --> 33:16.000
把job把merito拿掉

33:16.000 --> 33:18.000
我只把age跟education

33:18.000 --> 33:20.000
考進去我的prediction model

33:20.000 --> 33:22.000
然後我今天它原本是

33:22.000 --> 33:24.000
不能還房貸的預測結果

33:24.000 --> 33:26.000
突然變成說你可以還得起房貸

33:26.000 --> 33:28.000
那這說明什麼

33:28.000 --> 33:30.000
這三個feature很重要

33:30.000 --> 33:32.000
這三個feature是導致它預測正確的

33:32.000 --> 33:34.000
一個key feature

33:34.000 --> 33:36.000
ok

33:36.000 --> 33:38.000
所以透過這種不斷的

33:38.000 --> 33:40.000
這種iterate的過程

33:40.000 --> 33:42.000
去找到說每一個feature的

33:42.000 --> 33:44.000
重要的分數

33:44.000 --> 33:46.000
這個就是shopping value

33:46.000 --> 33:48.000
ok

33:48.000 --> 33:50.000
懂經濟學的人應該

33:50.000 --> 33:52.000
看到這個東西會覺得蠻

33:52.000 --> 33:54.000
蠻親切的

33:54.000 --> 33:56.000
ok

33:56.000 --> 33:58.000
好那

33:58.000 --> 34:00.000
我shopping value不能用的話

34:00.000 --> 34:02.000
那怎麼辦

34:02.000 --> 34:04.000
這就是我在做研究

34:04.000 --> 34:06.000
就是我想要把這個

34:06.000 --> 34:08.000
explanation的這個過程給加速

34:08.000 --> 34:10.000
我想要讓

34:10.000 --> 34:12.000
因為每一個feature

34:12.000 --> 34:14.000
每一個

34:14.000 --> 34:16.000
我們在做local explanation

34:16.000 --> 34:18.000
每一個使用者都必須要

34:18.000 --> 34:20.000
給一個客製化的

34:20.000 --> 34:22.000
explanation的結果

34:22.000 --> 34:24.000
那我剛才提到每一個客製化的explanation的結果

34:24.000 --> 34:26.000
都必須要有一個獨立的

34:26.000 --> 34:28.000
explanator

34:28.000 --> 34:30.000
就是一個解釋prediction model的模型

34:30.000 --> 34:32.000
那這個過程本身

34:32.000 --> 34:34.000
就是一個非常耗

34:34.000 --> 34:36.000
耗時間的一個過程

34:36.000 --> 34:38.000
像是你如果

34:38.000 --> 34:40.000
給一個推薦結果然後你叫使用者

34:40.000 --> 34:42.000
說不好意思等我10秒

34:42.000 --> 34:44.000
我給你一個推薦我給你一個解釋

34:44.000 --> 34:46.000
使用者一定就不能接受

34:46.000 --> 34:48.000
如果你一個網頁叫你等10秒

34:48.000 --> 34:50.000
你一定就左上角插他就按掉了

34:50.000 --> 34:52.000
類似這樣

34:52.000 --> 34:54.000
所以這個加速結果是很重要

34:54.000 --> 34:56.000
那在這邊這個work

34:56.000 --> 34:58.000
這是我的去年投稿上的一個work

34:58.000 --> 35:00.000
這個work

35:00.000 --> 35:02.000
這個work就是

35:02.000 --> 35:04.000
我們目的是在加速這個

35:04.000 --> 35:06.000
shopping value的

35:06.000 --> 35:08.000
這個運算過程

35:08.000 --> 35:10.000
然後把這個過程的結果

35:10.000 --> 35:12.000
提供給使用者當作

35:12.000 --> 35:14.000
解釋的結果

35:14.000 --> 35:16.000
ok那我們做了什麼像我剛才在講說

35:16.000 --> 35:18.000
shopping value他其實

35:18.000 --> 35:20.000
因為我要go through all the combination

35:20.000 --> 35:22.000
of the feature set

35:22.000 --> 35:24.000
所以我們今天假設有五個feature

35:24.000 --> 35:26.000
那就是2的五次方

35:26.000 --> 35:28.000
那是不是每一個feature的組合

35:28.000 --> 35:30.000
對模型來說很重要呢

35:30.000 --> 35:32.000
不見得為什麼就是有些feature

35:32.000 --> 35:34.000
他的交互作用其實對模型來說

35:34.000 --> 35:36.000
是不重要的

35:36.000 --> 35:38.000
那這個資訊可不可以在模型訓練完

35:38.000 --> 35:40.000
之後就得到呢

35:40.000 --> 35:42.000
其實是可以的就是我們其實可以把

35:42.000 --> 35:44.000
模型的訓練的權重

35:44.000 --> 35:46.000
給拿出來去看到說

35:46.000 --> 35:48.000
feature1跟feature2

35:48.000 --> 35:50.000
黑色代表很重要那feature

35:50.000 --> 35:52.000
x1跟x2他很重要

35:52.000 --> 35:54.000
x1跟x3

35:54.000 --> 35:56.000
突然之間沒有跟x2那麼重要

35:56.000 --> 35:58.000
x1跟x5來說

35:58.000 --> 36:00.000
其實完全不重要

36:00.000 --> 36:02.000
所以這個時候我們就可以把x1x5

36:02.000 --> 36:04.000
這個組合從算shopping value

36:04.000 --> 36:06.000
的這個組合給拿掉

36:06.000 --> 36:08.000
那拿掉之後

36:08.000 --> 36:10.000
例如說我們原本要2的五次方可能要32次

36:10.000 --> 36:12.000
可能透過

36:12.000 --> 36:14.000
這種拿掉的過程例如說

36:14.000 --> 36:16.000
我們把這個灰色以下的

36:16.000 --> 36:18.000
色階的色塊的

36:18.000 --> 36:20.000
組合全部拿掉那我們

36:20.000 --> 36:22.000
裡面所需要算的次數就變成是

36:22.000 --> 36:24.000
可能2 2 4 6 8

36:24.000 --> 36:26.000
還有14次

36:26.000 --> 36:28.000
所以其實是減了一半的過程

36:28.000 --> 36:30.000
對吧這個過程其實是

36:30.000 --> 36:32.000
可以加速這個

36:32.000 --> 36:34.000
shopping value的計算

36:34.000 --> 36:36.000
然後把這個東西變成一個

36:36.000 --> 36:38.000
使用者可以接受的

36:38.000 --> 36:40.000
一個等待時間

36:40.000 --> 36:42.000
那最後提供給使用者

36:42.000 --> 36:44.000
比較快速的這個解釋的

36:44.000 --> 36:46.000
解釋的

36:46.000 --> 36:48.000
這個結果

36:48.000 --> 36:50.000
所以這個數學其實就是在表達說

36:50.000 --> 36:52.000
我們今天在shopping value

36:52.000 --> 36:54.000
裡面我們需要

36:54.000 --> 36:56.000
去經歷所有的

36:56.000 --> 36:58.000
所有的feature的組合

36:58.000 --> 37:00.000
那我們今天把它變成是

37:00.000 --> 37:02.000
我們透過模型的權重

37:02.000 --> 37:04.000
去看到feature跟feature之間的

37:04.000 --> 37:06.000
interaction的重要性

37:06.000 --> 37:08.000
然後把不重要拿掉

37:08.000 --> 37:10.000
最後我們去算

37:10.000 --> 37:12.000
approximate的shopping value

37:12.000 --> 37:14.000
把這個shopping value當作

37:14.000 --> 37:16.000
當作是

37:16.000 --> 37:18.000
explanation的important score

37:18.000 --> 37:20.000
提供給使用者

37:20.000 --> 37:22.000
就是這個work的精神

37:22.000 --> 37:24.000
ok

37:24.000 --> 37:26.000
所以你可以看到說

37:26.000 --> 37:28.000
我們這個work其實有一個

37:28.000 --> 37:30.000
application的scenario

37:30.000 --> 37:32.000
就是我們今天在train的一個

37:32.000 --> 37:34.000
prediction model

37:34.000 --> 37:36.000
我們去預測說

37:36.000 --> 37:38.000
他的income是高還是低

37:38.000 --> 37:40.000
這個東西,我們其實可以用我們的模型

37:40.000 --> 37:42.000
去解釋這個訓練好的

37:42.000 --> 37:44.000
這個prediction model

37:44.000 --> 37:46.000
然後進而去提供說

37:46.000 --> 37:48.000
為什麼他可能是因為你的

37:48.000 --> 37:50.000
高學歷,例如說你是

37:50.000 --> 37:52.000
稻乾工作人,類似那種

37:52.000 --> 37:54.000
然後是造成說

37:54.000 --> 37:56.000
我判斷你是高工資

37:56.000 --> 37:58.000
的這種結果

37:58.000 --> 38:00.000
所以我們這個模型其實是

38:00.000 --> 38:02.000
可以用一個比較快速的

38:02.000 --> 38:04.000
快速的這個explanation的方式

38:04.000 --> 38:06.000
然後在

38:06.000 --> 38:08.000
提供給使用者的這個情境

38:08.000 --> 38:10.000
上面做使用

38:10.000 --> 38:12.000
那右下角這個圖其實可以看到

38:12.000 --> 38:14.000
左邊這個藍色的這個bar

38:14.000 --> 38:16.000
其實是

38:16.000 --> 38:18.000
ground truth

38:18.000 --> 38:20.000
也就是說我們用經濟學

38:20.000 --> 38:22.000
跟theory的那個公式去算出來

38:22.000 --> 38:24.000
說屬於這個

38:24.000 --> 38:26.000
屬於這個feature來說

38:26.000 --> 38:28.000
真正他應該要有

38:28.000 --> 38:30.000
那可以看到其實

38:30.000 --> 38:32.000
我們就算把那些不重要的

38:32.000 --> 38:34.000
feature跟interaction給拿掉

38:34.000 --> 38:36.000
我們的shopping value在

38:36.000 --> 38:38.000
我們的這個model在預測出來

38:38.000 --> 38:40.000
這個

38:40.000 --> 38:42.000
importance的這個score

38:42.000 --> 38:44.000
他其實跟shopping value是非常靠近

38:44.000 --> 38:46.000
完全一模一樣

38:46.000 --> 38:48.000
畢竟我們把一些combination拿掉了

38:48.000 --> 38:50.000
所以這個東西其實是

38:50.000 --> 38:52.000
更快速但是我們同樣不失去

38:52.000 --> 38:54.000
那個shopping value的那個

38:54.000 --> 38:56.000
準度所以我們在提供給

38:56.000 --> 38:58.000
每個feature的重要性

38:58.000 --> 39:00.000
給user的時候我們可以說

39:00.000 --> 39:02.000
學歷這一塊的feature

39:02.000 --> 39:04.000
對於我們的

39:04.000 --> 39:06.000
模型來說預測你是

39:06.000 --> 39:08.000
高薪水低薪水的

39:08.000 --> 39:10.000
這個

39:10.000 --> 39:12.000
模型來說是很重要的

39:12.000 --> 39:14.000
所以我們會把這個例如分數

39:14.000 --> 39:16.000
或是把這個重要程度的

39:16.000 --> 39:18.000
ranking的結果提供給

39:18.000 --> 39:20.000
使用者作為他

39:20.000 --> 39:22.000
想不想相信這個

39:22.000 --> 39:24.000
這個預測模型的一個參考

39:24.000 --> 39:26.000
ok

39:26.000 --> 39:28.000
好

39:28.000 --> 39:30.000
那下一個工作

39:30.000 --> 39:32.000
那我加速一下

39:32.000 --> 39:34.000
下一個工作的話是

39:34.000 --> 39:36.000
因為我們前面是

39:36.000 --> 39:38.000
加速那個shopping value

39:38.000 --> 39:40.000
的計算過程

39:40.000 --> 39:42.000
對吧

39:42.000 --> 39:44.000
這個工作是我們

39:44.000 --> 39:46.000
與其要去加速那個過程

39:46.000 --> 39:48.000
那倒不如說我們去訓練一個模型

39:48.000 --> 39:50.000
去把shopping value

39:50.000 --> 39:52.000
計算的那個distribution

39:52.000 --> 39:54.000
計算那個distribution給學起來

39:54.000 --> 39:56.000
那這個東西其實

39:56.000 --> 39:58.000
可以更快地去

39:58.000 --> 40:00.000
給預測結果

40:00.000 --> 40:02.000
為什麼呢是因為

40:02.000 --> 40:04.000
我們這個傳統的

40:04.000 --> 40:06.000
一般的這種DNA模型

40:06.000 --> 40:08.000
我們剛才是每一個

40:08.000 --> 40:10.000
每一個

40:10.000 --> 40:12.000
user我們要給他一個

40:12.000 --> 40:14.000
客製化的結果我們就必須要有一個

40:14.000 --> 40:16.000
explanation model

40:16.000 --> 40:18.000
所以我們今天要預測十萬個user

40:18.000 --> 40:20.000
我們就要給十萬個model

40:20.000 --> 40:22.000
那在這邊我們只要給一個user

40:22.000 --> 40:24.000
我們只要給一個model去

40:24.000 --> 40:26.000
預測所有user的

40:26.000 --> 40:28.000
explanation結果就可以了

40:28.000 --> 40:30.000
那這個過程其實上也是加速

40:30.000 --> 40:32.000
加速我們給

40:32.000 --> 40:34.000
explanation過程的這個進程

40:34.000 --> 40:36.000
ok那為什麼是因為

40:36.000 --> 40:38.000
一般的這種

40:38.000 --> 40:40.000
深度學習這種網路

40:40.000 --> 40:42.000
我們只要給例如說我們可以

40:42.000 --> 40:44.000
一次把十萬個user當作

40:44.000 --> 40:46.000
data送進去那個

40:46.000 --> 40:48.000
預測網路然後這個預測網路

40:48.000 --> 40:50.000
他就可以突出屬於這十萬個

40:50.000 --> 40:52.000
不同的結果

40:52.000 --> 40:54.000
ok所以我們再也不需要

40:54.000 --> 40:56.000
我們再也不需要一個使用者

40:56.000 --> 40:58.000
對應一個獨立的

40:58.000 --> 41:00.000
explanator

41:00.000 --> 41:02.000
我們現在只需要一個explanator

41:02.000 --> 41:04.000
去對應所有user

41:04.000 --> 41:06.000
這個訓練過程

41:06.000 --> 41:08.000
其實是我另外一個work

41:08.000 --> 41:10.000
那這篇也在投稿當中

41:10.000 --> 41:12.000
那他主要就是說

41:12.000 --> 41:14.000
我們是不是可以透過一些

41:14.000 --> 41:16.000
sorry

41:16.000 --> 41:18.000
透過一些正樣本跟負樣本的這種

41:18.000 --> 41:20.000
學習什麼是正樣本

41:20.000 --> 41:22.000
例如說

41:22.000 --> 41:24.000
我們今天把某些一張圖片

41:24.000 --> 41:26.000
把某些重要的

41:26.000 --> 41:28.000
特徵給遮掉

41:28.000 --> 41:30.000
如果我們遮掉這個特徵呢

41:30.000 --> 41:32.000
是屬於不重要的特徵

41:32.000 --> 41:34.000
比如說我們遮掉狗的眼睛

41:34.000 --> 41:36.000
那因為他有另外一隻眼睛

41:36.000 --> 41:38.000
所以可能一隻眼睛相對來說不重要

41:38.000 --> 41:40.000
那模型在判斷

41:40.000 --> 41:42.000
這張圖是不是一隻狗的時候

41:42.000 --> 41:44.000
他還是做出一樣的判斷

41:44.000 --> 41:46.000
這個就是所謂的正樣本

41:46.000 --> 41:48.000
那這個正樣本其實可以幫助

41:48.000 --> 41:50.000
幫助我們去解釋

41:50.000 --> 41:52.000
例如說我們說

41:52.000 --> 41:54.000
我們希望一個人判斷說

41:54.000 --> 41:56.000
這張圖片

41:56.000 --> 41:58.000
希望一個模型判斷說他是不是一隻狗

41:58.000 --> 42:00.000
那我們給出的解釋

42:00.000 --> 42:02.000
因為狗

42:02.000 --> 42:04.000
狗在乎這些特徵

42:04.000 --> 42:06.000
所以我模型做出

42:06.000 --> 42:08.000
這個prediction的結果

42:08.000 --> 42:10.000
那這些東西是正向

42:10.000 --> 42:12.000
那我們可以給一些負向

42:12.000 --> 42:14.000
例如說我們這邊給一個場景錄

42:14.000 --> 42:16.000
場景錄就不是狗

42:16.000 --> 42:18.000
所以模型就會知道說

42:18.000 --> 42:20.000
今天遇到這個場景錄的時候

42:20.000 --> 42:22.000
他就是屬於這個狗的

42:22.000 --> 42:24.000
negative sample

42:24.000 --> 42:26.000
就是所謂的負面樣

42:26.000 --> 42:28.000
負面教材

42:28.000 --> 42:30.000
負面教材跟負面教材的時候

42:30.000 --> 42:32.000
我就可以去學

42:32.000 --> 42:34.000
就是我希望我的模型

42:34.000 --> 42:36.000
給出的解釋

42:36.000 --> 42:38.000
要越靠近正面教材

42:38.000 --> 42:40.000
然後離負面教材越來越遠

42:40.000 --> 42:42.000
這就是我要做的事情

42:42.000 --> 42:44.000
所以我透過我的這個

42:44.000 --> 42:46.000
設計的這個模型

42:46.000 --> 42:48.000
那這個過程

42:48.000 --> 42:50.000
其實有一個名詞叫contrastive learning

42:50.000 --> 42:52.000
就是中文叫

42:52.000 --> 42:54.000
對比學習吧

42:54.000 --> 42:56.000
應該是這個對比學習的過程

42:56.000 --> 42:58.000
然後讓模型知道什麼是對

42:58.000 --> 43:00.000
什麼是錯

43:00.000 --> 43:02.000
然後我希望我對的跟錯的

43:02.000 --> 43:04.000
之間的距離越來越大

43:04.000 --> 43:06.000
然後讓模型往正確方向去訓練

43:06.000 --> 43:08.000
那為什麼後面還有一個

43:08.000 --> 43:10.000
fine tuning的過程

43:10.000 --> 43:12.000
這邊的fine tuning的過程是

43:12.000 --> 43:14.000
為什麼我剛剛提到shoppy value

43:14.000 --> 43:16.000
是因為shoppy value有很強大的

43:16.000 --> 43:18.000
經濟學game theory的一些

43:18.000 --> 43:20.000
理論上的support

43:20.000 --> 43:22.000
所以如果我們能

43:22.000 --> 43:24.000
如果我們能製造出

43:24.000 --> 43:26.000
這些

43:26.000 --> 43:28.000
這些important score

43:28.000 --> 43:30.000
他也具有shoppy value

43:30.000 --> 43:32.000
這些特性的話

43:32.000 --> 43:34.000
那

43:34.000 --> 43:36.000
在提供給

43:36.000 --> 43:38.000
我不敢說提供給使用者用

43:38.000 --> 43:40.000
但至少在提供給一些

43:40.000 --> 43:42.000
例如machine learning engineer

43:42.000 --> 43:44.000
或是一些高層

43:44.000 --> 43:46.000
他們要去看說去evaluate說

43:46.000 --> 43:48.000
這些evaluation的

43:48.000 --> 43:50.000
這些explanation的結果是不是可信

43:50.000 --> 43:52.000
那這個過程其實上

43:52.000 --> 43:54.000
一是去讓人家更幸福這個過程

43:54.000 --> 43:56.000
二是我透過這個fine tuning的過程

43:56.000 --> 43:58.000
我希望我的

43:58.000 --> 44:00.000
我的這些東西不要離shoppy value

44:00.000 --> 44:02.000
太遠

44:02.000 --> 44:04.000
那這個不要離shoppy value太遠的

44:04.000 --> 44:06.000
這個特性他somehow可以去

44:06.000 --> 44:08.000
因為這邊我完全沒有使用任何的

44:08.000 --> 44:10.000
一些shoppy value

44:10.000 --> 44:12.000
作為label去訓練

44:12.000 --> 44:14.000
那後面我給少量的label

44:14.000 --> 44:16.000
我是可以把他拉回shoppy value的正軌

44:16.000 --> 44:18.000
讓整個

44:18.000 --> 44:20.000
整個大模型在訓練的時候

44:20.000 --> 44:22.000
不要離我們剛才所謂的shoppy value

44:22.000 --> 44:24.000
給explanation這個framework太遠

44:24.000 --> 44:26.000
讓我們可以

44:26.000 --> 44:28.000
拿這個結果去

44:28.000 --> 44:30.000
讓別人相信說

44:30.000 --> 44:32.000
我們不是隨便去製造一個

44:32.000 --> 44:34.000
important score的預測

44:34.000 --> 44:36.000
我們是基於shoppy value的

44:36.000 --> 44:38.000
這個大框架去做

44:38.000 --> 44:40.000
prediction

44:40.000 --> 44:42.000
所以這個模型其實有

44:42.000 --> 44:44.000
幾個優點

44:44.000 --> 44:46.000
第一個是他可以很快

44:46.000 --> 44:48.000
因為他只要一個模型就可以應付所有的user

44:48.000 --> 44:50.000
不像是以前一個模型

44:50.000 --> 44:52.000
只能應付一個user

44:52.000 --> 44:54.000
然後再來就是他提供了正樣本跟負樣本的選取

44:54.000 --> 44:56.000
然後讓整個訓練的

44:56.000 --> 44:58.000
過程中我們不需要

44:58.000 --> 45:00.000
太大量的樣本

45:00.000 --> 45:02.000
不需要太大量的label

45:02.000 --> 45:04.000
然後也可以去訓練出對於所有user的

45:04.000 --> 45:06.000
這個訓練結果

45:06.000 --> 45:08.000
好那這個我就

45:08.000 --> 45:10.000
就不講了我就跳過

45:10.000 --> 45:12.000
可以看到右邊這個

45:12.000 --> 45:14.000
右邊這個兩張圖就是

45:14.000 --> 45:16.000
我們訓練的結果

45:16.000 --> 45:18.000
這個訓練的結果就是可以看到說

45:18.000 --> 45:20.000
這是一隻鳥嘛

45:20.000 --> 45:22.000
那這個鳥的中間被框住

45:22.000 --> 45:24.000
紅色就代表重要,藍色就代表不重要

45:24.000 --> 45:26.000
所以我們就知道

45:26.000 --> 45:28.000
這個image classification model

45:28.000 --> 45:30.000
他其實上是在

45:30.000 --> 45:32.000
為什麼會預測他是鳥

45:32.000 --> 45:34.000
那就是因為他抓到了這個鳥的特徵

45:34.000 --> 45:36.000
所以我們就可以去相信

45:36.000 --> 45:38.000
這個image classification model是做對的

45:38.000 --> 45:40.000
然後也可以去看說

45:40.000 --> 45:42.000
我們這個explainer是trend好的

45:42.000 --> 45:44.000
是trend對的

45:44.000 --> 45:46.000
下面這個狗

45:46.000 --> 45:48.000
最重要的這個特徵是臉

45:48.000 --> 45:50.000
狗的這個臉

45:50.000 --> 45:52.000
那我們就把我們的這個

45:52.000 --> 45:54.000
explainer訓練結果

45:54.000 --> 45:56.000
他其實就是說這個臉很重要

45:56.000 --> 45:58.000
那我們把這個臉highlight起來

45:58.000 --> 46:00.000
那就是最後的explanation

46:00.000 --> 46:02.000
結果

46:04.000 --> 46:06.000
好那有幾個

46:06.000 --> 46:08.000
有幾個application

46:08.000 --> 46:10.000
就是例如說

46:10.000 --> 46:12.000
其實我前面大致上都提過

46:12.000 --> 46:14.000
XAI在recommended system上面

46:14.000 --> 46:16.000
的重要性

46:16.000 --> 46:18.000
是因為可以針對幾個不同的

46:18.000 --> 46:20.000
人

46:20.000 --> 46:22.000
幾個不同的使用情境來說

46:22.000 --> 46:24.000
例如說我對於客人來說

46:24.000 --> 46:26.000
我可以讓他知道我拿什麼

46:26.000 --> 46:28.000
怎麼逼選給你訓練

46:28.000 --> 46:30.000
那為什麼你會做出這個結果

46:30.000 --> 46:32.000
讓大家覺得

46:32.000 --> 46:34.000
這個decision是對

46:34.000 --> 46:36.000
然後更安心

46:36.000 --> 46:38.000
對於這個recommended system更安心

46:38.000 --> 46:40.000
那對於商人來說

46:40.000 --> 46:42.000
那他可以透過這個

46:42.000 --> 46:44.000
例如說我投放廣告

46:44.000 --> 46:46.000
那你總要跟廣告商說

46:46.000 --> 46:48.000
因為我

46:48.000 --> 46:50.000
截取了使用者

46:50.000 --> 46:52.000
使用者的A feature B feature

46:52.000 --> 46:54.000
C feature D feature

46:54.000 --> 46:56.000
然後導致

46:56.000 --> 46:58.000
我推薦這個使用者廣告

46:58.000 --> 47:00.000
所以商人可以透過這種

47:00.000 --> 47:02.000
顯示的分析去做出

47:02.000 --> 47:04.000
更好的例如說

47:04.000 --> 47:06.000
因為我發現可能這個市場

47:06.000 --> 47:08.000
某些使用者會在乎什麼

47:08.000 --> 47:10.000
商品的特徵

47:10.000 --> 47:12.000
我可以去對這些商品的特徵做調整

47:12.000 --> 47:14.000
然後他也可以更信服

47:14.000 --> 47:16.000
這個廣告的投放

47:16.000 --> 47:18.000
那對於我們

47:18.000 --> 47:20.000
這種工程師來說

47:20.000 --> 47:22.000
提供這種

47:22.000 --> 47:24.000
explanation的結果

47:24.000 --> 47:26.000
他其實可以幫我們去

47:26.000 --> 47:28.000
debug這個system

47:28.000 --> 47:30.000
也可以讓我們去改善這個system

47:30.000 --> 47:32.000
例如說我們如果抓錯了feature的話

47:32.000 --> 47:34.000
我們要怎麼去調整

47:34.000 --> 47:36.000
那再來就是health care

47:36.000 --> 47:38.000
那就是例如說

47:38.000 --> 47:40.000
我們對於病人來說

47:40.000 --> 47:42.000
我們可以去說服病人說

47:42.000 --> 47:44.000
相信這個診斷結果是沒錯

47:44.000 --> 47:46.000
那對於醫生來說

47:46.000 --> 47:48.000
可以去協助他做正確的結論

47:48.000 --> 47:50.000
就是醫生憑什麼相信

47:50.000 --> 47:52.000
這個prediction model

47:52.000 --> 47:54.000
但你總要說服他說

47:54.000 --> 47:56.000
因為我detect到什麼症狀

47:56.000 --> 47:58.000
這個東西其實是

47:58.000 --> 48:00.000
可以協助醫生

48:00.000 --> 48:02.000
但我必須要說

48:02.000 --> 48:04.000
他不能取代醫生的重要程度

48:04.000 --> 48:06.000
所有的AI model

48:06.000 --> 48:08.000
他都是在協助為本質

48:08.000 --> 48:10.000
這個outlier

48:10.000 --> 48:12.000
我覺得就先跳過

48:12.000 --> 48:14.000
OK

48:14.000 --> 48:16.000
好那最後的話

48:16.000 --> 48:18.000
我來講幾個常見的

48:18.000 --> 48:20.000
這個open source package

48:20.000 --> 48:22.000
這些都是免費完全免費

48:22.000 --> 48:24.000
不用付任何錢

48:24.000 --> 48:26.000
第一個就是Captain

48:26.000 --> 48:28.000
Captain其實是Facebook開發的一個

48:28.000 --> 48:30.000
可解釋性的

48:30.000 --> 48:32.000
開源的套件

48:32.000 --> 48:34.000
那這個套件

48:34.000 --> 48:36.000
使用的方式很簡單

48:36.000 --> 48:38.000
就我們要使用這種

48:38.000 --> 48:40.000
套件的時候

48:40.000 --> 48:42.000
首先我們必須要有一個

48:42.000 --> 48:44.000
我們要解釋的模型

48:44.000 --> 48:46.000
如果沒有解釋模型

48:46.000 --> 48:48.000
那這個XI就沒用了

48:48.000 --> 48:50.000
所以我們

48:50.000 --> 48:52.000
這個code的話就是

48:52.000 --> 48:54.000
首先我們要有一個

48:54.000 --> 48:56.000
我們要去解釋的目標模型

48:56.000 --> 48:58.000
那這個integrated gradient

48:58.000 --> 49:00.000
這個東西

49:00.000 --> 49:02.000
其實就是

49:02.000 --> 49:04.000
其中一個我們剛才說

49:04.000 --> 49:06.000
一直在說的explainer

49:06.000 --> 49:08.000
就是解釋模型的

49:08.000 --> 49:10.000
機器學系模型

49:10.000 --> 49:12.000
有點local

49:12.000 --> 49:14.000
所以我們把這個

49:14.000 --> 49:16.000
image classifier丟進這個

49:16.000 --> 49:18.000
的explainer裡面

49:18.000 --> 49:20.000
那他的

49:20.000 --> 49:22.000
這個explainer的

49:22.000 --> 49:24.000
output結果就會是

49:24.000 --> 49:26.000
我去highlight一張圖片哪裡重要

49:26.000 --> 49:28.000
所以右邊這個結果就是

49:28.000 --> 49:30.000
這個東西 output出來的結果

49:30.000 --> 49:32.000
這是一隻天鵝

49:32.000 --> 49:34.000
那他predict是不是一隻天鵝

49:34.000 --> 49:36.000
是因為他

49:36.000 --> 49:38.000
看了

49:38.000 --> 49:40.000
這些重要的feature

49:40.000 --> 49:42.000
所以我們就可以透過這個輪廓

49:42.000 --> 49:44.000
他真的是detect到這個天鵝

49:44.000 --> 49:46.000
而不是detect到旁邊這些水

49:46.000 --> 49:48.000
這就是過程

49:48.000 --> 49:50.000
那不然就是

49:50.000 --> 49:52.000
例如說我們有一些文字的模型

49:52.000 --> 49:54.000
就是文字的classifier的結果

49:54.000 --> 49:56.000
那是因為為什麼他會說

49:56.000 --> 49:58.000
這句話是一個

49:58.000 --> 50:00.000
positive的attitude

50:00.000 --> 50:02.000
那是因為沒有

50:02.000 --> 50:04.000
例如fantastic的字

50:04.000 --> 50:06.000
那為什麼他是positive

50:06.000 --> 50:08.000
因為他有best

50:08.000 --> 50:10.000
那為什麼他不好

50:10.000 --> 50:12.000
例如說negative

50:12.000 --> 50:14.000
類似這種showcase

50:14.000 --> 50:16.000
所以這整個package的

50:16.000 --> 50:18.000
使用方式非常簡單

50:18.000 --> 50:20.000
所以只要幾行

50:20.000 --> 50:22.000
一行兩行三行

50:22.000 --> 50:24.000
只要三行就可以搞定

50:24.000 --> 50:26.000
explanation的process

50:26.000 --> 50:28.000
那當然就是這裡面還有一些

50:28.000 --> 50:30.000
模型調教的一些

50:30.000 --> 50:32.000
trick或是一些經驗談

50:32.000 --> 50:34.000
那這個就是要靠

50:34.000 --> 50:36.000
大家平常常去用的一些

50:36.000 --> 50:38.000
technique才會知道的一些

50:38.000 --> 50:40.000
makeup

50:40.000 --> 50:42.000
那另外一個package叫shop

50:42.000 --> 50:44.000
那他shop他也是

50:44.000 --> 50:46.000
用同樣的concept在

50:46.000 --> 50:48.000
設計這個package

50:48.000 --> 50:50.000
一樣他就是例如說

50:50.000 --> 50:52.000
我需要有一個prediction model在這裡

50:52.000 --> 50:54.000
然後把他寫成一個function

50:54.000 --> 50:56.000
那我已經有一個prediction model在這裡

50:56.000 --> 50:58.000
我已經有一個explanator在這裡

50:58.000 --> 51:00.000
那這個explanator的目的

51:00.000 --> 51:02.000
就是要去製造說

51:02.000 --> 51:04.000
每一個圖片

51:04.000 --> 51:06.000
或是每一個

51:06.000 --> 51:08.000
每一句話或是每一個user

51:08.000 --> 51:10.000
他重要的feature或重要的字

51:10.000 --> 51:12.000
或是重要的pixel是什麼

51:12.000 --> 51:14.000
那就是透過這個

51:14.000 --> 51:16.000
這個explanator

51:16.000 --> 51:18.000
來最後做一個製造

51:18.000 --> 51:20.000
所以他output出來的shop event

51:20.000 --> 51:22.000
就是屬於每一個pixel

51:22.000 --> 51:24.000
重要的important score

51:24.000 --> 51:26.000
ok所以整個過程就會比較像是這樣

51:26.000 --> 51:28.000
我已經有一個黑盒模型

51:28.000 --> 51:30.000
然後最後把他打開

51:30.000 --> 51:32.000
然後知道每一個feature的重要程度是什麼

51:32.000 --> 51:34.000
ok

51:34.000 --> 51:36.000
所以整個feature會比較像是這樣

51:36.000 --> 51:38.000
就例如說這是一艘船

51:38.000 --> 51:40.000
所以他就會說

51:40.000 --> 51:42.000
highlight船的這個

51:42.000 --> 51:44.000
部件在這裡

51:44.000 --> 51:46.000
那就讓人家知道說

51:46.000 --> 51:48.000
這個船很重要

51:48.000 --> 51:50.000
就可以了解到這個船的特徵

51:50.000 --> 51:52.000
ok

51:52.000 --> 51:54.000
那後面這些圖是

51:54.000 --> 51:56.000
說今天錯誤的樣本

51:56.000 --> 51:58.000
會發生什麼事

51:58.000 --> 52:00.000
其實後面就不用管

52:00.000 --> 52:02.000
因為我們只在

52:02.000 --> 52:04.000
XAI這個模型

52:04.000 --> 52:06.000
其實我們不能去導正

52:06.000 --> 52:08.000
直接去導正

52:08.000 --> 52:10.000
prediction model不好的結果

52:10.000 --> 52:12.000
我們就是基於prediction model

52:12.000 --> 52:14.000
給出來的結果做最忠實

52:14.000 --> 52:16.000
最真誠的

52:16.000 --> 52:18.000
這個解釋的結果

52:18.000 --> 52:20.000
所以今天不能說

52:20.000 --> 52:22.000
為什麼這個

52:22.000 --> 52:24.000
他在fountain這個level下

52:24.000 --> 52:26.000
他的explanation的結果好像很爛

52:26.000 --> 52:28.000
那廢話因為他prediction

52:28.000 --> 52:30.000
本來就是錯

52:30.000 --> 52:32.000
因為你本來決策就錯了

52:32.000 --> 52:34.000
所以我基於你做錯的決策

52:34.000 --> 52:36.000
去做錯誤的explanation

52:36.000 --> 52:38.000
其實是

52:38.000 --> 52:40.000
XAI是不能管

52:40.000 --> 52:42.000
我們只是基於你給我的

52:42.000 --> 52:44.000
prediction model最忠實的決定

52:44.000 --> 52:46.000
所以在看這個speed

52:46.000 --> 52:48.000
這個boat

52:48.000 --> 52:50.000
這個level才是比較

52:50.000 --> 52:52.000
合理的

52:52.000 --> 52:54.000
好

52:54.000 --> 52:56.000
可能多拖了五分鐘

52:56.000 --> 52:58.000
那今天演講就

52:58.000 --> 53:00.000
大概到這裡

53:00.000 --> 53:02.000
謝謝大家

53:06.000 --> 53:08.000
有問題都可以提問

53:10.000 --> 53:12.000
我想確認一下

53:12.000 --> 53:14.000
因為現在沒有什麼設定上的問題

53:14.000 --> 53:16.000
所以說

53:16.000 --> 53:18.000
有問題的人

53:18.000 --> 53:20.000
可以直接問一下

53:36.000 --> 53:38.000
你好

53:38.000 --> 53:40.000
我在聊天室有一個問題

53:40.000 --> 53:42.000
好想要問

53:42.000 --> 53:44.000
因為我主要是做tree-based model

53:44.000 --> 53:46.000
就是提升資源區那種

53:46.000 --> 53:48.000
然後因為

53:48.000 --> 53:50.000
就是在很多那種數據競賽

53:50.000 --> 53:52.000
或是實用目的可能比較少

53:52.000 --> 53:54.000
那數據競賽

53:54.000 --> 53:56.000
最後都會結合很多

53:56.000 --> 53:58.000
模型然後塞在一起

53:58.000 --> 54:00.000
那這種也可以用XAI這種方式嗎

54:02.000 --> 54:04.000
如果是post hoc的話

54:04.000 --> 54:06.000
你是用什麼

54:06.000 --> 54:08.000
XGBoost或是LightGBM

54:08.000 --> 54:10.000
例如說我最近就有研究

54:10.000 --> 54:12.000
就是XGBoost加LightGBM

54:12.000 --> 54:14.000
然後結果再把它拼進去

54:14.000 --> 54:16.000
OK

54:16.000 --> 54:18.000
我必須要說XGBoost那些

54:18.000 --> 54:20.000
縱使它是樹模型

54:20.000 --> 54:22.000
但是它中間有很多不同的layer

54:22.000 --> 54:24.000
它其實是不能解釋的

54:24.000 --> 54:26.000
所以一個可解

54:26.000 --> 54:28.000
如果我們要說一個模型本身具有可解釋性

54:28.000 --> 54:30.000
那它每一個component都要是清楚的

54:30.000 --> 54:32.000
就縱使中間有一個

54:32.000 --> 54:34.000
是不清楚的

54:34.000 --> 54:36.000
那它整個就是不清楚的

54:36.000 --> 54:38.000
XGBoost或是LightGBM

54:38.000 --> 54:40.000
都必須要用post hoc的方式

54:40.000 --> 54:42.000
所以用那些package

54:42.000 --> 54:44.000
應該是可以去幫你做

54:44.000 --> 54:46.000
解釋上面的動作

54:48.000 --> 54:50.000
那如果我今天的模型的預測

54:50.000 --> 54:52.000
是基於這四種模型的

54:52.000 --> 54:54.000
預測結果去做平均的話

54:54.000 --> 54:56.000
那我有辦法

54:56.000 --> 54:58.000
一樣用你剛說的工具去回推

54:58.000 --> 55:00.000
就是它綜合的

55:00.000 --> 55:02.000
這樣子的性能

55:02.000 --> 55:04.000
然後它的feature對哪一些

55:04.000 --> 55:06.000
feature是哪一些重要的嗎

55:06.000 --> 55:08.000
你四個模型

55:08.000 --> 55:10.000
都是不同的模型嗎

55:10.000 --> 55:12.000
就是其實我是用

55:12.000 --> 55:14.000
不同的模型

55:14.000 --> 55:16.000
然後去訓練

55:16.000 --> 55:18.000
然後去預測出結果

55:18.000 --> 55:20.000
只是說我最終的預測是這四個模型的輸出

55:20.000 --> 55:22.000
再去做平均

55:22.000 --> 55:24.000
ok 就單純的insampling這樣

55:24.000 --> 55:26.000
ok

55:26.000 --> 55:28.000
那我覺得是不太

55:28.000 --> 55:30.000
不太能對四個模型的綜合

55:30.000 --> 55:32.000
做評估

55:32.000 --> 55:34.000
四個模型各做一個評估

55:34.000 --> 55:36.000
對對

55:36.000 --> 55:38.000
例如說我explanation score

55:38.000 --> 55:40.000
把它加起來平均

55:40.000 --> 55:42.000
對對對

55:42.000 --> 55:44.000
ok 謝謝

55:44.000 --> 55:46.000
ok 謝謝

55:46.000 --> 55:48.000
謝謝你的問題

55:48.000 --> 55:50.000
然後推薦系統中

55:50.000 --> 55:52.000
遇到全新的商品

55:52.000 --> 55:54.000
可能啊可能

55:54.000 --> 55:56.000
例如說我就舉個最簡單的例子

55:56.000 --> 55:58.000
就為什麼一些

55:58.000 --> 56:00.000
假設Netflix它一進去

56:00.000 --> 56:02.000
它叫你按說你喜歡什麼歌

56:02.000 --> 56:04.000
你喜歡什麼藝人

56:04.000 --> 56:06.000
你看過什麼影片

56:06.000 --> 56:08.000
這些東西就是在建立你的個人的profile

56:08.000 --> 56:10.000
那透過這些相同的profile

56:10.000 --> 56:12.000
我可以去找到跟你

56:12.000 --> 56:14.000
習性比較像的使用者

56:14.000 --> 56:16.000
那我就把這些使用者的先前的習慣

56:16.000 --> 56:18.000
當作你最一開始的推薦結果

56:18.000 --> 56:20.000
那你當然之後

56:20.000 --> 56:22.000
拿到這些推薦結果你會繼續再做你自己的使用嘛

56:22.000 --> 56:24.000
那再把你自己的使用的

56:24.000 --> 56:26.000
這些東西加上別人的

56:26.000 --> 56:28.000
一些以前的習慣

56:28.000 --> 56:30.000
如何來弄起來

56:30.000 --> 56:32.000
然後做最後的

56:32.000 --> 56:34.000
例如說你使用半年之後

56:34.000 --> 56:36.000
你會發現結果越來越準

56:36.000 --> 56:38.000
就是因為它推薦系統模型

56:38.000 --> 56:40.000
你在經歷的過程

56:40.000 --> 56:42.000
ok 希望有回答到你的問題

56:44.000 --> 56:46.000
推薦的入門

56:46.000 --> 56:48.000
ok 有

56:48.000 --> 56:50.000
就是你可以上網搜

56:50.000 --> 56:52.000
XAI什麼block

56:52.000 --> 56:54.000
有一個寫的蠻好的

56:54.000 --> 56:56.000
算是一本書吧

56:56.000 --> 56:58.000
五個章節還是十個章節

56:58.000 --> 57:00.000
然後我覺得他寫的蠻好的

57:00.000 --> 57:02.000
然後包括現在這個XAI很多人在做

57:02.000 --> 57:04.000
所以你去youtube上面

57:04.000 --> 57:06.000
打例如introduction to

57:06.000 --> 57:08.000
XAI可能也會跳出一些

57:08.000 --> 57:10.000
不錯的影片

57:10.000 --> 57:12.000
但我蠻推薦我剛才說的那個block

57:12.000 --> 57:14.000
ok

57:14.000 --> 57:16.000
特徵

57:16.000 --> 57:18.000
ok 在這邊

57:18.000 --> 57:20.000
其實都

57:20.000 --> 57:22.000
ok

57:22.000 --> 57:24.000
在我剛才講的

57:24.000 --> 57:26.000
這個假設

57:26.000 --> 57:28.000
它其實都假設每個特徵

57:28.000 --> 57:30.000
是獨立的

57:30.000 --> 57:32.000
那也有人在

57:32.000 --> 57:34.000
研究說

57:34.000 --> 57:36.000
那今天特徵相異的時候

57:36.000 --> 57:38.000
我應該要怎麼去

57:38.000 --> 57:40.000
去分析

57:40.000 --> 57:42.000
那這些人就是比較著重在研究

57:42.000 --> 57:44.000
feature interaction的這種XAI的方式

57:44.000 --> 57:46.000
有有有

57:46.000 --> 57:48.000
有些人在做這些方式

57:48.000 --> 57:50.000
我記得上海

57:50.000 --> 57:52.000
上海交通大學的

57:52.000 --> 57:54.000
張全時老師的實驗室

57:54.000 --> 57:56.000
有一些papers在研究

57:56.000 --> 57:58.000
這方面的東西

57:58.000 --> 58:00.000
如果你有興趣可以去看一下

58:00.000 --> 58:02.000
我相信問出這麼specific的問題的人應該

58:02.000 --> 58:04.000
對這方面應該是稍微有研究

58:04.000 --> 58:06.000
或是其實是個expert

58:06.000 --> 58:08.000
所以這樣講應該

58:08.000 --> 58:10.000
希望能回答到你的問題

58:10.000 --> 58:12.000
ok

58:12.000 --> 58:14.000
ok

58:14.000 --> 58:16.000
這種

58:16.000 --> 58:18.000
這種全新商品的推薦模型

58:18.000 --> 58:20.000
叫做

58:20.000 --> 58:22.000
co-start

58:22.000 --> 58:24.000
recommendation

58:24.000 --> 58:26.000
ok

58:26.000 --> 58:28.000
很多很多

58:28.000 --> 58:30.000
很多很多方式

58:30.000 --> 58:32.000
我宣傳一下

58:32.000 --> 58:34.000
我之前發了一篇paper

58:34.000 --> 58:36.000
叫做TPR

58:36.000 --> 58:38.000
textual Preference

58:38.000 --> 58:40.000
Ranking

58:40.000 --> 58:42.000
這個東西它可以透過

58:42.000 --> 58:44.000
不同的使用者特徵

58:44.000 --> 58:46.000
然後來達到這種

58:46.000 --> 58:48.000
你剛才所謂的這種

58:48.000 --> 58:50.000
co-start recommendation

58:50.000 --> 58:52.000
的推薦效果

58:52.000 --> 58:54.000
對

58:54.000 --> 58:56.000
這是我之前自己發的

58:56.000 --> 58:58.000
跟我們商量一下

58:58.000 --> 59:00.000
或者是你可以去找

59:00.000 --> 59:02.000
找有一個

59:02.000 --> 59:04.000
UCSD有一個老師叫Julia McCauley

59:04.000 --> 59:06.000
他們實驗室也有很多人在做

59:06.000 --> 59:08.000
這種co-start recommendation

59:08.000 --> 59:10.000
然後你打這個關鍵字

59:10.000 --> 59:12.000
應該可以跳出很多

59:12.000 --> 59:14.000
ok

59:14.000 --> 59:16.000
希望回答到你的問題

59:20.000 --> 59:22.000
喂

59:22.000 --> 59:24.000
不好意思我想要問一個問題

59:24.000 --> 59:26.000
就是你剛剛在訓練

59:26.000 --> 59:28.000
那個狗狗的圖

59:28.000 --> 59:30.000
就是positive跟negative

59:30.000 --> 59:32.000
的那個狗狗的圖

59:32.000 --> 59:34.000
跟對比下面

59:34.000 --> 59:36.000
下面一張

59:36.000 --> 59:38.000
那個錯誤的圖

59:38.000 --> 59:40.000
然後你把狗狗的那個眼睛拔掉了

59:40.000 --> 59:42.000
那我在想說

59:42.000 --> 59:44.000
你在那個錯誤的答案

59:44.000 --> 59:46.000
是否就是

59:46.000 --> 59:48.000
也許把那隻狗狗的眼睛貼上去

59:48.000 --> 59:50.000
會不會是

59:50.000 --> 59:52.000
會不會讓訓練的效果比較好

59:52.000 --> 59:54.000
喔

59:54.000 --> 59:56.000
我這邊只是給一個

59:56.000 --> 59:58.000
例子

59:58.000 --> 01:00:00.000
但其實像我真的在訓練的時候

01:00:00.000 --> 01:00:02.000
例如說我是

01:00:02.000 --> 01:00:04.000
假設他是一個32x32的圖片

01:00:04.000 --> 01:00:06.000
那我就是給一個random mask

01:00:06.000 --> 01:00:08.000
那這個random mask套上去的時候

01:00:08.000 --> 01:00:10.000
他其實並不會

01:00:10.000 --> 01:00:12.000
例如說並不會單純聚在這

01:00:12.000 --> 01:00:14.000
他可能是散佈在一張圖片的不同地方

01:00:14.000 --> 01:00:16.000
那例如說散佈在不同圖片的地方

01:00:16.000 --> 01:00:18.000
我怎麼確認這個

01:00:18.000 --> 01:00:20.000
被遮住過後的

01:00:20.000 --> 01:00:22.000
這種

01:00:22.000 --> 01:00:24.000
example是positive

01:00:24.000 --> 01:00:26.000
我就把他丟到原本的prediction model去看說

01:00:26.000 --> 01:00:28.000
他prediction的

01:00:28.000 --> 01:00:30.000
例如說predict出來他是狗的分數

01:00:30.000 --> 01:00:32.000
是不是掉很多

01:00:32.000 --> 01:00:34.000
如果他幾乎沒掉

01:00:34.000 --> 01:00:36.000
那我就可以肯定說我遮住的這些feature

01:00:36.000 --> 01:00:38.000
所以我覺得你剛才說的是對的

01:00:38.000 --> 01:00:40.000
例如說我今天把這個東西

01:00:40.000 --> 01:00:42.000
遮到其他地方是有可能

01:00:42.000 --> 01:00:44.000
幫助模型訓練是有可能

01:00:44.000 --> 01:00:46.000
所以我在

01:00:46.000 --> 01:00:48.000
這個是比較technical detail

01:00:48.000 --> 01:00:50.000
我在訓練的時候其實是真的有做這件事情

01:00:50.000 --> 01:00:52.000
所以像

01:00:52.000 --> 01:00:54.000
像這種方式直接遮的話

01:00:54.000 --> 01:00:56.000
那他有可能遮出來的東西

01:00:56.000 --> 01:00:58.000
是例如說

01:00:58.000 --> 01:01:00.000
我就直接遮掉很重要的東西是有可能

01:01:00.000 --> 01:01:02.000
我覺得你的問題

01:01:02.000 --> 01:01:04.000
是一個很好的問題

01:01:04.000 --> 01:01:06.000
ok謝謝

01:01:06.000 --> 01:01:08.000
好謝謝

01:01:08.000 --> 01:01:10.000
喂你好

01:01:10.000 --> 01:01:12.000
哈囉

01:01:12.000 --> 01:01:14.000
欸sorry我的那個

01:01:14.000 --> 01:01:16.000
鏡頭壞掉我把你打開

01:01:16.000 --> 01:01:18.000
沒關係沒關係我們很free

01:01:18.000 --> 01:01:20.000
對

01:01:20.000 --> 01:01:22.000
那Alan跟我想問一下那個

01:01:22.000 --> 01:01:24.000
第12頁那個sharp t value那邊

01:01:24.000 --> 01:01:26.000
ok

01:01:26.000 --> 01:01:28.000
就是我想確認我的那個觀念沒有錯

01:01:28.000 --> 01:01:30.000
ok

01:01:30.000 --> 01:01:32.000
我可以理解他是

01:01:32.000 --> 01:01:34.000
我可以理解那difference那邊

01:01:34.000 --> 01:01:36.000
是我把某些feature拿掉以後

01:01:36.000 --> 01:01:38.000
然後他的difference如果很大的話

01:01:38.000 --> 01:01:40.000
就代表那個feature是重要的

01:01:40.000 --> 01:01:42.000
是這樣理解嗎

01:01:42.000 --> 01:01:44.000
對沒錯

01:01:44.000 --> 01:01:46.000
因為他改變了原本的prediction的pattern

01:01:46.000 --> 01:01:48.000
那下一頁

01:01:48.000 --> 01:01:50.000
那個第13頁就是

01:01:50.000 --> 01:01:52.000
在您的paper中

01:01:52.000 --> 01:01:54.000
就是有建立各個feature之間的

01:01:54.000 --> 01:01:56.000
的關係

01:01:56.000 --> 01:01:58.000
那個矩陣

01:01:58.000 --> 01:02:00.000
我可以把它理解成是那種

01:02:00.000 --> 01:02:02.000
的這種關係嗎

01:02:02.000 --> 01:02:04.000
可以

01:02:04.000 --> 01:02:06.000
那為什麼

01:02:06.000 --> 01:02:08.000
為什麼他們相關係數

01:02:08.000 --> 01:02:10.000
的高或低會跟

01:02:10.000 --> 01:02:12.000
就是我預測出來他這個feature

01:02:12.000 --> 01:02:14.000
對結果的重要性是有關的

01:02:14.000 --> 01:02:16.000
之間的分別

01:02:16.000 --> 01:02:18.000
跟

01:02:18.000 --> 01:02:20.000
你還有抱歉打斷你

01:02:20.000 --> 01:02:22.000
我太興奮了

01:02:22.000 --> 01:02:24.000
ok好

01:02:24.000 --> 01:02:26.000
那這個東西為什麼跟那個prediction有關

01:02:26.000 --> 01:02:28.000
是因為我這個矩陣

01:02:28.000 --> 01:02:30.000
你可以把它理解成一種相關係數

01:02:30.000 --> 01:02:32.000
但他不是完全是correlation

01:02:32.000 --> 01:02:34.000
他是基於我們model

01:02:34.000 --> 01:02:36.000
predict出來的中間那些prediction

01:02:36.000 --> 01:02:38.000
predict出來的那個model weight

01:02:38.000 --> 01:02:40.000
我們把那個model weight抽出來

01:02:40.000 --> 01:02:42.000
去建立這個

01:02:42.000 --> 01:02:44.000
feature跟feature之間關係的

01:02:44.000 --> 01:02:46.000
這種權重的

01:02:46.000 --> 01:02:48.000
的矩陣

01:02:48.000 --> 01:02:50.000
的這種權重圖

01:02:50.000 --> 01:02:52.000
所以他這裡面每一個圖片的

01:02:52.000 --> 01:02:54.000
每一個色塊的這種interaction

01:02:54.000 --> 01:02:56.000
的高或低都是

01:02:56.000 --> 01:02:58.000
based on

01:02:58.000 --> 01:03:00.000
model prediction的結果

01:03:00.000 --> 01:03:02.000
所以我們才可以拿這個東西

01:03:02.000 --> 01:03:04.000
當作去減少這種

01:03:04.000 --> 01:03:06.000
計算combination

01:03:06.000 --> 01:03:08.000
的依據

01:03:08.000 --> 01:03:10.000
他不單單只是

01:03:10.000 --> 01:03:12.000
對

01:03:12.000 --> 01:03:14.000
就是例如說我可能

01:03:14.000 --> 01:03:16.000
換了另外一個input

01:03:16.000 --> 01:03:18.000
然後可能兩個feature同時的可能增加

01:03:18.000 --> 01:03:20.000
或減少或是之間的關聯

01:03:20.000 --> 01:03:22.000
然後是based在這個weight的變化

01:03:22.000 --> 01:03:24.000
去建那個matrix

01:03:24.000 --> 01:03:26.000
例如說你可以

01:03:26.000 --> 01:03:28.000
把

01:03:28.000 --> 01:03:30.000
今天這個model裡面某一個

01:03:30.000 --> 01:03:32.000
乘的weight拿出來

01:03:32.000 --> 01:03:34.000
那我們就可以去透過這個weight

01:03:34.000 --> 01:03:36.000
可以去看說今天這個x1跟這個x2

01:03:36.000 --> 01:03:38.000
他是不是

01:03:38.000 --> 01:03:40.000
例如說他interaction是不是比較大

01:03:40.000 --> 01:03:42.000
或是今天x1跟x3是不是比較大

01:03:42.000 --> 01:03:44.000
類似這樣

01:03:46.000 --> 01:03:48.000
可以想像是變化的

01:03:48.000 --> 01:03:50.000
一起變化的那種感覺嗎

01:03:50.000 --> 01:03:52.000
就一起變

01:03:52.000 --> 01:03:54.000
就例如說

01:03:54.000 --> 01:03:56.000
今天這個x1加x3

01:03:56.000 --> 01:03:58.000
變化嗎

01:03:58.000 --> 01:04:00.000
應該說今天這個x1x3對於

01:04:00.000 --> 01:04:02.000
模型的訓練

01:04:02.000 --> 01:04:04.000
這邊先不牽扯到變化

01:04:04.000 --> 01:04:06.000
我們單純看說

01:04:06.000 --> 01:04:08.000
就單純把這個模型扒開

01:04:08.000 --> 01:04:10.000
然後強制拿裡面的某一個

01:04:10.000 --> 01:04:12.000
訓練的權重出來

01:04:12.000 --> 01:04:14.000
拿出這個訓練的權重出來

01:04:14.000 --> 01:04:16.000
我們可以去提取

01:04:16.000 --> 01:04:18.000
就是說這個x1跟x3

01:04:18.000 --> 01:04:20.000
加在一起的時候

01:04:20.000 --> 01:04:22.000
對於這個prediction來說到底多重要

01:04:22.000 --> 01:04:24.000
類似這樣

01:04:24.000 --> 01:04:26.000
例如說他的x1加x3

01:04:26.000 --> 01:04:28.000
那條weight很粗

01:04:28.000 --> 01:04:30.000
例如說可能是0.8

01:04:30.000 --> 01:04:32.000
那我們就把這個0.8

01:04:32.000 --> 01:04:34.000
貼在這,類似這樣

01:04:34.000 --> 01:04:36.000
先沒有牽扯到後面

01:04:38.000 --> 01:04:40.000
那還有一個小問題想問

01:04:40.000 --> 01:04:42.000
沒事沒事

01:04:42.000 --> 01:04:44.000
因為我們現在可能討論說

01:04:44.000 --> 01:04:46.000
單一某一個feature對結果的重要程度

01:04:46.000 --> 01:04:48.000
到底是高還低

01:04:48.000 --> 01:04:50.000
那會不會說有一種狀況是說

01:04:50.000 --> 01:04:52.000
獨立看A他其實影響不大

01:04:52.000 --> 01:04:54.000
獨立看B他影響也不大

01:04:54.000 --> 01:04:56.000
但是然而只有A跟B

01:04:56.000 --> 01:04:58.000
同時存在的時候

01:04:58.000 --> 01:05:00.000
他對結果是影響很大的

01:05:00.000 --> 01:05:02.000
那這種情況我要怎麼去解釋他

01:05:02.000 --> 01:05:04.000
ok

01:05:04.000 --> 01:05:06.000
我覺得這個問題很好

01:05:06.000 --> 01:05:08.000
我覺得這個問題跟前面

01:05:08.000 --> 01:05:10.000
就是

01:05:10.000 --> 01:05:12.000
有一個人提到

01:05:12.000 --> 01:05:14.000
就是今天這個feature

01:05:14.000 --> 01:05:16.000
不是相依的這種情況

01:05:16.000 --> 01:05:18.000
產生你剛才那個問題

01:05:18.000 --> 01:05:20.000
就是今天就是A B不是獨立的

01:05:20.000 --> 01:05:22.000
那A B加在一起

01:05:22.000 --> 01:05:24.000
如果A B不是獨立

01:05:24.000 --> 01:05:26.000
那A不重要B不重要

01:05:26.000 --> 01:05:28.000
那A加B有可能重要

01:05:28.000 --> 01:05:30.000
有可能產生這種情形

01:05:30.000 --> 01:05:32.000
那這種方式要怎麼去判斷

01:05:32.000 --> 01:05:34.000
例如說我們去設計這個

01:05:34.000 --> 01:05:36.000
explanation model的時候

01:05:36.000 --> 01:05:38.000
我們本來就要去考慮

01:05:38.000 --> 01:05:40.000
就不能把這種feature獨立的這種

01:05:40.000 --> 01:05:42.000
假設加在我們explanation model的設計

01:05:42.000 --> 01:05:44.000
所以像是我今天這兩個word

01:05:44.000 --> 01:05:46.000
就是不管是剛才那個

01:05:46.000 --> 01:05:48.000
九宮格或是這個word

01:05:48.000 --> 01:05:50.000
我們都假設每一個feature是獨立的

01:05:50.000 --> 01:05:52.000
所以我們在算的時候我們都把每一個feature當作

01:05:52.000 --> 01:05:54.000
算是一個

01:05:54.000 --> 01:05:56.000
特徵來算

01:05:56.000 --> 01:05:58.000
但如果你今天要考慮那種feature不同組合的話

01:05:58.000 --> 01:06:00.000
那你考慮的層面可能

01:06:00.000 --> 01:06:02.000
例如說你並不單只是

01:06:02.000 --> 01:06:04.000
考慮A B C D

01:06:04.000 --> 01:06:06.000
你可能還要考慮A B C D

01:06:06.000 --> 01:06:08.000
A加B A加C A加D

01:06:08.000 --> 01:06:10.000
當作A加D等於

01:06:10.000 --> 01:06:12.000
假設等於F類似這種

01:06:12.000 --> 01:06:14.000
就是你把A加B

01:06:14.000 --> 01:06:16.000
當作一個新的feature來分析

01:06:16.000 --> 01:06:18.000
這是我目前想要比較naive的

01:06:18.000 --> 01:06:20.000
解法

01:06:20.000 --> 01:06:22.000
我記得就是我剛才講的交通大學

01:06:22.000 --> 01:06:24.000
上海交通大學的那個張宣世老師

01:06:24.000 --> 01:06:26.000
他們的實驗室

01:06:26.000 --> 01:06:28.000
有人在研究這種

01:06:28.000 --> 01:06:30.000
這種feature interaction

01:06:30.000 --> 01:06:32.000
組合的

01:06:32.000 --> 01:06:34.000
貢獻度

01:06:34.000 --> 01:06:36.000
對於最後prediction

01:06:36.000 --> 01:06:38.000
就是explanation的performance的影響

01:06:38.000 --> 01:06:40.000
是有人在做這個

01:06:40.000 --> 01:06:42.000
但我的研究就比較

01:06:42.000 --> 01:06:44.000
focus在基於

01:06:44.000 --> 01:06:46.000
這種獨立feature的

01:06:46.000 --> 01:06:48.000
對對對

01:06:48.000 --> 01:06:50.000
了解

01:06:50.000 --> 01:06:52.000
謝謝Alan

01:06:52.000 --> 01:06:54.000
感謝你的問題

01:07:04.000 --> 01:07:06.000
討論區裡面有一個人有問說

01:07:06.000 --> 01:07:08.000
random mask影像的時候

01:07:08.000 --> 01:07:10.000
mask size要怎麼選擇

01:07:10.000 --> 01:07:12.000
mask size

01:07:12.000 --> 01:07:14.000
你是說整張mask的

01:07:14.000 --> 01:07:16.000
大小嗎

01:07:16.000 --> 01:07:18.000
還是

01:07:18.000 --> 01:07:20.000
比如說像狗的眼睛

01:07:20.000 --> 01:07:22.000
那一個

01:07:22.000 --> 01:07:24.000
比如說你今天

01:07:24.000 --> 01:07:26.000
你今天mask如果是

01:07:26.000 --> 01:07:28.000
比眼睛還要小的話

01:07:28.000 --> 01:07:30.000
這個mask是有效的

01:07:30.000 --> 01:07:32.000
ok

01:07:32.000 --> 01:07:34.000
我們在做這種東西的時候

01:07:34.000 --> 01:07:36.000
我們最小單位是一個pixel

01:07:36.000 --> 01:07:38.000
對所以

01:07:38.000 --> 01:07:40.000
我們應該不可能比pixel還要小

01:07:40.000 --> 01:07:42.000
對所以例如說

01:07:42.000 --> 01:07:44.000
如果mask size你再問說

01:07:44.000 --> 01:07:46.000
要mask幾個的話

01:07:46.000 --> 01:07:48.000
這種東西就是

01:07:48.000 --> 01:07:50.000
例如說我們在random的時候完全不去

01:07:50.000 --> 01:07:52.000
我在這個works裡面完全沒有去限制

01:07:52.000 --> 01:07:54.000
但我知道有人是會去限制說

01:07:54.000 --> 01:07:56.000
我不希望我遮掉的東西太多

01:07:56.000 --> 01:07:58.000
就是為什麼不希望

01:07:58.000 --> 01:08:00.000
遮掉的東西太多是因為

01:08:00.000 --> 01:08:02.000
如果遮太多的話其實會反而導致

01:08:02.000 --> 01:08:04.000
他那個random出來的結果不這麼positive

01:08:04.000 --> 01:08:06.000
類似這樣

01:08:06.000 --> 01:08:08.000
或是根據他的目的

01:08:08.000 --> 01:08:10.000
而有不同的那個

01:08:10.000 --> 01:08:12.000
選擇類似這樣

01:08:12.000 --> 01:08:14.000
或是他可能只要挑幾個很重要的

01:08:14.000 --> 01:08:16.000
feature出來

01:08:16.000 --> 01:08:18.000
那我今天如果mask掉了

01:08:18.000 --> 01:08:20.000
例如說很少部分

01:08:20.000 --> 01:08:22.000
那其實就不是我們樂見的結果

01:08:22.000 --> 01:08:24.000
類似這樣

01:08:24.000 --> 01:08:26.000
但如果你是說mask的那個

01:08:26.000 --> 01:08:28.000
那個dimension的話

01:08:28.000 --> 01:08:30.000
那他必須要跟原本的image的dimension是一樣

01:08:30.000 --> 01:08:32.000
不然我蓋不上去

01:08:32.000 --> 01:08:34.000
對

01:08:34.000 --> 01:08:36.000
OK

01:08:36.000 --> 01:08:38.000
希望有回答到你的問題

01:08:40.000 --> 01:08:42.000
這是安德嗎

01:08:44.000 --> 01:08:46.000
OK

01:08:46.000 --> 01:08:48.000
好 謝謝

01:08:48.000 --> 01:08:50.000
感覺這個有點人為決定

01:08:50.000 --> 01:08:52.000
因為我會覺得說像比如說你今天mask size

01:08:52.000 --> 01:08:54.000
如果今天你的mask size

01:08:54.000 --> 01:08:56.000
是

01:08:56.000 --> 01:08:58.000
感覺上是有一種所謂的feature

01:08:58.000 --> 01:09:00.000
的最小單位的這種概念

01:09:00.000 --> 01:09:02.000
就是說因為你今天眼睛

01:09:02.000 --> 01:09:04.000
feature的最小單位

01:09:04.000 --> 01:09:06.000
你一定會超過一個pixel

01:09:06.000 --> 01:09:08.000
因為你的

01:09:08.000 --> 01:09:10.000
情況下面

01:09:10.000 --> 01:09:12.000
我不知道 我只是覺得說

01:09:12.000 --> 01:09:14.000
這個問題我也不知道怎麼回答

01:09:14.000 --> 01:09:16.000
或者是說這個問題

01:09:16.000 --> 01:09:18.000
有時候感覺上你這個size的選擇

01:09:18.000 --> 01:09:20.000
是不是其實蠻主觀的

01:09:20.000 --> 01:09:22.000
這個選擇

01:09:22.000 --> 01:09:24.000
其實有人在做

01:09:24.000 --> 01:09:26.000
就是例如說

01:09:26.000 --> 01:09:28.000
也不是有人在做

01:09:28.000 --> 01:09:30.000
其中一個方法就是例如說

01:09:30.000 --> 01:09:32.000
他知道他指這些component

01:09:32.000 --> 01:09:34.000
這些object都是比較大

01:09:34.000 --> 01:09:36.000
例如眼睛是比較大

01:09:36.000 --> 01:09:38.000
那他就例如說他的mask他就不是random

01:09:38.000 --> 01:09:40.000
每一個pixel 他是random

01:09:40.000 --> 01:09:42.000
每4x4 pixel

01:09:42.000 --> 01:09:44.000
random每6x6 pixel

01:09:44.000 --> 01:09:46.000
那透過這個方式去遮

01:09:46.000 --> 01:09:48.000
也有人是這樣做

01:09:48.000 --> 01:09:50.000
但我覺得這個東西就是

01:09:50.000 --> 01:09:52.000
因為我們都已經不知道模型

01:09:52.000 --> 01:09:54.000
他在乎的pixel可能是眼睛的某

01:09:54.000 --> 01:09:56.000
右上加左下

01:09:56.000 --> 01:09:58.000
類似這種

01:09:58.000 --> 01:10:00.000
如果給這麼強的假設

01:10:00.000 --> 01:10:02.000
我覺得模型訓練會比較不好

01:10:02.000 --> 01:10:04.000
就是這種

01:10:04.000 --> 01:10:06.000
explanation model會比較不好

01:10:06.000 --> 01:10:08.000
當然你說的是

01:10:08.000 --> 01:10:10.000
exactly right

01:10:10.000 --> 01:10:12.000
真的是這樣子

01:10:12.000 --> 01:10:14.000
其實有人詬病說

01:10:14.000 --> 01:10:16.000
這種做出來的解釋

01:10:16.000 --> 01:10:18.000
他其實就是大概

01:10:18.000 --> 01:10:20.000
遮一下遮一下

01:10:20.000 --> 01:10:22.000
例如說大概秀出

01:10:22.000 --> 01:10:24.000
其實這邊眼睛也沒遮到

01:10:24.000 --> 01:10:26.000
其實有可能就是

01:10:26.000 --> 01:10:28.000
因為這種mask

01:10:28.000 --> 01:10:30.000
一開始還沒有做好的原因

01:10:30.000 --> 01:10:32.000
導致我最後給出來的東西

01:10:32.000 --> 01:10:34.000
不這麼comprehensive

01:10:36.000 --> 01:10:38.000
這是有可能

01:10:38.000 --> 01:10:40.000
我覺得這個問題超好的

01:10:40.000 --> 01:10:42.000
好

01:10:52.000 --> 01:10:54.000
感覺上

01:10:54.000 --> 01:10:56.000
聊天室裡面沒有新的問題

01:10:58.000 --> 01:11:00.000
那我們再謝謝一次

01:11:00.000 --> 01:11:02.000
裕能今天的演講

01:11:04.000 --> 01:11:06.000
謝謝大家

01:11:10.000 --> 01:11:12.000
謝謝

01:11:12.000 --> 01:11:14.000
感謝大家的捧場

01:11:14.000 --> 01:11:16.000
今天有幾個人

01:11:16.000 --> 01:11:18.000
我比較好奇

01:11:18.000 --> 01:11:20.000
因為我看不到

01:11:20.000 --> 01:11:22.000
今天我這邊看到是有

01:11:22.000 --> 01:11:24.000
最高有到32

01:11:24.000 --> 01:11:26.000
才這麼多

01:11:26.000 --> 01:11:28.000
我發現好多人

01:11:28.000 --> 01:11:30.000
是怎樣間領域的關係

01:11:32.000 --> 01:11:34.000
可是沒有人幫我的facebook按讚

01:11:34.000 --> 01:11:36.000
開玩笑

01:11:36.000 --> 01:11:38.000
必須說我之前

01:11:38.000 --> 01:11:40.000
給過一次我本身是物理專業的

01:11:40.000 --> 01:11:42.000
total五個人

01:11:44.000 --> 01:11:46.000
可能這個東西

01:11:46.000 --> 01:11:48.000
大家比較好奇

01:11:48.000 --> 01:11:50.000
然後不知道怎麼入門吧

01:11:50.000 --> 01:11:52.000
我也不知道

01:11:52.000 --> 01:11:54.000
不過蠻開心有這個機會

01:11:54.000 --> 01:11:56.000
分享一下所學

01:11:56.000 --> 01:11:58.000
不然都在吃社會資源

01:11:58.000 --> 01:12:00.000
總要貢獻一下

01:12:00.000 --> 01:12:02.000
吃社會資源

01:12:02.000 --> 01:12:04.000
對

01:12:04.000 --> 01:12:06.000
你是高虹安嗎

01:12:06.000 --> 01:12:08.000
沒有

01:12:08.000 --> 01:12:10.000
我不是台大的

01:12:10.000 --> 01:12:12.000
沒事

01:12:12.000 --> 01:12:14.000
我全身而退

01:12:14.000 --> 01:12:16.000
OK

01:12:16.000 --> 01:12:18.000
感謝

01:12:18.000 --> 01:12:20.000
突然在外查

01:12:20.000 --> 01:12:22.000
補問一個問題

01:12:22.000 --> 01:12:24.000
比如說你前一張投影片裡面

01:12:26.000 --> 01:12:28.000
這個嗎

01:12:28.000 --> 01:12:30.000
那個舉證的

01:12:30.000 --> 01:12:32.000
這個嗎

01:12:32.000 --> 01:12:34.000
對對對

01:12:34.000 --> 01:12:36.000
舉證對角

01:12:36.000 --> 01:12:38.000
所有這種x1 x1 x2 x2

01:12:38.000 --> 01:12:40.000
你自動忽略是這個意思嗎

01:12:40.000 --> 01:12:42.000
因為他們在這個

01:12:42.000 --> 01:12:44.000
自己跟自己的關係

01:12:44.000 --> 01:12:46.000
就是不討論

01:12:46.000 --> 01:12:48.000
我們就不討論

01:12:48.000 --> 01:12:50.000
因為可能在這個問題定義

01:12:50.000 --> 01:12:52.000
定義裡面就沒有意義的

01:12:52.000 --> 01:12:54.000
感覺

01:12:54.000 --> 01:12:56.000
因為shopping value

01:12:56.000 --> 01:12:58.000
自己跟自己還是自己

01:12:58.000 --> 01:13:00.000
自己跟自己的組合還是自己

01:13:00.000 --> 01:13:02.000
所以我們就不討論

01:13:02.000 --> 01:13:04.000
那搶單兒自己跟自己

01:13:04.000 --> 01:13:06.000
前提是高度相關

01:13:06.000 --> 01:13:08.000
對

01:13:10.000 --> 01:13:12.000
其實我不知道怎麼接觸分享

01:13:12.000 --> 01:13:14.000
呵呵

01:13:16.000 --> 01:13:18.000
那大概我想一下

01:13:18.000 --> 01:13:20.000
那我可能先stop recording

01:13:20.000 --> 01:13:22.000
OK

