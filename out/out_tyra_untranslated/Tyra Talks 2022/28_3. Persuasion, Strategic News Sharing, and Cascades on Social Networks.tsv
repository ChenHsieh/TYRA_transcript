start	end	text
0	3320	謝謝大家今天參與今天Project Teal的演講
3320	7940	我是今天的主持人,我是在魯汶大學念PhD的高賢
7940	10500	今天的講者是靜江
10500	16900	靜江是MIT現在的博士候選人
16900	23560	他在進入MIT之前,他在台大拿到了電機系的一個士學位
23560	29560	那他現在的研究主要是專長在人類的決策
29560	35560	還有人跟我們的社會社群媒體如何的互動
35560	40060	尤其當我們的社群媒體充斥著很多真實的訊息
40060	42060	還有假訊息的情況下
42060	47560	人跟我們的社會媒體社會網絡是怎麼互動的
47560	51560	尤其是我們的人類的行為如何隨著時間
51560	55560	漸漸的演化成一個大型規模的社會現象
55560	57560	這是他所關心的議題
57560	61560	那他也會特別用不同的理論角度
61560	65560	不同的研究方法去關心他剛剛所講的這個研究的興趣
65560	68560	那今天我們邀請他來演講這個題目呢
68560	72560	就是跟Persuasion,跟策略性的新聞的分享
72560	76560	跟社會網絡之間的關心的一個題目
76560	78560	那我就先,我就不多說了
78560	81560	馬上就請我們靜嘉來進行今天的報告
81560	84560	對,那靜嘉,the floor is yours
84560	85560	OK, thank you
85560	86560	謝謝高雄的介紹
86560	89560	大家好,我現在在MIT念書
89560	92560	然後這是我,應該是最後一年了
92560	94560	今天我要介紹的一個是我的project叫
94560	99560	Persuasion, New Sharing and Cascade on Social Networks
99560	103560	然後這是一個join work with my post-doc
104560	106560	還有我的advisor
106560	109560	然後我現在其實在engineering school底下
109560	110560	然後這個問題
110560	113560	但是我其實做的東西是比較偏economics theory
113560	116560	好,那我們開始吧
116560	119560	首先就是現在呢
119560	122560	自從這個社交媒體發明了之後
122560	126560	就其實大家改變了這種消費新聞的行為
126560	129560	像美國人大概五分之一的人就會說
129560	132560	他們其實通常都是從社交媒體上面得到新聞
132560	134560	但是這種社交媒體呢
134560	135560	然後社交媒體呢
135560	138560	它也會增進大家的這種peer-to-peer interaction
138560	139560	就是人際之間的互動
139560	143560	然後其實它也會加速這樣子訊息的流通
143560	145560	但另外我們也發現說
145560	146560	其實在social media上面
146560	149560	其實你是有點像是被動的接收訊息
149560	152560	因為通常都是你的同事
152560	155560	你的朋友決定分享給你新聞
155560	158560	或者是這些演算法去推送給你新聞
158560	160560	其實是他們subjectively
160560	164560	選擇這些新聞在你被動的去接收這樣的新聞
164560	165560	另外還有social media
165560	168560	他們主要還是想要賺錢嘛
168560	171560	那其實他們在
171560	174560	在分配這些廣告或什麼之類的時候
174560	176560	其實無形中他們也可能是
176560	178560	加速了misinformation的
178560	182560	就amplify misinformation on the platform
182560	186560	然後今天我要study的問題就是說
186560	188560	就是大家分享新聞的動機是什麼
188560	190560	就是說這個其實是大家
190560	192560	現在學界還蠻關心的問題
192560	194560	就是說到底大家分享新聞
194560	197560	是為了要去告訴別人一些事情呢
197560	198560	還是表達自己的觀點
198560	202560	或者是說他其實是想要找同溫層
202560	203560	或者是想要去
203560	206560	就persuade別人
206560	207560	那另外就是說
207560	209560	在這樣的動機之下
209560	210560	那什麼樣的新聞
210560	214560	其實會在社交媒體上面傳得比較遠
214560	218560	就包含說像可信度的這個影響呢
218560	221560	還有以及他怎麼樣去
221560	223560	以及這樣子
223560	225560	應該說他
225560	228560	跟社會的一些
228560	229560	characteristic
229560	231560	就是他的一些特性有沒有什麼關係
231560	234560	然後像這個領域他現在就像
234560	237560	我切入的角度會是economics的角度
237560	238560	然後有一些psychologist
238560	240560	他可能就會去study就是說
240560	242560	他們有沒有一些behavioral bias
242560	244560	那political science他們就比較focus
244560	245560	在misinformation
245560	248560	在政治上面的影響
248560	251560	以及他們兩黨不同的新聞
251560	253560	他們可能會比較願意去傳
253560	256560	是不是自己跟自己黨相關的新聞呢
256560	257560	或者是
257560	259560	然後他們會怎麼樣去看待
259560	261560	跟自己是同黨的新聞
261560	262560	他是可信度比較高呢
262560	263560	或可能比較少
263560	264560	然後其實像這樣子的領域
264560	267560	他們都還是還蠻仰賴CS的幫忙
267560	270560	就是他們還是會做一些computational的事情
270560	273560	那今天我的這個問題呢
273560	275560	就是接下來的model
275560	276560	都會based on一個question
276560	279560	就是說如果大家在分享的新聞
279560	280560	上面的動機
280560	283560	其實是想要去讓你的follower
283560	284560	跟你想的比較接近
284560	286560	那麼會發生什麼事
286560	287560	所以其實這個model
287560	289560	算是一個what if的question
289560	291560	就是說如果這是大家的動機
291560	292560	大家想要去讓別人
292560	294560	跟你想的比較類似的話
294560	296560	那麼
296560	298560	什麼樣的新聞你會去傳
298560	300560	然後以及什麼樣的新聞
300560	302560	可能在什麼樣的社會
302560	305560	會傳得比較遠
305560	308560	OK
308560	309560	好
309560	311560	那就我剛才講的
311560	313560	就是我是用economics的角度
313560	314560	切入這個問題
314560	316560	所以接下來我會用
316560	320560	就是所謂的賽局理論來講這個model
320560	321560	然後另外就是我們這篇work
321560	323560	已經submit到
323560	325560	就是Journal of Economics Theory
325560	328560	然後這邊有一個SSR的link
328560	330560	如果大家感興趣那個Journal version
330560	334560	想要看的話就歡迎掃碼一下
334560	336560	首先我跟大家講一下這個model
336560	339560	一些比較大的assumption
339560	340560	當然我們會考慮一個society
340560	344560	就是每一個人的這種觀念不一樣
344560	346560	就大家不是同心協力
346560	349560	都覺得就是偏左或偏右
349560	352560	大家就是可以是有不同的觀點
352560	354560	然後
354560	356560	一個人他會希望說我的
356560	359560	同儕跟我的這個想法比較類似
359560	362560	然後另外是
362560	364560	我們在這裡會用的persuasion這個詞
364560	366560	代表的是說
366560	370560	你給別人一個objective的information
370560	371560	就是一個新的information
371560	374560	去影響他的belief
374560	376560	所以當我在講persuasion的話
376560	378560	我的意思是這個意思
378560	381560	另外在分享的時候
381560	383560	會有一些成本出現
383560	386560	就是說你可以把它想成是
386560	387560	我平常一直在滑手機
387560	388560	滑到一半的時候
388560	390560	我突然覺得我得
390560	391560	突然想要傳這個新聞
391560	393560	你可以思考了一下
393560	394560	你原本是一直在滑手機
394560	395560	然後你突然得停下來
395560	398560	就model它是有一個小小的成本在裡頭
398560	400560	因為有這個小小的成本在
400560	401560	所以其實是有一個
401560	404560	在經濟上我們叫它
404560	406560	策略性替代的行為
406560	407560	strategic substitute
407560	408560	就是說
408560	410560	當別人會做這件事情的時候
410560	411560	你就不想去做這件事情
411560	412560	因為你覺得別人會幫你做
412560	414560	所以這邊你為了要省那個成本
414560	415560	你可能會覺得
415560	417560	如果別人已經都傳了這個新聞的話
417560	419560	那我就沒有傳的必要
419560	422560	另外是最後會apply這個model
422560	423560	到一個society
423560	425560	就是有分化的情形
425560	426560	兩黨分化的情形
426560	427560	然後去看說
427560	430560	在兩黨分化的這個社會底下
430560	433560	什麼狀況會導致
433560	435560	比較不準確的新聞
435560	441560	會傳的比準確的新聞還要遠
441560	443560	不好意思我可以問個問題嗎
443560	444560	我可以打斷嗎
444560	445560	沒事沒事
445560	447560	我們應該還蠻確定的
447560	449560	我想要問一下就是
449560	451560	二跟三之間的
451560	452560	算是關係吧
452560	454560	因為如果說
454560	456560	我在social media上面分享東西
456560	457560	我希望
457560	459560	就是我希望我在我的同溫層裡面
459560	462560	那
462560	463560	這時候我們講的persuasion
463560	465560	好像跟平常是我們想像的persuasion
465560	466560	很不一樣
466560	467560	譬如說如果
467560	468560	上面都是我的同溫層
468560	470560	那我PO一個東西
470560	471560	我的理解會是
471560	473560	或者是我會希望他們就接受了
473560	476560	就是沒有很多的persuasion
476560	478560	那通常我們想像的persuasion是
478560	480560	那些人可能原本是中立的
480560	482560	或甚至跟我是不一樣的
482560	483560	就是觀點
483560	484560	不一樣的立場這樣
484560	486560	然後我經過
486560	487560	分享一些新的資訊
487560	488560	來說服他們
488560	490560	譬如說加入我的陣營這樣
490560	491560	對那所以
491560	493560	如果說
493560	495560	你的model原本就是預設說
495560	496560	大家都會希望
496560	497560	待在自己的同溫層裡面
497560	499560	那他persuasion的力道
499560	500560	是不是就會比較
500560	502560	相對會比較弱一點
502560	503560	喔不好意思
503560	504560	我可能剛才沒有解釋好
504560	505560	這個第二點
505560	506560	第二點他不是只是說
506560	507560	你想在同溫層裡面
507560	508560	是說
508560	510560	我的動機是希望別人
510560	512560	跟我的想法類似
512560	513560	那個preference
513560	514560	這個意思
514560	515560	就是說你希望別人的想法
515560	516560	跟你的
516560	517560	別人做的事情
517560	519560	跟你的想法是類似的
519560	521560	他有那個persuasion的動機
521560	523560	所以其實我是在解釋
523560	524560	對
524560	525560	OK
525560	526560	謝謝
526560	527560	可能是我剛剛沒聽清楚
527560	528560	不好意思
528560	529560	好
529560	530560	這個這裡的
530560	532560	persuasion的communication mechanism
532560	533560	有在乎是一對一
533560	535560	或是一對多的嗎
535560	536560	喔那是個good question
536560	537560	就是說
537560	539560	這邊的persuasion是
539560	541560	因為我們這個model
541560	542560	比較像是在考慮
542560	543560	twitter這種東西
543560	544560	所以你就想說
544560	545560	我一
545560	546560	分享這個新聞之後
546560	547560	就是等於是
547560	549560	你所有的朋友都會看到
549560	551560	對就是一個比較
551560	552560	簡單的something
552560	553560	就是說就是broadcast
553560	555560	就是我一分享了
555560	556560	那我就可以
556560	557560	而且我知道
557560	559560	我知道我所有的朋友都會看到
559560	560560	就是比較
560560	562560	所以才會有strategic substitute
562560	564560	因為別人可以broadcast
564560	566560	對對對
566560	567560	了解
567560	568560	是的是的
568560	569560	yeah
569560	570560	that's a good question
570560	571560	對
571560	573560	就是你沒有說你特別去選
573560	574560	我要傳給誰
574560	575560	就是我一
575560	576560	我傳就只有決定說
576560	577560	我要傳
577560	578560	然後我知道我所有的follow
578560	579560	我都會看到
579560	580560	這樣
580560	582560	OK
582560	583560	嗯
583560	585560	然後這邊是給大家一個
585560	586560	就是broad picture
586560	587560	about
587560	588560	我們的這個主要的結果
588560	589560	第一個就是說
589560	590560	我們當然是刻劃這個
590560	591560	equilibrium
591560	592560	到底長什麼樣子
592560	593560	嗯
593560	595560	然後包含像我們identify
595560	596560	呃
596560	598560	大家在分享新聞的這些
598560	599560	就是說你是based on
599560	600560	這個persuasion的模式
600560	601560	但其實
601560	603560	在進一步把它decompose
603560	604560	就是把它分析出來
604560	605560	終於會發現說
605560	606560	其實更極端的人
606560	607560	他當然會想要
607560	609560	更有那一種
609560	610560	呃
610560	611560	動力去分享
611560	613560	而且當然是分享那些
613560	614560	呃
614560	616560	跟我想法是align
616560	617560	就是說哦
617560	618560	如果是偏右的極端的人
618560	619560	那當然我會去
619560	621560	想要去分享偏右的
621560	622560	我不會去
622560	623560	分享偏左的
623560	624560	另外是
624560	625560	呃
625560	626560	persuasiveness of news
626560	628560	就是說這個新聞
628560	629560	我一旦傳了出去之後
629560	630560	我可以改變
630560	632560	多少人的
632560	633560	belief
633560	634560	就是說
634560	635560	到底他成效如何
635560	636560	假如我一直傳出去
636560	637560	哎
637560	638560	其實也沒有人會相信
638560	639560	這個東西
639560	640560	沒有人在改變他的想法的話
640560	641560	那我幹嘛傳
641560	642560	呃
642560	643560	最後一個東西
643560	644560	我幫你講
644560	645560	就是說
645560	646560	你在傳的時候
646560	648560	你會去評估說
648560	649560	哎
649560	650560	到底
650560	651560	嗯
651560	652560	在均衡狀況底下
652560	653560	到底有多少人在傳這個東西
653560	654560	然後你會覺得說
654560	655560	哦
655560	656560	其實我的follower
656560	658560	說不定很大的機率
658560	659560	已經會聽到這個新聞
659560	660560	我就不需要傳了
660560	661560	對
661560	662560	所以這是
662560	664560	這主要的三個component
664560	665560	呃
665560	666560	可以再clarify
666560	667560	均衡的概念嗎
667560	668560	哦
668560	669560	均衡就是說
669560	670560	均衡
670560	671560	哦
671560	672560	對
672560	673560	均衡這個概念就是說
673560	674560	呃
674560	676560	在這個model底下
676560	677560	大家
677560	680560	你去預測別人的行為在做什麼
680560	681560	然後
681560	683560	你預測那個行為之下
683560	684560	你
684560	687560	你會做一個選擇去
687560	688560	在那個
688560	690560	當你在預測別人的行為之下
690560	691560	你會
691560	693560	然後optimize你自己的行為嗎
693560	694560	就是說
694560	696560	我如果預期到大家都去超市
696560	697560	把東西買光的話
697560	699560	那我也會去超市
699560	700560	買光這樣
700560	701560	但是
701560	702560	呃
702560	703560	也跟著去超市買東西之類的
703560	704560	所以
704560	705560	均衡的話就是說
705560	707560	在你預測別人的行為之下
707560	710560	你就會做這樣子對你最好的行為
710560	711560	但是你也同時知道說
711560	712560	哦
712560	713560	你做了這個最好的行為之後
713560	715560	別人也在同時
715560	717560	在設想你會做什麼樣的行為
717560	719560	於是在這種交互的行為之下
719560	720560	他達到一個均衡
720560	721560	就是說
721560	722560	呃
722560	723560	這個情況下
723560	725560	沒有人會改變他的行為
725560	726560	所以
726560	727560	你知道漸漸的就是
727560	729560	達到一個平衡
729560	730560	也不是說漸漸的達到平衡
730560	731560	就是他在平衡狀態下
731560	732560	就是
732560	733560	你做這個行為
733560	734560	我現在做這個行為
734560	735560	就是對你最好的
735560	736560	呃
736560	737560	response
737560	738560	然後同時是
738560	739560	所以我不會去改變我的行為
739560	741560	然後你也不會再去改變你的行為
741560	743560	所以就達到一個均衡
743560	747560	所以這個game theory下面的一個assumption
747560	748560	不是assumption就是說
748560	749560	呃
749560	750560	不能講assumption
750560	751560	就是說equilibrium
751560	752560	就是說game
752560	753560	就是說
753560	754560	game就
754560	755560	對在economy裡頭
755560	756560	game
756560	758560	其實就像你在玩
758560	760560	撲克牌遊戲一樣
760560	761560	我會覺得
761560	763560	喔別人在玩什麼招數
763560	765560	然後我要想辦法去抵抗他
765560	766560	我會做
766560	767560	假設他做了這件事情
767560	768560	那我就是
768560	770560	挑我最好的那個
770560	771560	呃
771560	772560	牌組之類的
772560	773560	但是你也同時在想說
773560	775560	喔別人也知道
775560	777560	別人也在猜你的行為是什麼
777560	778560	然後變成是說均衡底下
778560	779560	你可能會覺得
779560	780560	喔我應該怎麼樣出手
780560	781560	然後大家都不會
781560	782560	呃
782560	784560	有任何動機去改變他的行為
784560	785560	就他達到了一個
785560	787560	互相抵抗的一個
787560	788560	呃
788560	789560	呃
789560	790560	平衡
790560	791560	所以那個時候
791560	793560	那個狀態我們就都要加均衡
793560	794560	了解
794560	795560	那我想問一下
795560	796560	就是說
796560	797560	比如說prison dilemma
797560	799560	這樣算是會有均衡的state嗎
799560	800560	那個就是均衡
800560	801560	prison dilemma就是均衡
801560	802560	那個就是均衡
802560	803560	那個就是因為
803560	805560	我知道說如果我不講了
805560	806560	我不講
806560	807560	但是對方可能
807560	808560	就是說
808560	810560	我知道對方講或不講
810560	811560	我
811560	813560	講了都對我比較好
813560	814560	所以我就會選擇去講
814560	816560	然後對方也知道說啊那個
816560	817560	不管講或不講
817560	818560	他還是講比較
818560	819560	就就變兩個人都講
819560	821560	然後兩個人都被懲罰
821560	822560	這個意思
822560	823560	了解謝謝
823560	824560	好
824560	825560	呃
825560	826560	不好意思我想要
826560	828560	就是再問一下均衡的這個問題
828560	829560	嗯那
829560	830560	呃
830560	832560	就是考慮均衡的時候
832560	834560	會討論個體差異嗎
834560	836560	因為每一個人
836560	837560	呃
837560	838560	之前會長得不太一樣
838560	839560	呃
839560	841560	他們有很呃
841560	842560	一看裡面
842560	844560	他們會把這個叫做type
844560	845560	就是你是什麼樣的類型
845560	846560	然後另外是說
846560	848560	他一定可以考慮個體差異的
848560	849560	只是說
849560	850560	一看裡面
850560	852560	常常會問的一個問題是說
852560	854560	你到底知不知道這個個體差異
854560	855560	就是說
855560	857560	假設我今天到底知不知道你
857560	858560	我如果已經知道
858560	859560	這樣我們就知道
859560	860560	川普他就是支持
860560	861560	共和黨
861560	862560	但是你現在走在路上
862560	863560	你看到某一個人
863560	865560	你也不知道他到底是支持共和黨
865560	866560	或者是
866560	867560	民主黨對吧
867560	868560	就等於是說
868560	870560	他們會用這種方式
870560	872560	呃
872560	873560	嗯
873560	874560	他們是會考慮個體差異
874560	876560	然後他們會去看哦
876560	877560	我不知道你是
877560	878560	主要是我不知道
878560	880560	你到底是什麼樣子類型的人
880560	882560	或者是我知道你是什麼樣類型的人
882560	884560	會導致有不一樣的strategy
884560	885560	對他的
885560	886560	他會對對
886560	888560	然後像我們這個model裡面
888560	889560	我們會假設
889560	891560	大家都不知道別人在想什麼
891560	892560	但是
892560	894560	我知道
894560	895560	我們會make assumption就是說
895560	896560	哦我知道
896560	898560	整體社會大致上在想什麼
898560	899560	但是我不知道
899560	901560	specifically like
901560	902560	高賢他在想什麼
902560	903560	我不知道
903560	904560	但是我知道
904560	905560	哦我們可能這一群人
905560	907560	普遍都是支持
907560	908560	呀
908560	909560	臺灣怎麼樣
909560	910560	或者是ok
910560	911560	你懂我意思
911560	912560	對
912560	913560	可是這個這個assumption
913560	915560	好像跟我們平常是在
915560	918560	社交網站上面的
918560	921560	互動有一點點不太一樣
921560	923560	就是我我通常就是譬如說
923560	925560	不管在推特上或者在
925560	926560	呃facebook上
926560	928560	我通常會大概知道我的follower
928560	929560	大概知道了
929560	931560	但不是百分百知道
931560	933560	但我大概可以猜測出來他們在
933560	934560	想什麼
934560	935560	那對
935560	936560	哦
936560	937560	不好意思你先說
937560	938560	不好意思
938560	938560	
938560	940560	所以我們在這邊其實沒有
940560	941560	呃到時候你看到那個商品
941560	943560	就是說我大概只是知道說
943560	944560	我的那個distribution是什麼
944560	946560	但我不知道確切
946560	947560	這樣
947560	948560	ok
948560	949560	對ok
949560	951560	我會問這個那個
951560	953560	不好意思你先說
953560	954560	沒有你講講講
954560	955560	我有聽過
955560	958560	我會問這個那個equilibrium的問題
958560	959560	其實是因為
959560	961560	第一個是就算是假設
961560	962560	所有人都是rational的
962560	964560	他們的optimization可能會不太一樣
964560	965560	就是每個人要的東西
965560	966560	感覺會有點不太一樣
966560	968560	那這個感覺你剛剛的
968560	970560	呃回答有
970560	972560	就是有cover到這一部分
972560	974560	那可是因為你還有另一個
974560	975560	假設是
975560	977560	就是在你這個study裡面
977560	978560	是人是rational的嗎
978560	980560	那如果你把人不是rational的
980560	981560	考慮進來
981560	982560	等於就又多了一個dimension
982560	984560	所以有些人他如果不是
984560	985560	呃不是rational的
985560	987560	那他的equilibrium又會長得
987560	989560	跟rational那些人不太一樣
989560	991560	所以好像就會變得非常複雜
991560	992560	呃對
992560	995560	但是目前就是先考
995560	997560	因為我們try to就
997560	998560	其實有點mess
998560	999560	我們的主要的message
999560	1000560	就跟大家講是說
1000560	1001560	哦
1001560	1002560	因為其實你說那rational的話
1002560	1004560	那其實很多事情都很好解釋
1004560	1005560	因為那rational就直接
1005560	1006560	那rational就說
1006560	1008560	啊他就想這樣做
1008560	1009560	但是然後等於是說
1009560	1010560	我們的model會變成是說
1010560	1011560	哦
1011560	1012560	even
1012560	1013560	你覺得大家都是rational的
1013560	1015560	狀況底下
1015560	1016560	這種比較不準確的新聞
1016560	1017560	他還是願意傳
1017560	1018560	然後會發生什麼事情
1018560	1019560	所以就變了
1019560	1021560	這是主要這種rational model
1021560	1022560	在try to
1022560	1023560	不然因為
1023560	1024560	如果你說
1024560	1025560	大家都是irrational的話
1025560	1026560	就會變成說
1026560	1028560	哦其實
1028560	1030560	那就直接把它歸類為irrational
1030560	1031560	然後但是
1031560	1032560	其實ecom他們有時候
1032560	1033560	也會考慮bounded rational
1033560	1034560	就是說他有
1034560	1035560	有一些部分他是rational
1035560	1036560	有一些部分irrational
1036560	1038560	或者是像這種
1038560	1039560	另外還有一種model
1039560	1040560	就是像你剛才講
1040560	1041560	就一部
1041560	1042560	有可能大部分的人是rational
1042560	1043560	但有少部分人irrational
1043560	1044560	那又會發生什麼事
1044560	1045560	但我們這個model
1045560	1046560	就是考慮
1046560	1047560	就是
1047560	1048560	大家都是rational
1048560	1049560	比較簡單一點
1049560	1050560	了解了解
1050560	1051560	我先講一下
1051560	1052560	反正我們這邊人也不多
1052560	1053560	我們大概
1053560	1054560	等一下可能就會是這個
1054560	1055560	非常互動的討論形式
1055560	1056560	因為我自己是psychologist
1056560	1058560	所以我是做experimental
1058560	1059560	我是experimental psychologist
1059560	1060560	所以
1060560	1062560	experimental psychologist的
1062560	1063560	那個
1063560	1064560	出發點
1064560	1065560	就會比較像是
1065560	1066560	人by default
1066560	1067560	就是irrational
1067560	1068560	但我們當然不會說
1068560	1069560	啊他就irrational
1069560	1070560	所以我們就不用研究他的行為
1070560	1071560	反正他就irrational
1071560	1072560	但我們會去嘗試解釋
1072560	1074560	他irrational背後的原因是什麼
1074560	1075560	譬如說他可能會受到
1075560	1076560	什麼東西
1076560	1077560	呃
1077560	1078560	影響啊
1078560	1079560	或者他會
1079560	1080560	在什麼情況
1080560	1081560	他會用heuristics
1081560	1082560	什麼情況他會
1082560	1083560	就是reason
1083560	1084560	你去做一個決定
1084560	1085560	等等這樣
1085560	1086560	對但是就是
1086560	1087560	我先把我的那個
1087560	1088560	stance講出來
1088560	1089560	OKOKOK
1089560	1091560	為什麼我會問這個問題
1091560	1092560	好好好
1092560	1093560	我知道我知道你的意思
1093560	1094560	好
1094560	1095560	等於是說
1095560	1096560	主要是在這裡
1096560	1097560	就沒有什麼behavioral bias
1097560	1099560	就純粹大家就是
1099560	1100560	你可以把大家
1100560	1101560	就想像他就是
1101560	1102560	很精密的儀器
1102560	1103560	他可以去算很多東西
1103560	1104560	然後
1104560	1105560	even在這樣的情況下
1105560	1107560	大家可能還是想要穿
1107560	1108560	不準確的行為
1108560	1109560	這樣
1109560	1110560	OKOK
1110560	1111560	OK
1111560	1112560	好
1112560	1113560	嗯
1113560	1114560	好另外還有一個
1114560	1115560	就是主要
1115560	1116560	其實這是更主要的結果
1116560	1117560	就是說
1117560	1118560	在這種底下
1118560	1119560	呃
1119560	1120560	我們去探討說這種
1120560	1122560	傳播的這個尺度
1122560	1123560	以及新聞可信度
1123560	1124560	以及
1124560	1125560	就是這種
1125560	1126560	呃
1126560	1128560	這個社會的一些特性的關聯
1128560	1130560	然後我們就會發現說
1130560	1131560	哦其實
1131560	1132560	就是比較
1132560	1133560	呃
1133560	1134560	沒有那麼可信的新聞
1134560	1135560	其實就有點不準確的新聞呢
1135560	1136560	他反而可以
1136560	1137560	傳的更遠
1137560	1138560	如果說
1138560	1139560	這一個
1139560	1141560	network的
1141560	1143560	連結是非常緊密
1143560	1144560	就是說
1144560	1145560	每個人都好多個follow的話
1145560	1146560	那其實這可能會
1146560	1147560	促進那一些
1147560	1149560	比較不準確的新聞
1149560	1150560	去傳播的更遠
1150560	1151560	而另外還有說
1151560	1154560	就是如果這個社會的分化
1154560	1155560	更嚴重的話
1155560	1158560	那麼他可能把這個threshold
1158560	1159560	在這個
1159560	1160560	呃
1160560	1162560	網路緊密度的這個threshold降低
1162560	1163560	就是說
1163560	1165560	我剛才是講說
1165560	1166560	如果這個社會
1166560	1167560	如果越緊密的話呢
1167560	1168560	呃
1168560	1169560	這種比較不準確的新聞
1169560	1170560	就可以傳的越遠
1170560	1172560	那如果你又讓這個
1172560	1174560	極化的程度更高的話
1174560	1175560	反而那些
1175560	1176560	就是甚至連那些
1176560	1178560	可能不是很緊密連接的網路呢
1178560	1179560	也能使得
1179560	1180560	呃
1180560	1181560	不準確的新聞
1181560	1182560	去傳播的更遠
1182560	1183560	然後另外
1183560	1184560	大家可能會想說
1184560	1185560	哎那萬一我去增加
1185560	1187560	兩檔之內
1187560	1188560	呃
1188560	1189560	就是兩檔之內的
1189560	1192560	他們自己各自檔內的這種diversity的話
1192560	1194560	其實會不會去
1194560	1195560	幫助抑制這種
1195560	1196560	less credible news
1196560	1198560	to create a larger cascade
1198560	1199560	但是我們的
1199560	1200560	呃
1200560	1201560	結果會發現說
1201560	1202560	其實也不一定
1202560	1203560	就是說
1203560	1204560	在某些情況下
1204560	1205560	的確可以去降低
1205560	1206560	呃
1206560	1207560	呃
1207560	1208560	這個
1208560	1210560	比較不準確新聞的
1210560	1211560	傳播的
1211560	1212560	呃
1212560	1213560	速度
1213560	1214560	不能講速度
1214560	1215560	就是怕遲度
1215560	1216560	但是其實有一些情況下
1216560	1217560	他是會發現說
1217560	1218560	其實你增加這種diversity
1218560	1219560	反而會增加
1219560	1220560	呃
1220560	1222560	不準確的新聞去傳播的更遠
1222560	1223560	所以這主要是我們的
1223560	1224560	呃
1224560	1225560	結論
1225560	1226560	好
1226560	1227560	那我就
1227560	1228560	繼續了
1228560	1229560	就好
1229560	1230560	那我現在這是主要我們的結論
1230560	1231560	然後我就稍微講
1231560	1232560	開始講一下我們的model
1232560	1234560	讓大家知道說這個model到底
1234560	1236560	大致上在做些什麼
1236560	1238560	首先我們這個model有三個stage
1238560	1240560	最一開始的stage0呢
1240560	1242560	就是對一些fundamental的事情
1242560	1243560	就這邊有一個social network
1243560	1244560	然後大家每個人
1244560	1246560	他自己對某件事情
1246560	1247560	假設就投
1247560	1249560	到底要投川普還是拜登的這種想法
1249560	1251560	然後新聞就有
1251560	1253560	有一個有一個新聞就是被
1253560	1255560	realize
1255560	1257560	然後他可能就給某一些人看
1257560	1258560	呃
1258560	1259560	然後第二個stage呢
1259560	1260560	就是你知道
1260560	1261560	在投票之前
1261560	1262560	大家的互動的stage
1262560	1263560	然後大家可以
1263560	1264560	大家可能如果看到新聞呢
1264560	1265560	他就決定
1265560	1266560	到底要不要傳
1266560	1267560	如果傳的話呢
1267560	1268560	我
1268560	1269560	我這邊有一個decision
1269560	1270560	variable
1270560	1271560	SI meaning that
1271560	1272560	1的話就是
1272560	1273560	喔我傳了這個新聞
1273560	1275560	0的話就沒有
1275560	1276560	然後最後呢
1276560	1278560	在這個interaction stage
1278560	1279560	結束之後
1279560	1280560	最後大家就決定要不要vote
1280560	1282560	然後AI
1282560	1283560	就是那個action
1283560	1284560	就是說喔我到底要投
1284560	1286560	假設負1就是偏左
1286560	1287560	正1就是偏右
1287560	1288560	就其實就是一個binary variable
1288560	1290560	然後你choose 1
1290560	1291560	嗯
1291560	1293560	然後在這個model裡頭呢
1293560	1294560	因為呃
1294560	1295560	有一些technical assumption
1295560	1297560	就我是假設
1297560	1299560	你知道這個agent是非常多
1299560	1300560	多到爆炸
1300560	1302560	就是無窮多個
1302560	1303560	好不好
1303560	1304560	就是你可以想像這個population非常的大
1304560	1305560	但是他是用
1305560	1307560	0到1之間的這種時數系來
1307560	1309560	index每一個人
1309560	1311560	所以總共
1311560	1313560	不可數的無窮多個人
1313560	1315560	在這個社會裡頭
1315560	1317560	然後他是一個connected
1317560	1319560	by this social network
1319560	1321560	然後這個social network是directed
1321560	1323560	就是你知道他是一個follower的這種型態
1323560	1325560	假設我
1325560	1327560	有一個箭頭指向高顯的話
1327560	1329560	就代表我follow了高顯
1329560	1331560	在這個social network裡頭
1331560	1333560	然後
1333560	1335560	這個social network怎麼形成的呢
1335560	1337560	就因為我們也不是說
1337560	1339560	真的去apply data到這裡
1339560	1341560	然後為了要讓我們的analysis比較
1341560	1342560	tractable
1342560	1343560	我們會用一個呃
1343560	1345560	大家傳統大家都會用的
1345560	1347560	random network model
1347560	1349560	就是說你這個network是randomly
1349560	1351560	製造出來的
1351560	1353560	符合了某些特性之後的條件之下
1353560	1355560	random製造出來的
1355560	1357560	嗯
1357560	1359560	然後我在這裡頭呢
1359560	1361560	主要有一個參數是
1361560	1363560	告訴大家說這個connectivity lambda
1363560	1365560	connectivity lambda代表的是
1365560	1367560	平均每一個人
1367560	1369560	他follow了多少人
1369560	1371560	呃不是說平均每一個人
1371560	1373560	他有多少個followers
1373560	1375560	就大家在這個
1375560	1377560	model裡面大概記得這個字型就好
1377560	1379560	就好
1379560	1381560	這個variable
1381560	1383560	那當然這個新聞一開始給
1383560	1385560	很少的人去聽到然後他們決定要不要
1385560	1387560	傳然後到最後達到
1387560	1389560	steady state就是你知道他們一直傳
1389560	1391560	總是會converge
1391560	1393560	嗯我會
1393560	1395560	叫最後那個
1395560	1397560	你說這個社會裡頭有多少比例的人
1397560	1399560	他到底最後
1399560	1401560	有聽到這個新聞
1401560	1403560	那我就把這個size
1403560	1405560	這個大小叫做Q
1405560	1407560	你看全部總共是1嘛
1407560	1409560	那假設有80%的人
1409560	1411560	那Q就是0.8聽到的
1411560	1413560	這個新聞然後這個就是我們
1413560	1415560	接下來會探討的一個主要的variable
1415560	1417560	嗯
1417560	1419560	可以嗎到目前為止
1419560	1421560	Q算是dependent variable
1421560	1423560	就是你的target metric
1423560	1425560	對對對它是一個我等一下會講它
1425560	1427560	對它是一個variable它不是一個我外
1427560	1429560	你知道它鐵定就是你知道這個系統
1429560	1431560	它自己產生出來的話你會觀察到
1431560	1433560	喔有一個Q
1433560	1435560	然後lambda是我給定的
1435560	1437560	對我應該對不起我應該
1437560	1439560	make it more specific
1439560	1441560	就是lambda是我一開始說喔
1441560	1443560	我就考慮這個network裡頭
1443560	1445560	平均每個人有10個follower
1445560	1447560	就設定為10然後去generate
1447560	1449560	這個dependent network model
1449560	1451560	但是Q是根據他們
1451560	1453560	的strategy去做出來
1453560	1455560	然後會產生出來的一個數字
1455560	1457560	ok
1457560	1459560	嗯
1459560	1461560	然後我剛才講說
1461560	1463560	每個人其實都有他自己的
1463560	1465560	一個想法嘛那其實我們在這個
1465560	1467560	模型裡頭因為一些utility function
1467560	1469560	我們在定義的過程當中
1469560	1471560	其實我們只需要去注意每個人
1471560	1473560	他對這件事情的
1473560	1475560	期望值的想法
1475560	1477560	也就是說我今天問你說
1477560	1479560	你覺得
1479560	1481560	這個通膨率是多少
1481560	1483560	你可能覺得說我覺得平均應該就0.5吧
1483560	1485560	就這樣所以我們只需要care這個
1485560	1487560	這個平均值就好所以我叫
1487560	1489560	然後
1489560	1491560	每個人都其實看不到別人的
1491560	1493560	平均值就是說我其實
1493560	1495560	也不知道高顯對這件事情的看法
1495560	1497560	但是我大概知道說這整個社會裡頭
1497560	1499560	呃
1499560	1501560	呃這個分佈是怎麼樣可能
1501560	1503560	偏向說今年這個通膨率
1503560	1505560	是偏高大家可能都落在
1505560	1507560	這個0.7附近
1507560	1509560	就是一個function cdf function
1509560	1511560	然後我就假設說大概有一半
1511560	1513560	的人覺得是小於0一半的人覺得
1513560	1515560	大於0這樣這只是一個
1515560	1517560	呃就simplify一些
1517560	1519560	notation的方式而已
1519560	1521560	呃另外
1521560	1523560	這個事件呢在一開始他就generate
1523560	1525560	這個news就他產生一個
1525560	1527560	news然後他是一個時數
1527560	1529560	然後另外他有credibility data
1529560	1531560	就是道義之間的一個時數
1531560	1533560	這個credibility
1533560	1535560	呢其實本質上
1535560	1537560	的意思是說
1537560	1539560	當你看到新聞之後
1539560	1541560	你會對這個
1541560	1543560	新聞有多大的sensitivity
1543560	1545560	就是說我會很相信他的話
1545560	1547560	那我當然就完全的
1547560	1549560	把我的belief都改成是這個news
1549560	1551560	但如果你完全不相信他的話
1551560	1553560	你可能就你就會覺得說
1553560	1555560	我就把他當成是garbage
1555560	1557560	message我就直接還是preserve
1557560	1559560	my expectation
1559560	1561560	然後在這裡呢
1561560	1563560	beta等於1呢我就叫他叫做
1563560	1565560	fully credible的case
1565560	1567560	因為我們之後我們主要想要探討的是說
1567560	1569560	如果fully credible news是這樣子
1569560	1571560	那如果比fully credible再小一點
1571560	1573560	把他的credibility
1573560	1575560	降低的話會不會有更大的
1575560	1577560	news cascade
1577560	1579560	不好意思
1579560	1581560	我可以問個問題嗎
1581560	1583560	credibility在這邊
1583560	1585560	就聽你剛剛的解釋聽起來有點像是
1585560	1587560	那個新聞有多
1587560	1589560	influential你的定義是
1589560	1591560	應該
1591560	1593560	其實他可以被
1593560	1595560	對其實你可以就是想像
1595560	1597560	其實他有點像是一個我覺得有點
1597560	1599560	在這邊其實有點terminology的問題
1599560	1601560	就是說可能我講credibility有一些人會覺得
1601560	1603560	他是有一個subjective的
1603560	1605560	觀念在裡頭但是
1605560	1607560	這裡的話我們就是假定說你可以
1607560	1609560	當你收到新聞的時候你知道說
1609560	1611560	這是CNN的news然後我們大家都
1611560	1613560	同意CNN的可信度
1613560	1615560	在那個在什麼level
1615560	1617560	這樣子
1617560	1619560	是prior belief
1619560	1621560	但是可信度
1621560	1623560	這是homogeneous
1623560	1625560	對對這是一個common knowledge
1625560	1627560	就是如果大家收到新聞就ok
1627560	1629560	這個就是一個非常可信的新聞
1629560	1631560	沒有大家沒有我沒有讓他
1631560	1633560	變成heterogeneous
1633560	1635560	ok
1635560	1637560	就我剛聽你的
1637560	1639560	解釋方法聽起來比較像是
1639560	1641560	他是主觀
1641560	1643560	衡量的一個東西就如果
1643560	1645560	我現在收到一個東西然後我覺得他的可信度
1645560	1647560	很高
1647560	1649560	那他的credibility就很高在你這個model
1649560	1651560	裡面可是
1651560	1653560	就是感覺credibility
1653560	1655560	可以用比較客觀的方法
1655560	1657560	來量就是說如果你把像你
1657560	1659560	剛講的譬如說如果是CNN你把某一些新聞
1659560	1661560	然後列出來
1661560	1663560	然後我們可能可以
1663560	1665560	通過
1665560	1667560	不知道可能還是要靠
1667560	1669560	某一種rating之類的然後
1669560	1671560	用非常多人的rating然後來排名說
1671560	1673560	那如果我們來看客觀來講
1673560	1675560	哪一個不用rating
1675560	1677560	我們來看譬如說每一個新聞的outlet他產出
1677560	1679560	正確的新聞跟
1679560	1681560	不正確的新聞的比例是怎樣
1681560	1683560	那如果他正確的比例很高他credibility就很高
1683560	1685560	那就不用靠任何的人工的rating
1685560	1687560	但是聽起來你們是
1687560	1689560	你在這邊的credibility定義是
1689560	1691560	就是基本上
1691560	1693560	how influential it is
1693560	1695560	其實就他
1695560	1697560	其實如果你用一些
1697560	1699560	像一些什麼Gaussian的belief去看
1699560	1701560	就是你可以其實有一些數學
1701560	1703560	的東西去解釋他你可以說是
1703560	1705560	這個news可能是一個signal
1705560	1707560	然後你說這個beta其實
1707560	1709560	就是一個
1709560	1711560	signal多準確
1711560	1713560	如果他越準確的話其實就變成是說
1713560	1715560	我越相信這個news
1715560	1717560	所以那個準確的意思是說
1717560	1719560	他更接近那個人原本的prior嗎
1719560	1721560	不是那個準確是
1721560	1723560	你知道這個東西
1723560	1725560	signal裡面沒有noise
1725560	1727560	然後我知道這個東西觀察到的就是那個東西
1727560	1729560	沒有任何的
1729560	1731560	觀察上的誤差
1731560	1733560	我覺得
1733560	1735560	這應該就是我的問題的
1735560	1737560	根源就是
1737560	1739560	我觀察到一個東西沒有noise的時候
1739560	1741560	我會假定那個東西
1741560	1743560	我其實不知道那個東西是真的還是假的
1743560	1745560	所以有兩個approach的方法
1745560	1747560	第一個就是
1747560	1749560	我感覺這個東西沒有noise
1749560	1751560	然後第二個是
1751560	1753560	因為那個東西客觀上
1753560	1755560	就譬如說我們有某一種評量
1755560	1757560	他是一個從一個很好的outlet出來的
1757560	1759560	所以我猜
1759560	1761560	就是因為那個原因所以他沒有noise
1761560	1763560	所以我比較知道你在講的是哪一種
1763560	1765560	我在講
1765560	1767560	是我們大家都知道
1767560	1769560	這是一個observation然後他沒有noise
1769560	1771560	ok
1771560	1773560	對observation on the state
1773560	1775560	然後他沒有noise
1775560	1777560	所以等於是我就看到那個state
1777560	1779560	等於說那個牌就翻出來
1779560	1781560	我們大家全部都親眼看到他是什麼意思
1781560	1783560	就是說
1783560	1785560	反正這邊我講credibility就是說
1785560	1787560	其實就是說
1787560	1789560	大家都當他看到這個新聞的時候
1789560	1791560	大家都agree
1791560	1793560	這樣子的新聞他的準確度在哪裡
1793560	1795560	然後我說的準確度是說他跟
1795560	1797560	跟那個
1797560	1799560	真正你想觀察的那個東西的
1799560	1801560	誤差
1801560	1803560	就是
1803560	1805560	越接近那個
1805560	1807560	如果你跟我講說這個signal很準確的話
1807560	1809560	那我的意思就是說
1809560	1811560	我完全的反映了那一個我想觀察的東西
1811560	1813560	如果你說這個signal不準確的話
1813560	1815560	那他可能
1815560	1817560	somehow有點noise然後可能看到家務
1817560	1819560	或監務的這種東西
1819560	1821560	那我把它換成那個
1821560	1823560	真實世界的角度來看
1823560	1825560	換句話說如果今天有一個左派的人他看左派的媒體
1825560	1827560	跟右派的人看右派的媒體
1827560	1829560	對他們來講
1829560	1831560	那個情況的credibility都是maximum
1833560	1835560	我們這個model
1835560	1837560	我們這個model裡面就是假設說
1837560	1839560	大家都agree
1839560	1841560	這一個我們大家都
1841560	1843560	我也知道你在
1843560	1845560	當然你可以說現實生活中
1845560	1847560	democrats如果他看到CNN的news
1847560	1849560	決定說這個我相信他
1849560	1851560	然後可能那個共和黨人看到CNN
1851560	1853560	就說啊什麼東西啊就我不相信
1853560	1855560	但是
1855560	1857560	我這個model是假設說
1857560	1859560	不管怎麼樣的人
1859560	1861560	他如果看到這個新聞之後我們大家都相信
1861560	1863560	他的準確度在那個程度
1863560	1865560	就是說我們是一個假定的狀態
1865560	1867560	就是假定
1867560	1869560	我大家都agree
1869560	1871560	CNN的這個準確程度在0.9
1871560	1873560	OK
1873560	1875560	OK
1875560	1877560	你想的那個問題
1877560	1879560	其實我的另外一個paper有在study
1879560	1881560	就是說
1881560	1883560	現在是白邊
1883560	1885560	不好意思
1885560	1887560	就是說
1887560	1889560	我們現在的狀況是
1889560	1891560	我另外一個paper有看
1891560	1893560	有在study這樣的問題
1893560	1895560	就是說我們先假定
1895560	1897560	大家都agreeCNN就是0.9
1897560	1899560	我們看到CNN的我們都同意
1899560	1901560	他是0.9
1901560	1903560	那我就暫時把他想像成
1903560	1905560	我們可能有一個
1905560	1907560	類似那種國營媒體好了
1907560	1909560	假設像是日本的NHK
1909560	1911560	當然也不是所有日本人都喜歡NHK
1911560	1913560	假設有一個國營媒體然後那個東西是
1913560	1915560	highly credible
1915560	1917560	然後其他的媒體我們先不管他
1917560	1919560	反正他的東西如果出現大家都相信
1919560	1921560	然後他的credibility就是最高
1921560	1923560	他不一定是最高
1923560	1925560	反正我現在就是會調整那個數字
1925560	1927560	但我只是問說
1927560	1929560	然後
1929560	1931560	很好
1931560	1933560	我們這裡就是說
1933560	1935560	如果你沒有聽到新聞的話
1935560	1937560	你就繼續保持你原本的
1937560	1939560	這個想法
1939560	1941560	但如果你聽到想法的話就是我剛才講的
1941560	1943560	你可能會你就是做一個complex combination
1943560	1945560	就是說
1945560	1947560	你可以看出來如果是beta等於1
1947560	1949560	是fully credible的話我完全
1949560	1951560	是考慮我自己原本的μi
1951560	1953560	對吧 因為如果是beta等於1的話
1953560	1955560	我就是完全的改變了
1955560	1957560	我的想法變成x 但是如果beta
1957560	1959560	等於0就是我知道這東西完全是garbage
1959560	1961560	我就是preserve my
1961560	1963560	prior expectation
1963560	1965560	就是這是一個
1965560	1967560	分析比較簡單的方法
1967560	1969560	然後
1969560	1971560	再來是我想因為econ裡面
1971560	1973560	你就是你做任何的事情
1973560	1975560	其實你都是想try to maximize
1975560	1977560	your expected utility
1977560	1979560	然後首先是
1979560	1981560	如果你決定去分享這個新聞的時候
1981560	1983560	就像我剛才講的
1983560	1985560	你得突然在那邊滑手機滑得很開心
1985560	1987560	突然要停下來然後就是要
1987560	1989560	我們把interpret as a
1989560	1991560	status quo
1991560	1993560	bias就是說你不想
1993560	1995560	破壞原本的現狀你可能要突然又
1995560	1997560	做一件事情所以你要花了一個cost
1997560	1999560	然後在這個
1999560	2001560	狀況底下我們formulate這個utility function
2001560	2003560	像這樣子就是說
2003560	2005560	我會一步一步來解釋說
2005560	2007560	這個utility function代表了什麼意思
2007560	2009560	第一個是先
2009560	2011560	因為我剛才沒有定義這個network model
2011560	2013560	第一個是
2013560	2015560	我的utility是
2015560	2017560	根據我
2017560	2019560	關心的這個state data
2019560	2021560	以及我做了一個
2021560	2023560	sharing decision si 還有
2023560	2025560	以後我之後我會voting ai
2025560	2027560	但我也考慮到
2027560	2029560	我這些follower的
2029560	2031560	voting decision 以及
2031560	2033560	最後我可能如果我真的sharing
2033560	2035560	這個news的話我真的要
2035560	2037560	花了一個cost
2037560	2039560	所以第一個是這個其實就是
2039560	2041560	你aggregate就你考慮到了
2041560	2043560	你這個follower
2043560	2045560	的這個action
2045560	2047560	第一個部分就是direct utility from voting
2047560	2049560	就是我直接投了然後你可以看得出來
2049560	2051560	就是你盡可能的你想要
2051560	2053560	match這個state的sign
2053560	2055560	你可以看到ai data
2055560	2057560	你基本上你會挑那個action
2057560	2061560	去match the sign of the state
2061560	2063560	你想要知道它到底是偏右還是偏左
2063560	2065560	另外這個blue part
2065560	2067560	就是說這個東西
2067560	2069560	是你去aggregate我考慮到了
2069560	2071560	我這些follower的
2071560	2073560	utility
2073560	2075560	你internalize
2075560	2077560	你的follower
2077560	2079560	你其實是想要為了它們好
2079560	2081560	你只是覺得說我希望你們去投的跟我想的
2081560	2083560	一樣就這個theta
2083560	2085560	還是根據你自己的believe去
2085560	2087560	estimate
2087560	2089560	但是至於那個action
2089560	2091560	是depends on
2091560	2093560	你follower自己的believe
2093560	2095560	所以你想要try to inference它們的這個action
2097560	2099560	然後其實這邊我先快速的
2099560	2101560	跟大家講一下就是我剛才講的
2101560	2103560	你在voting的狀況底下
2103560	2105560	其實你是想要match the sign of the state
2105560	2107560	所以其實這邊有一個很快的
2107560	2109560	結論就是說
2111560	2113560	當我有收到的新聞之後
2113560	2115560	然後你知道就是剛才那個
2115560	2117560	beta s加一點beta mu i
2117560	2119560	然後它大於0了
2119560	2121560	那當然就代表我現在覺得
2121560	2123560	這個state應該是比較偏右
2123560	2125560	那我就投1
2125560	2127560	偏左的話就投-1
2127560	2129560	所以其實如果你有收到新聞的話
2129560	2131560	你會投
2131560	2133560	正義的condition
2133560	2135560	就是說這個mu i
2135560	2137560	足夠大,大過於這個threshold
2137560	2139560	如果你沒有聽到任何新聞
2139560	2141560	那當然一開始你覺得已經是偏低
2141560	2143560	如果一開始你的期望值是
2143560	2145560	正的那你當然就投
2145560	2147560	共和黨
2147560	2149560	如果你覺得一開始你的期望值是負的
2149560	2151560	那你就投民主黨
2151560	2153560	所以這個voting decision是比較簡單的部分
2153560	2155560	然後
2155560	2157560	我接下來要開始講的就是比較有趣的
2157560	2159560	就是這個sharing decision
2159560	2161560	我有一個問題
2161560	2163560	我不知道
2163560	2165560	可能這是你等一下要講
2165560	2167560	utility function
2167560	2169560	其中一個input是voting decision
2169560	2171560	但是自己那個agent也有可能
2171560	2173560	會改變
2173560	2175560	AI star
2175560	2177560	但是AI star跟這個AI是沒有關的
2177560	2179560	是這樣嗎
2179560	2181560	不好意思
2181560	2183560	AI star其實就是均衡
2183560	2185560	好
2185560	2187560	就是說我現在剛才定義了這個utility function
2187560	2189560	沒有打信號的部分
2189560	2191560	就是單純的
2191560	2193560	我的utility是這樣子
2193560	2195560	我現在打了信號的意思代表說
2195560	2197560	在均衡底下之後其實我就是會做這件事情
2197560	2199560	因為你還記得voting是在最後一刻發生的
2199560	2201560	對吧
2201560	2203560	所以大概大家都已經interact完了之後
2203560	2205560	如果我在那個時間點
2205560	2207560	在voting的時間點
2207560	2209560	我有收到新聞的話
2209560	2211560	那麼我的最好的action
2211560	2213560	因為我那個時間點投票
2213560	2215560	我已經沒辦法再影響任何人了
2215560	2217560	我其實能投的事情就是
2217560	2219560	我就是要根據我自己的belief去match
2219560	2221560	那個state
2221560	2223560	所以那個時候如果你的期望值
2223560	2225560	就是你如果已經收過新聞了
2225560	2227560	那你的期望值是1
2227560	2229560	你的期望值是正的
2229560	2231560	那那個condition就是這個
2231560	2233560	然後你會投1
2233560	2235560	但是如果在那個投票的時間點
2235560	2237560	你沒有收到任何新聞
2237560	2239560	那你就是根據你原本的期望值
2239560	2241560	如果他是正的我就投1
2241560	2243560	如果是負的我就投負1
2243560	2245560	ok嗎
2245560	2247560	聽起來的感覺是
2247560	2249560	這個utility function是可以
2249560	2251560	是在
2251560	2253560	這個agent在present state就可以預測
2253560	2255560	預測到他的stage 2的
2255560	2257560	voting decision
2257560	2259560	對
2259560	2261560	他其實他在stage 1
2261560	2263560	對
2263560	2265560	good question
2265560	2267560	就是說其實
2267560	2269560	因為我其實沒有想講的太technical
2269560	2271560	但是其實應該跟大家講的就是說
2271560	2273560	你在解這個equilibrium的時候
2273560	2275560	你可能是倒推回來的
2275560	2277560	到最後一步我知道大家根據他們自己的belief
2277560	2279560	他會怎麼投
2279560	2281560	有收到新聞的人他就是按照上面這個方式投
2281560	2283560	沒收到新聞的人就按照下面這個方式投
2283560	2285560	沒收到新聞的人就按照下面這個方式投
2285560	2287560	然後
2287560	2289560	所以這個時候你已經知道他們在voting stage
2289560	2291560	的時候會做出這樣的事情的時候
2291560	2293560	你在sharing stage的時候你就會考慮說
2293560	2295560	那我會怎麼樣影響他們最後投票的
2295560	2297560	這個程度
2297560	2299560	因為我知道如果我分享
2299560	2301560	他們就會看到
2301560	2303560	他們就會照著上面那個方式投
2303560	2305560	如果我沒有分享他們就會照著下面那個方式投
2305560	2307560	之類的所以你就要去想這件事情
2307560	2309560	ok嗎
2309560	2311560	好
2311560	2313560	那我趕快講一下這個
2313560	2315560	因為這個比較有趣一點
2315560	2317560	就是說到了sharing decision的時候
2317560	2319560	就變成說你現在已經知道說他們最後
2319560	2321560	那個stage他們會怎麼vote
2321560	2323560	然後你在想的就是說
2323560	2325560	那我到底該不該
2325560	2327560	分享我的新聞去改變他們的想法
2327560	2329560	因為我改變的話他們就會照著
2329560	2331560	那個beta s加一減beta
2331560	2333560	如果沒有的話就是preserve
2333560	2335560	他們原本的expectation
2335560	2337560	對就我剛才講
2337560	2339560	如果分享的話我就知道說
2339560	2341560	我的follower都會看到這個新聞
2341560	2343560	當然有可能
2343560	2345560	對就是我們就assume
2345560	2347560	你的follower真的就會看到這個新聞
2347560	2349560	另外一個就是說如果你沒分享的話
2349560	2351560	那你就要想
2351560	2353560	我沒有分享但是
2353560	2355560	我的follower也有可能從別的地方
2355560	2357560	聽到新聞對吧
2357560	2359560	因為你現在已經知道有這個新聞在這個社會裡頭
2359560	2361560	所以你已經知道說
2361560	2363560	有一些人也在傳這個新聞
2363560	2365560	那到底我的follower有多可能聽到這個新聞
2365560	2367560	那其實這個東西呢
2367560	2369560	在這個分析當中
2369560	2371560	你會預測說
2371560	2373560	在均衡的底下
2373560	2375560	到底這個新聞
2375560	2377560	可以傳的多遠
2377560	2379560	所以你在想的就是這個sizeQ
2379560	2381560	你可以想像就是說
2381560	2383560	這個sizeQ鐵定會正相關的影響到
2383560	2385560	你的follower
2385560	2387560	多可能聽到這個新聞
2387560	2389560	在這個equally
2389560	2391560	從別人那邊聽到的新聞
2391560	2393560	而不是你自己
2393560	2395560	因為如果你它假設這個Q是1
2395560	2397560	那你就知道鐵定他會從別人那邊聽到新聞
2397560	2399560	因為每個人都在傳
2399560	2401560	就是這個size非常大
2401560	2403560	就等於是說那我就不用傳
2403560	2405560	對
2405560	2407560	那接下來這個問題就來了
2407560	2409560	就是它現在
2409560	2411560	決定了sharing decision之後
2411560	2413560	它就形成了這個sharing strategy
2413560	2415560	就是它的策略是這樣
2415560	2417560	然後每一個人都做了這些事情之後呢
2417560	2419560	經過這個spread dynamics
2419560	2421560	就是你知道這個傳播的這個鏈
2421560	2423560	它才會去決定了
2423560	2425560	這個new spread sizeQ
2425560	2427560	所以其實它形成了一個loop
2427560	2429560	大家有沒有看出來
2429560	2431560	就是我每一個人分享了一個新聞之後呢
2431560	2433560	經過了
2433560	2435560	因為新聞會這樣子慢慢的傳出去嘛
2435560	2437560	然後它就形成了spread size
2437560	2439560	可是這個其實你也預測到說
2439560	2441560	喔我傳出去之後這種spread size
2441560	2443560	進而又影響了我到底
2443560	2445560	我的follower有多可能聽到這樣的新聞
2445560	2447560	所以那個時候可能你覺得說
2447560	2449560	所以就是它是一個feedback
2449560	2451560	就等於是說如果你覺得好像有很多人會傳
2451560	2453560	那你就不傳
2453560	2455560	你又知道說
2455560	2457560	那現在我又應該傳了
2457560	2459560	因為大家都覺得好像我不應該傳
2459560	2461560	就是它會有一個feedback
2461560	2463560	然後會達到一個均衡
2463560	2465560	所以這個Q呢
2465560	2467560	是一個indigenous generated variable
2467560	2469560	它是一個
2469560	2471560	它不是一個
2471560	2473560	它是有一個feedback去決定它
2473560	2475560	這個均衡底下這個數字應該長什麼樣
2475560	2477560	然後我就
2477560	2479560	如果大家感興趣那個
2479560	2481560	那些equation去怎麼樣
2481560	2483560	大家可以去看一下paper
2483560	2485560	但這是主要的idea就是說
2485560	2487560	你在決定sharing的時候你得去考慮
2487560	2489560	喔到均衡的時候
2489560	2491560	這個傳播的鏈有多大
2491560	2493560	但是有可能它就影響我到底
2493560	2495560	要不要去傳
2495560	2497560	然後這是我們的這個
2497560	2499560	第一個雷瑪就是說
2499560	2501560	characterize到底
2501560	2503560	假設你已經知道這spread size是Q的話
2503560	2505560	那你
2505560	2507560	會傳這個新聞的
2507560	2509560	condition是這樣
2509560	2511560	其實就是你的expectation
2511560	2513560	因為你現在是
2513560	2515560	拿到新聞了對吧
2515560	2517560	你拿到新聞的expectation
2517560	2519560	然後乘上你覺得這個news
2519560	2521560	它可以影響多少的
2521560	2523560	你的follower
2523560	2525560	假設你傳了結果你發現所有的follower
2525560	2527560	不會因為你的這個decision而改變他的想法的話
2527560	2529560	那你就沒有意義
2529560	2531560	去傳這個新聞
2531560	2533560	所以這個成績如果大於C的話
2533560	2535560	那他就會傳
2535560	2537560	那這裡就是說
2537560	2539560	其實就是
2539560	2541560	就是我剛才講的那個strategic substitute
2541560	2543560	如果你說這個Q已經很多
2543560	2545560	就是你知道這個spread size
2545560	2547560	可以很大的時候
2547560	2549560	那你就可以看到這個成績就會變小對吧
2549560	2551560	因為它是反向的
2551560	2553560	所以你就越不想傳
2553560	2555560	然後另外是說從這個雷瑪
2555560	2557560	你可以導出一個threshold strategy
2557560	2559560	就是說你可以發現
2559560	2561560	其實每一個人收到新聞之後呢
2561560	2563560	如果這個新聞是
2563560	2565560	偏向共和黨的
2565560	2567560	那你就知道說
2567560	2569560	如果一開始這個人的expectation
2569560	2571560	大過了某一個threshold
2571560	2573560	那他就會傳這個新聞
2573560	2575560	然後小於這個threshold的人他都不會傳這個新聞
2575560	2577560	如果這個新聞是
2577560	2579560	支持democrats的話呢
2579560	2581560	那麼你知道說
2581560	2583560	如果那些
2583560	2585560	原本就偏左派的人呢
2585560	2587560	他就會分享這個新聞
2587560	2589560	但是偏右的人就不會想分享這個新聞
2589560	2591560	所以這是一個threshold
2591560	2593560	我們叫它是一個threshold strategy
2593560	2595560	就是你知道
2595560	2597560	大於他的就傳
2597560	2599560	小於他的就不傳
2601560	2603560	那接下來的問題就是說
2603560	2605560	什麼時候這一個
2605560	2607560	news cascade會
2607560	2609560	可以maximize
2609560	2611560	我應該再
2611560	2613560	more specific一點就是說
2613560	2615560	什麼樣的credibility
2615560	2617560	可以使得這個news
2617560	2619560	傳得最遠
2619560	2621560	傳得最大
2621560	2623560	Fully credible
2623560	2625560	for credibility就是最好的
2625560	2627560	然後首先我們要回答這個問題
2627560	2629560	就要去
2629560	2631560	看一下說到底
2631560	2633560	Fully credible news
2633560	2635560	他會產生的這個spread size到底有多大
2635560	2637560	然後第一個是
2637560	2639560	這個圖呢
2639560	2641560	他其實是根據x的
2641560	2643560	這個magnitude在畫的
2643560	2645560	我只考慮正的,因為負的話
2645560	2647560	其實就是一個symmetry的case
2647560	2649560	所以大家可以不用太care
2649560	2651560	然後就0到c
2651560	2653560	就是當你這個magnitude
2653560	2655560	of the news是很小很小的時候
2655560	2657560	你會發現說
2657560	2659560	even是fully credible news
2659560	2661560	就是沒有人要傳
2661560	2663560	就是他會導致說這個新聞傳不出去
2663560	2665560	是0
2665560	2667560	如果說這個magnitude of news很強大的話
2667560	2669560	就是你知道
2669560	2671560	這個news他既是credible
2671560	2673560	然後他又告訴我們
2673560	2675560	一定要投一下拜登的話
2675560	2677560	不是拜登,因為現在是偏右
2677560	2679560	大家很相信他
2679560	2681560	然後他又那麼的強
2681560	2683560	就是他的magnitude很強的話
2683560	2685560	那大家就會很願意去傳他
2685560	2687560	很願意去告訴別人說你應該要投川普
2687560	2689560	這個時候他會
2689560	2691560	這個傳播會達到他的極大值
2691560	2693560	這個極大值就是
2693560	2695560	這個社會能形成的極大值
2695560	2697560	我叫他QG
2697560	2699560	就是一個join component
2699560	2701560	就是他有connected最大的那一個component
2701560	2703560	但是如果這個news
2703560	2705560	在中間這個範圍
2705560	2707560	他是因為我剛才講的
2707560	2709560	就是有一個strategic substitute
2709560	2711560	就是我知道別人要傳的話我就不傳
2711560	2713560	但是我如果不傳
2713560	2715560	別人又知道他要傳
2715560	2717560	然後就是在那個均衡底下呢
2717560	2719560	這個數字會導致大家會覺得
2719560	2721560	他在boundary
2721560	2723560	他會覺得我其實沒有特別覺得
2723560	2725560	該傳或不傳
2725560	2727560	就是剛好在那個feel indifferent的那個
2727560	2729560	界線
2729560	2731560	所以在
2731560	2733560	news他是moderate的情況下
2733560	2735560	其實你可以characterize
2735560	2737560	就是他的spread size
2737560	2739560	那我們現在已經知道說
2739560	2741560	Fully credible news
2741560	2743560	會是這樣
2743560	2745560	他的size會是這樣子
2745560	2747560	那麼現在很明顯的就是
2747560	2749560	如果news在這個範圍裡頭呢
2749560	2751560	你就知道說
2751560	2753560	Fully credible news不會傳出去
2753560	2755560	因為他鐵定是0嘛
2755560	2757560	他鐵定是最差的
2757560	2759560	另外一個是
2759560	2761560	這邊最右邊他已經達到他最大值
2761560	2763560	好討論的
2763560	2765560	因為就變成說你知道啊
2765560	2767560	這個Fully credible是最好或最差
2767560	2769560	但在中間這個時候呢
2769560	2771560	就我們有另外一個proposition
2771560	2773560	就告訴你說
2773560	2775560	怎麼樣情況下這種last credible news
2775560	2777560	last credible news就是說
2777560	2779560	比beta還小一點的那些news
2779560	2781560	他反而可以去
2781560	2783560	使得這個cascade size
2783560	2785560	比beta等於1
2785560	2787560	也就是Fully credible news
2787560	2789560	還要大
2789560	2791560	news的這個threshold
2791560	2793560	我剛才不是講他是一個threshold strategy嗎
2793560	2795560	如果這個threshold大於x的話
2795560	2797560	然後我跟
2797560	2799560	就他可能比較有點technical
2799560	2801560	的這邊就是說
2801560	2803560	其實給大家一個比較intuition
2803560	2805560	的狀況底下
2805560	2807560	你可以想像就是說
2807560	2809560	這是一個剛才的decision rule嘛
2809560	2811560	就是expectation乘上了一個
2811560	2813560	你可以改變的人的數量
2813560	2815560	然後會大於1
2815560	2817560	但是當beta等於1的時候
2817560	2819560	其實你會reduce到說
2819560	2821560	這邊x就完全是x
2821560	2823560	然後根據剛才算的
2823560	2825560	這個Q star其實是1-c
2825560	2827560	over x 所以其實這邊是c
2827560	2829560	over x 其實是大於等於c
2829560	2831560	就是在那個
2831560	2833560	在中間這個紅色
2833560	2835560	這個階段的時候其實大家都是feel
2835560	2837560	都是感覺到indifferent
2837560	2839560	要傳不傳他都覺得可以
2839560	2841560	但是如果你把beta就是稍微
2841560	2843560	降低的話就讓他比較
2843560	2845560	沒有那麼precise的時候
2845560	2847560	但是這個時候你可以
2847560	2849560	先假定說這個Q star
2849560	2851560	還是接近c over x
2851560	2853560	就是這個1-Q star還是接近c over x
2853560	2855560	你會發現說當你把beta
2855560	2857560	小於1的時候這個時候
2857560	2859560	一旦μi大於x
2859560	2861560	他們都會嚴格的大於c
2861560	2863560	所以他們都會願意傳
2863560	2865560	所以就變成說當你beta小於
2865560	2867560	1的時候呢這些μi
2867560	2869560	大於x的人其實都願意傳
2869560	2871560	懂嗎
2871560	2873560	所以如果這個
2873560	2875560	對不起就是說
2875560	2877560	就是你把他等於1
2877560	2879560	然後再往小於1的部分
2879560	2881560	也降低一點的話
2881560	2883560	你會發現說這個threshold
2883560	2885560	就是μi那些比x
2885560	2887560	還要大的人呢都願意去
2887560	2889560	傳的這個新聞
2889560	2891560	所以只要這個
2891560	2893560	你可以知道說至少這些人都願意去傳
2893560	2895560	所以如果這個threshold呢
2895560	2897560	比這個x還要小的話
2897560	2899560	你就知道說其實你把beta
2899560	2901560	小於1降低降到小於1
2901560	2903560	的時候你可以讓更多人去傳
2903560	2905560	對這邊可能有點technical
2905560	2907560	但是就是一個
2907560	2909560	就是如果intuitively
2909560	2911560	如果cost 0
2911560	2913560	的話會
2913560	2915560	怎樣
2915560	2917560	因為現在有cost的話
2917560	2919560	現在感覺有我覺得比較tricky是有cost
2919560	2921560	然後又有那個
2921560	2923560	beta又varies然後還有x
2923560	2925560	然後所以
2925560	2927560	你可以把cost想像
2927560	2929560	你不要去改變cost在我們這個case
2929560	2931560	裡面我們沒有去改變任何的cost
2931560	2933560	因為我們覺得他就是一個fundamental的東西
2933560	2935560	然後我們比較好奇的就只是說
2935560	2937560	credibility跟news之間的一些
2937560	2939560	關係所以你不要去改變cost
2939560	2941560	between 0 and 1嗎
2941560	2943560	還是什麼0 to x
2943560	2945560	沒有就是他是一個大於0的數
2945560	2947560	cost就是一個大於0
2947560	2949560	對就是大於0的數就好
2949560	2951560	如果他等於0的話其實大家就
2951560	2953560	不管怎樣他都願意傳因為他覺得
2953560	2955560	就算是garbage他也願意傳
2955560	2957560	因為反正不影響我嘛就是你知道
2957560	2959560	有什麼關係反正我傳了
2959560	2961560	我縱使照美人山但也沒有什麼關係
2961560	2963560	因為我沒有花任何的cost
2963560	2965560	所以cost其實還是有點影響
2965560	2967560	那個spread size就會最大
2967560	2969560	對因為
2969560	2971560	等於是說大家其實沒有
2971560	2973560	然後那個就會變成一個
2973560	2975560	很trivial case
2975560	2977560	我們還是有點依賴這個
2977560	2979560	有點cost的情況
2979560	2981560	我覺得這個地方就可能大家
2981560	2983560	稍微知道一下說中間這個地方
2983560	2985560	我們是有一個condition
2985560	2987560	去告訴大家說到底
2987560	2989560	for中間這個線段
2989560	2991560	這個紅色moderate news的線段
2991560	2993560	怎麼樣的什麼樣
2993560	2995560	credibility的news可以create a logic case
2995560	2997560	就是說其實如果滿足了這個條件
2997560	2999560	那麼其實比較
2999560	3001560	less credible的news他就可以
3001560	3003560	所以我們用這樣子一個condition
3003560	3005560	去討論
3005560	3007560	接下來的問題是還有
3007560	3009560	這個是非常重要就是說
3009560	3011560	這個colorary告訴我們
3011560	3013560	說對這些
3013560	3015560	只要news要大於c
3015560	3017560	因為如果news小於c的話
3017560	3019560	那你知道說這個
3019560	3021560	beta等於1鐵定是
3021560	3023560	導致沒有人在傳這個新聞
3023560	3025560	所以我們只考慮s大於c
3025560	3027560	當你知道s大於c的時候你可以知道說
3027560	3029560	只要我這個network
3029560	3031560	足夠的連結在一起的話
3031560	3033560	就是你知道他有一個threshold
3033560	3035560	就是你只要大於那一個數字
3035560	3037560	就是你平均的followers
3037560	3039560	只要大於那個數字
3039560	3041560	那麼鐵定比較
3041560	3043560	不credible的news
3043560	3045560	可以create a logic case than fully credible news
3045560	3047560	所以這是一個
3047560	3049560	還滿重要的colorary
3049560	3051560	就是告訴你說
3051560	3053560	只要這個network足夠的
3053560	3055560	緊密
3055560	3057560	只要你有足夠
3057560	3059560	平均足夠多的followers的時候
3059560	3061560	你就會讓那些
3061560	3063560	比較不準確的新聞去create a logic case
3063560	3065560	其實這邊的intuition很簡單
3065560	3067560	其實就只是告訴你說
3067560	3069560	因為network越緊密的話
3069560	3071560	越緊密
3071560	3073560	那我的followers
3073560	3075560	很容易就聽到別人傳來的訊息
3075560	3077560	因為他有很多個管道可以去聽到別人的訊息
3077560	3079560	那麼你就
3079560	3081560	這就會進而去導致說
3081560	3083560	那我可能就不太想要傳訊息
3083560	3085560	因為你知道說
3085560	3087560	大家連結這麼緊密
3087560	3089560	只要他稍微從某個管道聽到
3089560	3091560	他就聽到了
3091560	3093560	那麼這個時候會變成說
3093560	3095560	那什麼樣的人在這樣的情況下
3095560	3097560	他還願意傳新聞呢
3097560	3099560	這是一個非常非常非常極端的人
3099560	3101560	然後如果說
3101560	3103560	這個
3103560	3105560	新聞是比較
3105560	3107560	lower credibility的話
3107560	3109560	你知道說因為lower credibility
3109560	3111560	他不太能去
3111560	3113560	改變大家的想法
3113560	3115560	因為看到他有點lower credibility
3115560	3117560	我可能還是會比較考慮到我原本的想法
3117560	3119560	所以這個時候其實
3119560	3121560	極端的人還是很多的
3121560	3123560	反而相對於如果你讓一個
3123560	3125560	fully credible放下去之後
3125560	3127560	很converge到同一個點
3127560	3129560	那個時候反而是沒有什麼extremism
3129560	3131560	所以這是為什麼在
3131560	3133560	在這種情況底下
3133560	3135560	lower credibility news can create logic
3135560	3137560	因為這個時候population裡面還是
3137560	3139560	很多的extremism
3139560	3141560	但是反而在你讓那個news
3141560	3143560	非常fully credible的時候反而沒有什麼
3143560	3145560	extremism所以就沒有
3145560	3147560	這種news spread的產生
3147560	3149560	就是說news spread的size會比較小
3151560	3153560	好
3153560	3155560	那最後一個階段比較有趣一點
3155560	3157560	就是說我們考慮到說
3157560	3159560	這個population的case
3159560	3161560	其實你可以大家就直接想像
3161560	3163560	每個人他的
3163560	3165560	either他是from共和黨
3165560	3167560	either from民主黨
3167560	3169560	如果他是from共和黨就是紅色這個
3169560	3171560	這個curve
3171560	3173560	然後他就是randomly drawn from
3173560	3175560	這個distribution
3175560	3177560	然後如果是藍色
3177560	3179560	如果他一開始是偏民主黨的話
3179560	3181560	那他可能就是在左邊
3181560	3183560	這個random這個population
3183560	3185560	跟黨內的這個
3185560	3187560	多元化的程度
3187560	3189560	第一個就是μbar
3189560	3191560	如果這個μbar很大的話就代表說population越強
3191560	3193560	兩端的這個peak是
3193560	3195560	離各自越遠
3195560	3197560	然後σμ的話就代表說
3197560	3199560	這個variance在
3199560	3201560	這個黨內的大小是多少
3201560	3203560	就大家可以只要記得這些事情就好了
3205560	3207560	然後這個圖呢
3207560	3209560	其實就最有趣的圖就是這樣子
3209560	3211560	就是說
3211560	3213560	給定的news和cost
3213560	3215560	然後我去問說
3215560	3217560	因為我剛才已經講了嘛
3217560	3219560	只要這個network
3219560	3221560	足夠的
3221560	3223560	就是只要你的平均的
3223560	3225560	follower數
3225560	3227560	超過了一個threshold之後呢
3227560	3229560	你就會使得這些
3229560	3231560	less credible news
3231560	3233560	傳得比較遠
3233560	3235560	所以我只需要去討論
3235560	3237560	到底這個population跟這個
3237560	3239560	diversity如何去影響
3239560	3241560	這樣子的一個threshold
3241560	3243560	如果我讓他的threshold降得更低
3243560	3245560	那代表有更多的network
3245560	3247560	可以使得這種
3247560	3249560	less credible news
3249560	3251560	如果我讓這個threshold變得更高
3251560	3253560	那代表說只有那一些
3253560	3255560	非常非常緊密的network
3255560	3257560	才有可能使得
3257560	3259560	less credible news就是傳得比較遠
3259560	3261560	所以這邊第一個
3261560	3263560	結論就是說
3263560	3265560	沿著這條綠色線你會發現說
3265560	3267560	把population變大的時候呢
3267560	3269560	你降低了
3269560	3271560	這個threshold
3271560	3273560	meaning that
3273560	3275560	其實有更多的network可以使得
3275560	3277560	就是甚至連不緊密的network
3277560	3279560	都可以使得這種
3279560	3281560	less credible news
3281560	3283560	傳得更遠
3283560	3285560	另外是
3285560	3287560	那就我剛才講的就是
3287560	3289560	這ingroup diversity到底能不能幫忙
3289560	3291560	改變這樣子的一個
3291560	3293560	態勢
3293560	3295560	你會發現說其實
3295560	3297560	第一可是如果說
3297560	3299560	這個population其實很小很小的時候
3299560	3301560	如果你在一個population
3301560	3303560	不大的一個社會
3303560	3305560	你去改變這個ingroup diversity的時候
3305560	3307560	會發生什麼事?其實你去
3307560	3309560	你這時候加了ingroup diversity
3309560	3311560	你其實反而是去增加了
3311560	3313560	更多的極端的那一些人的想法
3313560	3315560	就是你知道你把variance變大之後
3315560	3317560	就更多人在極端的那個部分
3317560	3319560	所以反而其實它會增加
3319560	3321560	那一些less credible news
3321560	3323560	被傳播的
3323560	3325560	可能性所以它就
3325560	3327560	導致那個spread size其實是
3327560	3329560	更容易變大
3329560	3331560	然後降低了這個threshold
3331560	3333560	network threshold
3333560	3335560	但如果說其實這個population
3335560	3337560	其實somehow還蠻大
3337560	3339560	大過這個news本身的話
3339560	3341560	其實如果你增加的話
3341560	3343560	其實是可以降低
3343560	3345560	這個news
3345560	3347560	這個less credible news被傳播的
3347560	3349560	大小
3349560	3351560	但是一旦到你
3351560	3353560	大過了某一個程度的時候
3353560	3355560	其實又反向的它會
3357560	3359560	使得這個less credible news
3359560	3361560	被傳播的更遠
3361560	3363560	原因是因為當你把diversity
3363560	3365560	這樣增高的太快的時候
3365560	3367560	其實你又
3367560	3369560	再一次的使得這個
3369560	3371560	社會出現了很多的這種極端的
3371560	3373560	想法的人,然後他們其實都很願意
3373560	3375560	去傳一些less credible news
3375560	3377560	所以
3377560	3379560	為什麼其實include diversity
3379560	3381560	你去增加它,有時候是可以的
3381560	3383560	但是增加太過不是一件好事
3387560	3389560	你要conclude,但是我問一下最後一個點
3389560	3391560	你剛剛是說diversity增加的時候
3391560	3393560	極端的人
3393560	3395560	就會變多
3395560	3397560	對對
3397560	3399560	我給大家看一下這個圖就好了
3399560	3401560	就是說
3401560	3403560	因為我現在都是等於是說給定一個數字
3403560	3405560	我只
3405560	3407560	變換了另外一個的東西
3407560	3409560	的magnitude
3409560	3411560	對吧
3411560	3413560	所以其實你看這個圖
3413560	3415560	只要看紅色的部分就好了
3415560	3417560	假設紅色的peak固定在1這個點
3417560	3419560	其實它就固定在1這個點
3419560	3421560	你把這個variance變大的時候
3421560	3423560	其實它同時
3423560	3425560	以distribution來講兩邊
3425560	3427560	其實它有點把
3427560	3429560	其實是有把一些
3429560	3431560	extremist推得更極端
3431560	3433560	所以導致說
3433560	3435560	對
3435560	3437560	這感覺是一個很強的assumption
3437560	3439560	因為第一個是你不要先假設
3439560	3441560	右派或左派
3441560	3443560	它的分佈是高懸
3443560	3445560	然後第二個是
3445560	3447560	第二個是
3447560	3449560	他們
3449560	3451560	對啊這個好像
3451560	3453560	非常複雜
3453560	3455560	假設你變得比較寬一點
3455560	3457560	在你這個情況裡面就diversity變高
3457560	3459560	那
3459560	3461560	這些比較extreme的人
3461560	3463560	群體的影響到底是什麼
3463560	3465560	好像
3465560	3467560	等於是說
3467560	3469560	在我們這個model裡面其實就是說
3469560	3471560	我剛才講extremist他沒有很強
3471560	3473560	比那種moderate鐵定
3473560	3475560	他的強
3475560	3477560	因為我剛才其實有看
3477560	3479560	那個utility function的話你會知道說
3479560	3481560	他如果覺得大家投錯的話對他來講
3481560	3483560	傷害比較大
3483560	3485560	他希望大家真的去投對
3485560	3487560	所以像
3487560	3489560	這個情況底下就是說其實如果
3489560	3491560	很多人都還蠻
3491560	3493560	polarization不大的時候
3493560	3495560	反而你去增加這個variance的時候
3495560	3497560	你其實是把很多人都push away到那個極端去
3497560	3499560	所以才會導致說其實那個時候
3499560	3501560	如果這個社會已經不是很分化的話
3501560	3503560	其實你沒有必要去增加diversity
3503560	3505560	但
3505560	3507560	當你這個社會比較極端一點
3507560	3509560	就像你剛才講的
3509560	3511560	然後你把variance變大的時候
3511560	3513560	其實你一開始是有點降低了
3513560	3515560	在極端的那一部分的人的
3515560	3517560	數量
3517560	3519560	但是其實
3519560	3521560	比較詭異的事情是如果你增加diversity
3521560	3523560	你反而增加了
3523560	3525560	然後原本他是民主黨的人
3525560	3527560	結果你增加了diversity之後反而他居然有一些
3527560	3529560	很極端在民主黨裡面的人
3529560	3531560	居然去支持了川普
3531560	3533560	是蠻奇怪的啦
3533560	3535560	但是就是這是一個你知道
3535560	3537560	在分析上說我們導出
3537560	3539560	這樣的結果
3539560	3541560	然後的確他
3541560	3543560	他其實也不是我講Gaussian like
3543560	3545560	他也不是只有Gaussian
3545560	3547560	他還是有一些
3547560	3549560	我沒有把他們比較generalize一點
3549560	3551560	但是就是說他還是在某一個class的function
3551560	3553560	底下
3553560	3555560	就是他會滿足這樣子的現象
3555560	3557560	但如果你考慮了一些function
3557560	3559560	out of那個class的話
3559560	3561560	那可能就會有不一樣的結果
3561560	3563560	但是對這是我們目前得到的一些insight
3563560	3565560	ok
3565560	3567560	對然後
3567560	3569560	然後我就conclude
3569560	3571560	等於是說我們
3571560	3573560	provide這個model
3573560	3575560	去study some new sharing decision
3575560	3577560	然後還有非常這種individual sharing decision
3577560	3579560	什麼樣的新聞會傳得比較遠
3579560	3581560	然後包含像
3581560	3583560	他怎麼跟這個
3585560	3587560	這個社會的一些特性有關
3587560	3589560	然後我們就
3589560	3591560	在裡頭我們會發現其實
3591560	3593560	polarization一旦你把它增大的話
3593560	3595560	鐵定就是會使得這種
3595560	3597560	less credible news
3597560	3599560	更容易被傳
3599560	3601560	因為他其實somehow增加兩端
3601560	3603560	extremist的
3603560	3605560	數量
3605560	3607560	所以他其實使得這個社會
3607560	3609560	更容易去傳的這種less credible news
3609560	3611560	那另外就是說如果你去增加
3611560	3613560	這種perspective discrepancy
3613560	3615560	就是我們剛才在討論
3615560	3617560	跟Sean在討論的問題就是說
3617560	3619560	其實
3619560	3621560	it really depends on
3621560	3623560	你到底怎麼樣影響
3623560	3625560	那些extremist的分布
3625560	3627560	好謝謝大家
3627560	3629560	我希望這個
3629560	3631560	就是如果大家真的很感興趣
3631560	3633560	裡頭那些technical的部分的話
3633560	3635560	就可以看一下
3635560	3637560	那個paper
3637560	3639560	因為我其實obstruct蠻多的
3639560	3641560	technical的部分
3641560	3643560	大家可能有些地方有點聽到了
3643560	3645560	有點覺得好像不太懂
3645560	3647560	對所以不好意思
3647560	3649560	不好意思我可以再問
3649560	3651560	再追問一下那個diversity
3651560	3653560	的問題嗎
3653560	3655560	有一個情況是
3655560	3657560	如果我們單純考慮你剛剛的
3657560	3659560	兩個高懸
3659560	3661560	譬如說右邊的那個
3661560	3663560	distribution他的
3663560	3665560	極端值
3665560	3667560	他的左側極端值
3667560	3669560	其實會接近另一個distribution的
3669560	3671560	中間值
3671560	3673560	他的tail如果沒有這麼大的話
3673560	3675560	所以換句話說在某些情況底下
3675560	3677560	在某一個檔裡面的比較extreme的人
3677560	3679560	可能反而是比較
3679560	3681560	就是
3681560	3683560	接近兩群人的比較中間的分布
3683560	3685560	懂我意思嗎譬如說
3685560	3687560	就像你剛剛舉的那個例子
3687560	3689560	假設我今天是民主黨的人
3689560	3691560	然後我去支持一個共和黨的候選人
3691560	3693560	就Trump是一個極端中的極端
3693560	3695560	我們就假設一個一般的正常的共和黨候選人
3695560	3697560	那可能表示的不是
3697560	3699560	就是同時表示的
3699560	3701560	我是民主黨裡面比較極端的人
3701560	3703560	可是同時在整群人裡面的distribution裡面
3703560	3705560	我可能是比較接近中間的
3705560	3707560	懂我意思嗎
3707560	3709560	對
3709560	3711560	換句話說就是
3711560	3713560	每一個distribution裡面都有兩端
3713560	3715560	那一端就會是真正的extremist
3715560	3717560	對不對
3717560	3719560	就是那一端譬如說
3719560	3721560	他是左派中的左派或右派中的右派
3721560	3723560	可是如果是右派中的左派
3723560	3725560	跟左派中的右派呢
3725560	3727560	好像他反而會往群體之間集中
3727560	3729560	那他們兩群人表現出來的行為
3729560	3731560	好像會有一點點不太一樣
3731560	3733560	我也有類似的問題
3733560	3735560	就是這個右下角這個圖
3735560	3737560	比如說
3737560	3739560	為什麼在zero
3739560	3741560	就是說在紅色zero左邊的還是紅色
3741560	3743560	他就不是變成藍色
3743560	3745560	就是那個
3745560	3747560	我這邊的紅色跟藍色
3747560	3749560	就是說他一開始他挑的黨是什麼
3749560	3751560	然後但是他可能在這件事情
3751560	3753560	大家在這件事情裡頭
3753560	3755560	你知道就像
3755560	3757560	共和黨裡面也有人覺得
3757560	3759560	他不喜歡川普這種感覺
3759560	3761560	所以這個grouping跟這個
3761560	3763560	vote decision是
3763560	3765560	兩件
3765560	3767560	這不是一個overlap的事情
3767560	3769560	對等於是說
3769560	3771560	這個model就是說一開始我們要怎麼樣去決定
3771560	3773560	每一個人的perspective
3773560	3775560	就是他的perspective是怎麼被randomly抽出來
3775560	3777560	一開始就說我先投一個硬幣
3777560	3779560	一半一半他是共和黨一半是民主黨
3779560	3781560	但是他是民主黨之後我就知道說
3781560	3783560	他可能concentrate在那個peak
3783560	3785560	在E那個附近
3785560	3787560	但他也有可能在這件事情他可能是
3787560	3789560	你知道他可能非常極端
3789560	3791560	他跑去支持了偏民主黨的部分
3791560	3793560	對
3793560	3795560	然後
3795560	3797560	其實剛才根據那個Sean講的東西
3797560	3799560	就是說其實
3799560	3801560	呃
3801560	3803560	我想一下因為
3803560	3805560	等於是說你可以
3805560	3807560	我可以再elaborate一下意思就是說
3807560	3809560	當有在某一群人裡面的
3809560	3811560	polarization
3811560	3813560	在你這個裡面第一是diversity
3813560	3815560	在那群人裡面的diversity產生的時候
3815560	3817560	diversify產生的時候
3817560	3819560	一邊的人其實是會往
3819560	3821560	average那邊
3821560	3823560	就是移過去
3823560	3825560	然後另一個人其實是整整的
3825560	3827560	就是extremist
3827560	3829560	那這兩群人的行為感覺
3829560	3831560	是可以分開來討論的
3831560	3833560	對對對其實應該這樣講
3833560	3835560	為什麼我一開始說
3835560	3837560	其實如果polarization變大的時候
3837560	3839560	其實一開始你增加diversity
3839560	3841560	不是反而更好嗎
3841560	3843560	就你還記得我
3843560	3845560	你還記得我剛才那個結論其實是
3845560	3847560	呃
3847560	3849560	下面的嗎就是說其實
3849560	3851560	increasing是好的
3851560	3853560	因為你讓那個threshold變大就是變得更難的意思
3853560	3855560	所以就是說
3855560	3857560	一開始其實你在
3857560	3859560	你知道它其實它都有一個相對的極端值
3859560	3861560	就是如果你polarization已經很大
3861560	3863560	就等於說你的peak其實有點落在
3863560	3865560	其實不是1可能是落在一個你知道
3865560	3867560	3的位置然後如果你這個時候
3867560	3869560	然後這個新聞其實比較
3869560	3871560	沒那麼極端
3871560	3873560	然後等於是說如果你
3873560	3875560	把diversity降低的時候
3875560	3877560	你等於是somehow你把那個distribution
3877560	3879560	壓平了對吧
3879560	3881560	所以其實你也把
3881560	3883560	diversity升高吧
3883560	3885560	diversity升高之後其實你有點
3885560	3887560	那個shape原本是這樣子
3887560	3889560	因為diversity小的時候
3889560	3891560	其實它就比較寬
3891560	3893560	對但你看變寬的時候其實你會發現說
3893560	3895560	其實這個越extremist
3895560	3897560	原本是偏右的extremist
3897560	3899560	那個數量其實是會減低的
3899560	3901560	是會減低
3901560	3903560	因為它對吧因為你現在
3903560	3905560	假設你peak在這邊
3905560	3907560	然後如果你把它壓低的話
3907560	3909560	很多人其實開始往左邊跑
3909560	3911560	另外的為什麼到後來
3911560	3913560	它又開始有點
3913560	3915560	反向的又變化是因為
3915560	3917560	就我剛才講的就其實就像你講的
3917560	3919560	剛才民主黨有一些人他原本
3919560	3921560	你知道接近那個民主黨的peak
3921560	3923560	但是你一旦把那個diversity變高的時候
3923560	3925560	它開始去接近了
3925560	3927560	共和黨的部分它可能一開始在moderate
3927560	3929560	那個時候可能還在
3929560	3931560	increasing的階段但是一旦它
3931560	3933560	也變成了共和黨的
3933560	3935560	extremist的時候它就變decreasing
3935560	3937560	所以那個是就是那個
3937560	3939560	consistion的變化是這樣
3939560	3941560	所以比較像是
3941560	3943560	比較像是在model
3943560	3945560	裡面比較會產生的事情
3945560	3947560	現實狀況好像不太容易產生
3947560	3949560	對等於說現實狀況
3949560	3951560	底下是說
3951560	3953560	pratically就是說問題在於
3953560	3955560	你要怎麼measure這個polarization
3955560	3957560	跟那個diversity就是說
3957560	3959560	我覺得還是可以somehow可以用Gaussian like
3959560	3961560	去model這個問題
3961560	3963560	就是說這個diversity的那個量度
3963560	3965560	到底在哪裡還有那個polarization
3965560	3967560	然後還有那個news本身
3967560	3969560	所以就變成是這些問題
3969560	3971560	就是說你可能會覺得
3971560	3973560	這個嗎給一個8好像有點誇張
3973560	3975560	但是有可能現實生活中其實我們可能只考慮
3975560	3977560	那個2-4的那個範圍
3977560	3979560	對就是
3979560	3981560	有點像是給大家
3981560	3983560	一個idea說這個趨勢會是怎麼樣
3983560	3985560	但實際上在哪一個範圍就不知道
3985560	3987560	了解了解
3987560	3989560	不好意思我再追問一個問題
3989560	3991560	就是因為你的那個credibility是
3991560	3993560	基本上在0跟1
3993560	3995560	換句話來說你的
3995560	3997560	less credible或者是
3997560	3999560	lower credibility其實意思是相對於
3999560	4001560	1來講
4001560	4003560	相對於
4003560	4005560	fully credible的news來講
4005560	4007560	你往低一點點的地方走
4007560	4009560	反而傳播的會就是尺度會比較大
4009560	4011560	那
4011560	4013560	會不會對應到
4013560	4015560	真實世界上你的
4015560	4017560	lower credibility的news
4017560	4019560	其實就是像CNN這樣的outlet
4019560	4021560	因為在現實世界上不會有
4021560	4023560	真正是1的news outlet
4023560	4025560	所以你的
4025560	4027560	less credible news傳的比較遠
4027560	4029560	其實是對應到真實世界的
4029560	4031560	我們覺得有很高的credible
4031560	4033560	的那些news outlet或者是news
4033560	4035560	對
4035560	4037560	我們也有在想
4037560	4039560	這個問題就是說
4043560	4045560	對因為
4045560	4047560	我們的確有在想過這個問題就是說
4047560	4049560	其實生活中其實不是
4049560	4051560	那麼的fully credible news
4051560	4053560	但是
4053560	4055560	但是你還是可以知道說
4055560	4057560	就是
4057560	4059560	因為這個地方可能只有一些
4059560	4061560	quotative的case就是它只告訴你說
4061560	4063560	跟1比的話它
4063560	4065560	下降你把1往下降之後
4065560	4067560	它可能這個
4067560	4069560	scale會變大spread size會變大
4069560	4071560	但它沒有
4071560	4073560	足夠的訊息告訴你說它到底是
4075560	4077560	哪一段它是會變大的
4077560	4079560	我只有告訴你那個趨勢是會變大
4079560	4081560	但是你可能說說不定它
4081560	4083560	1到0.95的時候是變大
4083560	4085560	但是0.95之後又下去了
4085560	4087560	對吧有可能它的peak是發展在0.9
4087560	4089560	所以那個地方我就
4089560	4091560	不知道了
4091560	4093560	所以你們現在可以估計出來
4093560	4095560	它的peak大概會長在什麼地方
4095560	4097560	現在是沒有辦法
4097560	4099560	因為它這個等於你要solve
4099560	4101560	那個optimization question
4101560	4103560	然後就會等於是
4103560	4105560	所以我們後來就只有
4105560	4107560	比較像是告訴你說
4107560	4109560	所以我們的argument都比較偏向
4109560	4111560	less credible跟
4111560	4113560	fully credible
4113560	4115560	因為我猜測就是
4115560	4117560	因為講less credible的時候大家心目中
4117560	4119560	跑出來的東西有點不太一樣
4119560	4121560	因為在你的model裡面
4121560	4123560	less credible的意思就是相對於
4123560	4125560	比較不credible嘛
4125560	4127560	那我剛剛問這個問題
4127560	4129560	應該是下一個問題其實就是
4129560	4131560	如果你們能夠預測出來
4131560	4133560	你們可以estimate出來那個peak在哪裡
4133560	4135560	有可能你把那個model應用到
4135560	4137560	真實生活中的時候
4137560	4139560	你可以來算每一個新聞outlet的peak
4139560	4141560	那或許算出來你們的
4141560	4143560	less credible的
4143560	4145560	在你們model裡面的news outlet
4145560	4147560	對應到真實世界其實就是我們覺得
4147560	4149560	比較有credible的那些
4149560	4151560	就是如果兩邊可以對應起來的話
4151560	4153560	故事就會通
4153560	4155560	應該這樣講就是說
4155560	4157560	我
4157560	4159560	我們有所有的equation你可以用
4159560	4161560	就甚至你要用
4161560	4163560	用那種
4163560	4165560	去算那個peak也可以
4165560	4167560	只是
4167560	4169560	因為我沒有辦法去證明說那個peak
4169560	4171560	到底是
4171560	4173560	會落在什麼區間你知道嗎
4173560	4175560	就是我能mathematically
4175560	4177560	proof的東西就是告訴你說
4177560	4179560	它有這個趨勢就是
4179560	4181560	那個condition是什麼但是
4181560	4183560	我們有所有的equation就是你可以去
4183560	4185560	用collaboration就是說去算
4185560	4187560	那到底那個peak會長什麼
4187560	4189560	在哪個位置如果你用computational
4189560	4191560	可以算只是說
4191560	4193560	我沒有辦法mathematically proof
4193560	4195560	那個peak的點在哪裡
4195560	4197560	或者是它到哪裡會下降
4197560	4199560	就有點
4199560	4201560	那個就對我們來講比較難
4201560	4203560	謝謝
4209560	4211560	主持人回來了
4211560	4213560	就是我們這個演講
4213560	4215560	基本上是一個小時
4215560	4217560	之前跟靜嘉講是一個小時
4217560	4219560	大概50分鐘的演講然後大概10到
4219560	4221560	20分鐘的Q&A討論
4221560	4223560	但是我們今天的形式所以變得比較像是
4223560	4225560	就是在演講過程中
4225560	4227560	就參雜了Q&A
4227560	4229560	所以我基本上就是當作我們已經
4229560	4231560	把Q&A結束了所以現在就是看說
4231560	4233560	靜嘉願不願意再多
4233560	4235560	留一點時間然後
4235560	4237560	可以再繼續做討論那我不知道你
4237560	4239560	大概願意再多大概10分鐘嗎
4239560	4241560	再繼續簡單討論
4243560	4245560	可以3點半嗎
4245560	4247560	3點半因為我剛好跟嘉佑約了
4247560	4249560	3點半沒關係
4249560	4251560	ok所以就是7分鐘後
4251560	4253560	對但大家也可以
4253560	4255560	就是如果還有什麼其他問題的話
4255560	4257560	可以就是email我
4257560	4259560	然後我可以在這邊先answer一些
4259560	4261560	就是可能比較大方向的question
4261560	4263560	好那我先做
4263560	4265560	另外一件事情就是我可不可以
4265560	4267560	我就先把錄影先關掉了
4267560	4269560	然後就是大家就
4269560	4271560	對好那我就先關錄影
