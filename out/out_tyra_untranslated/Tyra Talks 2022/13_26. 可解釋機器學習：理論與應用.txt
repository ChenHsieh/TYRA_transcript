這樣能看到嗎?
可以
OK
大家好
我是莊育南
可以叫我Allen
我現在是
Rice University的PhD學生
我讀的是
Computer Science
我現在是二年級
我在做的研究就是
像是剛才那個
主持人提到的
Predictable Machine Learning
或者是這種Explainable
Artificial Intelligence
目的就是在
介紹
這個Machine Learning Model
為什麼會做出這種Prediction
你要去給他一個解釋
讓人家去
信服你說
這個Prediction Result
不是亂來的
OK
講一下我的
自我介紹
我以前在台灣念碩班
我以前做的是
推薦系統
做推薦系統相關
等下Q&A如果有推薦系統相關的問題
也可以問我
但我會盡量回答的出來
OK
我以前是
大學的時候念數學
我大學在四年級
畢業以前我是不會寫Code
我連一行Code都看不懂
所以
在這邊鼓勵
還想學Code的人
不要放棄
都有機會
努力練練練
應該是大家都有機會
可以去做這件事
OK
那今天的話主要是
OK
那個
我只是想要講說
這開頭太勵志了
反正
今天我不是什麼大師
我只是一個
普通的二年級的PhD學生
所以你如果要打斷我的話
你就直接開啟你的麥克風
然後把我打斷都沒關係
OK
那我就開始吧
OK
今天的Outline會比較是
著重在四個點
第一個就是我來介紹一下
什麼是XAI
什麼是可解釋機器學習
然後再來就是
接下來就是比較偏向
我的研究的部分
在可解釋機器學習裡面
主要會有兩種
一種是模型的本身就有可解釋性
舉個例子來說
大家可能有聽過二分法術
就是Decision Tree
這種Decision Tree的話
它在做Prediction
我們可以看到它的決策過程
那這種就是
它本身就是具有可解釋性的
Machine Learning Model
那另外一種是像是
你們比較常聽到的神經網路
然後像是
Deep Neural Network
就是那種Fully Connected
那種就是Black Box Model
那它本身就不具有可解釋性
因為你不知道它裡面的那些Learning Weight
就是那些學習的那些Weight
到底怎麼分布
或是怎麼去讓這個Model
學習的這個動作
所以我們就需要一個
事後的解釋
就是我在模型學Prediction Model
學完之後
我再額外去Learn一個
我們叫Explainer
就是專門用來解釋的模型
去解釋那個Prediction Model
然後這種的學習方式叫做Post-Hoc
就是After Prediction Process
OK
然後再來就是我來講完這個之後
我來講一下
這個XAI目前有什麼應用
大家如果不是Computer Science
背景的人可能也不要
就是會覺得
我到底學的有什麼用
我講一下現在有什麼Open Source Package
就是
可以做這個
Explainable Model
那當然現在就是大家都用Python
所以我就是講了兩個Python的
Open Source Package
然後跟大家說其實
這個東西不是想像中的遙不可及
就是每個人在每個人領域
當你有Prediction Model
你想要做Explainable
那人人都可以有機會
可以踏進這個領域
OK好那接下來就來
講一下什麼是
什麼是XAI
OK那就是
人工智慧在
在我們日常生活中
已經有很多很多應用了
像是這最近很
前陣子比較流行的
就是DeepMind不是
就是去玩這個
然後他可以下贏那個
人類的Expert
那前陣子
還有DeepMind
有給了一個叫Alpha Tensor
跟Alpha那個叫
算蛋白質的那個東西
那其實上他們都是
就是那種機器人
強化學
強化式學習的
這種應用在日常生活
領域當中
像是一些Medical Diagnosis
就是例如說我給你一個X光片
然後你希望這個
機器人的Model判斷你有沒有腦瘤
那他就會說
有或沒有類似這種
或是甚至說你給今天一個
機器人的Model說你今天有
咳嗽有什麼症狀
然後他幫你判斷你到底得了什麼病
類似這種的Medical Diagnosis
或是像是
Tesla的Autopilot
就是這種
自動駕駛
他不是會Detect
就是有什麼物體
然後他會給你一個最佳路徑
讓你這個車子去開
再來就是Voice Recognition
像是Alexa
不知道大家有沒有用過智慧音響
你只要對這個音響講的時候
他就會偵測你的
語音然後把這個東西
轉換成
文字
然後透過這個文字
Online Server去Request
或是像最近Meta有個很紅的
Speech to Speech
就是把台語轉換成英文
那個也很神
所以就類似這種AI的
應用在日常生活中越來越常見
OK
那但是
就是我們其實很難知道
就是這些模型就為什麼會去
做出這些判斷像是
假設你去看一個醫生
然後今天醫生推了一個
然後你把
你的病狀全部輸入這個
模型他就說
你今天只是感冒你信嗎
為什麼就問你為什麼會做出這個
判斷或者是
今天Tesla給你一個
最佳的
自動駕駛
路線規劃
他給你給出來之後
你真的按照他這樣開你真的覺得
他不會出事嗎
這個決策過程中
如果能夠提供
說今天是因為
這個模型學到什麼什麼特徵
或是他感知到了
例如說有一個行人走過去他停下來
或是他覺得他detect
前方是綠燈
這種感知的
這種為什麼的這種過程
如果提供給使用者的話
那他其實上可以增加
人們對這個模型
對prediction model這種機器學習
模型的這種可信度
對吧
所以整個這個xai
他其實就是在做一件事
他希望提供給使用者一個
合理的原因
例如說因為這個模型
學了什麼特徵
或是這個模型
因為看到了什麼或是這個模型
因為學習了某兩三個特徵
而導致我做出這個prediction
的結果
那xai就是在
把這個過程給完善
像是左邊這個
假設這個autopilot我剛才講過
就是我們xai要做什麼
其實就是想要提供
就是為什麼這個autopilot
會給你的
autopilot做的路徑是安全的
所以你就要跟使用者講說
因為我偵測到模型
我的模型偵測到人
我的模型偵測到前方
可能50米內沒有車子
那再來就是例如說
像是右邊這個圖
這個醫療的診斷
醫療診斷它其實就是要
例如說你要說服這個醫生
或是說服這個病人說
因為我在你
腦的X光片裡面
看到某一塊是腦瘤
我把它框起來
所以我判斷你的腦瘤
這個動作其實上
看起來就是沒有這麼的
在整個prediction的過程中
看起來沒有這麼的
這麼有需求
但是如果你提供給使用者
這個步驟的話
就是為什麼這個步驟的話
使用者其實會對於你的決策模型
感到就是非常的
例如說他可以放下心來去
相信這個決策
當然還有很多我沒有提到的應用
像是例如做股市交易
或是什麼的
你今天股市說你就買50萬的Meta股票
類似這樣
Meta現在已經跌到
骨折了
他叫你賣掉
你說為什麼他不會跌超過
低於100塊
這模型如果預測錯了
他叫你趕快賣掉
他說因為股票可能跌超過100塊
類似這種東西
你總要說服買賣的trader
說因為我看到
他的財報全部都不達標
什麼什麼貴的
這種檢測到特徵
你才能說服使用者說
這模型是可信的
所以
這種東西其實
牽扯到幾個點
第一個是我們剛才講到你要說服使用者
然後再來就是你要提供安全
還有一個很重要的東西就是
就是一些法規的東西
像是GDPR
像是Facebook在前陣子
就是Zack Scruber
被叫去歐洲議會罵
或是Google CEO
被叫去美國的
眾議院被叫去罵
其實就是在說
你們使用的
這些privacy data
或是你們使用的個人資料
這些資料的使用
你沒有提供給使用者
一個合理或是讓他
覺得安全幸福的這種
使用方法的準則
你不能就是無條件無上限
去使用這些東西
這些feature當作訓練模型的一個依據
所以如果你今天能透過
XAI的這些方法
提供給這些使用者
我今天是因為用了你
這些feature
然後導致我提供給你
這些個人化的一些
不管是推薦結果或是預測結果
那其實使用者他在
某種程度上他會比較安心說
原來你是使用過這些東西
縱使他是隱私資料
但是總比就是完全關起門來
然後做事然後就跟你講結果
好得非常多
這是XAI為什麼
我不敢說這一兩年
或是這三四年
越來越多人關注的一個
原因
好
再來就是
剛才那些都是一些
比較
high level層面
那我們現在就講一些
比較
科技層面的東西
反正就是如果我們現在在做這種
image classification
就是我們這個圖片分類
我們在判斷這個模型好不好
就是我們單純做實驗我們要
train一個我們要訓練一個
圖片分類的模型
這個圖片分類模型我們總不能
說
當然可以說就是prediction
就是例如說我們去看它說
預測準不準確就例如說看它
accuracy好不好啊
我有沒有成功的預測到屬於這樣圖的
當然是可以這樣做
你可以其實可以想一件事就是
今天這張圖是一隻青蛙
那
今天這張圖是一隻青蛙那今天這個模型
是真的學到青蛙的特徵嗎
也就是說換句話說就是
他真的學到青蛙這個頭嗎
就是這個image classification
這個模型真的學到這個青蛙
沒有人知道他有可能是學
假設後面有一個
一個水塘然後上面有一個荷葉
然後他就覺得
大部分的青蛙圖片都是因為青蛙
在水塘跟荷葉上面
所以他就反而去抓那個水塘
跟荷葉的特徵
那這件事情不是我們樂見的為什麼
那當今天有一個像是這種圖片的
來那學到水塘跟青蛙
的這種的圖片
分類的這種的model
是不是就會predict錯誤
所以這個東西其實是我們不樂見
所以透過這種XAI的方法
就是我們去看說到底哪一個
哪一塊哪一塊重要
那最後如果我們得到我們最後的解釋是
因為
他判斷這張圖是
青蛙的原因是因為
他有這個很具代表性的
這個頭那我們就知道
這個模型其實是
學往對的方向而不是
在抓一些背景的漏洞
或是一些shortcut
就是一些捷徑
捷徑的feature來讓我們的
image classification的accuracy上升
那或是像是在
medical diagnosis就例如說
我們就假設
假設今天有一個檢測
你是不是得covid的classifier
那大家都知道
covid就是很大
很大的機率就會例如
發燒喉嚨痛那今天假設
你只是感冒
那你只要輸入發燒跟
喉嚨痛這兩個症狀
進去他就說你的covid
那你就是慌了就為什麼
所以這時候如果我們要去
判斷說今天這個
的這個
是不是真的學到我們要
學的特徵就例如說我們
也提供了一些肺部X光照片
啊什麼的
他就會去說因為他做出這個covid診斷
並不單單只是
透過你有
喉嚨痛跟發燒這兩個特徵
而得出你有的covid這個結論
所以我們今天
如果在
提供這種prediction result給
使用者的時候我們附帶上
我們因為detect到
你有什麼什麼特徵
使用者在相信
這一種prediction model
就會更加的幸福
好
那我喝個水
所以
總結來說就是AI跟XAI
他其實上他是一個相輔相成
的一個循環的
循環的一個過程
如果今天沒有prediction model
那我們今天如果沒有AI的預測
那我們根本就不需要去解釋
這個東西所以XAI就不會
不會出現
所以我們今天XAI其實
主要是透過
我們另外訓練一個
模型或是模型這個prediction模型
本身就具有可解釋的特性
然後來達到這個
提供解釋的這個動作
所以你可以看
這邊有一張柴犬
然後這邊有一個prediction model
他是一隻柴犬
那為什麼
有可能是這個柴犬趴在地上
因為大部分的狗都趴在地上
我學到這邊比較
偏旁的這些特徵
導致他判斷他是柴犬
也有可能或是因為他
只看到他是橘色的毛色
就說他是柴犬
類似這種我們不知道
因為他是一個黑盒子的模型
就是我們所謂的black box model
所以我們需要另外一個
XAI的這種model
去把這個黑盒打開
把這個black box給打開
然後我們就看到說為什麼他給出
這個圖片是一隻柴犬
然後因為他看到柴犬細細的眼睛
跟柴犬特有的耳朵
那因為這個特徵的
給予
的學習所以我們
讓這個圖片被判斷為柴犬
那這時候這個prediction model
就非常的可信了
因為我們知道他學習到
正確的方向
而不是他去偷懶
去偷懶
學習別的特徵造成這個prediction的結果
所以這個模型
prediction model會是比較一個可信的
一個程度
ok
那剛才那個就是
一個XAI的
大概念的一個解釋
那我們現在來
比較深入一點
因為我不確定這邊是不是全部都是
CS background的人
的同學或是老師
或是一些博士
碩士生所以我今天就是
會給一個比較high level
的一個
talk那我們現在就是
深入了解就是
這種XAI有什麼
現行有什麼
sota technique
就是可以去執行這個
explanation的動作
ok
那在講這個之前我們先
可以大概看一下
幾個不同的scope
那這邊有分成global跟local
我其實就大概講一下
這個global其實就是
我們在看模型
假設今天這個image classifier
這個classifier
in total
就是這個模型在乎什麼特徵
也就是說我們今天如果都
都例如在train一個
假設狗的品種分類
的這種圖片分類
的model
那我們可以就是
想要知道這個模型到底在乎什麼
例如說狗的很多不同品種
耳朵都不一樣所以耳朵就是
這個模型在乎的一個特徵
這就是global的
scope那什麼是local呢
local比較偏向是
我們去解釋每一張
圖片他在乎什麼
像是我舉個例子
像剛剛那張柴犬
我們要做global的話我們就是
必須要去看上面的classification
model他在乎的
在所有的狗裡面他在乎的什麼特徵
但是如果在local scope下面的話
我們就可以我們就必須一張一張
圖片去看說
這張圖片他是因為
柴犬的臉那下一張圖片
可能因為什麼邊境牧羊犬
的尾巴或是邊境牧羊犬的
耳朵這種所以在
local scope底下每一張圖片
都會具有每一張圖片的解釋
那這其實有各有各的好處就是
我們當今天如果要去宏觀的看說
這個模型可不可信那我們當然就
使用global就可以了嘛
我們就要去提供給使用者說
in total來說我們這個
autopilot for example就是
in total來說我們這個假設autopilot
這個模型因為他
會去
detect紅綠燈
有沒有亮或是有沒有行人或是路上
有沒有障礙物這種in total的
方式我們去說服使用者那我們當然
就使用global那local的話
就是我們說在現在這個
場景裡面就這一秒
那現在這一秒我們這個
sensor detect到的這個街景
那因為他有什麼什麼
下一秒又可以提供到什麼
所以當我們今天要去
去分析每一秒的
這種街景圖的時候
那我們就必須使用local
的模型那local模型就會
提供比較
算是比較客製化的一個
解釋的結果給使用者
ok那這個是
global跟local那再來旁邊兩個
有不同的manners就是一個是intrinsic
一個是prototype那intrinsic
就是剛才有提到說模型本身
就具有可解釋性那什麼
要具有可解釋性本身就具有可解釋性
像這個決策數
decision tree就是模型最後說
喔我今天要預測
他的結果是c
那因為我們就可以從這個c
往後走是假設他因為
這個走到這個走到這個例如
這樣子所以我們就可以知道模型在做
決策的時候他是因為經過了
什麼條件然後來達到這個
最後prediction的結果
那這個模型本身
就是具有可解釋性因為
我們可以從模型的決策過程中
知道他做了
截取了什麼特徵或是做了什麼
做了什麼condition的選擇
ok這個就是intrinsic
model或是像右邊這個就是
如果大家有稍微聽
過一些machine learning的講座
或是課程的話應該就會知道
BERT這個東西那BERT這個東西
他裡面字跟字在模型訓練裡面
字會有一個
東西叫做tension score
所謂的tension score就是例如說
我今天把一個句子
丟到這個BERT model裡面
那我每一個字對於模型最後
預測出假設我要做semantic
prediction我預測這句話是
正向或是負向
的情緒那
我每一個字對於這個正向或負向
情緒的貢獻程度
都有加有減就是例如說
I love this morning那love對於
他最後判斷出是正向
絕對有很大的幫助因為他love這個字
本身就是一個正向的
動詞那
這個
很大貢獻這個分數
我們可以理解為
model的tension weight
就是這邊比較high level
所以我們可以透過這些字跟字之間
不同的重要的這種對於
model prediction不同的重要程度
來去判斷說
今天這句話
到底為什麼
會做出他是
正向的這個prediction result
透過
去拿裡面的
important score來最後
做prediction的結果
那這個我剛才講的這兩個模型
都是intrinsic
就是模型本身是具有prediction模型
模型本身是具有可理解之心的
這個範疇那再來就是post hoc
那post hoc我覺得
我等一下可以講一下為什麼我比較focus
在這個點上
不是前面不好是因為我覺得
現在目前大部分就是
machine learning model都是用一些比較黑盒子的
模型像是我可能就隨便
建一個深度學習的
網路
就開始疊一些積木啊
疊一些什麼全連接層
什麼的convolution什麼的疊疊疊
那這些
這些prediction model縱使他有很好的
結果但是他本身
因為我不知道他裡面幹了什麼事
就是例如說我不知道他
他去學了什麼feature什麼
所以這種比較
黑盒的學習就需要
這種post hoc的解釋
就是我需要在模型訓練
預測模型訓練完後再提供
一個解釋專門用來解釋的
模型來去解釋
前面那個預測的模型
所以這種模型
在日常生活的這種
scenario下是
比較常遇到的就舉個例子
像是例如說我們做推薦系統
我們做推薦系統可能會用一些
神經網路的
預測模型來做訓練
那這時候
他神經網路的這個推薦系統
本身就不具有可解釋性
對吧所以我們就需要一個
事後的解釋性模型
來去解釋那個推薦的
預測模型
那這邊一樣有提供
這個事後解釋模型一樣有兩個
scope一個就是global一個是local
那global的話就一樣就是
我去看說這個神經網路
預測模型呢他到底
這個模型本身注重了什麼
大方向的feature
那local的話我就去說
這個神經網路對於每一個
使用者來說因為
我看到使用者A
他的年齡過
18歲然後例如說
他是男性然後例如說
他是例如說他居住在
亞洲那居住在
亞洲呢他會說中文
他會說台語所以
所以我推那個高五人的歌給他
對就是每一個人都有不同的
特徵所以我根據每一個人去提供
不同的模型解釋這個就是在
local scope在做的事情
ok好那
所以in conclusion就是
今天推
比較主流的分成這兩種
intrinsic跟post hoc
對然後
我今天會比較著重在討論
這個post hoc的explanation
然後再更進一步
就是我會比較著重在local的部分
ok因為local比較是
屬於更日常生活中
會碰到的像我剛剛舉的
推薦系統的場景
ok
好那
稍微來講一下
intrinsic local
那就是這邊有一個
這邊有一個圖片
然後這是一篇文章
那例如說
我今天說欸這篇文章很重要
為什麼例如說
因為他detect到這個什麼ent23
或是他detect到
什麼natural
tenacity
這種字眼所以
透過這種
我說哪一個字哪一個字比較重要的
這個過程
我提供給這個使用者
使用者說欸我今天為什麼
拿這篇文章做出以下的預測
是因為我在
我的模型吃了這個ent23
今天我模型吃了這種
比較具有情緒的字眼
這種的過程
然後讓使用者更加信服
說我的這種文字
預測模型預測出來的
結果是可以信服的
ok這就是intrinsic
本身的
intrinsic本身要提供的
的東西
那再來就是
intrinsic global
我剛說global主要是
在模型本身注重什麼
所以例如說我們今天設計一個
可解釋的
CNN就是
是屬於image classification
的其中一個比較具有代表性的
的模型
那我今天就是要去說
為什麼這個CNN會成功預測出
這個貓是
這個貓的圖片是一隻貓
那是因為它detect到
這個貓的頭
那如果我們從CNN裡面
把裡面抽取出一層
例如說其中一層model weight
那這個model weight我把它蓋在圖片上
我們是不是可以去看說
因為這個模型去
對其中某些feature做convolution
得到了某些
特定的feature
是不是這個CNN的模型去做出
它是貓的prediction的這個結果
那從模型中
抽出這個mask
或是抽出這個feature map
然後來讓這個
最後的結果是
可以讓使用者來看說
其實這個
最後的預測結果
是可以信服的
這個過程它其實就是
我們所謂的intrinsic的過程
那所謂intrinsic就是
模型本身就具有
可以製造解釋的這個功能
或是模型本身就具有
可以提煉出某些component
然後進而提供使用者解釋的
這個
過程我們就叫做
intrinsic的過程
ok
那另外一個就是
我們今天會比較著重的post hoc
post hoc的explanation
那post hoc explanation
它主要是分成兩個步驟
前面就會是我們的
black box model
所謂的black box model就是我們的prediction model
我們會有data
我們會有model去訓練
那接下來我們會有一個explainer
就是我們的所謂的拿來解釋
這個prediction model
的machine learning model
ok
那再來我們會需要一些metric
就是需要一些評估準則
來去看說
這個explainer到底學得好不好
並不是說我今天給你一個
explainer你就要五條天去相信它
所以我們當然可以透過
很多很多不同種的
metric來去判斷它好不好
那其中最有名的就是
屬於這個sharp e-value
如果這邊有經濟學背景的
或是曾經有學過經濟學的
人呢
可能會對這個字滿熟悉
它就是同一個東西
其實就只是要去算說
每一個feature
對於這個prediction model
的貢獻程度是多少
我們藉由這個
game theory這個理論裡面的
sharp e-value
來製造我們的
所謂的正確答案然後去做evaluation
對
那這個explainer有很多
很多方法
像是sharp啊像是line啊
這種
那這邊
列出來這些方法其實
有一個很大的缺點就是
如果我們要做剛剛我們所說的
local explanation的話
那我們每張圖都必須要提供一個解釋
那以現在目前的
比較
常見的方法
我不能說全部因為現在有
另外一種方法就是
現在比較常見的方法呢
就是我們一張圖
例如說我們要給一張圖片解釋
我們就必須要有一個explainer
也就是說我們今天有十萬張圖片
因為我們要給每一張圖片一個
客製化的解釋結果
所以我們就必須要有十萬
那會導致什麼導致這個過程非常非常
慢所以
這些
方法就有一些缺點
當然就有些人說你今天有
sharp e就是經濟學
game theory這麼好的一個
一個
準則你為什麼不拿來用
那我等一下會解釋一下為什麼我們
不能直接拿sharp e來做
最後模型解釋的一個結論
ok
好那我們就來講一下
比較在網
裡面挖一點那什麼是
sharp e value呢其實sharp e value其實
就是我們把game theory的一個
東西借過來然後來
來就是給模型
的prediction做解釋
好那我們
要製造sharp e value必須要做一件事
就是我們需要
知道每一個feature我們的目的就是
知道每一個feature的
important score就是我們需要知道每一個feature
的重要程度的分數
那我們要知道每一個feature
的重要分數
必須要做一些什麼事
其實很簡單就是我們要把
每一個假設我們這邊有
user0有四個feature
5個12345
我們要判斷這個user0
是不是他可以
還清這個房貸
ok那我們就必須要知道說
每一個feature對於
能不能還清房貸的重要程度
我們要做一件事就是例如說
我們必須要
這個job加這個
marital status
婚姻狀況的兩個feature
的共同貢獻
對於這個房貸
能不能還清房貸這個prediction
的重要程度
所以我們必須要去
go through
all the combination
feature的combination
也就是說我們必須要說
我們必須要去走例如age加job
加marital加education
或是age加job加marital
或是age加job
這種所有的feature的組合我們才可以知道
在shopping eval裡面我們才可以知道
每一個
每一個單一feature對於最後prediction
結果的重要程度
所以這會導致說什麼
這會導致說如果我這邊只有
5個feature那倒還好
因為這個組合量就是2的5次方
要不要要不要
所以是2的5次方
那今天如果我在
大公司裡面做推薦系統
我有2000個feature
那就會是2的2000次方
這個一個user就需要
2的2000次方的計算的
次數
那還得了我今天有11個user
那撐下去
那就是算不完了
整個運算過程算不完了
所以今天這個shopping eval
為什麼不能用
它其實本身是一個NP-hard problem
就是它的複雜度太高了
而且我們不能用
低複雜度的方法去
去驗證它
我們算出來的東西是不是對
這個東西就NP-hard
所以我們在
我們在直接拿shopping eval
這個詞來當作
最後解釋的important score的話
其實可以但是
是不現實的
因為它太
太吃時間
它需要太大的計算的複雜度
所以導致我們
沒辦法使用這個狀況
ok
剛才漏講一下後面這個
其實後面這個東西它就是在算說
如果我今天把
例如說我今天只在乎job跟merito
我今天把另外三個拿掉後
那我的model會
的prediction會做出什麼結果
我model prediction
會發生什麼變化
如果我今天把這個balance
把job把merito拿掉
我只把age跟education
考進去我的prediction model
然後我今天它原本是
不能還房貸的預測結果
突然變成說你可以還得起房貸
那這說明什麼
這三個feature很重要
這三個feature是導致它預測正確的
一個key feature
ok
所以透過這種不斷的
這種iterate的過程
去找到說每一個feature的
重要的分數
這個就是shopping value
ok
懂經濟學的人應該
看到這個東西會覺得蠻
蠻親切的
ok
好那
我shopping value不能用的話
那怎麼辦
這就是我在做研究
就是我想要把這個
explanation的這個過程給加速
我想要讓
因為每一個feature
每一個
我們在做local explanation
每一個使用者都必須要
給一個客製化的
explanation的結果
那我剛才提到每一個客製化的explanation的結果
都必須要有一個獨立的
explanator
就是一個解釋prediction model的模型
那這個過程本身
就是一個非常耗
耗時間的一個過程
像是你如果
給一個推薦結果然後你叫使用者
說不好意思等我10秒
我給你一個推薦我給你一個解釋
使用者一定就不能接受
如果你一個網頁叫你等10秒
你一定就左上角插他就按掉了
類似這樣
所以這個加速結果是很重要
那在這邊這個work
這是我的去年投稿上的一個work
這個work
這個work就是
我們目的是在加速這個
shopping value的
這個運算過程
然後把這個過程的結果
提供給使用者當作
解釋的結果
ok那我們做了什麼像我剛才在講說
shopping value他其實
因為我要go through all the combination
of the feature set
所以我們今天假設有五個feature
那就是2的五次方
那是不是每一個feature的組合
對模型來說很重要呢
不見得為什麼就是有些feature
他的交互作用其實對模型來說
是不重要的
那這個資訊可不可以在模型訓練完
之後就得到呢
其實是可以的就是我們其實可以把
模型的訓練的權重
給拿出來去看到說
feature1跟feature2
黑色代表很重要那feature
x1跟x2他很重要
x1跟x3
突然之間沒有跟x2那麼重要
x1跟x5來說
其實完全不重要
所以這個時候我們就可以把x1x5
這個組合從算shopping value
的這個組合給拿掉
那拿掉之後
例如說我們原本要2的五次方可能要32次
可能透過
這種拿掉的過程例如說
我們把這個灰色以下的
色階的色塊的
組合全部拿掉那我們
裡面所需要算的次數就變成是
可能2 2 4 6 8
還有14次
所以其實是減了一半的過程
對吧這個過程其實是
可以加速這個
shopping value的計算
然後把這個東西變成一個
使用者可以接受的
一個等待時間
那最後提供給使用者
比較快速的這個解釋的
解釋的
這個結果
所以這個數學其實就是在表達說
我們今天在shopping value
裡面我們需要
去經歷所有的
所有的feature的組合
那我們今天把它變成是
我們透過模型的權重
去看到feature跟feature之間的
interaction的重要性
然後把不重要拿掉
最後我們去算
approximate的shopping value
把這個shopping value當作
當作是
explanation的important score
提供給使用者
就是這個work的精神
ok
所以你可以看到說
我們這個work其實有一個
application的scenario
就是我們今天在train的一個
prediction model
我們去預測說
他的income是高還是低
這個東西,我們其實可以用我們的模型
去解釋這個訓練好的
這個prediction model
然後進而去提供說
為什麼他可能是因為你的
高學歷,例如說你是
稻乾工作人,類似那種
然後是造成說
我判斷你是高工資
的這種結果
所以我們這個模型其實是
可以用一個比較快速的
快速的這個explanation的方式
然後在
提供給使用者的這個情境
上面做使用
那右下角這個圖其實可以看到
左邊這個藍色的這個bar
其實是
ground truth
也就是說我們用經濟學
跟theory的那個公式去算出來
說屬於這個
屬於這個feature來說
真正他應該要有
那可以看到其實
我們就算把那些不重要的
feature跟interaction給拿掉
我們的shopping value在
我們的這個model在預測出來
這個
importance的這個score
他其實跟shopping value是非常靠近
完全一模一樣
畢竟我們把一些combination拿掉了
所以這個東西其實是
更快速但是我們同樣不失去
那個shopping value的那個
準度所以我們在提供給
每個feature的重要性
給user的時候我們可以說
學歷這一塊的feature
對於我們的
模型來說預測你是
高薪水低薪水的
這個
模型來說是很重要的
所以我們會把這個例如分數
或是把這個重要程度的
ranking的結果提供給
使用者作為他
想不想相信這個
這個預測模型的一個參考
ok
好
那下一個工作
那我加速一下
下一個工作的話是
因為我們前面是
加速那個shopping value
的計算過程
對吧
這個工作是我們
與其要去加速那個過程
那倒不如說我們去訓練一個模型
去把shopping value
計算的那個distribution
計算那個distribution給學起來
那這個東西其實
可以更快地去
給預測結果
為什麼呢是因為
我們這個傳統的
一般的這種DNA模型
我們剛才是每一個
每一個
user我們要給他一個
客製化的結果我們就必須要有一個
explanation model
所以我們今天要預測十萬個user
我們就要給十萬個model
那在這邊我們只要給一個user
我們只要給一個model去
預測所有user的
explanation結果就可以了
那這個過程其實上也是加速
加速我們給
explanation過程的這個進程
ok那為什麼是因為
一般的這種
深度學習這種網路
我們只要給例如說我們可以
一次把十萬個user當作
data送進去那個
預測網路然後這個預測網路
他就可以突出屬於這十萬個
不同的結果
ok所以我們再也不需要
我們再也不需要一個使用者
對應一個獨立的
explanator
我們現在只需要一個explanator
去對應所有user
這個訓練過程
其實是我另外一個work
那這篇也在投稿當中
那他主要就是說
我們是不是可以透過一些
sorry
透過一些正樣本跟負樣本的這種
學習什麼是正樣本
例如說
我們今天把某些一張圖片
把某些重要的
特徵給遮掉
如果我們遮掉這個特徵呢
是屬於不重要的特徵
比如說我們遮掉狗的眼睛
那因為他有另外一隻眼睛
所以可能一隻眼睛相對來說不重要
那模型在判斷
這張圖是不是一隻狗的時候
他還是做出一樣的判斷
這個就是所謂的正樣本
那這個正樣本其實可以幫助
幫助我們去解釋
例如說我們說
我們希望一個人判斷說
這張圖片
希望一個模型判斷說他是不是一隻狗
那我們給出的解釋
因為狗
狗在乎這些特徵
所以我模型做出
這個prediction的結果
那這些東西是正向
那我們可以給一些負向
例如說我們這邊給一個場景錄
場景錄就不是狗
所以模型就會知道說
今天遇到這個場景錄的時候
他就是屬於這個狗的
negative sample
就是所謂的負面樣
負面教材
負面教材跟負面教材的時候
我就可以去學
就是我希望我的模型
給出的解釋
要越靠近正面教材
然後離負面教材越來越遠
這就是我要做的事情
所以我透過我的這個
設計的這個模型
那這個過程
其實有一個名詞叫contrastive learning
就是中文叫
對比學習吧
應該是這個對比學習的過程
然後讓模型知道什麼是對
什麼是錯
然後我希望我對的跟錯的
之間的距離越來越大
然後讓模型往正確方向去訓練
那為什麼後面還有一個
fine tuning的過程
這邊的fine tuning的過程是
為什麼我剛剛提到shoppy value
是因為shoppy value有很強大的
經濟學game theory的一些
理論上的support
所以如果我們能
如果我們能製造出
這些
這些important score
他也具有shoppy value
這些特性的話
那
在提供給
我不敢說提供給使用者用
但至少在提供給一些
例如machine learning engineer
或是一些高層
他們要去看說去evaluate說
這些evaluation的
這些explanation的結果是不是可信
那這個過程其實上
一是去讓人家更幸福這個過程
二是我透過這個fine tuning的過程
我希望我的
我的這些東西不要離shoppy value
太遠
那這個不要離shoppy value太遠的
這個特性他somehow可以去
因為這邊我完全沒有使用任何的
一些shoppy value
作為label去訓練
那後面我給少量的label
我是可以把他拉回shoppy value的正軌
讓整個
整個大模型在訓練的時候
不要離我們剛才所謂的shoppy value
給explanation這個framework太遠
讓我們可以
拿這個結果去
讓別人相信說
我們不是隨便去製造一個
important score的預測
我們是基於shoppy value的
這個大框架去做
prediction
所以這個模型其實有
幾個優點
第一個是他可以很快
因為他只要一個模型就可以應付所有的user
不像是以前一個模型
只能應付一個user
然後再來就是他提供了正樣本跟負樣本的選取
然後讓整個訓練的
過程中我們不需要
太大量的樣本
不需要太大量的label
然後也可以去訓練出對於所有user的
這個訓練結果
好那這個我就
就不講了我就跳過
可以看到右邊這個
右邊這個兩張圖就是
我們訓練的結果
這個訓練的結果就是可以看到說
這是一隻鳥嘛
那這個鳥的中間被框住
紅色就代表重要,藍色就代表不重要
所以我們就知道
這個image classification model
他其實上是在
為什麼會預測他是鳥
那就是因為他抓到了這個鳥的特徵
所以我們就可以去相信
這個image classification model是做對的
然後也可以去看說
我們這個explainer是trend好的
是trend對的
下面這個狗
最重要的這個特徵是臉
狗的這個臉
那我們就把我們的這個
explainer訓練結果
他其實就是說這個臉很重要
那我們把這個臉highlight起來
那就是最後的explanation
結果
好那有幾個
有幾個application
就是例如說
其實我前面大致上都提過
XAI在recommended system上面
的重要性
是因為可以針對幾個不同的
人
幾個不同的使用情境來說
例如說我對於客人來說
我可以讓他知道我拿什麼
怎麼逼選給你訓練
那為什麼你會做出這個結果
讓大家覺得
這個decision是對
然後更安心
對於這個recommended system更安心
那對於商人來說
那他可以透過這個
例如說我投放廣告
那你總要跟廣告商說
因為我
截取了使用者
使用者的A feature B feature
C feature D feature
然後導致
我推薦這個使用者廣告
所以商人可以透過這種
顯示的分析去做出
更好的例如說
因為我發現可能這個市場
某些使用者會在乎什麼
商品的特徵
我可以去對這些商品的特徵做調整
然後他也可以更信服
這個廣告的投放
那對於我們
這種工程師來說
提供這種
explanation的結果
他其實可以幫我們去
debug這個system
也可以讓我們去改善這個system
例如說我們如果抓錯了feature的話
我們要怎麼去調整
那再來就是health care
那就是例如說
我們對於病人來說
我們可以去說服病人說
相信這個診斷結果是沒錯
那對於醫生來說
可以去協助他做正確的結論
就是醫生憑什麼相信
這個prediction model
但你總要說服他說
因為我detect到什麼症狀
這個東西其實是
可以協助醫生
但我必須要說
他不能取代醫生的重要程度
所有的AI model
他都是在協助為本質
這個outlier
我覺得就先跳過
OK
好那最後的話
我來講幾個常見的
這個open source package
這些都是免費完全免費
不用付任何錢
第一個就是Captain
Captain其實是Facebook開發的一個
可解釋性的
開源的套件
那這個套件
使用的方式很簡單
就我們要使用這種
套件的時候
首先我們必須要有一個
我們要解釋的模型
如果沒有解釋模型
那這個XI就沒用了
所以我們
這個code的話就是
首先我們要有一個
我們要去解釋的目標模型
那這個integrated gradient
這個東西
其實就是
其中一個我們剛才說
一直在說的explainer
就是解釋模型的
機器學系模型
有點local
所以我們把這個
image classifier丟進這個
的explainer裡面
那他的
這個explainer的
output結果就會是
我去highlight一張圖片哪裡重要
所以右邊這個結果就是
這個東西 output出來的結果
這是一隻天鵝
那他predict是不是一隻天鵝
是因為他
看了
這些重要的feature
所以我們就可以透過這個輪廓
他真的是detect到這個天鵝
而不是detect到旁邊這些水
這就是過程
那不然就是
例如說我們有一些文字的模型
就是文字的classifier的結果
那是因為為什麼他會說
這句話是一個
positive的attitude
那是因為沒有
例如fantastic的字
那為什麼他是positive
因為他有best
那為什麼他不好
例如說negative
類似這種showcase
所以這整個package的
使用方式非常簡單
所以只要幾行
一行兩行三行
只要三行就可以搞定
explanation的process
那當然就是這裡面還有一些
模型調教的一些
trick或是一些經驗談
那這個就是要靠
大家平常常去用的一些
technique才會知道的一些
makeup
那另外一個package叫shop
那他shop他也是
用同樣的concept在
設計這個package
一樣他就是例如說
我需要有一個prediction model在這裡
然後把他寫成一個function
那我已經有一個prediction model在這裡
我已經有一個explanator在這裡
那這個explanator的目的
就是要去製造說
每一個圖片
或是每一個
每一句話或是每一個user
他重要的feature或重要的字
或是重要的pixel是什麼
那就是透過這個
這個explanator
來最後做一個製造
所以他output出來的shop event
就是屬於每一個pixel
重要的important score
ok所以整個過程就會比較像是這樣
我已經有一個黑盒模型
然後最後把他打開
然後知道每一個feature的重要程度是什麼
ok
所以整個feature會比較像是這樣
就例如說這是一艘船
所以他就會說
highlight船的這個
部件在這裡
那就讓人家知道說
這個船很重要
就可以了解到這個船的特徵
ok
那後面這些圖是
說今天錯誤的樣本
會發生什麼事
其實後面就不用管
因為我們只在
XAI這個模型
其實我們不能去導正
直接去導正
prediction model不好的結果
我們就是基於prediction model
給出來的結果做最忠實
最真誠的
這個解釋的結果
所以今天不能說
為什麼這個
他在fountain這個level下
他的explanation的結果好像很爛
那廢話因為他prediction
本來就是錯
因為你本來決策就錯了
所以我基於你做錯的決策
去做錯誤的explanation
其實是
XAI是不能管
我們只是基於你給我的
prediction model最忠實的決定
所以在看這個speed
這個boat
這個level才是比較
合理的
好
可能多拖了五分鐘
那今天演講就
大概到這裡
謝謝大家
有問題都可以提問
我想確認一下
因為現在沒有什麼設定上的問題
所以說
有問題的人
可以直接問一下
你好
我在聊天室有一個問題
好想要問
因為我主要是做tree-based model
就是提升資源區那種
然後因為
就是在很多那種數據競賽
或是實用目的可能比較少
那數據競賽
最後都會結合很多
模型然後塞在一起
那這種也可以用XAI這種方式嗎
如果是post hoc的話
你是用什麼
XGBoost或是LightGBM
例如說我最近就有研究
就是XGBoost加LightGBM
然後結果再把它拼進去
OK
我必須要說XGBoost那些
縱使它是樹模型
但是它中間有很多不同的layer
它其實是不能解釋的
所以一個可解
如果我們要說一個模型本身具有可解釋性
那它每一個component都要是清楚的
就縱使中間有一個
是不清楚的
那它整個就是不清楚的
XGBoost或是LightGBM
都必須要用post hoc的方式
所以用那些package
應該是可以去幫你做
解釋上面的動作
那如果我今天的模型的預測
是基於這四種模型的
預測結果去做平均的話
那我有辦法
一樣用你剛說的工具去回推
就是它綜合的
這樣子的性能
然後它的feature對哪一些
feature是哪一些重要的嗎
你四個模型
都是不同的模型嗎
就是其實我是用
不同的模型
然後去訓練
然後去預測出結果
只是說我最終的預測是這四個模型的輸出
再去做平均
ok 就單純的insampling這樣
ok
那我覺得是不太
不太能對四個模型的綜合
做評估
四個模型各做一個評估
對對
例如說我explanation score
把它加起來平均
對對對
ok 謝謝
ok 謝謝
謝謝你的問題
然後推薦系統中
遇到全新的商品
可能啊可能
例如說我就舉個最簡單的例子
就為什麼一些
假設Netflix它一進去
它叫你按說你喜歡什麼歌
你喜歡什麼藝人
你看過什麼影片
這些東西就是在建立你的個人的profile
那透過這些相同的profile
我可以去找到跟你
習性比較像的使用者
那我就把這些使用者的先前的習慣
當作你最一開始的推薦結果
那你當然之後
拿到這些推薦結果你會繼續再做你自己的使用嘛
那再把你自己的使用的
這些東西加上別人的
一些以前的習慣
如何來弄起來
然後做最後的
例如說你使用半年之後
你會發現結果越來越準
就是因為它推薦系統模型
你在經歷的過程
ok 希望有回答到你的問題
推薦的入門
ok 有
就是你可以上網搜
XAI什麼block
有一個寫的蠻好的
算是一本書吧
五個章節還是十個章節
然後我覺得他寫的蠻好的
然後包括現在這個XAI很多人在做
所以你去youtube上面
打例如introduction to
XAI可能也會跳出一些
不錯的影片
但我蠻推薦我剛才說的那個block
ok
特徵
ok 在這邊
其實都
ok
在我剛才講的
這個假設
它其實都假設每個特徵
是獨立的
那也有人在
研究說
那今天特徵相異的時候
我應該要怎麼去
去分析
那這些人就是比較著重在研究
feature interaction的這種XAI的方式
有有有
有些人在做這些方式
我記得上海
上海交通大學的
張全時老師的實驗室
有一些papers在研究
這方面的東西
如果你有興趣可以去看一下
我相信問出這麼specific的問題的人應該
對這方面應該是稍微有研究
或是其實是個expert
所以這樣講應該
希望能回答到你的問題
ok
ok
這種
這種全新商品的推薦模型
叫做
co-start
recommendation
ok
很多很多
很多很多方式
我宣傳一下
我之前發了一篇paper
叫做TPR
textual Preference
Ranking
這個東西它可以透過
不同的使用者特徵
然後來達到這種
你剛才所謂的這種
co-start recommendation
的推薦效果
對
這是我之前自己發的
跟我們商量一下
或者是你可以去找
找有一個
UCSD有一個老師叫Julia McCauley
他們實驗室也有很多人在做
這種co-start recommendation
然後你打這個關鍵字
應該可以跳出很多
ok
希望回答到你的問題
喂
不好意思我想要問一個問題
就是你剛剛在訓練
那個狗狗的圖
就是positive跟negative
的那個狗狗的圖
跟對比下面
下面一張
那個錯誤的圖
然後你把狗狗的那個眼睛拔掉了
那我在想說
你在那個錯誤的答案
是否就是
也許把那隻狗狗的眼睛貼上去
會不會是
會不會讓訓練的效果比較好
喔
我這邊只是給一個
例子
但其實像我真的在訓練的時候
例如說我是
假設他是一個32x32的圖片
那我就是給一個random mask
那這個random mask套上去的時候
他其實並不會
例如說並不會單純聚在這
他可能是散佈在一張圖片的不同地方
那例如說散佈在不同圖片的地方
我怎麼確認這個
被遮住過後的
這種
example是positive
我就把他丟到原本的prediction model去看說
他prediction的
例如說predict出來他是狗的分數
是不是掉很多
如果他幾乎沒掉
那我就可以肯定說我遮住的這些feature
所以我覺得你剛才說的是對的
例如說我今天把這個東西
遮到其他地方是有可能
幫助模型訓練是有可能
所以我在
這個是比較technical detail
我在訓練的時候其實是真的有做這件事情
所以像
像這種方式直接遮的話
那他有可能遮出來的東西
是例如說
我就直接遮掉很重要的東西是有可能
我覺得你的問題
是一個很好的問題
ok謝謝
好謝謝
喂你好
哈囉
欸sorry我的那個
鏡頭壞掉我把你打開
沒關係沒關係我們很free
對
那Alan跟我想問一下那個
第12頁那個sharp t value那邊
ok
就是我想確認我的那個觀念沒有錯
ok
我可以理解他是
我可以理解那difference那邊
是我把某些feature拿掉以後
然後他的difference如果很大的話
就代表那個feature是重要的
是這樣理解嗎
對沒錯
因為他改變了原本的prediction的pattern
那下一頁
那個第13頁就是
在您的paper中
就是有建立各個feature之間的
的關係
那個矩陣
我可以把它理解成是那種
的這種關係嗎
可以
那為什麼
為什麼他們相關係數
的高或低會跟
就是我預測出來他這個feature
對結果的重要性是有關的
之間的分別
跟
你還有抱歉打斷你
我太興奮了
ok好
那這個東西為什麼跟那個prediction有關
是因為我這個矩陣
你可以把它理解成一種相關係數
但他不是完全是correlation
他是基於我們model
predict出來的中間那些prediction
predict出來的那個model weight
我們把那個model weight抽出來
去建立這個
feature跟feature之間關係的
這種權重的
的矩陣
的這種權重圖
所以他這裡面每一個圖片的
每一個色塊的這種interaction
的高或低都是
based on
model prediction的結果
所以我們才可以拿這個東西
當作去減少這種
計算combination
的依據
他不單單只是
對
就是例如說我可能
換了另外一個input
然後可能兩個feature同時的可能增加
或減少或是之間的關聯
然後是based在這個weight的變化
去建那個matrix
例如說你可以
把
今天這個model裡面某一個
乘的weight拿出來
那我們就可以去透過這個weight
可以去看說今天這個x1跟這個x2
他是不是
例如說他interaction是不是比較大
或是今天x1跟x3是不是比較大
類似這樣
可以想像是變化的
一起變化的那種感覺嗎
就一起變
就例如說
今天這個x1加x3
變化嗎
應該說今天這個x1x3對於
模型的訓練
這邊先不牽扯到變化
我們單純看說
就單純把這個模型扒開
然後強制拿裡面的某一個
訓練的權重出來
拿出這個訓練的權重出來
我們可以去提取
就是說這個x1跟x3
加在一起的時候
對於這個prediction來說到底多重要
類似這樣
例如說他的x1加x3
那條weight很粗
例如說可能是0.8
那我們就把這個0.8
貼在這,類似這樣
先沒有牽扯到後面
那還有一個小問題想問
沒事沒事
因為我們現在可能討論說
單一某一個feature對結果的重要程度
到底是高還低
那會不會說有一種狀況是說
獨立看A他其實影響不大
獨立看B他影響也不大
但是然而只有A跟B
同時存在的時候
他對結果是影響很大的
那這種情況我要怎麼去解釋他
ok
我覺得這個問題很好
我覺得這個問題跟前面
就是
有一個人提到
就是今天這個feature
不是相依的這種情況
產生你剛才那個問題
就是今天就是A B不是獨立的
那A B加在一起
如果A B不是獨立
那A不重要B不重要
那A加B有可能重要
有可能產生這種情形
那這種方式要怎麼去判斷
例如說我們去設計這個
explanation model的時候
我們本來就要去考慮
就不能把這種feature獨立的這種
假設加在我們explanation model的設計
所以像是我今天這兩個word
就是不管是剛才那個
九宮格或是這個word
我們都假設每一個feature是獨立的
所以我們在算的時候我們都把每一個feature當作
算是一個
特徵來算
但如果你今天要考慮那種feature不同組合的話
那你考慮的層面可能
例如說你並不單只是
考慮A B C D
你可能還要考慮A B C D
A加B A加C A加D
當作A加D等於
假設等於F類似這種
就是你把A加B
當作一個新的feature來分析
這是我目前想要比較naive的
解法
我記得就是我剛才講的交通大學
上海交通大學的那個張宣世老師
他們的實驗室
有人在研究這種
這種feature interaction
組合的
貢獻度
對於最後prediction
就是explanation的performance的影響
是有人在做這個
但我的研究就比較
focus在基於
這種獨立feature的
對對對
了解
謝謝Alan
感謝你的問題
討論區裡面有一個人有問說
random mask影像的時候
mask size要怎麼選擇
mask size
你是說整張mask的
大小嗎
還是
比如說像狗的眼睛
那一個
比如說你今天
你今天mask如果是
比眼睛還要小的話
這個mask是有效的
ok
我們在做這種東西的時候
我們最小單位是一個pixel
對所以
我們應該不可能比pixel還要小
對所以例如說
如果mask size你再問說
要mask幾個的話
這種東西就是
例如說我們在random的時候完全不去
我在這個works裡面完全沒有去限制
但我知道有人是會去限制說
我不希望我遮掉的東西太多
就是為什麼不希望
遮掉的東西太多是因為
如果遮太多的話其實會反而導致
他那個random出來的結果不這麼positive
類似這樣
或是根據他的目的
而有不同的那個
選擇類似這樣
或是他可能只要挑幾個很重要的
feature出來
那我今天如果mask掉了
例如說很少部分
那其實就不是我們樂見的結果
類似這樣
但如果你是說mask的那個
那個dimension的話
那他必須要跟原本的image的dimension是一樣
不然我蓋不上去
對
OK
希望有回答到你的問題
這是安德嗎
OK
好 謝謝
感覺這個有點人為決定
因為我會覺得說像比如說你今天mask size
如果今天你的mask size
是
感覺上是有一種所謂的feature
的最小單位的這種概念
就是說因為你今天眼睛
feature的最小單位
你一定會超過一個pixel
因為你的
情況下面
我不知道 我只是覺得說
這個問題我也不知道怎麼回答
或者是說這個問題
有時候感覺上你這個size的選擇
是不是其實蠻主觀的
這個選擇
其實有人在做
就是例如說
也不是有人在做
其中一個方法就是例如說
他知道他指這些component
這些object都是比較大
例如眼睛是比較大
那他就例如說他的mask他就不是random
每一個pixel 他是random
每4x4 pixel
random每6x6 pixel
那透過這個方式去遮
也有人是這樣做
但我覺得這個東西就是
因為我們都已經不知道模型
他在乎的pixel可能是眼睛的某
右上加左下
類似這種
如果給這麼強的假設
我覺得模型訓練會比較不好
就是這種
explanation model會比較不好
當然你說的是
exactly right
真的是這樣子
其實有人詬病說
這種做出來的解釋
他其實就是大概
遮一下遮一下
例如說大概秀出
其實這邊眼睛也沒遮到
其實有可能就是
因為這種mask
一開始還沒有做好的原因
導致我最後給出來的東西
不這麼comprehensive
這是有可能
我覺得這個問題超好的
好
感覺上
聊天室裡面沒有新的問題
那我們再謝謝一次
裕能今天的演講
謝謝大家
謝謝
感謝大家的捧場
今天有幾個人
我比較好奇
因為我看不到
今天我這邊看到是有
最高有到32
才這麼多
我發現好多人
是怎樣間領域的關係
可是沒有人幫我的facebook按讚
開玩笑
必須說我之前
給過一次我本身是物理專業的
total五個人
可能這個東西
大家比較好奇
然後不知道怎麼入門吧
我也不知道
不過蠻開心有這個機會
分享一下所學
不然都在吃社會資源
總要貢獻一下
吃社會資源
對
你是高虹安嗎
沒有
我不是台大的
沒事
我全身而退
OK
感謝
突然在外查
補問一個問題
比如說你前一張投影片裡面
這個嗎
那個舉證的
這個嗎
對對對
舉證對角
所有這種x1 x1 x2 x2
你自動忽略是這個意思嗎
因為他們在這個
自己跟自己的關係
就是不討論
我們就不討論
因為可能在這個問題定義
定義裡面就沒有意義的
感覺
因為shopping value
自己跟自己還是自己
自己跟自己的組合還是自己
所以我們就不討論
那搶單兒自己跟自己
前提是高度相關
對
其實我不知道怎麼接觸分享
呵呵
那大概我想一下
那我可能先stop recording
OK
