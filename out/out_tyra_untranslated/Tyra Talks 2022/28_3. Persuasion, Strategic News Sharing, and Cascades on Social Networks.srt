1
00:00:00,000 --> 00:00:03,320
謝謝大家今天參與今天Project Teal的演講

2
00:00:03,320 --> 00:00:07,940
我是今天的主持人,我是在魯汶大學念PhD的高賢

3
00:00:07,940 --> 00:00:10,500
今天的講者是靜江

4
00:00:10,500 --> 00:00:16,900
靜江是MIT現在的博士候選人

5
00:00:16,900 --> 00:00:23,560
他在進入MIT之前,他在台大拿到了電機系的一個士學位

6
00:00:23,560 --> 00:00:29,560
那他現在的研究主要是專長在人類的決策

7
00:00:29,560 --> 00:00:35,560
還有人跟我們的社會社群媒體如何的互動

8
00:00:35,560 --> 00:00:40,060
尤其當我們的社群媒體充斥著很多真實的訊息

9
00:00:40,060 --> 00:00:42,060
還有假訊息的情況下

10
00:00:42,060 --> 00:00:47,560
人跟我們的社會媒體社會網絡是怎麼互動的

11
00:00:47,560 --> 00:00:51,560
尤其是我們的人類的行為如何隨著時間

12
00:00:51,560 --> 00:00:55,560
漸漸的演化成一個大型規模的社會現象

13
00:00:55,560 --> 00:00:57,560
這是他所關心的議題

14
00:00:57,560 --> 00:01:01,560
那他也會特別用不同的理論角度

15
00:01:01,560 --> 00:01:05,560
不同的研究方法去關心他剛剛所講的這個研究的興趣

16
00:01:05,560 --> 00:01:08,560
那今天我們邀請他來演講這個題目呢

17
00:01:08,560 --> 00:01:12,560
就是跟Persuasion,跟策略性的新聞的分享

18
00:01:12,560 --> 00:01:16,560
跟社會網絡之間的關心的一個題目

19
00:01:16,560 --> 00:01:18,560
那我就先,我就不多說了

20
00:01:18,560 --> 00:01:21,560
馬上就請我們靜嘉來進行今天的報告

21
00:01:21,560 --> 00:01:24,560
對,那靜嘉,the floor is yours

22
00:01:24,560 --> 00:01:25,560
OK, thank you

23
00:01:25,560 --> 00:01:26,560
謝謝高雄的介紹

24
00:01:26,560 --> 00:01:29,560
大家好,我現在在MIT念書

25
00:01:29,560 --> 00:01:32,560
然後這是我,應該是最後一年了

26
00:01:32,560 --> 00:01:34,560
今天我要介紹的一個是我的project叫

27
00:01:34,560 --> 00:01:39,560
Persuasion, New Sharing and Cascade on Social Networks

28
00:01:39,560 --> 00:01:43,560
然後這是一個join work with my post-doc

29
00:01:44,560 --> 00:01:46,560
還有我的advisor

30
00:01:46,560 --> 00:01:49,560
然後我現在其實在engineering school底下

31
00:01:49,560 --> 00:01:50,560
然後這個問題

32
00:01:50,560 --> 00:01:53,560
但是我其實做的東西是比較偏economics theory

33
00:01:53,560 --> 00:01:56,560
好,那我們開始吧

34
00:01:56,560 --> 00:01:59,560
首先就是現在呢

35
00:01:59,560 --> 00:02:02,560
自從這個社交媒體發明了之後

36
00:02:02,560 --> 00:02:06,560
就其實大家改變了這種消費新聞的行為

37
00:02:06,560 --> 00:02:09,560
像美國人大概五分之一的人就會說

38
00:02:09,560 --> 00:02:12,560
他們其實通常都是從社交媒體上面得到新聞

39
00:02:12,560 --> 00:02:14,560
但是這種社交媒體呢

40
00:02:14,560 --> 00:02:15,560
然後社交媒體呢

41
00:02:15,560 --> 00:02:18,560
它也會增進大家的這種peer-to-peer interaction

42
00:02:18,560 --> 00:02:19,560
就是人際之間的互動

43
00:02:19,560 --> 00:02:23,560
然後其實它也會加速這樣子訊息的流通

44
00:02:23,560 --> 00:02:25,560
但另外我們也發現說

45
00:02:25,560 --> 00:02:26,560
其實在social media上面

46
00:02:26,560 --> 00:02:29,560
其實你是有點像是被動的接收訊息

47
00:02:29,560 --> 00:02:32,560
因為通常都是你的同事

48
00:02:32,560 --> 00:02:35,560
你的朋友決定分享給你新聞

49
00:02:35,560 --> 00:02:38,560
或者是這些演算法去推送給你新聞

50
00:02:38,560 --> 00:02:40,560
其實是他們subjectively

51
00:02:40,560 --> 00:02:44,560
選擇這些新聞在你被動的去接收這樣的新聞

52
00:02:44,560 --> 00:02:45,560
另外還有social media

53
00:02:45,560 --> 00:02:48,560
他們主要還是想要賺錢嘛

54
00:02:48,560 --> 00:02:51,560
那其實他們在

55
00:02:51,560 --> 00:02:54,560
在分配這些廣告或什麼之類的時候

56
00:02:54,560 --> 00:02:56,560
其實無形中他們也可能是

57
00:02:56,560 --> 00:02:58,560
加速了misinformation的

58
00:02:58,560 --> 00:03:02,560
就amplify misinformation on the platform

59
00:03:02,560 --> 00:03:06,560
然後今天我要study的問題就是說

60
00:03:06,560 --> 00:03:08,560
就是大家分享新聞的動機是什麼

61
00:03:08,560 --> 00:03:10,560
就是說這個其實是大家

62
00:03:10,560 --> 00:03:12,560
現在學界還蠻關心的問題

63
00:03:12,560 --> 00:03:14,560
就是說到底大家分享新聞

64
00:03:14,560 --> 00:03:17,560
是為了要去告訴別人一些事情呢

65
00:03:17,560 --> 00:03:18,560
還是表達自己的觀點

66
00:03:18,560 --> 00:03:22,560
或者是說他其實是想要找同溫層

67
00:03:22,560 --> 00:03:23,560
或者是想要去

68
00:03:23,560 --> 00:03:26,560
就persuade別人

69
00:03:26,560 --> 00:03:27,560
那另外就是說

70
00:03:27,560 --> 00:03:29,560
在這樣的動機之下

71
00:03:29,560 --> 00:03:30,560
那什麼樣的新聞

72
00:03:30,560 --> 00:03:34,560
其實會在社交媒體上面傳得比較遠

73
00:03:34,560 --> 00:03:38,560
就包含說像可信度的這個影響呢

74
00:03:38,560 --> 00:03:41,560
還有以及他怎麼樣去

75
00:03:41,560 --> 00:03:43,560
以及這樣子

76
00:03:43,560 --> 00:03:45,560
應該說他

77
00:03:45,560 --> 00:03:48,560
跟社會的一些

78
00:03:48,560 --> 00:03:49,560
characteristic

79
00:03:49,560 --> 00:03:51,560
就是他的一些特性有沒有什麼關係

80
00:03:51,560 --> 00:03:54,560
然後像這個領域他現在就像

81
00:03:54,560 --> 00:03:57,560
我切入的角度會是economics的角度

82
00:03:57,560 --> 00:03:58,560
然後有一些psychologist

83
00:03:58,560 --> 00:04:00,560
他可能就會去study就是說

84
00:04:00,560 --> 00:04:02,560
他們有沒有一些behavioral bias

85
00:04:02,560 --> 00:04:04,560
那political science他們就比較focus

86
00:04:04,560 --> 00:04:05,560
在misinformation

87
00:04:05,560 --> 00:04:08,560
在政治上面的影響

88
00:04:08,560 --> 00:04:11,560
以及他們兩黨不同的新聞

89
00:04:11,560 --> 00:04:13,560
他們可能會比較願意去傳

90
00:04:13,560 --> 00:04:16,560
是不是自己跟自己黨相關的新聞呢

91
00:04:16,560 --> 00:04:17,560
或者是

92
00:04:17,560 --> 00:04:19,560
然後他們會怎麼樣去看待

93
00:04:19,560 --> 00:04:21,560
跟自己是同黨的新聞

94
00:04:21,560 --> 00:04:22,560
他是可信度比較高呢

95
00:04:22,560 --> 00:04:23,560
或可能比較少

96
00:04:23,560 --> 00:04:24,560
然後其實像這樣子的領域

97
00:04:24,560 --> 00:04:27,560
他們都還是還蠻仰賴CS的幫忙

98
00:04:27,560 --> 00:04:30,560
就是他們還是會做一些computational的事情

99
00:04:30,560 --> 00:04:33,560
那今天我的這個問題呢

100
00:04:33,560 --> 00:04:35,560
就是接下來的model

101
00:04:35,560 --> 00:04:36,560
都會based on一個question

102
00:04:36,560 --> 00:04:39,560
就是說如果大家在分享的新聞

103
00:04:39,560 --> 00:04:40,560
上面的動機

104
00:04:40,560 --> 00:04:43,560
其實是想要去讓你的follower

105
00:04:43,560 --> 00:04:44,560
跟你想的比較接近

106
00:04:44,560 --> 00:04:46,560
那麼會發生什麼事

107
00:04:46,560 --> 00:04:47,560
所以其實這個model

108
00:04:47,560 --> 00:04:49,560
算是一個what if的question

109
00:04:49,560 --> 00:04:51,560
就是說如果這是大家的動機

110
00:04:51,560 --> 00:04:52,560
大家想要去讓別人

111
00:04:52,560 --> 00:04:54,560
跟你想的比較類似的話

112
00:04:54,560 --> 00:04:56,560
那麼

113
00:04:56,560 --> 00:04:58,560
什麼樣的新聞你會去傳

114
00:04:58,560 --> 00:05:00,560
然後以及什麼樣的新聞

115
00:05:00,560 --> 00:05:02,560
可能在什麼樣的社會

116
00:05:02,560 --> 00:05:05,560
會傳得比較遠

117
00:05:05,560 --> 00:05:08,560
OK

118
00:05:08,560 --> 00:05:09,560
好

119
00:05:09,560 --> 00:05:11,560
那就我剛才講的

120
00:05:11,560 --> 00:05:13,560
就是我是用economics的角度

121
00:05:13,560 --> 00:05:14,560
切入這個問題

122
00:05:14,560 --> 00:05:16,560
所以接下來我會用

123
00:05:16,560 --> 00:05:20,560
就是所謂的賽局理論來講這個model

124
00:05:20,560 --> 00:05:21,560
然後另外就是我們這篇work

125
00:05:21,560 --> 00:05:23,560
已經submit到

126
00:05:23,560 --> 00:05:25,560
就是Journal of Economics Theory

127
00:05:25,560 --> 00:05:28,560
然後這邊有一個SSR的link

128
00:05:28,560 --> 00:05:30,560
如果大家感興趣那個Journal version

129
00:05:30,560 --> 00:05:34,560
想要看的話就歡迎掃碼一下

130
00:05:34,560 --> 00:05:36,560
首先我跟大家講一下這個model

131
00:05:36,560 --> 00:05:39,560
一些比較大的assumption

132
00:05:39,560 --> 00:05:40,560
當然我們會考慮一個society

133
00:05:40,560 --> 00:05:44,560
就是每一個人的這種觀念不一樣

134
00:05:44,560 --> 00:05:46,560
就大家不是同心協力

135
00:05:46,560 --> 00:05:49,560
都覺得就是偏左或偏右

136
00:05:49,560 --> 00:05:52,560
大家就是可以是有不同的觀點

137
00:05:52,560 --> 00:05:54,560
然後

138
00:05:54,560 --> 00:05:56,560
一個人他會希望說我的

139
00:05:56,560 --> 00:05:59,560
同儕跟我的這個想法比較類似

140
00:05:59,560 --> 00:06:02,560
然後另外是

141
00:06:02,560 --> 00:06:04,560
我們在這裡會用的persuasion這個詞

142
00:06:04,560 --> 00:06:06,560
代表的是說

143
00:06:06,560 --> 00:06:10,560
你給別人一個objective的information

144
00:06:10,560 --> 00:06:11,560
就是一個新的information

145
00:06:11,560 --> 00:06:14,560
去影響他的belief

146
00:06:14,560 --> 00:06:16,560
所以當我在講persuasion的話

147
00:06:16,560 --> 00:06:18,560
我的意思是這個意思

148
00:06:18,560 --> 00:06:21,560
另外在分享的時候

149
00:06:21,560 --> 00:06:23,560
會有一些成本出現

150
00:06:23,560 --> 00:06:26,560
就是說你可以把它想成是

151
00:06:26,560 --> 00:06:27,560
我平常一直在滑手機

152
00:06:27,560 --> 00:06:28,560
滑到一半的時候

153
00:06:28,560 --> 00:06:30,560
我突然覺得我得

154
00:06:30,560 --> 00:06:31,560
突然想要傳這個新聞

155
00:06:31,560 --> 00:06:33,560
你可以思考了一下

156
00:06:33,560 --> 00:06:34,560
你原本是一直在滑手機

157
00:06:34,560 --> 00:06:35,560
然後你突然得停下來

158
00:06:35,560 --> 00:06:38,560
就model它是有一個小小的成本在裡頭

159
00:06:38,560 --> 00:06:40,560
因為有這個小小的成本在

160
00:06:40,560 --> 00:06:41,560
所以其實是有一個

161
00:06:41,560 --> 00:06:44,560
在經濟上我們叫它

162
00:06:44,560 --> 00:06:46,560
策略性替代的行為

163
00:06:46,560 --> 00:06:47,560
strategic substitute

164
00:06:47,560 --> 00:06:48,560
就是說

165
00:06:48,560 --> 00:06:50,560
當別人會做這件事情的時候

166
00:06:50,560 --> 00:06:51,560
你就不想去做這件事情

167
00:06:51,560 --> 00:06:52,560
因為你覺得別人會幫你做

168
00:06:52,560 --> 00:06:54,560
所以這邊你為了要省那個成本

169
00:06:54,560 --> 00:06:55,560
你可能會覺得

170
00:06:55,560 --> 00:06:57,560
如果別人已經都傳了這個新聞的話

171
00:06:57,560 --> 00:06:59,560
那我就沒有傳的必要

172
00:06:59,560 --> 00:07:02,560
另外是最後會apply這個model

173
00:07:02,560 --> 00:07:03,560
到一個society

174
00:07:03,560 --> 00:07:05,560
就是有分化的情形

175
00:07:05,560 --> 00:07:06,560
兩黨分化的情形

176
00:07:06,560 --> 00:07:07,560
然後去看說

177
00:07:07,560 --> 00:07:10,560
在兩黨分化的這個社會底下

178
00:07:10,560 --> 00:07:13,560
什麼狀況會導致

179
00:07:13,560 --> 00:07:15,560
比較不準確的新聞

180
00:07:15,560 --> 00:07:21,560
會傳的比準確的新聞還要遠

181
00:07:21,560 --> 00:07:23,560
不好意思我可以問個問題嗎

182
00:07:23,560 --> 00:07:24,560
我可以打斷嗎

183
00:07:24,560 --> 00:07:25,560
沒事沒事

184
00:07:25,560 --> 00:07:27,560
我們應該還蠻確定的

185
00:07:27,560 --> 00:07:29,560
我想要問一下就是

186
00:07:29,560 --> 00:07:31,560
二跟三之間的

187
00:07:31,560 --> 00:07:32,560
算是關係吧

188
00:07:32,560 --> 00:07:34,560
因為如果說

189
00:07:34,560 --> 00:07:36,560
我在social media上面分享東西

190
00:07:36,560 --> 00:07:37,560
我希望

191
00:07:37,560 --> 00:07:39,560
就是我希望我在我的同溫層裡面

192
00:07:39,560 --> 00:07:42,560
那

193
00:07:42,560 --> 00:07:43,560
這時候我們講的persuasion

194
00:07:43,560 --> 00:07:45,560
好像跟平常是我們想像的persuasion

195
00:07:45,560 --> 00:07:46,560
很不一樣

196
00:07:46,560 --> 00:07:47,560
譬如說如果

197
00:07:47,560 --> 00:07:48,560
上面都是我的同溫層

198
00:07:48,560 --> 00:07:50,560
那我PO一個東西

199
00:07:50,560 --> 00:07:51,560
我的理解會是

200
00:07:51,560 --> 00:07:53,560
或者是我會希望他們就接受了

201
00:07:53,560 --> 00:07:56,560
就是沒有很多的persuasion

202
00:07:56,560 --> 00:07:58,560
那通常我們想像的persuasion是

203
00:07:58,560 --> 00:08:00,560
那些人可能原本是中立的

204
00:08:00,560 --> 00:08:02,560
或甚至跟我是不一樣的

205
00:08:02,560 --> 00:08:03,560
就是觀點

206
00:08:03,560 --> 00:08:04,560
不一樣的立場這樣

207
00:08:04,560 --> 00:08:06,560
然後我經過

208
00:08:06,560 --> 00:08:07,560
分享一些新的資訊

209
00:08:07,560 --> 00:08:08,560
來說服他們

210
00:08:08,560 --> 00:08:10,560
譬如說加入我的陣營這樣

211
00:08:10,560 --> 00:08:11,560
對那所以

212
00:08:11,560 --> 00:08:13,560
如果說

213
00:08:13,560 --> 00:08:15,560
你的model原本就是預設說

214
00:08:15,560 --> 00:08:16,560
大家都會希望

215
00:08:16,560 --> 00:08:17,560
待在自己的同溫層裡面

216
00:08:17,560 --> 00:08:19,560
那他persuasion的力道

217
00:08:19,560 --> 00:08:20,560
是不是就會比較

218
00:08:20,560 --> 00:08:22,560
相對會比較弱一點

219
00:08:22,560 --> 00:08:23,560
喔不好意思

220
00:08:23,560 --> 00:08:24,560
我可能剛才沒有解釋好

221
00:08:24,560 --> 00:08:25,560
這個第二點

222
00:08:25,560 --> 00:08:26,560
第二點他不是只是說

223
00:08:26,560 --> 00:08:27,560
你想在同溫層裡面

224
00:08:27,560 --> 00:08:28,560
是說

225
00:08:28,560 --> 00:08:30,560
我的動機是希望別人

226
00:08:30,560 --> 00:08:32,560
跟我的想法類似

227
00:08:32,560 --> 00:08:33,560
那個preference

228
00:08:33,560 --> 00:08:34,560
這個意思

229
00:08:34,560 --> 00:08:35,560
就是說你希望別人的想法

230
00:08:35,560 --> 00:08:36,560
跟你的

231
00:08:36,560 --> 00:08:37,560
別人做的事情

232
00:08:37,560 --> 00:08:39,560
跟你的想法是類似的

233
00:08:39,560 --> 00:08:41,560
他有那個persuasion的動機

234
00:08:41,560 --> 00:08:43,560
所以其實我是在解釋

235
00:08:43,560 --> 00:08:44,560
對

236
00:08:44,560 --> 00:08:45,560
OK

237
00:08:45,560 --> 00:08:46,560
謝謝

238
00:08:46,560 --> 00:08:47,560
可能是我剛剛沒聽清楚

239
00:08:47,560 --> 00:08:48,560
不好意思

240
00:08:48,560 --> 00:08:49,560
好

241
00:08:49,560 --> 00:08:50,560
這個這裡的

242
00:08:50,560 --> 00:08:52,560
persuasion的communication mechanism

243
00:08:52,560 --> 00:08:53,560
有在乎是一對一

244
00:08:53,560 --> 00:08:55,560
或是一對多的嗎

245
00:08:55,560 --> 00:08:56,560
喔那是個good question

246
00:08:56,560 --> 00:08:57,560
就是說

247
00:08:57,560 --> 00:08:59,560
這邊的persuasion是

248
00:08:59,560 --> 00:09:01,560
因為我們這個model

249
00:09:01,560 --> 00:09:02,560
比較像是在考慮

250
00:09:02,560 --> 00:09:03,560
twitter這種東西

251
00:09:03,560 --> 00:09:04,560
所以你就想說

252
00:09:04,560 --> 00:09:05,560
我一

253
00:09:05,560 --> 00:09:06,560
分享這個新聞之後

254
00:09:06,560 --> 00:09:07,560
就是等於是

255
00:09:07,560 --> 00:09:09,560
你所有的朋友都會看到

256
00:09:09,560 --> 00:09:11,560
對就是一個比較

257
00:09:11,560 --> 00:09:12,560
簡單的something

258
00:09:12,560 --> 00:09:13,560
就是說就是broadcast

259
00:09:13,560 --> 00:09:15,560
就是我一分享了

260
00:09:15,560 --> 00:09:16,560
那我就可以

261
00:09:16,560 --> 00:09:17,560
而且我知道

262
00:09:17,560 --> 00:09:19,560
我知道我所有的朋友都會看到

263
00:09:19,560 --> 00:09:20,560
就是比較

264
00:09:20,560 --> 00:09:22,560
所以才會有strategic substitute

265
00:09:22,560 --> 00:09:24,560
因為別人可以broadcast

266
00:09:24,560 --> 00:09:26,560
對對對

267
00:09:26,560 --> 00:09:27,560
了解

268
00:09:27,560 --> 00:09:28,560
是的是的

269
00:09:28,560 --> 00:09:29,560
yeah

270
00:09:29,560 --> 00:09:30,560
that's a good question

271
00:09:30,560 --> 00:09:31,560
對

272
00:09:31,560 --> 00:09:33,560
就是你沒有說你特別去選

273
00:09:33,560 --> 00:09:34,560
我要傳給誰

274
00:09:34,560 --> 00:09:35,560
就是我一

275
00:09:35,560 --> 00:09:36,560
我傳就只有決定說

276
00:09:36,560 --> 00:09:37,560
我要傳

277
00:09:37,560 --> 00:09:38,560
然後我知道我所有的follow

278
00:09:38,560 --> 00:09:39,560
我都會看到

279
00:09:39,560 --> 00:09:40,560
這樣

280
00:09:40,560 --> 00:09:42,560
OK

281
00:09:42,560 --> 00:09:43,560
嗯

282
00:09:43,560 --> 00:09:45,560
然後這邊是給大家一個

283
00:09:45,560 --> 00:09:46,560
就是broad picture

284
00:09:46,560 --> 00:09:47,560
about

285
00:09:47,560 --> 00:09:48,560
我們的這個主要的結果

286
00:09:48,560 --> 00:09:49,560
第一個就是說

287
00:09:49,560 --> 00:09:50,560
我們當然是刻劃這個

288
00:09:50,560 --> 00:09:51,560
equilibrium

289
00:09:51,560 --> 00:09:52,560
到底長什麼樣子

290
00:09:52,560 --> 00:09:53,560
嗯

291
00:09:53,560 --> 00:09:55,560
然後包含像我們identify

292
00:09:55,560 --> 00:09:56,560
呃

293
00:09:56,560 --> 00:09:58,560
大家在分享新聞的這些

294
00:09:58,560 --> 00:09:59,560
就是說你是based on

295
00:09:59,560 --> 00:10:00,560
這個persuasion的模式

296
00:10:00,560 --> 00:10:01,560
但其實

297
00:10:01,560 --> 00:10:03,560
在進一步把它decompose

298
00:10:03,560 --> 00:10:04,560
就是把它分析出來

299
00:10:04,560 --> 00:10:05,560
終於會發現說

300
00:10:05,560 --> 00:10:06,560
其實更極端的人

301
00:10:06,560 --> 00:10:07,560
他當然會想要

302
00:10:07,560 --> 00:10:09,560
更有那一種

303
00:10:09,560 --> 00:10:10,560
呃

304
00:10:10,560 --> 00:10:11,560
動力去分享

305
00:10:11,560 --> 00:10:13,560
而且當然是分享那些

306
00:10:13,560 --> 00:10:14,560
呃

307
00:10:14,560 --> 00:10:16,560
跟我想法是align

308
00:10:16,560 --> 00:10:17,560
就是說哦

309
00:10:17,560 --> 00:10:18,560
如果是偏右的極端的人

310
00:10:18,560 --> 00:10:19,560
那當然我會去

311
00:10:19,560 --> 00:10:21,560
想要去分享偏右的

312
00:10:21,560 --> 00:10:22,560
我不會去

313
00:10:22,560 --> 00:10:23,560
分享偏左的

314
00:10:23,560 --> 00:10:24,560
另外是

315
00:10:24,560 --> 00:10:25,560
呃

316
00:10:25,560 --> 00:10:26,560
persuasiveness of news

317
00:10:26,560 --> 00:10:28,560
就是說這個新聞

318
00:10:28,560 --> 00:10:29,560
我一旦傳了出去之後

319
00:10:29,560 --> 00:10:30,560
我可以改變

320
00:10:30,560 --> 00:10:32,560
多少人的

321
00:10:32,560 --> 00:10:33,560
belief

322
00:10:33,560 --> 00:10:34,560
就是說

323
00:10:34,560 --> 00:10:35,560
到底他成效如何

324
00:10:35,560 --> 00:10:36,560
假如我一直傳出去

325
00:10:36,560 --> 00:10:37,560
哎

326
00:10:37,560 --> 00:10:38,560
其實也沒有人會相信

327
00:10:38,560 --> 00:10:39,560
這個東西

328
00:10:39,560 --> 00:10:40,560
沒有人在改變他的想法的話

329
00:10:40,560 --> 00:10:41,560
那我幹嘛傳

330
00:10:41,560 --> 00:10:42,560
呃

331
00:10:42,560 --> 00:10:43,560
最後一個東西

332
00:10:43,560 --> 00:10:44,560
我幫你講

333
00:10:44,560 --> 00:10:45,560
就是說

334
00:10:45,560 --> 00:10:46,560
你在傳的時候

335
00:10:46,560 --> 00:10:48,560
你會去評估說

336
00:10:48,560 --> 00:10:49,560
哎

337
00:10:49,560 --> 00:10:50,560
到底

338
00:10:50,560 --> 00:10:51,560
嗯

339
00:10:51,560 --> 00:10:52,560
在均衡狀況底下

340
00:10:52,560 --> 00:10:53,560
到底有多少人在傳這個東西

341
00:10:53,560 --> 00:10:54,560
然後你會覺得說

342
00:10:54,560 --> 00:10:55,560
哦

343
00:10:55,560 --> 00:10:56,560
其實我的follower

344
00:10:56,560 --> 00:10:58,560
說不定很大的機率

345
00:10:58,560 --> 00:10:59,560
已經會聽到這個新聞

346
00:10:59,560 --> 00:11:00,560
我就不需要傳了

347
00:11:00,560 --> 00:11:01,560
對

348
00:11:01,560 --> 00:11:02,560
所以這是

349
00:11:02,560 --> 00:11:04,560
這主要的三個component

350
00:11:04,560 --> 00:11:05,560
呃

351
00:11:05,560 --> 00:11:06,560
可以再clarify

352
00:11:06,560 --> 00:11:07,560
均衡的概念嗎

353
00:11:07,560 --> 00:11:08,560
哦

354
00:11:08,560 --> 00:11:09,560
均衡就是說

355
00:11:09,560 --> 00:11:10,560
均衡

356
00:11:10,560 --> 00:11:11,560
哦

357
00:11:11,560 --> 00:11:12,560
對

358
00:11:12,560 --> 00:11:13,560
均衡這個概念就是說

359
00:11:13,560 --> 00:11:14,560
呃

360
00:11:14,560 --> 00:11:16,560
在這個model底下

361
00:11:16,560 --> 00:11:17,560
大家

362
00:11:17,560 --> 00:11:20,560
你去預測別人的行為在做什麼

363
00:11:20,560 --> 00:11:21,560
然後

364
00:11:21,560 --> 00:11:23,560
你預測那個行為之下

365
00:11:23,560 --> 00:11:24,560
你

366
00:11:24,560 --> 00:11:27,560
你會做一個選擇去

367
00:11:27,560 --> 00:11:28,560
在那個

368
00:11:28,560 --> 00:11:30,560
當你在預測別人的行為之下

369
00:11:30,560 --> 00:11:31,560
你會

370
00:11:31,560 --> 00:11:33,560
然後optimize你自己的行為嗎

371
00:11:33,560 --> 00:11:34,560
就是說

372
00:11:34,560 --> 00:11:36,560
我如果預期到大家都去超市

373
00:11:36,560 --> 00:11:37,560
把東西買光的話

374
00:11:37,560 --> 00:11:39,560
那我也會去超市

375
00:11:39,560 --> 00:11:40,560
買光這樣

376
00:11:40,560 --> 00:11:41,560
但是

377
00:11:41,560 --> 00:11:42,560
呃

378
00:11:42,560 --> 00:11:43,560
也跟著去超市買東西之類的

379
00:11:43,560 --> 00:11:44,560
所以

380
00:11:44,560 --> 00:11:45,560
均衡的話就是說

381
00:11:45,560 --> 00:11:47,560
在你預測別人的行為之下

382
00:11:47,560 --> 00:11:50,560
你就會做這樣子對你最好的行為

383
00:11:50,560 --> 00:11:51,560
但是你也同時知道說

384
00:11:51,560 --> 00:11:52,560
哦

385
00:11:52,560 --> 00:11:53,560
你做了這個最好的行為之後

386
00:11:53,560 --> 00:11:55,560
別人也在同時

387
00:11:55,560 --> 00:11:57,560
在設想你會做什麼樣的行為

388
00:11:57,560 --> 00:11:59,560
於是在這種交互的行為之下

389
00:11:59,560 --> 00:12:00,560
他達到一個均衡

390
00:12:00,560 --> 00:12:01,560
就是說

391
00:12:01,560 --> 00:12:02,560
呃

392
00:12:02,560 --> 00:12:03,560
這個情況下

393
00:12:03,560 --> 00:12:05,560
沒有人會改變他的行為

394
00:12:05,560 --> 00:12:06,560
所以

395
00:12:06,560 --> 00:12:07,560
你知道漸漸的就是

396
00:12:07,560 --> 00:12:09,560
達到一個平衡

397
00:12:09,560 --> 00:12:10,560
也不是說漸漸的達到平衡

398
00:12:10,560 --> 00:12:11,560
就是他在平衡狀態下

399
00:12:11,560 --> 00:12:12,560
就是

400
00:12:12,560 --> 00:12:13,560
你做這個行為

401
00:12:13,560 --> 00:12:14,560
我現在做這個行為

402
00:12:14,560 --> 00:12:15,560
就是對你最好的

403
00:12:15,560 --> 00:12:16,560
呃

404
00:12:16,560 --> 00:12:17,560
response

405
00:12:17,560 --> 00:12:18,560
然後同時是

406
00:12:18,560 --> 00:12:19,560
所以我不會去改變我的行為

407
00:12:19,560 --> 00:12:21,560
然後你也不會再去改變你的行為

408
00:12:21,560 --> 00:12:23,560
所以就達到一個均衡

409
00:12:23,560 --> 00:12:27,560
所以這個game theory下面的一個assumption

410
00:12:27,560 --> 00:12:28,560
不是assumption就是說

411
00:12:28,560 --> 00:12:29,560
呃

412
00:12:29,560 --> 00:12:30,560
不能講assumption

413
00:12:30,560 --> 00:12:31,560
就是說equilibrium

414
00:12:31,560 --> 00:12:32,560
就是說game

415
00:12:32,560 --> 00:12:33,560
就是說

416
00:12:33,560 --> 00:12:34,560
game就

417
00:12:34,560 --> 00:12:35,560
對在economy裡頭

418
00:12:35,560 --> 00:12:36,560
game

419
00:12:36,560 --> 00:12:38,560
其實就像你在玩

420
00:12:38,560 --> 00:12:40,560
撲克牌遊戲一樣

421
00:12:40,560 --> 00:12:41,560
我會覺得

422
00:12:41,560 --> 00:12:43,560
喔別人在玩什麼招數

423
00:12:43,560 --> 00:12:45,560
然後我要想辦法去抵抗他

424
00:12:45,560 --> 00:12:46,560
我會做

425
00:12:46,560 --> 00:12:47,560
假設他做了這件事情

426
00:12:47,560 --> 00:12:48,560
那我就是

427
00:12:48,560 --> 00:12:50,560
挑我最好的那個

428
00:12:50,560 --> 00:12:51,560
呃

429
00:12:51,560 --> 00:12:52,560
牌組之類的

430
00:12:52,560 --> 00:12:53,560
但是你也同時在想說

431
00:12:53,560 --> 00:12:55,560
喔別人也知道

432
00:12:55,560 --> 00:12:57,560
別人也在猜你的行為是什麼

433
00:12:57,560 --> 00:12:58,560
然後變成是說均衡底下

434
00:12:58,560 --> 00:12:59,560
你可能會覺得

435
00:12:59,560 --> 00:13:00,560
喔我應該怎麼樣出手

436
00:13:00,560 --> 00:13:01,560
然後大家都不會

437
00:13:01,560 --> 00:13:02,560
呃

438
00:13:02,560 --> 00:13:04,560
有任何動機去改變他的行為

439
00:13:04,560 --> 00:13:05,560
就他達到了一個

440
00:13:05,560 --> 00:13:07,560
互相抵抗的一個

441
00:13:07,560 --> 00:13:08,560
呃

442
00:13:08,560 --> 00:13:09,560
呃

443
00:13:09,560 --> 00:13:10,560
平衡

444
00:13:10,560 --> 00:13:11,560
所以那個時候

445
00:13:11,560 --> 00:13:13,560
那個狀態我們就都要加均衡

446
00:13:13,560 --> 00:13:14,560
了解

447
00:13:14,560 --> 00:13:15,560
那我想問一下

448
00:13:15,560 --> 00:13:16,560
就是說

449
00:13:16,560 --> 00:13:17,560
比如說prison dilemma

450
00:13:17,560 --> 00:13:19,560
這樣算是會有均衡的state嗎

451
00:13:19,560 --> 00:13:20,560
那個就是均衡

452
00:13:20,560 --> 00:13:21,560
prison dilemma就是均衡

453
00:13:21,560 --> 00:13:22,560
那個就是均衡

454
00:13:22,560 --> 00:13:23,560
那個就是因為

455
00:13:23,560 --> 00:13:25,560
我知道說如果我不講了

456
00:13:25,560 --> 00:13:26,560
我不講

457
00:13:26,560 --> 00:13:27,560
但是對方可能

458
00:13:27,560 --> 00:13:28,560
就是說

459
00:13:28,560 --> 00:13:30,560
我知道對方講或不講

460
00:13:30,560 --> 00:13:31,560
我

461
00:13:31,560 --> 00:13:33,560
講了都對我比較好

462
00:13:33,560 --> 00:13:34,560
所以我就會選擇去講

463
00:13:34,560 --> 00:13:36,560
然後對方也知道說啊那個

464
00:13:36,560 --> 00:13:37,560
不管講或不講

465
00:13:37,560 --> 00:13:38,560
他還是講比較

466
00:13:38,560 --> 00:13:39,560
就就變兩個人都講

467
00:13:39,560 --> 00:13:41,560
然後兩個人都被懲罰

468
00:13:41,560 --> 00:13:42,560
這個意思

469
00:13:42,560 --> 00:13:43,560
了解謝謝

470
00:13:43,560 --> 00:13:44,560
好

471
00:13:44,560 --> 00:13:45,560
呃

472
00:13:45,560 --> 00:13:46,560
不好意思我想要

473
00:13:46,560 --> 00:13:48,560
就是再問一下均衡的這個問題

474
00:13:48,560 --> 00:13:49,560
嗯那

475
00:13:49,560 --> 00:13:50,560
呃

476
00:13:50,560 --> 00:13:52,560
就是考慮均衡的時候

477
00:13:52,560 --> 00:13:54,560
會討論個體差異嗎

478
00:13:54,560 --> 00:13:56,560
因為每一個人

479
00:13:56,560 --> 00:13:57,560
呃

480
00:13:57,560 --> 00:13:58,560
之前會長得不太一樣

481
00:13:58,560 --> 00:13:59,560
呃

482
00:13:59,560 --> 00:14:01,560
他們有很呃

483
00:14:01,560 --> 00:14:02,560
一看裡面

484
00:14:02,560 --> 00:14:04,560
他們會把這個叫做type

485
00:14:04,560 --> 00:14:05,560
就是你是什麼樣的類型

486
00:14:05,560 --> 00:14:06,560
然後另外是說

487
00:14:06,560 --> 00:14:08,560
他一定可以考慮個體差異的

488
00:14:08,560 --> 00:14:09,560
只是說

489
00:14:09,560 --> 00:14:10,560
一看裡面

490
00:14:10,560 --> 00:14:12,560
常常會問的一個問題是說

491
00:14:12,560 --> 00:14:14,560
你到底知不知道這個個體差異

492
00:14:14,560 --> 00:14:15,560
就是說

493
00:14:15,560 --> 00:14:17,560
假設我今天到底知不知道你

494
00:14:17,560 --> 00:14:18,560
我如果已經知道

495
00:14:18,560 --> 00:14:19,560
這樣我們就知道

496
00:14:19,560 --> 00:14:20,560
川普他就是支持

497
00:14:20,560 --> 00:14:21,560
共和黨

498
00:14:21,560 --> 00:14:22,560
但是你現在走在路上

499
00:14:22,560 --> 00:14:23,560
你看到某一個人

500
00:14:23,560 --> 00:14:25,560
你也不知道他到底是支持共和黨

501
00:14:25,560 --> 00:14:26,560
或者是

502
00:14:26,560 --> 00:14:27,560
民主黨對吧

503
00:14:27,560 --> 00:14:28,560
就等於是說

504
00:14:28,560 --> 00:14:30,560
他們會用這種方式

505
00:14:30,560 --> 00:14:32,560
呃

506
00:14:32,560 --> 00:14:33,560
嗯

507
00:14:33,560 --> 00:14:34,560
他們是會考慮個體差異

508
00:14:34,560 --> 00:14:36,560
然後他們會去看哦

509
00:14:36,560 --> 00:14:37,560
我不知道你是

510
00:14:37,560 --> 00:14:38,560
主要是我不知道

511
00:14:38,560 --> 00:14:40,560
你到底是什麼樣子類型的人

512
00:14:40,560 --> 00:14:42,560
或者是我知道你是什麼樣類型的人

513
00:14:42,560 --> 00:14:44,560
會導致有不一樣的strategy

514
00:14:44,560 --> 00:14:45,560
對他的

515
00:14:45,560 --> 00:14:46,560
他會對對

516
00:14:46,560 --> 00:14:48,560
然後像我們這個model裡面

517
00:14:48,560 --> 00:14:49,560
我們會假設

518
00:14:49,560 --> 00:14:51,560
大家都不知道別人在想什麼

519
00:14:51,560 --> 00:14:52,560
但是

520
00:14:52,560 --> 00:14:54,560
我知道

521
00:14:54,560 --> 00:14:55,560
我們會make assumption就是說

522
00:14:55,560 --> 00:14:56,560
哦我知道

523
00:14:56,560 --> 00:14:58,560
整體社會大致上在想什麼

524
00:14:58,560 --> 00:14:59,560
但是我不知道

525
00:14:59,560 --> 00:15:01,560
specifically like

526
00:15:01,560 --> 00:15:02,560
高賢他在想什麼

527
00:15:02,560 --> 00:15:03,560
我不知道

528
00:15:03,560 --> 00:15:04,560
但是我知道

529
00:15:04,560 --> 00:15:05,560
哦我們可能這一群人

530
00:15:05,560 --> 00:15:07,560
普遍都是支持

531
00:15:07,560 --> 00:15:08,560
呀

532
00:15:08,560 --> 00:15:09,560
臺灣怎麼樣

533
00:15:09,560 --> 00:15:10,560
或者是ok

534
00:15:10,560 --> 00:15:11,560
你懂我意思

535
00:15:11,560 --> 00:15:12,560
對

536
00:15:12,560 --> 00:15:13,560
可是這個這個assumption

537
00:15:13,560 --> 00:15:15,560
好像跟我們平常是在

538
00:15:15,560 --> 00:15:18,560
社交網站上面的

539
00:15:18,560 --> 00:15:21,560
互動有一點點不太一樣

540
00:15:21,560 --> 00:15:23,560
就是我我通常就是譬如說

541
00:15:23,560 --> 00:15:25,560
不管在推特上或者在

542
00:15:25,560 --> 00:15:26,560
呃facebook上

543
00:15:26,560 --> 00:15:28,560
我通常會大概知道我的follower

544
00:15:28,560 --> 00:15:29,560
大概知道了

545
00:15:29,560 --> 00:15:31,560
但不是百分百知道

546
00:15:31,560 --> 00:15:33,560
但我大概可以猜測出來他們在

547
00:15:33,560 --> 00:15:34,560
想什麼

548
00:15:34,560 --> 00:15:35,560
那對

549
00:15:35,560 --> 00:15:36,560
哦

550
00:15:36,560 --> 00:15:37,560
不好意思你先說

551
00:15:37,560 --> 00:15:38,560
不好意思

552
00:15:38,560 --> 00:15:38,560


553
00:15:38,560 --> 00:15:40,560
所以我們在這邊其實沒有

554
00:15:40,560 --> 00:15:41,560
呃到時候你看到那個商品

555
00:15:41,560 --> 00:15:43,560
就是說我大概只是知道說

556
00:15:43,560 --> 00:15:44,560
我的那個distribution是什麼

557
00:15:44,560 --> 00:15:46,560
但我不知道確切

558
00:15:46,560 --> 00:15:47,560
這樣

559
00:15:47,560 --> 00:15:48,560
ok

560
00:15:48,560 --> 00:15:49,560
對ok

561
00:15:49,560 --> 00:15:51,560
我會問這個那個

562
00:15:51,560 --> 00:15:53,560
不好意思你先說

563
00:15:53,560 --> 00:15:54,560
沒有你講講講

564
00:15:54,560 --> 00:15:55,560
我有聽過

565
00:15:55,560 --> 00:15:58,560
我會問這個那個equilibrium的問題

566
00:15:58,560 --> 00:15:59,560
其實是因為

567
00:15:59,560 --> 00:16:01,560
第一個是就算是假設

568
00:16:01,560 --> 00:16:02,560
所有人都是rational的

569
00:16:02,560 --> 00:16:04,560
他們的optimization可能會不太一樣

570
00:16:04,560 --> 00:16:05,560
就是每個人要的東西

571
00:16:05,560 --> 00:16:06,560
感覺會有點不太一樣

572
00:16:06,560 --> 00:16:08,560
那這個感覺你剛剛的

573
00:16:08,560 --> 00:16:10,560
呃回答有

574
00:16:10,560 --> 00:16:12,560
就是有cover到這一部分

575
00:16:12,560 --> 00:16:14,560
那可是因為你還有另一個

576
00:16:14,560 --> 00:16:15,560
假設是

577
00:16:15,560 --> 00:16:17,560
就是在你這個study裡面

578
00:16:17,560 --> 00:16:18,560
是人是rational的嗎

579
00:16:18,560 --> 00:16:20,560
那如果你把人不是rational的

580
00:16:20,560 --> 00:16:21,560
考慮進來

581
00:16:21,560 --> 00:16:22,560
等於就又多了一個dimension

582
00:16:22,560 --> 00:16:24,560
所以有些人他如果不是

583
00:16:24,560 --> 00:16:25,560
呃不是rational的

584
00:16:25,560 --> 00:16:27,560
那他的equilibrium又會長得

585
00:16:27,560 --> 00:16:29,560
跟rational那些人不太一樣

586
00:16:29,560 --> 00:16:31,560
所以好像就會變得非常複雜

587
00:16:31,560 --> 00:16:32,560
呃對

588
00:16:32,560 --> 00:16:35,560
但是目前就是先考

589
00:16:35,560 --> 00:16:37,560
因為我們try to就

590
00:16:37,560 --> 00:16:38,560
其實有點mess

591
00:16:38,560 --> 00:16:39,560
我們的主要的message

592
00:16:39,560 --> 00:16:40,560
就跟大家講是說

593
00:16:40,560 --> 00:16:41,560
哦

594
00:16:41,560 --> 00:16:42,560
因為其實你說那rational的話

595
00:16:42,560 --> 00:16:44,560
那其實很多事情都很好解釋

596
00:16:44,560 --> 00:16:45,560
因為那rational就直接

597
00:16:45,560 --> 00:16:46,560
那rational就說

598
00:16:46,560 --> 00:16:48,560
啊他就想這樣做

599
00:16:48,560 --> 00:16:49,560
但是然後等於是說

600
00:16:49,560 --> 00:16:50,560
我們的model會變成是說

601
00:16:50,560 --> 00:16:51,560
哦

602
00:16:51,560 --> 00:16:52,560
even

603
00:16:52,560 --> 00:16:53,560
你覺得大家都是rational的

604
00:16:53,560 --> 00:16:55,560
狀況底下

605
00:16:55,560 --> 00:16:56,560
這種比較不準確的新聞

606
00:16:56,560 --> 00:16:57,560
他還是願意傳

607
00:16:57,560 --> 00:16:58,560
然後會發生什麼事情

608
00:16:58,560 --> 00:16:59,560
所以就變了

609
00:16:59,560 --> 00:17:01,560
這是主要這種rational model

610
00:17:01,560 --> 00:17:02,560
在try to

611
00:17:02,560 --> 00:17:03,560
不然因為

612
00:17:03,560 --> 00:17:04,560
如果你說

613
00:17:04,560 --> 00:17:05,560
大家都是irrational的話

614
00:17:05,560 --> 00:17:06,560
就會變成說

615
00:17:06,560 --> 00:17:08,560
哦其實

616
00:17:08,560 --> 00:17:10,560
那就直接把它歸類為irrational

617
00:17:10,560 --> 00:17:11,560
然後但是

618
00:17:11,560 --> 00:17:12,560
其實ecom他們有時候

619
00:17:12,560 --> 00:17:13,560
也會考慮bounded rational

620
00:17:13,560 --> 00:17:14,560
就是說他有

621
00:17:14,560 --> 00:17:15,560
有一些部分他是rational

622
00:17:15,560 --> 00:17:16,560
有一些部分irrational

623
00:17:16,560 --> 00:17:18,560
或者是像這種

624
00:17:18,560 --> 00:17:19,560
另外還有一種model

625
00:17:19,560 --> 00:17:20,560
就是像你剛才講

626
00:17:20,560 --> 00:17:21,560
就一部

627
00:17:21,560 --> 00:17:22,560
有可能大部分的人是rational

628
00:17:22,560 --> 00:17:23,560
但有少部分人irrational

629
00:17:23,560 --> 00:17:24,560
那又會發生什麼事

630
00:17:24,560 --> 00:17:25,560
但我們這個model

631
00:17:25,560 --> 00:17:26,560
就是考慮

632
00:17:26,560 --> 00:17:27,560
就是

633
00:17:27,560 --> 00:17:28,560
大家都是rational

634
00:17:28,560 --> 00:17:29,560
比較簡單一點

635
00:17:29,560 --> 00:17:30,560
了解了解

636
00:17:30,560 --> 00:17:31,560
我先講一下

637
00:17:31,560 --> 00:17:32,560
反正我們這邊人也不多

638
00:17:32,560 --> 00:17:33,560
我們大概

639
00:17:33,560 --> 00:17:34,560
等一下可能就會是這個

640
00:17:34,560 --> 00:17:35,560
非常互動的討論形式

641
00:17:35,560 --> 00:17:36,560
因為我自己是psychologist

642
00:17:36,560 --> 00:17:38,560
所以我是做experimental

643
00:17:38,560 --> 00:17:39,560
我是experimental psychologist

644
00:17:39,560 --> 00:17:40,560
所以

645
00:17:40,560 --> 00:17:42,560
experimental psychologist的

646
00:17:42,560 --> 00:17:43,560
那個

647
00:17:43,560 --> 00:17:44,560
出發點

648
00:17:44,560 --> 00:17:45,560
就會比較像是

649
00:17:45,560 --> 00:17:46,560
人by default

650
00:17:46,560 --> 00:17:47,560
就是irrational

651
00:17:47,560 --> 00:17:48,560
但我們當然不會說

652
00:17:48,560 --> 00:17:49,560
啊他就irrational

653
00:17:49,560 --> 00:17:50,560
所以我們就不用研究他的行為

654
00:17:50,560 --> 00:17:51,560
反正他就irrational

655
00:17:51,560 --> 00:17:52,560
但我們會去嘗試解釋

656
00:17:52,560 --> 00:17:54,560
他irrational背後的原因是什麼

657
00:17:54,560 --> 00:17:55,560
譬如說他可能會受到

658
00:17:55,560 --> 00:17:56,560
什麼東西

659
00:17:56,560 --> 00:17:57,560
呃

660
00:17:57,560 --> 00:17:58,560
影響啊

661
00:17:58,560 --> 00:17:59,560
或者他會

662
00:17:59,560 --> 00:18:00,560
在什麼情況

663
00:18:00,560 --> 00:18:01,560
他會用heuristics

664
00:18:01,560 --> 00:18:02,560
什麼情況他會

665
00:18:02,560 --> 00:18:03,560
就是reason

666
00:18:03,560 --> 00:18:04,560
你去做一個決定

667
00:18:04,560 --> 00:18:05,560
等等這樣

668
00:18:05,560 --> 00:18:06,560
對但是就是

669
00:18:06,560 --> 00:18:07,560
我先把我的那個

670
00:18:07,560 --> 00:18:08,560
stance講出來

671
00:18:08,560 --> 00:18:09,560
OKOKOK

672
00:18:09,560 --> 00:18:11,560
為什麼我會問這個問題

673
00:18:11,560 --> 00:18:12,560
好好好

674
00:18:12,560 --> 00:18:13,560
我知道我知道你的意思

675
00:18:13,560 --> 00:18:14,560
好

676
00:18:14,560 --> 00:18:15,560
等於是說

677
00:18:15,560 --> 00:18:16,560
主要是在這裡

678
00:18:16,560 --> 00:18:17,560
就沒有什麼behavioral bias

679
00:18:17,560 --> 00:18:19,560
就純粹大家就是

680
00:18:19,560 --> 00:18:20,560
你可以把大家

681
00:18:20,560 --> 00:18:21,560
就想像他就是

682
00:18:21,560 --> 00:18:22,560
很精密的儀器

683
00:18:22,560 --> 00:18:23,560
他可以去算很多東西

684
00:18:23,560 --> 00:18:24,560
然後

685
00:18:24,560 --> 00:18:25,560
even在這樣的情況下

686
00:18:25,560 --> 00:18:27,560
大家可能還是想要穿

687
00:18:27,560 --> 00:18:28,560
不準確的行為

688
00:18:28,560 --> 00:18:29,560
這樣

689
00:18:29,560 --> 00:18:30,560
OKOK

690
00:18:30,560 --> 00:18:31,560
OK

691
00:18:31,560 --> 00:18:32,560
好

692
00:18:32,560 --> 00:18:33,560
嗯

693
00:18:33,560 --> 00:18:34,560
好另外還有一個

694
00:18:34,560 --> 00:18:35,560
就是主要

695
00:18:35,560 --> 00:18:36,560
其實這是更主要的結果

696
00:18:36,560 --> 00:18:37,560
就是說

697
00:18:37,560 --> 00:18:38,560
在這種底下

698
00:18:38,560 --> 00:18:39,560
呃

699
00:18:39,560 --> 00:18:40,560
我們去探討說這種

700
00:18:40,560 --> 00:18:42,560
傳播的這個尺度

701
00:18:42,560 --> 00:18:43,560
以及新聞可信度

702
00:18:43,560 --> 00:18:44,560
以及

703
00:18:44,560 --> 00:18:45,560
就是這種

704
00:18:45,560 --> 00:18:46,560
呃

705
00:18:46,560 --> 00:18:48,560
這個社會的一些特性的關聯

706
00:18:48,560 --> 00:18:50,560
然後我們就會發現說

707
00:18:50,560 --> 00:18:51,560
哦其實

708
00:18:51,560 --> 00:18:52,560
就是比較

709
00:18:52,560 --> 00:18:53,560
呃

710
00:18:53,560 --> 00:18:54,560
沒有那麼可信的新聞

711
00:18:54,560 --> 00:18:55,560
其實就有點不準確的新聞呢

712
00:18:55,560 --> 00:18:56,560
他反而可以

713
00:18:56,560 --> 00:18:57,560
傳的更遠

714
00:18:57,560 --> 00:18:58,560
如果說

715
00:18:58,560 --> 00:18:59,560
這一個

716
00:18:59,560 --> 00:19:01,560
network的

717
00:19:01,560 --> 00:19:03,560
連結是非常緊密

718
00:19:03,560 --> 00:19:04,560
就是說

719
00:19:04,560 --> 00:19:05,560
每個人都好多個follow的話

720
00:19:05,560 --> 00:19:06,560
那其實這可能會

721
00:19:06,560 --> 00:19:07,560
促進那一些

722
00:19:07,560 --> 00:19:09,560
比較不準確的新聞

723
00:19:09,560 --> 00:19:10,560
去傳播的更遠

724
00:19:10,560 --> 00:19:11,560
而另外還有說

725
00:19:11,560 --> 00:19:14,560
就是如果這個社會的分化

726
00:19:14,560 --> 00:19:15,560
更嚴重的話

727
00:19:15,560 --> 00:19:18,560
那麼他可能把這個threshold

728
00:19:18,560 --> 00:19:19,560
在這個

729
00:19:19,560 --> 00:19:20,560
呃

730
00:19:20,560 --> 00:19:22,560
網路緊密度的這個threshold降低

731
00:19:22,560 --> 00:19:23,560
就是說

732
00:19:23,560 --> 00:19:25,560
我剛才是講說

733
00:19:25,560 --> 00:19:26,560
如果這個社會

734
00:19:26,560 --> 00:19:27,560
如果越緊密的話呢

735
00:19:27,560 --> 00:19:28,560
呃

736
00:19:28,560 --> 00:19:29,560
這種比較不準確的新聞

737
00:19:29,560 --> 00:19:30,560
就可以傳的越遠

738
00:19:30,560 --> 00:19:32,560
那如果你又讓這個

739
00:19:32,560 --> 00:19:34,560
極化的程度更高的話

740
00:19:34,560 --> 00:19:35,560
反而那些

741
00:19:35,560 --> 00:19:36,560
就是甚至連那些

742
00:19:36,560 --> 00:19:38,560
可能不是很緊密連接的網路呢

743
00:19:38,560 --> 00:19:39,560
也能使得

744
00:19:39,560 --> 00:19:40,560
呃

745
00:19:40,560 --> 00:19:41,560
不準確的新聞

746
00:19:41,560 --> 00:19:42,560
去傳播的更遠

747
00:19:42,560 --> 00:19:43,560
然後另外

748
00:19:43,560 --> 00:19:44,560
大家可能會想說

749
00:19:44,560 --> 00:19:45,560
哎那萬一我去增加

750
00:19:45,560 --> 00:19:47,560
兩檔之內

751
00:19:47,560 --> 00:19:48,560
呃

752
00:19:48,560 --> 00:19:49,560
就是兩檔之內的

753
00:19:49,560 --> 00:19:52,560
他們自己各自檔內的這種diversity的話

754
00:19:52,560 --> 00:19:54,560
其實會不會去

755
00:19:54,560 --> 00:19:55,560
幫助抑制這種

756
00:19:55,560 --> 00:19:56,560
less credible news

757
00:19:56,560 --> 00:19:58,560
to create a larger cascade

758
00:19:58,560 --> 00:19:59,560
但是我們的

759
00:19:59,560 --> 00:20:00,560
呃

760
00:20:00,560 --> 00:20:01,560
結果會發現說

761
00:20:01,560 --> 00:20:02,560
其實也不一定

762
00:20:02,560 --> 00:20:03,560
就是說

763
00:20:03,560 --> 00:20:04,560
在某些情況下

764
00:20:04,560 --> 00:20:05,560
的確可以去降低

765
00:20:05,560 --> 00:20:06,560
呃

766
00:20:06,560 --> 00:20:07,560
呃

767
00:20:07,560 --> 00:20:08,560
這個

768
00:20:08,560 --> 00:20:10,560
比較不準確新聞的

769
00:20:10,560 --> 00:20:11,560
傳播的

770
00:20:11,560 --> 00:20:12,560
呃

771
00:20:12,560 --> 00:20:13,560
速度

772
00:20:13,560 --> 00:20:14,560
不能講速度

773
00:20:14,560 --> 00:20:15,560
就是怕遲度

774
00:20:15,560 --> 00:20:16,560
但是其實有一些情況下

775
00:20:16,560 --> 00:20:17,560
他是會發現說

776
00:20:17,560 --> 00:20:18,560
其實你增加這種diversity

777
00:20:18,560 --> 00:20:19,560
反而會增加

778
00:20:19,560 --> 00:20:20,560
呃

779
00:20:20,560 --> 00:20:22,560
不準確的新聞去傳播的更遠

780
00:20:22,560 --> 00:20:23,560
所以這主要是我們的

781
00:20:23,560 --> 00:20:24,560
呃

782
00:20:24,560 --> 00:20:25,560
結論

783
00:20:25,560 --> 00:20:26,560
好

784
00:20:26,560 --> 00:20:27,560
那我就

785
00:20:27,560 --> 00:20:28,560
繼續了

786
00:20:28,560 --> 00:20:29,560
就好

787
00:20:29,560 --> 00:20:30,560
那我現在這是主要我們的結論

788
00:20:30,560 --> 00:20:31,560
然後我就稍微講

789
00:20:31,560 --> 00:20:32,560
開始講一下我們的model

790
00:20:32,560 --> 00:20:34,560
讓大家知道說這個model到底

791
00:20:34,560 --> 00:20:36,560
大致上在做些什麼

792
00:20:36,560 --> 00:20:38,560
首先我們這個model有三個stage

793
00:20:38,560 --> 00:20:40,560
最一開始的stage0呢

794
00:20:40,560 --> 00:20:42,560
就是對一些fundamental的事情

795
00:20:42,560 --> 00:20:43,560
就這邊有一個social network

796
00:20:43,560 --> 00:20:44,560
然後大家每個人

797
00:20:44,560 --> 00:20:46,560
他自己對某件事情

798
00:20:46,560 --> 00:20:47,560
假設就投

799
00:20:47,560 --> 00:20:49,560
到底要投川普還是拜登的這種想法

800
00:20:49,560 --> 00:20:51,560
然後新聞就有

801
00:20:51,560 --> 00:20:53,560
有一個有一個新聞就是被

802
00:20:53,560 --> 00:20:55,560
realize

803
00:20:55,560 --> 00:20:57,560
然後他可能就給某一些人看

804
00:20:57,560 --> 00:20:58,560
呃

805
00:20:58,560 --> 00:20:59,560
然後第二個stage呢

806
00:20:59,560 --> 00:21:00,560
就是你知道

807
00:21:00,560 --> 00:21:01,560
在投票之前

808
00:21:01,560 --> 00:21:02,560
大家的互動的stage

809
00:21:02,560 --> 00:21:03,560
然後大家可以

810
00:21:03,560 --> 00:21:04,560
大家可能如果看到新聞呢

811
00:21:04,560 --> 00:21:05,560
他就決定

812
00:21:05,560 --> 00:21:06,560
到底要不要傳

813
00:21:06,560 --> 00:21:07,560
如果傳的話呢

814
00:21:07,560 --> 00:21:08,560
我

815
00:21:08,560 --> 00:21:09,560
我這邊有一個decision

816
00:21:09,560 --> 00:21:10,560
variable

817
00:21:10,560 --> 00:21:11,560
SI meaning that

818
00:21:11,560 --> 00:21:12,560
1的話就是

819
00:21:12,560 --> 00:21:13,560
喔我傳了這個新聞

820
00:21:13,560 --> 00:21:15,560
0的話就沒有

821
00:21:15,560 --> 00:21:16,560
然後最後呢

822
00:21:16,560 --> 00:21:18,560
在這個interaction stage

823
00:21:18,560 --> 00:21:19,560
結束之後

824
00:21:19,560 --> 00:21:20,560
最後大家就決定要不要vote

825
00:21:20,560 --> 00:21:22,560
然後AI

826
00:21:22,560 --> 00:21:23,560
就是那個action

827
00:21:23,560 --> 00:21:24,560
就是說喔我到底要投

828
00:21:24,560 --> 00:21:26,560
假設負1就是偏左

829
00:21:26,560 --> 00:21:27,560
正1就是偏右

830
00:21:27,560 --> 00:21:28,560
就其實就是一個binary variable

831
00:21:28,560 --> 00:21:30,560
然後你choose 1

832
00:21:30,560 --> 00:21:31,560
嗯

833
00:21:31,560 --> 00:21:33,560
然後在這個model裡頭呢

834
00:21:33,560 --> 00:21:34,560
因為呃

835
00:21:34,560 --> 00:21:35,560
有一些technical assumption

836
00:21:35,560 --> 00:21:37,560
就我是假設

837
00:21:37,560 --> 00:21:39,560
你知道這個agent是非常多

838
00:21:39,560 --> 00:21:40,560
多到爆炸

839
00:21:40,560 --> 00:21:42,560
就是無窮多個

840
00:21:42,560 --> 00:21:43,560
好不好

841
00:21:43,560 --> 00:21:44,560
就是你可以想像這個population非常的大

842
00:21:44,560 --> 00:21:45,560
但是他是用

843
00:21:45,560 --> 00:21:47,560
0到1之間的這種時數系來

844
00:21:47,560 --> 00:21:49,560
index每一個人

845
00:21:49,560 --> 00:21:51,560
所以總共

846
00:21:51,560 --> 00:21:53,560
不可數的無窮多個人

847
00:21:53,560 --> 00:21:55,560
在這個社會裡頭

848
00:21:55,560 --> 00:21:57,560
然後他是一個connected

849
00:21:57,560 --> 00:21:59,560
by this social network

850
00:21:59,560 --> 00:22:01,560
然後這個social network是directed

851
00:22:01,560 --> 00:22:03,560
就是你知道他是一個follower的這種型態

852
00:22:03,560 --> 00:22:05,560
假設我

853
00:22:05,560 --> 00:22:07,560
有一個箭頭指向高顯的話

854
00:22:07,560 --> 00:22:09,560
就代表我follow了高顯

855
00:22:09,560 --> 00:22:11,560
在這個social network裡頭

856
00:22:11,560 --> 00:22:13,560
然後

857
00:22:13,560 --> 00:22:15,560
這個social network怎麼形成的呢

858
00:22:15,560 --> 00:22:17,560
就因為我們也不是說

859
00:22:17,560 --> 00:22:19,560
真的去apply data到這裡

860
00:22:19,560 --> 00:22:21,560
然後為了要讓我們的analysis比較

861
00:22:21,560 --> 00:22:22,560
tractable

862
00:22:22,560 --> 00:22:23,560
我們會用一個呃

863
00:22:23,560 --> 00:22:25,560
大家傳統大家都會用的

864
00:22:25,560 --> 00:22:27,560
random network model

865
00:22:27,560 --> 00:22:29,560
就是說你這個network是randomly

866
00:22:29,560 --> 00:22:31,560
製造出來的

867
00:22:31,560 --> 00:22:33,560
符合了某些特性之後的條件之下

868
00:22:33,560 --> 00:22:35,560
random製造出來的

869
00:22:35,560 --> 00:22:37,560
嗯

870
00:22:37,560 --> 00:22:39,560
然後我在這裡頭呢

871
00:22:39,560 --> 00:22:41,560
主要有一個參數是

872
00:22:41,560 --> 00:22:43,560
告訴大家說這個connectivity lambda

873
00:22:43,560 --> 00:22:45,560
connectivity lambda代表的是

874
00:22:45,560 --> 00:22:47,560
平均每一個人

875
00:22:47,560 --> 00:22:49,560
他follow了多少人

876
00:22:49,560 --> 00:22:51,560
呃不是說平均每一個人

877
00:22:51,560 --> 00:22:53,560
他有多少個followers

878
00:22:53,560 --> 00:22:55,560
就大家在這個

879
00:22:55,560 --> 00:22:57,560
model裡面大概記得這個字型就好

880
00:22:57,560 --> 00:22:59,560
就好

881
00:22:59,560 --> 00:23:01,560
這個variable

882
00:23:01,560 --> 00:23:03,560
那當然這個新聞一開始給

883
00:23:03,560 --> 00:23:05,560
很少的人去聽到然後他們決定要不要

884
00:23:05,560 --> 00:23:07,560
傳然後到最後達到

885
00:23:07,560 --> 00:23:09,560
steady state就是你知道他們一直傳

886
00:23:09,560 --> 00:23:11,560
總是會converge

887
00:23:11,560 --> 00:23:13,560
嗯我會

888
00:23:13,560 --> 00:23:15,560
叫最後那個

889
00:23:15,560 --> 00:23:17,560
你說這個社會裡頭有多少比例的人

890
00:23:17,560 --> 00:23:19,560
他到底最後

891
00:23:19,560 --> 00:23:21,560
有聽到這個新聞

892
00:23:21,560 --> 00:23:23,560
那我就把這個size

893
00:23:23,560 --> 00:23:25,560
這個大小叫做Q

894
00:23:25,560 --> 00:23:27,560
你看全部總共是1嘛

895
00:23:27,560 --> 00:23:29,560
那假設有80%的人

896
00:23:29,560 --> 00:23:31,560
那Q就是0.8聽到的

897
00:23:31,560 --> 00:23:33,560
這個新聞然後這個就是我們

898
00:23:33,560 --> 00:23:35,560
接下來會探討的一個主要的variable

899
00:23:35,560 --> 00:23:37,560
嗯

900
00:23:37,560 --> 00:23:39,560
可以嗎到目前為止

901
00:23:39,560 --> 00:23:41,560
Q算是dependent variable

902
00:23:41,560 --> 00:23:43,560
就是你的target metric

903
00:23:43,560 --> 00:23:45,560
對對對它是一個我等一下會講它

904
00:23:45,560 --> 00:23:47,560
對它是一個variable它不是一個我外

905
00:23:47,560 --> 00:23:49,560
你知道它鐵定就是你知道這個系統

906
00:23:49,560 --> 00:23:51,560
它自己產生出來的話你會觀察到

907
00:23:51,560 --> 00:23:53,560
喔有一個Q

908
00:23:53,560 --> 00:23:55,560
然後lambda是我給定的

909
00:23:55,560 --> 00:23:57,560
對我應該對不起我應該

910
00:23:57,560 --> 00:23:59,560
make it more specific

911
00:23:59,560 --> 00:24:01,560
就是lambda是我一開始說喔

912
00:24:01,560 --> 00:24:03,560
我就考慮這個network裡頭

913
00:24:03,560 --> 00:24:05,560
平均每個人有10個follower

914
00:24:05,560 --> 00:24:07,560
就設定為10然後去generate

915
00:24:07,560 --> 00:24:09,560
這個dependent network model

916
00:24:09,560 --> 00:24:11,560
但是Q是根據他們

917
00:24:11,560 --> 00:24:13,560
的strategy去做出來

918
00:24:13,560 --> 00:24:15,560
然後會產生出來的一個數字

919
00:24:15,560 --> 00:24:17,560
ok

920
00:24:17,560 --> 00:24:19,560
嗯

921
00:24:19,560 --> 00:24:21,560
然後我剛才講說

922
00:24:21,560 --> 00:24:23,560
每個人其實都有他自己的

923
00:24:23,560 --> 00:24:25,560
一個想法嘛那其實我們在這個

924
00:24:25,560 --> 00:24:27,560
模型裡頭因為一些utility function

925
00:24:27,560 --> 00:24:29,560
我們在定義的過程當中

926
00:24:29,560 --> 00:24:31,560
其實我們只需要去注意每個人

927
00:24:31,560 --> 00:24:33,560
他對這件事情的

928
00:24:33,560 --> 00:24:35,560
期望值的想法

929
00:24:35,560 --> 00:24:37,560
也就是說我今天問你說

930
00:24:37,560 --> 00:24:39,560
你覺得

931
00:24:39,560 --> 00:24:41,560
這個通膨率是多少

932
00:24:41,560 --> 00:24:43,560
你可能覺得說我覺得平均應該就0.5吧

933
00:24:43,560 --> 00:24:45,560
就這樣所以我們只需要care這個

934
00:24:45,560 --> 00:24:47,560
這個平均值就好所以我叫

935
00:24:47,560 --> 00:24:49,560
然後

936
00:24:49,560 --> 00:24:51,560
每個人都其實看不到別人的

937
00:24:51,560 --> 00:24:53,560
平均值就是說我其實

938
00:24:53,560 --> 00:24:55,560
也不知道高顯對這件事情的看法

939
00:24:55,560 --> 00:24:57,560
但是我大概知道說這整個社會裡頭

940
00:24:57,560 --> 00:24:59,560
呃

941
00:24:59,560 --> 00:25:01,560
呃這個分佈是怎麼樣可能

942
00:25:01,560 --> 00:25:03,560
偏向說今年這個通膨率

943
00:25:03,560 --> 00:25:05,560
是偏高大家可能都落在

944
00:25:05,560 --> 00:25:07,560
這個0.7附近

945
00:25:07,560 --> 00:25:09,560
就是一個function cdf function

946
00:25:09,560 --> 00:25:11,560
然後我就假設說大概有一半

947
00:25:11,560 --> 00:25:13,560
的人覺得是小於0一半的人覺得

948
00:25:13,560 --> 00:25:15,560
大於0這樣這只是一個

949
00:25:15,560 --> 00:25:17,560
呃就simplify一些

950
00:25:17,560 --> 00:25:19,560
notation的方式而已

951
00:25:19,560 --> 00:25:21,560
呃另外

952
00:25:21,560 --> 00:25:23,560
這個事件呢在一開始他就generate

953
00:25:23,560 --> 00:25:25,560
這個news就他產生一個

954
00:25:25,560 --> 00:25:27,560
news然後他是一個時數

955
00:25:27,560 --> 00:25:29,560
然後另外他有credibility data

956
00:25:29,560 --> 00:25:31,560
就是道義之間的一個時數

957
00:25:31,560 --> 00:25:33,560
這個credibility

958
00:25:33,560 --> 00:25:35,560
呢其實本質上

959
00:25:35,560 --> 00:25:37,560
的意思是說

960
00:25:37,560 --> 00:25:39,560
當你看到新聞之後

961
00:25:39,560 --> 00:25:41,560
你會對這個

962
00:25:41,560 --> 00:25:43,560
新聞有多大的sensitivity

963
00:25:43,560 --> 00:25:45,560
就是說我會很相信他的話

964
00:25:45,560 --> 00:25:47,560
那我當然就完全的

965
00:25:47,560 --> 00:25:49,560
把我的belief都改成是這個news

966
00:25:49,560 --> 00:25:51,560
但如果你完全不相信他的話

967
00:25:51,560 --> 00:25:53,560
你可能就你就會覺得說

968
00:25:53,560 --> 00:25:55,560
我就把他當成是garbage

969
00:25:55,560 --> 00:25:57,560
message我就直接還是preserve

970
00:25:57,560 --> 00:25:59,560
my expectation

971
00:25:59,560 --> 00:26:01,560
然後在這裡呢

972
00:26:01,560 --> 00:26:03,560
beta等於1呢我就叫他叫做

973
00:26:03,560 --> 00:26:05,560
fully credible的case

974
00:26:05,560 --> 00:26:07,560
因為我們之後我們主要想要探討的是說

975
00:26:07,560 --> 00:26:09,560
如果fully credible news是這樣子

976
00:26:09,560 --> 00:26:11,560
那如果比fully credible再小一點

977
00:26:11,560 --> 00:26:13,560
把他的credibility

978
00:26:13,560 --> 00:26:15,560
降低的話會不會有更大的

979
00:26:15,560 --> 00:26:17,560
news cascade

980
00:26:17,560 --> 00:26:19,560
不好意思

981
00:26:19,560 --> 00:26:21,560
我可以問個問題嗎

982
00:26:21,560 --> 00:26:23,560
credibility在這邊

983
00:26:23,560 --> 00:26:25,560
就聽你剛剛的解釋聽起來有點像是

984
00:26:25,560 --> 00:26:27,560
那個新聞有多

985
00:26:27,560 --> 00:26:29,560
influential你的定義是

986
00:26:29,560 --> 00:26:31,560
應該

987
00:26:31,560 --> 00:26:33,560
其實他可以被

988
00:26:33,560 --> 00:26:35,560
對其實你可以就是想像

989
00:26:35,560 --> 00:26:37,560
其實他有點像是一個我覺得有點

990
00:26:37,560 --> 00:26:39,560
在這邊其實有點terminology的問題

991
00:26:39,560 --> 00:26:41,560
就是說可能我講credibility有一些人會覺得

992
00:26:41,560 --> 00:26:43,560
他是有一個subjective的

993
00:26:43,560 --> 00:26:45,560
觀念在裡頭但是

994
00:26:45,560 --> 00:26:47,560
這裡的話我們就是假定說你可以

995
00:26:47,560 --> 00:26:49,560
當你收到新聞的時候你知道說

996
00:26:49,560 --> 00:26:51,560
這是CNN的news然後我們大家都

997
00:26:51,560 --> 00:26:53,560
同意CNN的可信度

998
00:26:53,560 --> 00:26:55,560
在那個在什麼level

999
00:26:55,560 --> 00:26:57,560
這樣子

1000
00:26:57,560 --> 00:26:59,560
是prior belief

1001
00:26:59,560 --> 00:27:01,560
但是可信度

1002
00:27:01,560 --> 00:27:03,560
這是homogeneous

1003
00:27:03,560 --> 00:27:05,560
對對這是一個common knowledge

1004
00:27:05,560 --> 00:27:07,560
就是如果大家收到新聞就ok

1005
00:27:07,560 --> 00:27:09,560
這個就是一個非常可信的新聞

1006
00:27:09,560 --> 00:27:11,560
沒有大家沒有我沒有讓他

1007
00:27:11,560 --> 00:27:13,560
變成heterogeneous

1008
00:27:13,560 --> 00:27:15,560
ok

1009
00:27:15,560 --> 00:27:17,560
就我剛聽你的

1010
00:27:17,560 --> 00:27:19,560
解釋方法聽起來比較像是

1011
00:27:19,560 --> 00:27:21,560
他是主觀

1012
00:27:21,560 --> 00:27:23,560
衡量的一個東西就如果

1013
00:27:23,560 --> 00:27:25,560
我現在收到一個東西然後我覺得他的可信度

1014
00:27:25,560 --> 00:27:27,560
很高

1015
00:27:27,560 --> 00:27:29,560
那他的credibility就很高在你這個model

1016
00:27:29,560 --> 00:27:31,560
裡面可是

1017
00:27:31,560 --> 00:27:33,560
就是感覺credibility

1018
00:27:33,560 --> 00:27:35,560
可以用比較客觀的方法

1019
00:27:35,560 --> 00:27:37,560
來量就是說如果你把像你

1020
00:27:37,560 --> 00:27:39,560
剛講的譬如說如果是CNN你把某一些新聞

1021
00:27:39,560 --> 00:27:41,560
然後列出來

1022
00:27:41,560 --> 00:27:43,560
然後我們可能可以

1023
00:27:43,560 --> 00:27:45,560
通過

1024
00:27:45,560 --> 00:27:47,560
不知道可能還是要靠

1025
00:27:47,560 --> 00:27:49,560
某一種rating之類的然後

1026
00:27:49,560 --> 00:27:51,560
用非常多人的rating然後來排名說

1027
00:27:51,560 --> 00:27:53,560
那如果我們來看客觀來講

1028
00:27:53,560 --> 00:27:55,560
哪一個不用rating

1029
00:27:55,560 --> 00:27:57,560
我們來看譬如說每一個新聞的outlet他產出

1030
00:27:57,560 --> 00:27:59,560
正確的新聞跟

1031
00:27:59,560 --> 00:28:01,560
不正確的新聞的比例是怎樣

1032
00:28:01,560 --> 00:28:03,560
那如果他正確的比例很高他credibility就很高

1033
00:28:03,560 --> 00:28:05,560
那就不用靠任何的人工的rating

1034
00:28:05,560 --> 00:28:07,560
但是聽起來你們是

1035
00:28:07,560 --> 00:28:09,560
你在這邊的credibility定義是

1036
00:28:09,560 --> 00:28:11,560
就是基本上

1037
00:28:11,560 --> 00:28:13,560
how influential it is

1038
00:28:13,560 --> 00:28:15,560
其實就他

1039
00:28:15,560 --> 00:28:17,560
其實如果你用一些

1040
00:28:17,560 --> 00:28:19,560
像一些什麼Gaussian的belief去看

1041
00:28:19,560 --> 00:28:21,560
就是你可以其實有一些數學

1042
00:28:21,560 --> 00:28:23,560
的東西去解釋他你可以說是

1043
00:28:23,560 --> 00:28:25,560
這個news可能是一個signal

1044
00:28:25,560 --> 00:28:27,560
然後你說這個beta其實

1045
00:28:27,560 --> 00:28:29,560
就是一個

1046
00:28:29,560 --> 00:28:31,560
signal多準確

1047
00:28:31,560 --> 00:28:33,560
如果他越準確的話其實就變成是說

1048
00:28:33,560 --> 00:28:35,560
我越相信這個news

1049
00:28:35,560 --> 00:28:37,560
所以那個準確的意思是說

1050
00:28:37,560 --> 00:28:39,560
他更接近那個人原本的prior嗎

1051
00:28:39,560 --> 00:28:41,560
不是那個準確是

1052
00:28:41,560 --> 00:28:43,560
你知道這個東西

1053
00:28:43,560 --> 00:28:45,560
signal裡面沒有noise

1054
00:28:45,560 --> 00:28:47,560
然後我知道這個東西觀察到的就是那個東西

1055
00:28:47,560 --> 00:28:49,560
沒有任何的

1056
00:28:49,560 --> 00:28:51,560
觀察上的誤差

1057
00:28:51,560 --> 00:28:53,560
我覺得

1058
00:28:53,560 --> 00:28:55,560
這應該就是我的問題的

1059
00:28:55,560 --> 00:28:57,560
根源就是

1060
00:28:57,560 --> 00:28:59,560
我觀察到一個東西沒有noise的時候

1061
00:28:59,560 --> 00:29:01,560
我會假定那個東西

1062
00:29:01,560 --> 00:29:03,560
我其實不知道那個東西是真的還是假的

1063
00:29:03,560 --> 00:29:05,560
所以有兩個approach的方法

1064
00:29:05,560 --> 00:29:07,560
第一個就是

1065
00:29:07,560 --> 00:29:09,560
我感覺這個東西沒有noise

1066
00:29:09,560 --> 00:29:11,560
然後第二個是

1067
00:29:11,560 --> 00:29:13,560
因為那個東西客觀上

1068
00:29:13,560 --> 00:29:15,560
就譬如說我們有某一種評量

1069
00:29:15,560 --> 00:29:17,560
他是一個從一個很好的outlet出來的

1070
00:29:17,560 --> 00:29:19,560
所以我猜

1071
00:29:19,560 --> 00:29:21,560
就是因為那個原因所以他沒有noise

1072
00:29:21,560 --> 00:29:23,560
所以我比較知道你在講的是哪一種

1073
00:29:23,560 --> 00:29:25,560
我在講

1074
00:29:25,560 --> 00:29:27,560
是我們大家都知道

1075
00:29:27,560 --> 00:29:29,560
這是一個observation然後他沒有noise

1076
00:29:29,560 --> 00:29:31,560
ok

1077
00:29:31,560 --> 00:29:33,560
對observation on the state

1078
00:29:33,560 --> 00:29:35,560
然後他沒有noise

1079
00:29:35,560 --> 00:29:37,560
所以等於是我就看到那個state

1080
00:29:37,560 --> 00:29:39,560
等於說那個牌就翻出來

1081
00:29:39,560 --> 00:29:41,560
我們大家全部都親眼看到他是什麼意思

1082
00:29:41,560 --> 00:29:43,560
就是說

1083
00:29:43,560 --> 00:29:45,560
反正這邊我講credibility就是說

1084
00:29:45,560 --> 00:29:47,560
其實就是說

1085
00:29:47,560 --> 00:29:49,560
大家都當他看到這個新聞的時候

1086
00:29:49,560 --> 00:29:51,560
大家都agree

1087
00:29:51,560 --> 00:29:53,560
這樣子的新聞他的準確度在哪裡

1088
00:29:53,560 --> 00:29:55,560
然後我說的準確度是說他跟

1089
00:29:55,560 --> 00:29:57,560
跟那個

1090
00:29:57,560 --> 00:29:59,560
真正你想觀察的那個東西的

1091
00:29:59,560 --> 00:30:01,560
誤差

1092
00:30:01,560 --> 00:30:03,560
就是

1093
00:30:03,560 --> 00:30:05,560
越接近那個

1094
00:30:05,560 --> 00:30:07,560
如果你跟我講說這個signal很準確的話

1095
00:30:07,560 --> 00:30:09,560
那我的意思就是說

1096
00:30:09,560 --> 00:30:11,560
我完全的反映了那一個我想觀察的東西

1097
00:30:11,560 --> 00:30:13,560
如果你說這個signal不準確的話

1098
00:30:13,560 --> 00:30:15,560
那他可能

1099
00:30:15,560 --> 00:30:17,560
somehow有點noise然後可能看到家務

1100
00:30:17,560 --> 00:30:19,560
或監務的這種東西

1101
00:30:19,560 --> 00:30:21,560
那我把它換成那個

1102
00:30:21,560 --> 00:30:23,560
真實世界的角度來看

1103
00:30:23,560 --> 00:30:25,560
換句話說如果今天有一個左派的人他看左派的媒體

1104
00:30:25,560 --> 00:30:27,560
跟右派的人看右派的媒體

1105
00:30:27,560 --> 00:30:29,560
對他們來講

1106
00:30:29,560 --> 00:30:31,560
那個情況的credibility都是maximum

1107
00:30:33,560 --> 00:30:35,560
我們這個model

1108
00:30:35,560 --> 00:30:37,560
我們這個model裡面就是假設說

1109
00:30:37,560 --> 00:30:39,560
大家都agree

1110
00:30:39,560 --> 00:30:41,560
這一個我們大家都

1111
00:30:41,560 --> 00:30:43,560
我也知道你在

1112
00:30:43,560 --> 00:30:45,560
當然你可以說現實生活中

1113
00:30:45,560 --> 00:30:47,560
democrats如果他看到CNN的news

1114
00:30:47,560 --> 00:30:49,560
決定說這個我相信他

1115
00:30:49,560 --> 00:30:51,560
然後可能那個共和黨人看到CNN

1116
00:30:51,560 --> 00:30:53,560
就說啊什麼東西啊就我不相信

1117
00:30:53,560 --> 00:30:55,560
但是

1118
00:30:55,560 --> 00:30:57,560
我這個model是假設說

1119
00:30:57,560 --> 00:30:59,560
不管怎麼樣的人

1120
00:30:59,560 --> 00:31:01,560
他如果看到這個新聞之後我們大家都相信

1121
00:31:01,560 --> 00:31:03,560
他的準確度在那個程度

1122
00:31:03,560 --> 00:31:05,560
就是說我們是一個假定的狀態

1123
00:31:05,560 --> 00:31:07,560
就是假定

1124
00:31:07,560 --> 00:31:09,560
我大家都agree

1125
00:31:09,560 --> 00:31:11,560
CNN的這個準確程度在0.9

1126
00:31:11,560 --> 00:31:13,560
OK

1127
00:31:13,560 --> 00:31:15,560
OK

1128
00:31:15,560 --> 00:31:17,560
你想的那個問題

1129
00:31:17,560 --> 00:31:19,560
其實我的另外一個paper有在study

1130
00:31:19,560 --> 00:31:21,560
就是說

1131
00:31:21,560 --> 00:31:23,560
現在是白邊

1132
00:31:23,560 --> 00:31:25,560
不好意思

1133
00:31:25,560 --> 00:31:27,560
就是說

1134
00:31:27,560 --> 00:31:29,560
我們現在的狀況是

1135
00:31:29,560 --> 00:31:31,560
我另外一個paper有看

1136
00:31:31,560 --> 00:31:33,560
有在study這樣的問題

1137
00:31:33,560 --> 00:31:35,560
就是說我們先假定

1138
00:31:35,560 --> 00:31:37,560
大家都agreeCNN就是0.9

1139
00:31:37,560 --> 00:31:39,560
我們看到CNN的我們都同意

1140
00:31:39,560 --> 00:31:41,560
他是0.9

1141
00:31:41,560 --> 00:31:43,560
那我就暫時把他想像成

1142
00:31:43,560 --> 00:31:45,560
我們可能有一個

1143
00:31:45,560 --> 00:31:47,560
類似那種國營媒體好了

1144
00:31:47,560 --> 00:31:49,560
假設像是日本的NHK

1145
00:31:49,560 --> 00:31:51,560
當然也不是所有日本人都喜歡NHK

1146
00:31:51,560 --> 00:31:53,560
假設有一個國營媒體然後那個東西是

1147
00:31:53,560 --> 00:31:55,560
highly credible

1148
00:31:55,560 --> 00:31:57,560
然後其他的媒體我們先不管他

1149
00:31:57,560 --> 00:31:59,560
反正他的東西如果出現大家都相信

1150
00:31:59,560 --> 00:32:01,560
然後他的credibility就是最高

1151
00:32:01,560 --> 00:32:03,560
他不一定是最高

1152
00:32:03,560 --> 00:32:05,560
反正我現在就是會調整那個數字

1153
00:32:05,560 --> 00:32:07,560
但我只是問說

1154
00:32:07,560 --> 00:32:09,560
然後

1155
00:32:09,560 --> 00:32:11,560
很好

1156
00:32:11,560 --> 00:32:13,560
我們這裡就是說

1157
00:32:13,560 --> 00:32:15,560
如果你沒有聽到新聞的話

1158
00:32:15,560 --> 00:32:17,560
你就繼續保持你原本的

1159
00:32:17,560 --> 00:32:19,560
這個想法

1160
00:32:19,560 --> 00:32:21,560
但如果你聽到想法的話就是我剛才講的

1161
00:32:21,560 --> 00:32:23,560
你可能會你就是做一個complex combination

1162
00:32:23,560 --> 00:32:25,560
就是說

1163
00:32:25,560 --> 00:32:27,560
你可以看出來如果是beta等於1

1164
00:32:27,560 --> 00:32:29,560
是fully credible的話我完全

1165
00:32:29,560 --> 00:32:31,560
是考慮我自己原本的μi

1166
00:32:31,560 --> 00:32:33,560
對吧 因為如果是beta等於1的話

1167
00:32:33,560 --> 00:32:35,560
我就是完全的改變了

1168
00:32:35,560 --> 00:32:37,560
我的想法變成x 但是如果beta

1169
00:32:37,560 --> 00:32:39,560
等於0就是我知道這東西完全是garbage

1170
00:32:39,560 --> 00:32:41,560
我就是preserve my

1171
00:32:41,560 --> 00:32:43,560
prior expectation

1172
00:32:43,560 --> 00:32:45,560
就是這是一個

1173
00:32:45,560 --> 00:32:47,560
分析比較簡單的方法

1174
00:32:47,560 --> 00:32:49,560
然後

1175
00:32:49,560 --> 00:32:51,560
再來是我想因為econ裡面

1176
00:32:51,560 --> 00:32:53,560
你就是你做任何的事情

1177
00:32:53,560 --> 00:32:55,560
其實你都是想try to maximize

1178
00:32:55,560 --> 00:32:57,560
your expected utility

1179
00:32:57,560 --> 00:32:59,560
然後首先是

1180
00:32:59,560 --> 00:33:01,560
如果你決定去分享這個新聞的時候

1181
00:33:01,560 --> 00:33:03,560
就像我剛才講的

1182
00:33:03,560 --> 00:33:05,560
你得突然在那邊滑手機滑得很開心

1183
00:33:05,560 --> 00:33:07,560
突然要停下來然後就是要

1184
00:33:07,560 --> 00:33:09,560
我們把interpret as a

1185
00:33:09,560 --> 00:33:11,560
status quo

1186
00:33:11,560 --> 00:33:13,560
bias就是說你不想

1187
00:33:13,560 --> 00:33:15,560
破壞原本的現狀你可能要突然又

1188
00:33:15,560 --> 00:33:17,560
做一件事情所以你要花了一個cost

1189
00:33:17,560 --> 00:33:19,560
然後在這個

1190
00:33:19,560 --> 00:33:21,560
狀況底下我們formulate這個utility function

1191
00:33:21,560 --> 00:33:23,560
像這樣子就是說

1192
00:33:23,560 --> 00:33:25,560
我會一步一步來解釋說

1193
00:33:25,560 --> 00:33:27,560
這個utility function代表了什麼意思

1194
00:33:27,560 --> 00:33:29,560
第一個是先

1195
00:33:29,560 --> 00:33:31,560
因為我剛才沒有定義這個network model

1196
00:33:31,560 --> 00:33:33,560
第一個是

1197
00:33:33,560 --> 00:33:35,560
我的utility是

1198
00:33:35,560 --> 00:33:37,560
根據我

1199
00:33:37,560 --> 00:33:39,560
關心的這個state data

1200
00:33:39,560 --> 00:33:41,560
以及我做了一個

1201
00:33:41,560 --> 00:33:43,560
sharing decision si 還有

1202
00:33:43,560 --> 00:33:45,560
以後我之後我會voting ai

1203
00:33:45,560 --> 00:33:47,560
但我也考慮到

1204
00:33:47,560 --> 00:33:49,560
我這些follower的

1205
00:33:49,560 --> 00:33:51,560
voting decision 以及

1206
00:33:51,560 --> 00:33:53,560
最後我可能如果我真的sharing

1207
00:33:53,560 --> 00:33:55,560
這個news的話我真的要

1208
00:33:55,560 --> 00:33:57,560
花了一個cost

1209
00:33:57,560 --> 00:33:59,560
所以第一個是這個其實就是

1210
00:33:59,560 --> 00:34:01,560
你aggregate就你考慮到了

1211
00:34:01,560 --> 00:34:03,560
你這個follower

1212
00:34:03,560 --> 00:34:05,560
的這個action

1213
00:34:05,560 --> 00:34:07,560
第一個部分就是direct utility from voting

1214
00:34:07,560 --> 00:34:09,560
就是我直接投了然後你可以看得出來

1215
00:34:09,560 --> 00:34:11,560
就是你盡可能的你想要

1216
00:34:11,560 --> 00:34:13,560
match這個state的sign

1217
00:34:13,560 --> 00:34:15,560
你可以看到ai data

1218
00:34:15,560 --> 00:34:17,560
你基本上你會挑那個action

1219
00:34:17,560 --> 00:34:21,560
去match the sign of the state

1220
00:34:21,560 --> 00:34:23,560
你想要知道它到底是偏右還是偏左

1221
00:34:23,560 --> 00:34:25,560
另外這個blue part

1222
00:34:25,560 --> 00:34:27,560
就是說這個東西

1223
00:34:27,560 --> 00:34:29,560
是你去aggregate我考慮到了

1224
00:34:29,560 --> 00:34:31,560
我這些follower的

1225
00:34:31,560 --> 00:34:33,560
utility

1226
00:34:33,560 --> 00:34:35,560
你internalize

1227
00:34:35,560 --> 00:34:37,560
你的follower

1228
00:34:37,560 --> 00:34:39,560
你其實是想要為了它們好

1229
00:34:39,560 --> 00:34:41,560
你只是覺得說我希望你們去投的跟我想的

1230
00:34:41,560 --> 00:34:43,560
一樣就這個theta

1231
00:34:43,560 --> 00:34:45,560
還是根據你自己的believe去

1232
00:34:45,560 --> 00:34:47,560
estimate

1233
00:34:47,560 --> 00:34:49,560
但是至於那個action

1234
00:34:49,560 --> 00:34:51,560
是depends on

1235
00:34:51,560 --> 00:34:53,560
你follower自己的believe

1236
00:34:53,560 --> 00:34:55,560
所以你想要try to inference它們的這個action

1237
00:34:57,560 --> 00:34:59,560
然後其實這邊我先快速的

1238
00:34:59,560 --> 00:35:01,560
跟大家講一下就是我剛才講的

1239
00:35:01,560 --> 00:35:03,560
你在voting的狀況底下

1240
00:35:03,560 --> 00:35:05,560
其實你是想要match the sign of the state

1241
00:35:05,560 --> 00:35:07,560
所以其實這邊有一個很快的

1242
00:35:07,560 --> 00:35:09,560
結論就是說

1243
00:35:11,560 --> 00:35:13,560
當我有收到的新聞之後

1244
00:35:13,560 --> 00:35:15,560
然後你知道就是剛才那個

1245
00:35:15,560 --> 00:35:17,560
beta s加一點beta mu i

1246
00:35:17,560 --> 00:35:19,560
然後它大於0了

1247
00:35:19,560 --> 00:35:21,560
那當然就代表我現在覺得

1248
00:35:21,560 --> 00:35:23,560
這個state應該是比較偏右

1249
00:35:23,560 --> 00:35:25,560
那我就投1

1250
00:35:25,560 --> 00:35:27,560
偏左的話就投-1

1251
00:35:27,560 --> 00:35:29,560
所以其實如果你有收到新聞的話

1252
00:35:29,560 --> 00:35:31,560
你會投

1253
00:35:31,560 --> 00:35:33,560
正義的condition

1254
00:35:33,560 --> 00:35:35,560
就是說這個mu i

1255
00:35:35,560 --> 00:35:37,560
足夠大,大過於這個threshold

1256
00:35:37,560 --> 00:35:39,560
如果你沒有聽到任何新聞

1257
00:35:39,560 --> 00:35:41,560
那當然一開始你覺得已經是偏低

1258
00:35:41,560 --> 00:35:43,560
如果一開始你的期望值是

1259
00:35:43,560 --> 00:35:45,560
正的那你當然就投

1260
00:35:45,560 --> 00:35:47,560
共和黨

1261
00:35:47,560 --> 00:35:49,560
如果你覺得一開始你的期望值是負的

1262
00:35:49,560 --> 00:35:51,560
那你就投民主黨

1263
00:35:51,560 --> 00:35:53,560
所以這個voting decision是比較簡單的部分

1264
00:35:53,560 --> 00:35:55,560
然後

1265
00:35:55,560 --> 00:35:57,560
我接下來要開始講的就是比較有趣的

1266
00:35:57,560 --> 00:35:59,560
就是這個sharing decision

1267
00:35:59,560 --> 00:36:01,560
我有一個問題

1268
00:36:01,560 --> 00:36:03,560
我不知道

1269
00:36:03,560 --> 00:36:05,560
可能這是你等一下要講

1270
00:36:05,560 --> 00:36:07,560
utility function

1271
00:36:07,560 --> 00:36:09,560
其中一個input是voting decision

1272
00:36:09,560 --> 00:36:11,560
但是自己那個agent也有可能

1273
00:36:11,560 --> 00:36:13,560
會改變

1274
00:36:13,560 --> 00:36:15,560
AI star

1275
00:36:15,560 --> 00:36:17,560
但是AI star跟這個AI是沒有關的

1276
00:36:17,560 --> 00:36:19,560
是這樣嗎

1277
00:36:19,560 --> 00:36:21,560
不好意思

1278
00:36:21,560 --> 00:36:23,560
AI star其實就是均衡

1279
00:36:23,560 --> 00:36:25,560
好

1280
00:36:25,560 --> 00:36:27,560
就是說我現在剛才定義了這個utility function

1281
00:36:27,560 --> 00:36:29,560
沒有打信號的部分

1282
00:36:29,560 --> 00:36:31,560
就是單純的

1283
00:36:31,560 --> 00:36:33,560
我的utility是這樣子

1284
00:36:33,560 --> 00:36:35,560
我現在打了信號的意思代表說

1285
00:36:35,560 --> 00:36:37,560
在均衡底下之後其實我就是會做這件事情

1286
00:36:37,560 --> 00:36:39,560
因為你還記得voting是在最後一刻發生的

1287
00:36:39,560 --> 00:36:41,560
對吧

1288
00:36:41,560 --> 00:36:43,560
所以大概大家都已經interact完了之後

1289
00:36:43,560 --> 00:36:45,560
如果我在那個時間點

1290
00:36:45,560 --> 00:36:47,560
在voting的時間點

1291
00:36:47,560 --> 00:36:49,560
我有收到新聞的話

1292
00:36:49,560 --> 00:36:51,560
那麼我的最好的action

1293
00:36:51,560 --> 00:36:53,560
因為我那個時間點投票

1294
00:36:53,560 --> 00:36:55,560
我已經沒辦法再影響任何人了

1295
00:36:55,560 --> 00:36:57,560
我其實能投的事情就是

1296
00:36:57,560 --> 00:36:59,560
我就是要根據我自己的belief去match

1297
00:36:59,560 --> 00:37:01,560
那個state

1298
00:37:01,560 --> 00:37:03,560
所以那個時候如果你的期望值

1299
00:37:03,560 --> 00:37:05,560
就是你如果已經收過新聞了

1300
00:37:05,560 --> 00:37:07,560
那你的期望值是1

1301
00:37:07,560 --> 00:37:09,560
你的期望值是正的

1302
00:37:09,560 --> 00:37:11,560
那那個condition就是這個

1303
00:37:11,560 --> 00:37:13,560
然後你會投1

1304
00:37:13,560 --> 00:37:15,560
但是如果在那個投票的時間點

1305
00:37:15,560 --> 00:37:17,560
你沒有收到任何新聞

1306
00:37:17,560 --> 00:37:19,560
那你就是根據你原本的期望值

1307
00:37:19,560 --> 00:37:21,560
如果他是正的我就投1

1308
00:37:21,560 --> 00:37:23,560
如果是負的我就投負1

1309
00:37:23,560 --> 00:37:25,560
ok嗎

1310
00:37:25,560 --> 00:37:27,560
聽起來的感覺是

1311
00:37:27,560 --> 00:37:29,560
這個utility function是可以

1312
00:37:29,560 --> 00:37:31,560
是在

1313
00:37:31,560 --> 00:37:33,560
這個agent在present state就可以預測

1314
00:37:33,560 --> 00:37:35,560
預測到他的stage 2的

1315
00:37:35,560 --> 00:37:37,560
voting decision

1316
00:37:37,560 --> 00:37:39,560
對

1317
00:37:39,560 --> 00:37:41,560
他其實他在stage 1

1318
00:37:41,560 --> 00:37:43,560
對

1319
00:37:43,560 --> 00:37:45,560
good question

1320
00:37:45,560 --> 00:37:47,560
就是說其實

1321
00:37:47,560 --> 00:37:49,560
因為我其實沒有想講的太technical

1322
00:37:49,560 --> 00:37:51,560
但是其實應該跟大家講的就是說

1323
00:37:51,560 --> 00:37:53,560
你在解這個equilibrium的時候

1324
00:37:53,560 --> 00:37:55,560
你可能是倒推回來的

1325
00:37:55,560 --> 00:37:57,560
到最後一步我知道大家根據他們自己的belief

1326
00:37:57,560 --> 00:37:59,560
他會怎麼投

1327
00:37:59,560 --> 00:38:01,560
有收到新聞的人他就是按照上面這個方式投

1328
00:38:01,560 --> 00:38:03,560
沒收到新聞的人就按照下面這個方式投

1329
00:38:03,560 --> 00:38:05,560
沒收到新聞的人就按照下面這個方式投

1330
00:38:05,560 --> 00:38:07,560
然後

1331
00:38:07,560 --> 00:38:09,560
所以這個時候你已經知道他們在voting stage

1332
00:38:09,560 --> 00:38:11,560
的時候會做出這樣的事情的時候

1333
00:38:11,560 --> 00:38:13,560
你在sharing stage的時候你就會考慮說

1334
00:38:13,560 --> 00:38:15,560
那我會怎麼樣影響他們最後投票的

1335
00:38:15,560 --> 00:38:17,560
這個程度

1336
00:38:17,560 --> 00:38:19,560
因為我知道如果我分享

1337
00:38:19,560 --> 00:38:21,560
他們就會看到

1338
00:38:21,560 --> 00:38:23,560
他們就會照著上面那個方式投

1339
00:38:23,560 --> 00:38:25,560
如果我沒有分享他們就會照著下面那個方式投

1340
00:38:25,560 --> 00:38:27,560
之類的所以你就要去想這件事情

1341
00:38:27,560 --> 00:38:29,560
ok嗎

1342
00:38:29,560 --> 00:38:31,560
好

1343
00:38:31,560 --> 00:38:33,560
那我趕快講一下這個

1344
00:38:33,560 --> 00:38:35,560
因為這個比較有趣一點

1345
00:38:35,560 --> 00:38:37,560
就是說到了sharing decision的時候

1346
00:38:37,560 --> 00:38:39,560
就變成說你現在已經知道說他們最後

1347
00:38:39,560 --> 00:38:41,560
那個stage他們會怎麼vote

1348
00:38:41,560 --> 00:38:43,560
然後你在想的就是說

1349
00:38:43,560 --> 00:38:45,560
那我到底該不該

1350
00:38:45,560 --> 00:38:47,560
分享我的新聞去改變他們的想法

1351
00:38:47,560 --> 00:38:49,560
因為我改變的話他們就會照著

1352
00:38:49,560 --> 00:38:51,560
那個beta s加一減beta

1353
00:38:51,560 --> 00:38:53,560
如果沒有的話就是preserve

1354
00:38:53,560 --> 00:38:55,560
他們原本的expectation

1355
00:38:55,560 --> 00:38:57,560
對就我剛才講

1356
00:38:57,560 --> 00:38:59,560
如果分享的話我就知道說

1357
00:38:59,560 --> 00:39:01,560
我的follower都會看到這個新聞

1358
00:39:01,560 --> 00:39:03,560
當然有可能

1359
00:39:03,560 --> 00:39:05,560
對就是我們就assume

1360
00:39:05,560 --> 00:39:07,560
你的follower真的就會看到這個新聞

1361
00:39:07,560 --> 00:39:09,560
另外一個就是說如果你沒分享的話

1362
00:39:09,560 --> 00:39:11,560
那你就要想

1363
00:39:11,560 --> 00:39:13,560
我沒有分享但是

1364
00:39:13,560 --> 00:39:15,560
我的follower也有可能從別的地方

1365
00:39:15,560 --> 00:39:17,560
聽到新聞對吧

1366
00:39:17,560 --> 00:39:19,560
因為你現在已經知道有這個新聞在這個社會裡頭

1367
00:39:19,560 --> 00:39:21,560
所以你已經知道說

1368
00:39:21,560 --> 00:39:23,560
有一些人也在傳這個新聞

1369
00:39:23,560 --> 00:39:25,560
那到底我的follower有多可能聽到這個新聞

1370
00:39:25,560 --> 00:39:27,560
那其實這個東西呢

1371
00:39:27,560 --> 00:39:29,560
在這個分析當中

1372
00:39:29,560 --> 00:39:31,560
你會預測說

1373
00:39:31,560 --> 00:39:33,560
在均衡的底下

1374
00:39:33,560 --> 00:39:35,560
到底這個新聞

1375
00:39:35,560 --> 00:39:37,560
可以傳的多遠

1376
00:39:37,560 --> 00:39:39,560
所以你在想的就是這個sizeQ

1377
00:39:39,560 --> 00:39:41,560
你可以想像就是說

1378
00:39:41,560 --> 00:39:43,560
這個sizeQ鐵定會正相關的影響到

1379
00:39:43,560 --> 00:39:45,560
你的follower

1380
00:39:45,560 --> 00:39:47,560
多可能聽到這個新聞

1381
00:39:47,560 --> 00:39:49,560
在這個equally

1382
00:39:49,560 --> 00:39:51,560
從別人那邊聽到的新聞

1383
00:39:51,560 --> 00:39:53,560
而不是你自己

1384
00:39:53,560 --> 00:39:55,560
因為如果你它假設這個Q是1

1385
00:39:55,560 --> 00:39:57,560
那你就知道鐵定他會從別人那邊聽到新聞

1386
00:39:57,560 --> 00:39:59,560
因為每個人都在傳

1387
00:39:59,560 --> 00:40:01,560
就是這個size非常大

1388
00:40:01,560 --> 00:40:03,560
就等於是說那我就不用傳

1389
00:40:03,560 --> 00:40:05,560
對

1390
00:40:05,560 --> 00:40:07,560
那接下來這個問題就來了

1391
00:40:07,560 --> 00:40:09,560
就是它現在

1392
00:40:09,560 --> 00:40:11,560
決定了sharing decision之後

1393
00:40:11,560 --> 00:40:13,560
它就形成了這個sharing strategy

1394
00:40:13,560 --> 00:40:15,560
就是它的策略是這樣

1395
00:40:15,560 --> 00:40:17,560
然後每一個人都做了這些事情之後呢

1396
00:40:17,560 --> 00:40:19,560
經過這個spread dynamics

1397
00:40:19,560 --> 00:40:21,560
就是你知道這個傳播的這個鏈

1398
00:40:21,560 --> 00:40:23,560
它才會去決定了

1399
00:40:23,560 --> 00:40:25,560
這個new spread sizeQ

1400
00:40:25,560 --> 00:40:27,560
所以其實它形成了一個loop

1401
00:40:27,560 --> 00:40:29,560
大家有沒有看出來

1402
00:40:29,560 --> 00:40:31,560
就是我每一個人分享了一個新聞之後呢

1403
00:40:31,560 --> 00:40:33,560
經過了

1404
00:40:33,560 --> 00:40:35,560
因為新聞會這樣子慢慢的傳出去嘛

1405
00:40:35,560 --> 00:40:37,560
然後它就形成了spread size

1406
00:40:37,560 --> 00:40:39,560
可是這個其實你也預測到說

1407
00:40:39,560 --> 00:40:41,560
喔我傳出去之後這種spread size

1408
00:40:41,560 --> 00:40:43,560
進而又影響了我到底

1409
00:40:43,560 --> 00:40:45,560
我的follower有多可能聽到這樣的新聞

1410
00:40:45,560 --> 00:40:47,560
所以那個時候可能你覺得說

1411
00:40:47,560 --> 00:40:49,560
所以就是它是一個feedback

1412
00:40:49,560 --> 00:40:51,560
就等於是說如果你覺得好像有很多人會傳

1413
00:40:51,560 --> 00:40:53,560
那你就不傳

1414
00:40:53,560 --> 00:40:55,560
你又知道說

1415
00:40:55,560 --> 00:40:57,560
那現在我又應該傳了

1416
00:40:57,560 --> 00:40:59,560
因為大家都覺得好像我不應該傳

1417
00:40:59,560 --> 00:41:01,560
就是它會有一個feedback

1418
00:41:01,560 --> 00:41:03,560
然後會達到一個均衡

1419
00:41:03,560 --> 00:41:05,560
所以這個Q呢

1420
00:41:05,560 --> 00:41:07,560
是一個indigenous generated variable

1421
00:41:07,560 --> 00:41:09,560
它是一個

1422
00:41:09,560 --> 00:41:11,560
它不是一個

1423
00:41:11,560 --> 00:41:13,560
它是有一個feedback去決定它

1424
00:41:13,560 --> 00:41:15,560
這個均衡底下這個數字應該長什麼樣

1425
00:41:15,560 --> 00:41:17,560
然後我就

1426
00:41:17,560 --> 00:41:19,560
如果大家感興趣那個

1427
00:41:19,560 --> 00:41:21,560
那些equation去怎麼樣

1428
00:41:21,560 --> 00:41:23,560
大家可以去看一下paper

1429
00:41:23,560 --> 00:41:25,560
但這是主要的idea就是說

1430
00:41:25,560 --> 00:41:27,560
你在決定sharing的時候你得去考慮

1431
00:41:27,560 --> 00:41:29,560
喔到均衡的時候

1432
00:41:29,560 --> 00:41:31,560
這個傳播的鏈有多大

1433
00:41:31,560 --> 00:41:33,560
但是有可能它就影響我到底

1434
00:41:33,560 --> 00:41:35,560
要不要去傳

1435
00:41:35,560 --> 00:41:37,560
然後這是我們的這個

1436
00:41:37,560 --> 00:41:39,560
第一個雷瑪就是說

1437
00:41:39,560 --> 00:41:41,560
characterize到底

1438
00:41:41,560 --> 00:41:43,560
假設你已經知道這spread size是Q的話

1439
00:41:43,560 --> 00:41:45,560
那你

1440
00:41:45,560 --> 00:41:47,560
會傳這個新聞的

1441
00:41:47,560 --> 00:41:49,560
condition是這樣

1442
00:41:49,560 --> 00:41:51,560
其實就是你的expectation

1443
00:41:51,560 --> 00:41:53,560
因為你現在是

1444
00:41:53,560 --> 00:41:55,560
拿到新聞了對吧

1445
00:41:55,560 --> 00:41:57,560
你拿到新聞的expectation

1446
00:41:57,560 --> 00:41:59,560
然後乘上你覺得這個news

1447
00:41:59,560 --> 00:42:01,560
它可以影響多少的

1448
00:42:01,560 --> 00:42:03,560
你的follower

1449
00:42:03,560 --> 00:42:05,560
假設你傳了結果你發現所有的follower

1450
00:42:05,560 --> 00:42:07,560
不會因為你的這個decision而改變他的想法的話

1451
00:42:07,560 --> 00:42:09,560
那你就沒有意義

1452
00:42:09,560 --> 00:42:11,560
去傳這個新聞

1453
00:42:11,560 --> 00:42:13,560
所以這個成績如果大於C的話

1454
00:42:13,560 --> 00:42:15,560
那他就會傳

1455
00:42:15,560 --> 00:42:17,560
那這裡就是說

1456
00:42:17,560 --> 00:42:19,560
其實就是

1457
00:42:19,560 --> 00:42:21,560
就是我剛才講的那個strategic substitute

1458
00:42:21,560 --> 00:42:23,560
如果你說這個Q已經很多

1459
00:42:23,560 --> 00:42:25,560
就是你知道這個spread size

1460
00:42:25,560 --> 00:42:27,560
可以很大的時候

1461
00:42:27,560 --> 00:42:29,560
那你就可以看到這個成績就會變小對吧

1462
00:42:29,560 --> 00:42:31,560
因為它是反向的

1463
00:42:31,560 --> 00:42:33,560
所以你就越不想傳

1464
00:42:33,560 --> 00:42:35,560
然後另外是說從這個雷瑪

1465
00:42:35,560 --> 00:42:37,560
你可以導出一個threshold strategy

1466
00:42:37,560 --> 00:42:39,560
就是說你可以發現

1467
00:42:39,560 --> 00:42:41,560
其實每一個人收到新聞之後呢

1468
00:42:41,560 --> 00:42:43,560
如果這個新聞是

1469
00:42:43,560 --> 00:42:45,560
偏向共和黨的

1470
00:42:45,560 --> 00:42:47,560
那你就知道說

1471
00:42:47,560 --> 00:42:49,560
如果一開始這個人的expectation

1472
00:42:49,560 --> 00:42:51,560
大過了某一個threshold

1473
00:42:51,560 --> 00:42:53,560
那他就會傳這個新聞

1474
00:42:53,560 --> 00:42:55,560
然後小於這個threshold的人他都不會傳這個新聞

1475
00:42:55,560 --> 00:42:57,560
如果這個新聞是

1476
00:42:57,560 --> 00:42:59,560
支持democrats的話呢

1477
00:42:59,560 --> 00:43:01,560
那麼你知道說

1478
00:43:01,560 --> 00:43:03,560
如果那些

1479
00:43:03,560 --> 00:43:05,560
原本就偏左派的人呢

1480
00:43:05,560 --> 00:43:07,560
他就會分享這個新聞

1481
00:43:07,560 --> 00:43:09,560
但是偏右的人就不會想分享這個新聞

1482
00:43:09,560 --> 00:43:11,560
所以這是一個threshold

1483
00:43:11,560 --> 00:43:13,560
我們叫它是一個threshold strategy

1484
00:43:13,560 --> 00:43:15,560
就是你知道

1485
00:43:15,560 --> 00:43:17,560
大於他的就傳

1486
00:43:17,560 --> 00:43:19,560
小於他的就不傳

1487
00:43:21,560 --> 00:43:23,560
那接下來的問題就是說

1488
00:43:23,560 --> 00:43:25,560
什麼時候這一個

1489
00:43:25,560 --> 00:43:27,560
news cascade會

1490
00:43:27,560 --> 00:43:29,560
可以maximize

1491
00:43:29,560 --> 00:43:31,560
我應該再

1492
00:43:31,560 --> 00:43:33,560
more specific一點就是說

1493
00:43:33,560 --> 00:43:35,560
什麼樣的credibility

1494
00:43:35,560 --> 00:43:37,560
可以使得這個news

1495
00:43:37,560 --> 00:43:39,560
傳得最遠

1496
00:43:39,560 --> 00:43:41,560
傳得最大

1497
00:43:41,560 --> 00:43:43,560
Fully credible

1498
00:43:43,560 --> 00:43:45,560
for credibility就是最好的

1499
00:43:45,560 --> 00:43:47,560
然後首先我們要回答這個問題

1500
00:43:47,560 --> 00:43:49,560
就要去

1501
00:43:49,560 --> 00:43:51,560
看一下說到底

1502
00:43:51,560 --> 00:43:53,560
Fully credible news

1503
00:43:53,560 --> 00:43:55,560
他會產生的這個spread size到底有多大

1504
00:43:55,560 --> 00:43:57,560
然後第一個是

1505
00:43:57,560 --> 00:43:59,560
這個圖呢

1506
00:43:59,560 --> 00:44:01,560
他其實是根據x的

1507
00:44:01,560 --> 00:44:03,560
這個magnitude在畫的

1508
00:44:03,560 --> 00:44:05,560
我只考慮正的,因為負的話

1509
00:44:05,560 --> 00:44:07,560
其實就是一個symmetry的case

1510
00:44:07,560 --> 00:44:09,560
所以大家可以不用太care

1511
00:44:09,560 --> 00:44:11,560
然後就0到c

1512
00:44:11,560 --> 00:44:13,560
就是當你這個magnitude

1513
00:44:13,560 --> 00:44:15,560
of the news是很小很小的時候

1514
00:44:15,560 --> 00:44:17,560
你會發現說

1515
00:44:17,560 --> 00:44:19,560
even是fully credible news

1516
00:44:19,560 --> 00:44:21,560
就是沒有人要傳

1517
00:44:21,560 --> 00:44:23,560
就是他會導致說這個新聞傳不出去

1518
00:44:23,560 --> 00:44:25,560
是0

1519
00:44:25,560 --> 00:44:27,560
如果說這個magnitude of news很強大的話

1520
00:44:27,560 --> 00:44:29,560
就是你知道

1521
00:44:29,560 --> 00:44:31,560
這個news他既是credible

1522
00:44:31,560 --> 00:44:33,560
然後他又告訴我們

1523
00:44:33,560 --> 00:44:35,560
一定要投一下拜登的話

1524
00:44:35,560 --> 00:44:37,560
不是拜登,因為現在是偏右

1525
00:44:37,560 --> 00:44:39,560
大家很相信他

1526
00:44:39,560 --> 00:44:41,560
然後他又那麼的強

1527
00:44:41,560 --> 00:44:43,560
就是他的magnitude很強的話

1528
00:44:43,560 --> 00:44:45,560
那大家就會很願意去傳他

1529
00:44:45,560 --> 00:44:47,560
很願意去告訴別人說你應該要投川普

1530
00:44:47,560 --> 00:44:49,560
這個時候他會

1531
00:44:49,560 --> 00:44:51,560
這個傳播會達到他的極大值

1532
00:44:51,560 --> 00:44:53,560
這個極大值就是

1533
00:44:53,560 --> 00:44:55,560
這個社會能形成的極大值

1534
00:44:55,560 --> 00:44:57,560
我叫他QG

1535
00:44:57,560 --> 00:44:59,560
就是一個join component

1536
00:44:59,560 --> 00:45:01,560
就是他有connected最大的那一個component

1537
00:45:01,560 --> 00:45:03,560
但是如果這個news

1538
00:45:03,560 --> 00:45:05,560
在中間這個範圍

1539
00:45:05,560 --> 00:45:07,560
他是因為我剛才講的

1540
00:45:07,560 --> 00:45:09,560
就是有一個strategic substitute

1541
00:45:09,560 --> 00:45:11,560
就是我知道別人要傳的話我就不傳

1542
00:45:11,560 --> 00:45:13,560
但是我如果不傳

1543
00:45:13,560 --> 00:45:15,560
別人又知道他要傳

1544
00:45:15,560 --> 00:45:17,560
然後就是在那個均衡底下呢

1545
00:45:17,560 --> 00:45:19,560
這個數字會導致大家會覺得

1546
00:45:19,560 --> 00:45:21,560
他在boundary

1547
00:45:21,560 --> 00:45:23,560
他會覺得我其實沒有特別覺得

1548
00:45:23,560 --> 00:45:25,560
該傳或不傳

1549
00:45:25,560 --> 00:45:27,560
就是剛好在那個feel indifferent的那個

1550
00:45:27,560 --> 00:45:29,560
界線

1551
00:45:29,560 --> 00:45:31,560
所以在

1552
00:45:31,560 --> 00:45:33,560
news他是moderate的情況下

1553
00:45:33,560 --> 00:45:35,560
其實你可以characterize

1554
00:45:35,560 --> 00:45:37,560
就是他的spread size

1555
00:45:37,560 --> 00:45:39,560
那我們現在已經知道說

1556
00:45:39,560 --> 00:45:41,560
Fully credible news

1557
00:45:41,560 --> 00:45:43,560
會是這樣

1558
00:45:43,560 --> 00:45:45,560
他的size會是這樣子

1559
00:45:45,560 --> 00:45:47,560
那麼現在很明顯的就是

1560
00:45:47,560 --> 00:45:49,560
如果news在這個範圍裡頭呢

1561
00:45:49,560 --> 00:45:51,560
你就知道說

1562
00:45:51,560 --> 00:45:53,560
Fully credible news不會傳出去

1563
00:45:53,560 --> 00:45:55,560
因為他鐵定是0嘛

1564
00:45:55,560 --> 00:45:57,560
他鐵定是最差的

1565
00:45:57,560 --> 00:45:59,560
另外一個是

1566
00:45:59,560 --> 00:46:01,560
這邊最右邊他已經達到他最大值

1567
00:46:01,560 --> 00:46:03,560
好討論的

1568
00:46:03,560 --> 00:46:05,560
因為就變成說你知道啊

1569
00:46:05,560 --> 00:46:07,560
這個Fully credible是最好或最差

1570
00:46:07,560 --> 00:46:09,560
但在中間這個時候呢

1571
00:46:09,560 --> 00:46:11,560
就我們有另外一個proposition

1572
00:46:11,560 --> 00:46:13,560
就告訴你說

1573
00:46:13,560 --> 00:46:15,560
怎麼樣情況下這種last credible news

1574
00:46:15,560 --> 00:46:17,560
last credible news就是說

1575
00:46:17,560 --> 00:46:19,560
比beta還小一點的那些news

1576
00:46:19,560 --> 00:46:21,560
他反而可以去

1577
00:46:21,560 --> 00:46:23,560
使得這個cascade size

1578
00:46:23,560 --> 00:46:25,560
比beta等於1

1579
00:46:25,560 --> 00:46:27,560
也就是Fully credible news

1580
00:46:27,560 --> 00:46:29,560
還要大

1581
00:46:29,560 --> 00:46:31,560
news的這個threshold

1582
00:46:31,560 --> 00:46:33,560
我剛才不是講他是一個threshold strategy嗎

1583
00:46:33,560 --> 00:46:35,560
如果這個threshold大於x的話

1584
00:46:35,560 --> 00:46:37,560
然後我跟

1585
00:46:37,560 --> 00:46:39,560
就他可能比較有點technical

1586
00:46:39,560 --> 00:46:41,560
的這邊就是說

1587
00:46:41,560 --> 00:46:43,560
其實給大家一個比較intuition

1588
00:46:43,560 --> 00:46:45,560
的狀況底下

1589
00:46:45,560 --> 00:46:47,560
你可以想像就是說

1590
00:46:47,560 --> 00:46:49,560
這是一個剛才的decision rule嘛

1591
00:46:49,560 --> 00:46:51,560
就是expectation乘上了一個

1592
00:46:51,560 --> 00:46:53,560
你可以改變的人的數量

1593
00:46:53,560 --> 00:46:55,560
然後會大於1

1594
00:46:55,560 --> 00:46:57,560
但是當beta等於1的時候

1595
00:46:57,560 --> 00:46:59,560
其實你會reduce到說

1596
00:46:59,560 --> 00:47:01,560
這邊x就完全是x

1597
00:47:01,560 --> 00:47:03,560
然後根據剛才算的

1598
00:47:03,560 --> 00:47:05,560
這個Q star其實是1-c

1599
00:47:05,560 --> 00:47:07,560
over x 所以其實這邊是c

1600
00:47:07,560 --> 00:47:09,560
over x 其實是大於等於c

1601
00:47:09,560 --> 00:47:11,560
就是在那個

1602
00:47:11,560 --> 00:47:13,560
在中間這個紅色

1603
00:47:13,560 --> 00:47:15,560
這個階段的時候其實大家都是feel

1604
00:47:15,560 --> 00:47:17,560
都是感覺到indifferent

1605
00:47:17,560 --> 00:47:19,560
要傳不傳他都覺得可以

1606
00:47:19,560 --> 00:47:21,560
但是如果你把beta就是稍微

1607
00:47:21,560 --> 00:47:23,560
降低的話就讓他比較

1608
00:47:23,560 --> 00:47:25,560
沒有那麼precise的時候

1609
00:47:25,560 --> 00:47:27,560
但是這個時候你可以

1610
00:47:27,560 --> 00:47:29,560
先假定說這個Q star

1611
00:47:29,560 --> 00:47:31,560
還是接近c over x

1612
00:47:31,560 --> 00:47:33,560
就是這個1-Q star還是接近c over x

1613
00:47:33,560 --> 00:47:35,560
你會發現說當你把beta

1614
00:47:35,560 --> 00:47:37,560
小於1的時候這個時候

1615
00:47:37,560 --> 00:47:39,560
一旦μi大於x

1616
00:47:39,560 --> 00:47:41,560
他們都會嚴格的大於c

1617
00:47:41,560 --> 00:47:43,560
所以他們都會願意傳

1618
00:47:43,560 --> 00:47:45,560
所以就變成說當你beta小於

1619
00:47:45,560 --> 00:47:47,560
1的時候呢這些μi

1620
00:47:47,560 --> 00:47:49,560
大於x的人其實都願意傳

1621
00:47:49,560 --> 00:47:51,560
懂嗎

1622
00:47:51,560 --> 00:47:53,560
所以如果這個

1623
00:47:53,560 --> 00:47:55,560
對不起就是說

1624
00:47:55,560 --> 00:47:57,560
就是你把他等於1

1625
00:47:57,560 --> 00:47:59,560
然後再往小於1的部分

1626
00:47:59,560 --> 00:48:01,560
也降低一點的話

1627
00:48:01,560 --> 00:48:03,560
你會發現說這個threshold

1628
00:48:03,560 --> 00:48:05,560
就是μi那些比x

1629
00:48:05,560 --> 00:48:07,560
還要大的人呢都願意去

1630
00:48:07,560 --> 00:48:09,560
傳的這個新聞

1631
00:48:09,560 --> 00:48:11,560
所以只要這個

1632
00:48:11,560 --> 00:48:13,560
你可以知道說至少這些人都願意去傳

1633
00:48:13,560 --> 00:48:15,560
所以如果這個threshold呢

1634
00:48:15,560 --> 00:48:17,560
比這個x還要小的話

1635
00:48:17,560 --> 00:48:19,560
你就知道說其實你把beta

1636
00:48:19,560 --> 00:48:21,560
小於1降低降到小於1

1637
00:48:21,560 --> 00:48:23,560
的時候你可以讓更多人去傳

1638
00:48:23,560 --> 00:48:25,560
對這邊可能有點technical

1639
00:48:25,560 --> 00:48:27,560
但是就是一個

1640
00:48:27,560 --> 00:48:29,560
就是如果intuitively

1641
00:48:29,560 --> 00:48:31,560
如果cost 0

1642
00:48:31,560 --> 00:48:33,560
的話會

1643
00:48:33,560 --> 00:48:35,560
怎樣

1644
00:48:35,560 --> 00:48:37,560
因為現在有cost的話

1645
00:48:37,560 --> 00:48:39,560
現在感覺有我覺得比較tricky是有cost

1646
00:48:39,560 --> 00:48:41,560
然後又有那個

1647
00:48:41,560 --> 00:48:43,560
beta又varies然後還有x

1648
00:48:43,560 --> 00:48:45,560
然後所以

1649
00:48:45,560 --> 00:48:47,560
你可以把cost想像

1650
00:48:47,560 --> 00:48:49,560
你不要去改變cost在我們這個case

1651
00:48:49,560 --> 00:48:51,560
裡面我們沒有去改變任何的cost

1652
00:48:51,560 --> 00:48:53,560
因為我們覺得他就是一個fundamental的東西

1653
00:48:53,560 --> 00:48:55,560
然後我們比較好奇的就只是說

1654
00:48:55,560 --> 00:48:57,560
credibility跟news之間的一些

1655
00:48:57,560 --> 00:48:59,560
關係所以你不要去改變cost

1656
00:48:59,560 --> 00:49:01,560
between 0 and 1嗎

1657
00:49:01,560 --> 00:49:03,560
還是什麼0 to x

1658
00:49:03,560 --> 00:49:05,560
沒有就是他是一個大於0的數

1659
00:49:05,560 --> 00:49:07,560
cost就是一個大於0

1660
00:49:07,560 --> 00:49:09,560
對就是大於0的數就好

1661
00:49:09,560 --> 00:49:11,560
如果他等於0的話其實大家就

1662
00:49:11,560 --> 00:49:13,560
不管怎樣他都願意傳因為他覺得

1663
00:49:13,560 --> 00:49:15,560
就算是garbage他也願意傳

1664
00:49:15,560 --> 00:49:17,560
因為反正不影響我嘛就是你知道

1665
00:49:17,560 --> 00:49:19,560
有什麼關係反正我傳了

1666
00:49:19,560 --> 00:49:21,560
我縱使照美人山但也沒有什麼關係

1667
00:49:21,560 --> 00:49:23,560
因為我沒有花任何的cost

1668
00:49:23,560 --> 00:49:25,560
所以cost其實還是有點影響

1669
00:49:25,560 --> 00:49:27,560
那個spread size就會最大

1670
00:49:27,560 --> 00:49:29,560
對因為

1671
00:49:29,560 --> 00:49:31,560
等於是說大家其實沒有

1672
00:49:31,560 --> 00:49:33,560
然後那個就會變成一個

1673
00:49:33,560 --> 00:49:35,560
很trivial case

1674
00:49:35,560 --> 00:49:37,560
我們還是有點依賴這個

1675
00:49:37,560 --> 00:49:39,560
有點cost的情況

1676
00:49:39,560 --> 00:49:41,560
我覺得這個地方就可能大家

1677
00:49:41,560 --> 00:49:43,560
稍微知道一下說中間這個地方

1678
00:49:43,560 --> 00:49:45,560
我們是有一個condition

1679
00:49:45,560 --> 00:49:47,560
去告訴大家說到底

1680
00:49:47,560 --> 00:49:49,560
for中間這個線段

1681
00:49:49,560 --> 00:49:51,560
這個紅色moderate news的線段

1682
00:49:51,560 --> 00:49:53,560
怎麼樣的什麼樣

1683
00:49:53,560 --> 00:49:55,560
credibility的news可以create a logic case

1684
00:49:55,560 --> 00:49:57,560
就是說其實如果滿足了這個條件

1685
00:49:57,560 --> 00:49:59,560
那麼其實比較

1686
00:49:59,560 --> 00:50:01,560
less credible的news他就可以

1687
00:50:01,560 --> 00:50:03,560
所以我們用這樣子一個condition

1688
00:50:03,560 --> 00:50:05,560
去討論

1689
00:50:05,560 --> 00:50:07,560
接下來的問題是還有

1690
00:50:07,560 --> 00:50:09,560
這個是非常重要就是說

1691
00:50:09,560 --> 00:50:11,560
這個colorary告訴我們

1692
00:50:11,560 --> 00:50:13,560
說對這些

1693
00:50:13,560 --> 00:50:15,560
只要news要大於c

1694
00:50:15,560 --> 00:50:17,560
因為如果news小於c的話

1695
00:50:17,560 --> 00:50:19,560
那你知道說這個

1696
00:50:19,560 --> 00:50:21,560
beta等於1鐵定是

1697
00:50:21,560 --> 00:50:23,560
導致沒有人在傳這個新聞

1698
00:50:23,560 --> 00:50:25,560
所以我們只考慮s大於c

1699
00:50:25,560 --> 00:50:27,560
當你知道s大於c的時候你可以知道說

1700
00:50:27,560 --> 00:50:29,560
只要我這個network

1701
00:50:29,560 --> 00:50:31,560
足夠的連結在一起的話

1702
00:50:31,560 --> 00:50:33,560
就是你知道他有一個threshold

1703
00:50:33,560 --> 00:50:35,560
就是你只要大於那一個數字

1704
00:50:35,560 --> 00:50:37,560
就是你平均的followers

1705
00:50:37,560 --> 00:50:39,560
只要大於那個數字

1706
00:50:39,560 --> 00:50:41,560
那麼鐵定比較

1707
00:50:41,560 --> 00:50:43,560
不credible的news

1708
00:50:43,560 --> 00:50:45,560
可以create a logic case than fully credible news

1709
00:50:45,560 --> 00:50:47,560
所以這是一個

1710
00:50:47,560 --> 00:50:49,560
還滿重要的colorary

1711
00:50:49,560 --> 00:50:51,560
就是告訴你說

1712
00:50:51,560 --> 00:50:53,560
只要這個network足夠的

1713
00:50:53,560 --> 00:50:55,560
緊密

1714
00:50:55,560 --> 00:50:57,560
只要你有足夠

1715
00:50:57,560 --> 00:50:59,560
平均足夠多的followers的時候

1716
00:50:59,560 --> 00:51:01,560
你就會讓那些

1717
00:51:01,560 --> 00:51:03,560
比較不準確的新聞去create a logic case

1718
00:51:03,560 --> 00:51:05,560
其實這邊的intuition很簡單

1719
00:51:05,560 --> 00:51:07,560
其實就只是告訴你說

1720
00:51:07,560 --> 00:51:09,560
因為network越緊密的話

1721
00:51:09,560 --> 00:51:11,560
越緊密

1722
00:51:11,560 --> 00:51:13,560
那我的followers

1723
00:51:13,560 --> 00:51:15,560
很容易就聽到別人傳來的訊息

1724
00:51:15,560 --> 00:51:17,560
因為他有很多個管道可以去聽到別人的訊息

1725
00:51:17,560 --> 00:51:19,560
那麼你就

1726
00:51:19,560 --> 00:51:21,560
這就會進而去導致說

1727
00:51:21,560 --> 00:51:23,560
那我可能就不太想要傳訊息

1728
00:51:23,560 --> 00:51:25,560
因為你知道說

1729
00:51:25,560 --> 00:51:27,560
大家連結這麼緊密

1730
00:51:27,560 --> 00:51:29,560
只要他稍微從某個管道聽到

1731
00:51:29,560 --> 00:51:31,560
他就聽到了

1732
00:51:31,560 --> 00:51:33,560
那麼這個時候會變成說

1733
00:51:33,560 --> 00:51:35,560
那什麼樣的人在這樣的情況下

1734
00:51:35,560 --> 00:51:37,560
他還願意傳新聞呢

1735
00:51:37,560 --> 00:51:39,560
這是一個非常非常非常極端的人

1736
00:51:39,560 --> 00:51:41,560
然後如果說

1737
00:51:41,560 --> 00:51:43,560
這個

1738
00:51:43,560 --> 00:51:45,560
新聞是比較

1739
00:51:45,560 --> 00:51:47,560
lower credibility的話

1740
00:51:47,560 --> 00:51:49,560
你知道說因為lower credibility

1741
00:51:49,560 --> 00:51:51,560
他不太能去

1742
00:51:51,560 --> 00:51:53,560
改變大家的想法

1743
00:51:53,560 --> 00:51:55,560
因為看到他有點lower credibility

1744
00:51:55,560 --> 00:51:57,560
我可能還是會比較考慮到我原本的想法

1745
00:51:57,560 --> 00:51:59,560
所以這個時候其實

1746
00:51:59,560 --> 00:52:01,560
極端的人還是很多的

1747
00:52:01,560 --> 00:52:03,560
反而相對於如果你讓一個

1748
00:52:03,560 --> 00:52:05,560
fully credible放下去之後

1749
00:52:05,560 --> 00:52:07,560
很converge到同一個點

1750
00:52:07,560 --> 00:52:09,560
那個時候反而是沒有什麼extremism

1751
00:52:09,560 --> 00:52:11,560
所以這是為什麼在

1752
00:52:11,560 --> 00:52:13,560
在這種情況底下

1753
00:52:13,560 --> 00:52:15,560
lower credibility news can create logic

1754
00:52:15,560 --> 00:52:17,560
因為這個時候population裡面還是

1755
00:52:17,560 --> 00:52:19,560
很多的extremism

1756
00:52:19,560 --> 00:52:21,560
但是反而在你讓那個news

1757
00:52:21,560 --> 00:52:23,560
非常fully credible的時候反而沒有什麼

1758
00:52:23,560 --> 00:52:25,560
extremism所以就沒有

1759
00:52:25,560 --> 00:52:27,560
這種news spread的產生

1760
00:52:27,560 --> 00:52:29,560
就是說news spread的size會比較小

1761
00:52:31,560 --> 00:52:33,560
好

1762
00:52:33,560 --> 00:52:35,560
那最後一個階段比較有趣一點

1763
00:52:35,560 --> 00:52:37,560
就是說我們考慮到說

1764
00:52:37,560 --> 00:52:39,560
這個population的case

1765
00:52:39,560 --> 00:52:41,560
其實你可以大家就直接想像

1766
00:52:41,560 --> 00:52:43,560
每個人他的

1767
00:52:43,560 --> 00:52:45,560
either他是from共和黨

1768
00:52:45,560 --> 00:52:47,560
either from民主黨

1769
00:52:47,560 --> 00:52:49,560
如果他是from共和黨就是紅色這個

1770
00:52:49,560 --> 00:52:51,560
這個curve

1771
00:52:51,560 --> 00:52:53,560
然後他就是randomly drawn from

1772
00:52:53,560 --> 00:52:55,560
這個distribution

1773
00:52:55,560 --> 00:52:57,560
然後如果是藍色

1774
00:52:57,560 --> 00:52:59,560
如果他一開始是偏民主黨的話

1775
00:52:59,560 --> 00:53:01,560
那他可能就是在左邊

1776
00:53:01,560 --> 00:53:03,560
這個random這個population

1777
00:53:03,560 --> 00:53:05,560
跟黨內的這個

1778
00:53:05,560 --> 00:53:07,560
多元化的程度

1779
00:53:07,560 --> 00:53:09,560
第一個就是μbar

1780
00:53:09,560 --> 00:53:11,560
如果這個μbar很大的話就代表說population越強

1781
00:53:11,560 --> 00:53:13,560
兩端的這個peak是

1782
00:53:13,560 --> 00:53:15,560
離各自越遠

1783
00:53:15,560 --> 00:53:17,560
然後σμ的話就代表說

1784
00:53:17,560 --> 00:53:19,560
這個variance在

1785
00:53:19,560 --> 00:53:21,560
這個黨內的大小是多少

1786
00:53:21,560 --> 00:53:23,560
就大家可以只要記得這些事情就好了

1787
00:53:25,560 --> 00:53:27,560
然後這個圖呢

1788
00:53:27,560 --> 00:53:29,560
其實就最有趣的圖就是這樣子

1789
00:53:29,560 --> 00:53:31,560
就是說

1790
00:53:31,560 --> 00:53:33,560
給定的news和cost

1791
00:53:33,560 --> 00:53:35,560
然後我去問說

1792
00:53:35,560 --> 00:53:37,560
因為我剛才已經講了嘛

1793
00:53:37,560 --> 00:53:39,560
只要這個network

1794
00:53:39,560 --> 00:53:41,560
足夠的

1795
00:53:41,560 --> 00:53:43,560
就是只要你的平均的

1796
00:53:43,560 --> 00:53:45,560
follower數

1797
00:53:45,560 --> 00:53:47,560
超過了一個threshold之後呢

1798
00:53:47,560 --> 00:53:49,560
你就會使得這些

1799
00:53:49,560 --> 00:53:51,560
less credible news

1800
00:53:51,560 --> 00:53:53,560
傳得比較遠

1801
00:53:53,560 --> 00:53:55,560
所以我只需要去討論

1802
00:53:55,560 --> 00:53:57,560
到底這個population跟這個

1803
00:53:57,560 --> 00:53:59,560
diversity如何去影響

1804
00:53:59,560 --> 00:54:01,560
這樣子的一個threshold

1805
00:54:01,560 --> 00:54:03,560
如果我讓他的threshold降得更低

1806
00:54:03,560 --> 00:54:05,560
那代表有更多的network

1807
00:54:05,560 --> 00:54:07,560
可以使得這種

1808
00:54:07,560 --> 00:54:09,560
less credible news

1809
00:54:09,560 --> 00:54:11,560
如果我讓這個threshold變得更高

1810
00:54:11,560 --> 00:54:13,560
那代表說只有那一些

1811
00:54:13,560 --> 00:54:15,560
非常非常緊密的network

1812
00:54:15,560 --> 00:54:17,560
才有可能使得

1813
00:54:17,560 --> 00:54:19,560
less credible news就是傳得比較遠

1814
00:54:19,560 --> 00:54:21,560
所以這邊第一個

1815
00:54:21,560 --> 00:54:23,560
結論就是說

1816
00:54:23,560 --> 00:54:25,560
沿著這條綠色線你會發現說

1817
00:54:25,560 --> 00:54:27,560
把population變大的時候呢

1818
00:54:27,560 --> 00:54:29,560
你降低了

1819
00:54:29,560 --> 00:54:31,560
這個threshold

1820
00:54:31,560 --> 00:54:33,560
meaning that

1821
00:54:33,560 --> 00:54:35,560
其實有更多的network可以使得

1822
00:54:35,560 --> 00:54:37,560
就是甚至連不緊密的network

1823
00:54:37,560 --> 00:54:39,560
都可以使得這種

1824
00:54:39,560 --> 00:54:41,560
less credible news

1825
00:54:41,560 --> 00:54:43,560
傳得更遠

1826
00:54:43,560 --> 00:54:45,560
另外是

1827
00:54:45,560 --> 00:54:47,560
那就我剛才講的就是

1828
00:54:47,560 --> 00:54:49,560
這ingroup diversity到底能不能幫忙

1829
00:54:49,560 --> 00:54:51,560
改變這樣子的一個

1830
00:54:51,560 --> 00:54:53,560
態勢

1831
00:54:53,560 --> 00:54:55,560
你會發現說其實

1832
00:54:55,560 --> 00:54:57,560
第一可是如果說

1833
00:54:57,560 --> 00:54:59,560
這個population其實很小很小的時候

1834
00:54:59,560 --> 00:55:01,560
如果你在一個population

1835
00:55:01,560 --> 00:55:03,560
不大的一個社會

1836
00:55:03,560 --> 00:55:05,560
你去改變這個ingroup diversity的時候

1837
00:55:05,560 --> 00:55:07,560
會發生什麼事?其實你去

1838
00:55:07,560 --> 00:55:09,560
你這時候加了ingroup diversity

1839
00:55:09,560 --> 00:55:11,560
你其實反而是去增加了

1840
00:55:11,560 --> 00:55:13,560
更多的極端的那一些人的想法

1841
00:55:13,560 --> 00:55:15,560
就是你知道你把variance變大之後

1842
00:55:15,560 --> 00:55:17,560
就更多人在極端的那個部分

1843
00:55:17,560 --> 00:55:19,560
所以反而其實它會增加

1844
00:55:19,560 --> 00:55:21,560
那一些less credible news

1845
00:55:21,560 --> 00:55:23,560
被傳播的

1846
00:55:23,560 --> 00:55:25,560
可能性所以它就

1847
00:55:25,560 --> 00:55:27,560
導致那個spread size其實是

1848
00:55:27,560 --> 00:55:29,560
更容易變大

1849
00:55:29,560 --> 00:55:31,560
然後降低了這個threshold

1850
00:55:31,560 --> 00:55:33,560
network threshold

1851
00:55:33,560 --> 00:55:35,560
但如果說其實這個population

1852
00:55:35,560 --> 00:55:37,560
其實somehow還蠻大

1853
00:55:37,560 --> 00:55:39,560
大過這個news本身的話

1854
00:55:39,560 --> 00:55:41,560
其實如果你增加的話

1855
00:55:41,560 --> 00:55:43,560
其實是可以降低

1856
00:55:43,560 --> 00:55:45,560
這個news

1857
00:55:45,560 --> 00:55:47,560
這個less credible news被傳播的

1858
00:55:47,560 --> 00:55:49,560
大小

1859
00:55:49,560 --> 00:55:51,560
但是一旦到你

1860
00:55:51,560 --> 00:55:53,560
大過了某一個程度的時候

1861
00:55:53,560 --> 00:55:55,560
其實又反向的它會

1862
00:55:57,560 --> 00:55:59,560
使得這個less credible news

1863
00:55:59,560 --> 00:56:01,560
被傳播的更遠

1864
00:56:01,560 --> 00:56:03,560
原因是因為當你把diversity

1865
00:56:03,560 --> 00:56:05,560
這樣增高的太快的時候

1866
00:56:05,560 --> 00:56:07,560
其實你又

1867
00:56:07,560 --> 00:56:09,560
再一次的使得這個

1868
00:56:09,560 --> 00:56:11,560
社會出現了很多的這種極端的

1869
00:56:11,560 --> 00:56:13,560
想法的人,然後他們其實都很願意

1870
00:56:13,560 --> 00:56:15,560
去傳一些less credible news

1871
00:56:15,560 --> 00:56:17,560
所以

1872
00:56:17,560 --> 00:56:19,560
為什麼其實include diversity

1873
00:56:19,560 --> 00:56:21,560
你去增加它,有時候是可以的

1874
00:56:21,560 --> 00:56:23,560
但是增加太過不是一件好事

1875
00:56:27,560 --> 00:56:29,560
你要conclude,但是我問一下最後一個點

1876
00:56:29,560 --> 00:56:31,560
你剛剛是說diversity增加的時候

1877
00:56:31,560 --> 00:56:33,560
極端的人

1878
00:56:33,560 --> 00:56:35,560
就會變多

1879
00:56:35,560 --> 00:56:37,560
對對

1880
00:56:37,560 --> 00:56:39,560
我給大家看一下這個圖就好了

1881
00:56:39,560 --> 00:56:41,560
就是說

1882
00:56:41,560 --> 00:56:43,560
因為我現在都是等於是說給定一個數字

1883
00:56:43,560 --> 00:56:45,560
我只

1884
00:56:45,560 --> 00:56:47,560
變換了另外一個的東西

1885
00:56:47,560 --> 00:56:49,560
的magnitude

1886
00:56:49,560 --> 00:56:51,560
對吧

1887
00:56:51,560 --> 00:56:53,560
所以其實你看這個圖

1888
00:56:53,560 --> 00:56:55,560
只要看紅色的部分就好了

1889
00:56:55,560 --> 00:56:57,560
假設紅色的peak固定在1這個點

1890
00:56:57,560 --> 00:56:59,560
其實它就固定在1這個點

1891
00:56:59,560 --> 00:57:01,560
你把這個variance變大的時候

1892
00:57:01,560 --> 00:57:03,560
其實它同時

1893
00:57:03,560 --> 00:57:05,560
以distribution來講兩邊

1894
00:57:05,560 --> 00:57:07,560
其實它有點把

1895
00:57:07,560 --> 00:57:09,560
其實是有把一些

1896
00:57:09,560 --> 00:57:11,560
extremist推得更極端

1897
00:57:11,560 --> 00:57:13,560
所以導致說

1898
00:57:13,560 --> 00:57:15,560
對

1899
00:57:15,560 --> 00:57:17,560
這感覺是一個很強的assumption

1900
00:57:17,560 --> 00:57:19,560
因為第一個是你不要先假設

1901
00:57:19,560 --> 00:57:21,560
右派或左派

1902
00:57:21,560 --> 00:57:23,560
它的分佈是高懸

1903
00:57:23,560 --> 00:57:25,560
然後第二個是

1904
00:57:25,560 --> 00:57:27,560
第二個是

1905
00:57:27,560 --> 00:57:29,560
他們

1906
00:57:29,560 --> 00:57:31,560
對啊這個好像

1907
00:57:31,560 --> 00:57:33,560
非常複雜

1908
00:57:33,560 --> 00:57:35,560
假設你變得比較寬一點

1909
00:57:35,560 --> 00:57:37,560
在你這個情況裡面就diversity變高

1910
00:57:37,560 --> 00:57:39,560
那

1911
00:57:39,560 --> 00:57:41,560
這些比較extreme的人

1912
00:57:41,560 --> 00:57:43,560
群體的影響到底是什麼

1913
00:57:43,560 --> 00:57:45,560
好像

1914
00:57:45,560 --> 00:57:47,560
等於是說

1915
00:57:47,560 --> 00:57:49,560
在我們這個model裡面其實就是說

1916
00:57:49,560 --> 00:57:51,560
我剛才講extremist他沒有很強

1917
00:57:51,560 --> 00:57:53,560
比那種moderate鐵定

1918
00:57:53,560 --> 00:57:55,560
他的強

1919
00:57:55,560 --> 00:57:57,560
因為我剛才其實有看

1920
00:57:57,560 --> 00:57:59,560
那個utility function的話你會知道說

1921
00:57:59,560 --> 00:58:01,560
他如果覺得大家投錯的話對他來講

1922
00:58:01,560 --> 00:58:03,560
傷害比較大

1923
00:58:03,560 --> 00:58:05,560
他希望大家真的去投對

1924
00:58:05,560 --> 00:58:07,560
所以像

1925
00:58:07,560 --> 00:58:09,560
這個情況底下就是說其實如果

1926
00:58:09,560 --> 00:58:11,560
很多人都還蠻

1927
00:58:11,560 --> 00:58:13,560
polarization不大的時候

1928
00:58:13,560 --> 00:58:15,560
反而你去增加這個variance的時候

1929
00:58:15,560 --> 00:58:17,560
你其實是把很多人都push away到那個極端去

1930
00:58:17,560 --> 00:58:19,560
所以才會導致說其實那個時候

1931
00:58:19,560 --> 00:58:21,560
如果這個社會已經不是很分化的話

1932
00:58:21,560 --> 00:58:23,560
其實你沒有必要去增加diversity

1933
00:58:23,560 --> 00:58:25,560
但

1934
00:58:25,560 --> 00:58:27,560
當你這個社會比較極端一點

1935
00:58:27,560 --> 00:58:29,560
就像你剛才講的

1936
00:58:29,560 --> 00:58:31,560
然後你把variance變大的時候

1937
00:58:31,560 --> 00:58:33,560
其實你一開始是有點降低了

1938
00:58:33,560 --> 00:58:35,560
在極端的那一部分的人的

1939
00:58:35,560 --> 00:58:37,560
數量

1940
00:58:37,560 --> 00:58:39,560
但是其實

1941
00:58:39,560 --> 00:58:41,560
比較詭異的事情是如果你增加diversity

1942
00:58:41,560 --> 00:58:43,560
你反而增加了

1943
00:58:43,560 --> 00:58:45,560
然後原本他是民主黨的人

1944
00:58:45,560 --> 00:58:47,560
結果你增加了diversity之後反而他居然有一些

1945
00:58:47,560 --> 00:58:49,560
很極端在民主黨裡面的人

1946
00:58:49,560 --> 00:58:51,560
居然去支持了川普

1947
00:58:51,560 --> 00:58:53,560
是蠻奇怪的啦

1948
00:58:53,560 --> 00:58:55,560
但是就是這是一個你知道

1949
00:58:55,560 --> 00:58:57,560
在分析上說我們導出

1950
00:58:57,560 --> 00:58:59,560
這樣的結果

1951
00:58:59,560 --> 00:59:01,560
然後的確他

1952
00:59:01,560 --> 00:59:03,560
他其實也不是我講Gaussian like

1953
00:59:03,560 --> 00:59:05,560
他也不是只有Gaussian

1954
00:59:05,560 --> 00:59:07,560
他還是有一些

1955
00:59:07,560 --> 00:59:09,560
我沒有把他們比較generalize一點

1956
00:59:09,560 --> 00:59:11,560
但是就是說他還是在某一個class的function

1957
00:59:11,560 --> 00:59:13,560
底下

1958
00:59:13,560 --> 00:59:15,560
就是他會滿足這樣子的現象

1959
00:59:15,560 --> 00:59:17,560
但如果你考慮了一些function

1960
00:59:17,560 --> 00:59:19,560
out of那個class的話

1961
00:59:19,560 --> 00:59:21,560
那可能就會有不一樣的結果

1962
00:59:21,560 --> 00:59:23,560
但是對這是我們目前得到的一些insight

1963
00:59:23,560 --> 00:59:25,560
ok

1964
00:59:25,560 --> 00:59:27,560
對然後

1965
00:59:27,560 --> 00:59:29,560
然後我就conclude

1966
00:59:29,560 --> 00:59:31,560
等於是說我們

1967
00:59:31,560 --> 00:59:33,560
provide這個model

1968
00:59:33,560 --> 00:59:35,560
去study some new sharing decision

1969
00:59:35,560 --> 00:59:37,560
然後還有非常這種individual sharing decision

1970
00:59:37,560 --> 00:59:39,560
什麼樣的新聞會傳得比較遠

1971
00:59:39,560 --> 00:59:41,560
然後包含像

1972
00:59:41,560 --> 00:59:43,560
他怎麼跟這個

1973
00:59:45,560 --> 00:59:47,560
這個社會的一些特性有關

1974
00:59:47,560 --> 00:59:49,560
然後我們就

1975
00:59:49,560 --> 00:59:51,560
在裡頭我們會發現其實

1976
00:59:51,560 --> 00:59:53,560
polarization一旦你把它增大的話

1977
00:59:53,560 --> 00:59:55,560
鐵定就是會使得這種

1978
00:59:55,560 --> 00:59:57,560
less credible news

1979
00:59:57,560 --> 00:59:59,560
更容易被傳

1980
00:59:59,560 --> 01:00:01,560
因為他其實somehow增加兩端

1981
01:00:01,560 --> 01:00:03,560
extremist的

1982
01:00:03,560 --> 01:00:05,560
數量

1983
01:00:05,560 --> 01:00:07,560
所以他其實使得這個社會

1984
01:00:07,560 --> 01:00:09,560
更容易去傳的這種less credible news

1985
01:00:09,560 --> 01:00:11,560
那另外就是說如果你去增加

1986
01:00:11,560 --> 01:00:13,560
這種perspective discrepancy

1987
01:00:13,560 --> 01:00:15,560
就是我們剛才在討論

1988
01:00:15,560 --> 01:00:17,560
跟Sean在討論的問題就是說

1989
01:00:17,560 --> 01:00:19,560
其實

1990
01:00:19,560 --> 01:00:21,560
it really depends on

1991
01:00:21,560 --> 01:00:23,560
你到底怎麼樣影響

1992
01:00:23,560 --> 01:00:25,560
那些extremist的分布

1993
01:00:25,560 --> 01:00:27,560
好謝謝大家

1994
01:00:27,560 --> 01:00:29,560
我希望這個

1995
01:00:29,560 --> 01:00:31,560
就是如果大家真的很感興趣

1996
01:00:31,560 --> 01:00:33,560
裡頭那些technical的部分的話

1997
01:00:33,560 --> 01:00:35,560
就可以看一下

1998
01:00:35,560 --> 01:00:37,560
那個paper

1999
01:00:37,560 --> 01:00:39,560
因為我其實obstruct蠻多的

2000
01:00:39,560 --> 01:00:41,560
technical的部分

2001
01:00:41,560 --> 01:00:43,560
大家可能有些地方有點聽到了

2002
01:00:43,560 --> 01:00:45,560
有點覺得好像不太懂

2003
01:00:45,560 --> 01:00:47,560
對所以不好意思

2004
01:00:47,560 --> 01:00:49,560
不好意思我可以再問

2005
01:00:49,560 --> 01:00:51,560
再追問一下那個diversity

2006
01:00:51,560 --> 01:00:53,560
的問題嗎

2007
01:00:53,560 --> 01:00:55,560
有一個情況是

2008
01:00:55,560 --> 01:00:57,560
如果我們單純考慮你剛剛的

2009
01:00:57,560 --> 01:00:59,560
兩個高懸

2010
01:00:59,560 --> 01:01:01,560
譬如說右邊的那個

2011
01:01:01,560 --> 01:01:03,560
distribution他的

2012
01:01:03,560 --> 01:01:05,560
極端值

2013
01:01:05,560 --> 01:01:07,560
他的左側極端值

2014
01:01:07,560 --> 01:01:09,560
其實會接近另一個distribution的

2015
01:01:09,560 --> 01:01:11,560
中間值

2016
01:01:11,560 --> 01:01:13,560
他的tail如果沒有這麼大的話

2017
01:01:13,560 --> 01:01:15,560
所以換句話說在某些情況底下

2018
01:01:15,560 --> 01:01:17,560
在某一個檔裡面的比較extreme的人

2019
01:01:17,560 --> 01:01:19,560
可能反而是比較

2020
01:01:19,560 --> 01:01:21,560
就是

2021
01:01:21,560 --> 01:01:23,560
接近兩群人的比較中間的分布

2022
01:01:23,560 --> 01:01:25,560
懂我意思嗎譬如說

2023
01:01:25,560 --> 01:01:27,560
就像你剛剛舉的那個例子

2024
01:01:27,560 --> 01:01:29,560
假設我今天是民主黨的人

2025
01:01:29,560 --> 01:01:31,560
然後我去支持一個共和黨的候選人

2026
01:01:31,560 --> 01:01:33,560
就Trump是一個極端中的極端

2027
01:01:33,560 --> 01:01:35,560
我們就假設一個一般的正常的共和黨候選人

2028
01:01:35,560 --> 01:01:37,560
那可能表示的不是

2029
01:01:37,560 --> 01:01:39,560
就是同時表示的

2030
01:01:39,560 --> 01:01:41,560
我是民主黨裡面比較極端的人

2031
01:01:41,560 --> 01:01:43,560
可是同時在整群人裡面的distribution裡面

2032
01:01:43,560 --> 01:01:45,560
我可能是比較接近中間的

2033
01:01:45,560 --> 01:01:47,560
懂我意思嗎

2034
01:01:47,560 --> 01:01:49,560
對

2035
01:01:49,560 --> 01:01:51,560
換句話說就是

2036
01:01:51,560 --> 01:01:53,560
每一個distribution裡面都有兩端

2037
01:01:53,560 --> 01:01:55,560
那一端就會是真正的extremist

2038
01:01:55,560 --> 01:01:57,560
對不對

2039
01:01:57,560 --> 01:01:59,560
就是那一端譬如說

2040
01:01:59,560 --> 01:02:01,560
他是左派中的左派或右派中的右派

2041
01:02:01,560 --> 01:02:03,560
可是如果是右派中的左派

2042
01:02:03,560 --> 01:02:05,560
跟左派中的右派呢

2043
01:02:05,560 --> 01:02:07,560
好像他反而會往群體之間集中

2044
01:02:07,560 --> 01:02:09,560
那他們兩群人表現出來的行為

2045
01:02:09,560 --> 01:02:11,560
好像會有一點點不太一樣

2046
01:02:11,560 --> 01:02:13,560
我也有類似的問題

2047
01:02:13,560 --> 01:02:15,560
就是這個右下角這個圖

2048
01:02:15,560 --> 01:02:17,560
比如說

2049
01:02:17,560 --> 01:02:19,560
為什麼在zero

2050
01:02:19,560 --> 01:02:21,560
就是說在紅色zero左邊的還是紅色

2051
01:02:21,560 --> 01:02:23,560
他就不是變成藍色

2052
01:02:23,560 --> 01:02:25,560
就是那個

2053
01:02:25,560 --> 01:02:27,560
我這邊的紅色跟藍色

2054
01:02:27,560 --> 01:02:29,560
就是說他一開始他挑的黨是什麼

2055
01:02:29,560 --> 01:02:31,560
然後但是他可能在這件事情

2056
01:02:31,560 --> 01:02:33,560
大家在這件事情裡頭

2057
01:02:33,560 --> 01:02:35,560
你知道就像

2058
01:02:35,560 --> 01:02:37,560
共和黨裡面也有人覺得

2059
01:02:37,560 --> 01:02:39,560
他不喜歡川普這種感覺

2060
01:02:39,560 --> 01:02:41,560
所以這個grouping跟這個

2061
01:02:41,560 --> 01:02:43,560
vote decision是

2062
01:02:43,560 --> 01:02:45,560
兩件

2063
01:02:45,560 --> 01:02:47,560
這不是一個overlap的事情

2064
01:02:47,560 --> 01:02:49,560
對等於是說

2065
01:02:49,560 --> 01:02:51,560
這個model就是說一開始我們要怎麼樣去決定

2066
01:02:51,560 --> 01:02:53,560
每一個人的perspective

2067
01:02:53,560 --> 01:02:55,560
就是他的perspective是怎麼被randomly抽出來

2068
01:02:55,560 --> 01:02:57,560
一開始就說我先投一個硬幣

2069
01:02:57,560 --> 01:02:59,560
一半一半他是共和黨一半是民主黨

2070
01:02:59,560 --> 01:03:01,560
但是他是民主黨之後我就知道說

2071
01:03:01,560 --> 01:03:03,560
他可能concentrate在那個peak

2072
01:03:03,560 --> 01:03:05,560
在E那個附近

2073
01:03:05,560 --> 01:03:07,560
但他也有可能在這件事情他可能是

2074
01:03:07,560 --> 01:03:09,560
你知道他可能非常極端

2075
01:03:09,560 --> 01:03:11,560
他跑去支持了偏民主黨的部分

2076
01:03:11,560 --> 01:03:13,560
對

2077
01:03:13,560 --> 01:03:15,560
然後

2078
01:03:15,560 --> 01:03:17,560
其實剛才根據那個Sean講的東西

2079
01:03:17,560 --> 01:03:19,560
就是說其實

2080
01:03:19,560 --> 01:03:21,560
呃

2081
01:03:21,560 --> 01:03:23,560
我想一下因為

2082
01:03:23,560 --> 01:03:25,560
等於是說你可以

2083
01:03:25,560 --> 01:03:27,560
我可以再elaborate一下意思就是說

2084
01:03:27,560 --> 01:03:29,560
當有在某一群人裡面的

2085
01:03:29,560 --> 01:03:31,560
polarization

2086
01:03:31,560 --> 01:03:33,560
在你這個裡面第一是diversity

2087
01:03:33,560 --> 01:03:35,560
在那群人裡面的diversity產生的時候

2088
01:03:35,560 --> 01:03:37,560
diversify產生的時候

2089
01:03:37,560 --> 01:03:39,560
一邊的人其實是會往

2090
01:03:39,560 --> 01:03:41,560
average那邊

2091
01:03:41,560 --> 01:03:43,560
就是移過去

2092
01:03:43,560 --> 01:03:45,560
然後另一個人其實是整整的

2093
01:03:45,560 --> 01:03:47,560
就是extremist

2094
01:03:47,560 --> 01:03:49,560
那這兩群人的行為感覺

2095
01:03:49,560 --> 01:03:51,560
是可以分開來討論的

2096
01:03:51,560 --> 01:03:53,560
對對對其實應該這樣講

2097
01:03:53,560 --> 01:03:55,560
為什麼我一開始說

2098
01:03:55,560 --> 01:03:57,560
其實如果polarization變大的時候

2099
01:03:57,560 --> 01:03:59,560
其實一開始你增加diversity

2100
01:03:59,560 --> 01:04:01,560
不是反而更好嗎

2101
01:04:01,560 --> 01:04:03,560
就你還記得我

2102
01:04:03,560 --> 01:04:05,560
你還記得我剛才那個結論其實是

2103
01:04:05,560 --> 01:04:07,560
呃

2104
01:04:07,560 --> 01:04:09,560
下面的嗎就是說其實

2105
01:04:09,560 --> 01:04:11,560
increasing是好的

2106
01:04:11,560 --> 01:04:13,560
因為你讓那個threshold變大就是變得更難的意思

2107
01:04:13,560 --> 01:04:15,560
所以就是說

2108
01:04:15,560 --> 01:04:17,560
一開始其實你在

2109
01:04:17,560 --> 01:04:19,560
你知道它其實它都有一個相對的極端值

2110
01:04:19,560 --> 01:04:21,560
就是如果你polarization已經很大

2111
01:04:21,560 --> 01:04:23,560
就等於說你的peak其實有點落在

2112
01:04:23,560 --> 01:04:25,560
其實不是1可能是落在一個你知道

2113
01:04:25,560 --> 01:04:27,560
3的位置然後如果你這個時候

2114
01:04:27,560 --> 01:04:29,560
然後這個新聞其實比較

2115
01:04:29,560 --> 01:04:31,560
沒那麼極端

2116
01:04:31,560 --> 01:04:33,560
然後等於是說如果你

2117
01:04:33,560 --> 01:04:35,560
把diversity降低的時候

2118
01:04:35,560 --> 01:04:37,560
你等於是somehow你把那個distribution

2119
01:04:37,560 --> 01:04:39,560
壓平了對吧

2120
01:04:39,560 --> 01:04:41,560
所以其實你也把

2121
01:04:41,560 --> 01:04:43,560
diversity升高吧

2122
01:04:43,560 --> 01:04:45,560
diversity升高之後其實你有點

2123
01:04:45,560 --> 01:04:47,560
那個shape原本是這樣子

2124
01:04:47,560 --> 01:04:49,560
因為diversity小的時候

2125
01:04:49,560 --> 01:04:51,560
其實它就比較寬

2126
01:04:51,560 --> 01:04:53,560
對但你看變寬的時候其實你會發現說

2127
01:04:53,560 --> 01:04:55,560
其實這個越extremist

2128
01:04:55,560 --> 01:04:57,560
原本是偏右的extremist

2129
01:04:57,560 --> 01:04:59,560
那個數量其實是會減低的

2130
01:04:59,560 --> 01:05:01,560
是會減低

2131
01:05:01,560 --> 01:05:03,560
因為它對吧因為你現在

2132
01:05:03,560 --> 01:05:05,560
假設你peak在這邊

2133
01:05:05,560 --> 01:05:07,560
然後如果你把它壓低的話

2134
01:05:07,560 --> 01:05:09,560
很多人其實開始往左邊跑

2135
01:05:09,560 --> 01:05:11,560
另外的為什麼到後來

2136
01:05:11,560 --> 01:05:13,560
它又開始有點

2137
01:05:13,560 --> 01:05:15,560
反向的又變化是因為

2138
01:05:15,560 --> 01:05:17,560
就我剛才講的就其實就像你講的

2139
01:05:17,560 --> 01:05:19,560
剛才民主黨有一些人他原本

2140
01:05:19,560 --> 01:05:21,560
你知道接近那個民主黨的peak

2141
01:05:21,560 --> 01:05:23,560
但是你一旦把那個diversity變高的時候

2142
01:05:23,560 --> 01:05:25,560
它開始去接近了

2143
01:05:25,560 --> 01:05:27,560
共和黨的部分它可能一開始在moderate

2144
01:05:27,560 --> 01:05:29,560
那個時候可能還在

2145
01:05:29,560 --> 01:05:31,560
increasing的階段但是一旦它

2146
01:05:31,560 --> 01:05:33,560
也變成了共和黨的

2147
01:05:33,560 --> 01:05:35,560
extremist的時候它就變decreasing

2148
01:05:35,560 --> 01:05:37,560
所以那個是就是那個

2149
01:05:37,560 --> 01:05:39,560
consistion的變化是這樣

2150
01:05:39,560 --> 01:05:41,560
所以比較像是

2151
01:05:41,560 --> 01:05:43,560
比較像是在model

2152
01:05:43,560 --> 01:05:45,560
裡面比較會產生的事情

2153
01:05:45,560 --> 01:05:47,560
現實狀況好像不太容易產生

2154
01:05:47,560 --> 01:05:49,560
對等於說現實狀況

2155
01:05:49,560 --> 01:05:51,560
底下是說

2156
01:05:51,560 --> 01:05:53,560
pratically就是說問題在於

2157
01:05:53,560 --> 01:05:55,560
你要怎麼measure這個polarization

2158
01:05:55,560 --> 01:05:57,560
跟那個diversity就是說

2159
01:05:57,560 --> 01:05:59,560
我覺得還是可以somehow可以用Gaussian like

2160
01:05:59,560 --> 01:06:01,560
去model這個問題

2161
01:06:01,560 --> 01:06:03,560
就是說這個diversity的那個量度

2162
01:06:03,560 --> 01:06:05,560
到底在哪裡還有那個polarization

2163
01:06:05,560 --> 01:06:07,560
然後還有那個news本身

2164
01:06:07,560 --> 01:06:09,560
所以就變成是這些問題

2165
01:06:09,560 --> 01:06:11,560
就是說你可能會覺得

2166
01:06:11,560 --> 01:06:13,560
這個嗎給一個8好像有點誇張

2167
01:06:13,560 --> 01:06:15,560
但是有可能現實生活中其實我們可能只考慮

2168
01:06:15,560 --> 01:06:17,560
那個2-4的那個範圍

2169
01:06:17,560 --> 01:06:19,560
對就是

2170
01:06:19,560 --> 01:06:21,560
有點像是給大家

2171
01:06:21,560 --> 01:06:23,560
一個idea說這個趨勢會是怎麼樣

2172
01:06:23,560 --> 01:06:25,560
但實際上在哪一個範圍就不知道

2173
01:06:25,560 --> 01:06:27,560
了解了解

2174
01:06:27,560 --> 01:06:29,560
不好意思我再追問一個問題

2175
01:06:29,560 --> 01:06:31,560
就是因為你的那個credibility是

2176
01:06:31,560 --> 01:06:33,560
基本上在0跟1

2177
01:06:33,560 --> 01:06:35,560
換句話來說你的

2178
01:06:35,560 --> 01:06:37,560
less credible或者是

2179
01:06:37,560 --> 01:06:39,560
lower credibility其實意思是相對於

2180
01:06:39,560 --> 01:06:41,560
1來講

2181
01:06:41,560 --> 01:06:43,560
相對於

2182
01:06:43,560 --> 01:06:45,560
fully credible的news來講

2183
01:06:45,560 --> 01:06:47,560
你往低一點點的地方走

2184
01:06:47,560 --> 01:06:49,560
反而傳播的會就是尺度會比較大

2185
01:06:49,560 --> 01:06:51,560
那

2186
01:06:51,560 --> 01:06:53,560
會不會對應到

2187
01:06:53,560 --> 01:06:55,560
真實世界上你的

2188
01:06:55,560 --> 01:06:57,560
lower credibility的news

2189
01:06:57,560 --> 01:06:59,560
其實就是像CNN這樣的outlet

2190
01:06:59,560 --> 01:07:01,560
因為在現實世界上不會有

2191
01:07:01,560 --> 01:07:03,560
真正是1的news outlet

2192
01:07:03,560 --> 01:07:05,560
所以你的

2193
01:07:05,560 --> 01:07:07,560
less credible news傳的比較遠

2194
01:07:07,560 --> 01:07:09,560
其實是對應到真實世界的

2195
01:07:09,560 --> 01:07:11,560
我們覺得有很高的credible

2196
01:07:11,560 --> 01:07:13,560
的那些news outlet或者是news

2197
01:07:13,560 --> 01:07:15,560
對

2198
01:07:15,560 --> 01:07:17,560
我們也有在想

2199
01:07:17,560 --> 01:07:19,560
這個問題就是說

2200
01:07:23,560 --> 01:07:25,560
對因為

2201
01:07:25,560 --> 01:07:27,560
我們的確有在想過這個問題就是說

2202
01:07:27,560 --> 01:07:29,560
其實生活中其實不是

2203
01:07:29,560 --> 01:07:31,560
那麼的fully credible news

2204
01:07:31,560 --> 01:07:33,560
但是

2205
01:07:33,560 --> 01:07:35,560
但是你還是可以知道說

2206
01:07:35,560 --> 01:07:37,560
就是

2207
01:07:37,560 --> 01:07:39,560
因為這個地方可能只有一些

2208
01:07:39,560 --> 01:07:41,560
quotative的case就是它只告訴你說

2209
01:07:41,560 --> 01:07:43,560
跟1比的話它

2210
01:07:43,560 --> 01:07:45,560
下降你把1往下降之後

2211
01:07:45,560 --> 01:07:47,560
它可能這個

2212
01:07:47,560 --> 01:07:49,560
scale會變大spread size會變大

2213
01:07:49,560 --> 01:07:51,560
但它沒有

2214
01:07:51,560 --> 01:07:53,560
足夠的訊息告訴你說它到底是

2215
01:07:55,560 --> 01:07:57,560
哪一段它是會變大的

2216
01:07:57,560 --> 01:07:59,560
我只有告訴你那個趨勢是會變大

2217
01:07:59,560 --> 01:08:01,560
但是你可能說說不定它

2218
01:08:01,560 --> 01:08:03,560
1到0.95的時候是變大

2219
01:08:03,560 --> 01:08:05,560
但是0.95之後又下去了

2220
01:08:05,560 --> 01:08:07,560
對吧有可能它的peak是發展在0.9

2221
01:08:07,560 --> 01:08:09,560
所以那個地方我就

2222
01:08:09,560 --> 01:08:11,560
不知道了

2223
01:08:11,560 --> 01:08:13,560
所以你們現在可以估計出來

2224
01:08:13,560 --> 01:08:15,560
它的peak大概會長在什麼地方

2225
01:08:15,560 --> 01:08:17,560
現在是沒有辦法

2226
01:08:17,560 --> 01:08:19,560
因為它這個等於你要solve

2227
01:08:19,560 --> 01:08:21,560
那個optimization question

2228
01:08:21,560 --> 01:08:23,560
然後就會等於是

2229
01:08:23,560 --> 01:08:25,560
所以我們後來就只有

2230
01:08:25,560 --> 01:08:27,560
比較像是告訴你說

2231
01:08:27,560 --> 01:08:29,560
所以我們的argument都比較偏向

2232
01:08:29,560 --> 01:08:31,560
less credible跟

2233
01:08:31,560 --> 01:08:33,560
fully credible

2234
01:08:33,560 --> 01:08:35,560
因為我猜測就是

2235
01:08:35,560 --> 01:08:37,560
因為講less credible的時候大家心目中

2236
01:08:37,560 --> 01:08:39,560
跑出來的東西有點不太一樣

2237
01:08:39,560 --> 01:08:41,560
因為在你的model裡面

2238
01:08:41,560 --> 01:08:43,560
less credible的意思就是相對於

2239
01:08:43,560 --> 01:08:45,560
比較不credible嘛

2240
01:08:45,560 --> 01:08:47,560
那我剛剛問這個問題

2241
01:08:47,560 --> 01:08:49,560
應該是下一個問題其實就是

2242
01:08:49,560 --> 01:08:51,560
如果你們能夠預測出來

2243
01:08:51,560 --> 01:08:53,560
你們可以estimate出來那個peak在哪裡

2244
01:08:53,560 --> 01:08:55,560
有可能你把那個model應用到

2245
01:08:55,560 --> 01:08:57,560
真實生活中的時候

2246
01:08:57,560 --> 01:08:59,560
你可以來算每一個新聞outlet的peak

2247
01:08:59,560 --> 01:09:01,560
那或許算出來你們的

2248
01:09:01,560 --> 01:09:03,560
less credible的

2249
01:09:03,560 --> 01:09:05,560
在你們model裡面的news outlet

2250
01:09:05,560 --> 01:09:07,560
對應到真實世界其實就是我們覺得

2251
01:09:07,560 --> 01:09:09,560
比較有credible的那些

2252
01:09:09,560 --> 01:09:11,560
就是如果兩邊可以對應起來的話

2253
01:09:11,560 --> 01:09:13,560
故事就會通

2254
01:09:13,560 --> 01:09:15,560
應該這樣講就是說

2255
01:09:15,560 --> 01:09:17,560
我

2256
01:09:17,560 --> 01:09:19,560
我們有所有的equation你可以用

2257
01:09:19,560 --> 01:09:21,560
就甚至你要用

2258
01:09:21,560 --> 01:09:23,560
用那種

2259
01:09:23,560 --> 01:09:25,560
去算那個peak也可以

2260
01:09:25,560 --> 01:09:27,560
只是

2261
01:09:27,560 --> 01:09:29,560
因為我沒有辦法去證明說那個peak

2262
01:09:29,560 --> 01:09:31,560
到底是

2263
01:09:31,560 --> 01:09:33,560
會落在什麼區間你知道嗎

2264
01:09:33,560 --> 01:09:35,560
就是我能mathematically

2265
01:09:35,560 --> 01:09:37,560
proof的東西就是告訴你說

2266
01:09:37,560 --> 01:09:39,560
它有這個趨勢就是

2267
01:09:39,560 --> 01:09:41,560
那個condition是什麼但是

2268
01:09:41,560 --> 01:09:43,560
我們有所有的equation就是你可以去

2269
01:09:43,560 --> 01:09:45,560
用collaboration就是說去算

2270
01:09:45,560 --> 01:09:47,560
那到底那個peak會長什麼

2271
01:09:47,560 --> 01:09:49,560
在哪個位置如果你用computational

2272
01:09:49,560 --> 01:09:51,560
可以算只是說

2273
01:09:51,560 --> 01:09:53,560
我沒有辦法mathematically proof

2274
01:09:53,560 --> 01:09:55,560
那個peak的點在哪裡

2275
01:09:55,560 --> 01:09:57,560
或者是它到哪裡會下降

2276
01:09:57,560 --> 01:09:59,560
就有點

2277
01:09:59,560 --> 01:10:01,560
那個就對我們來講比較難

2278
01:10:01,560 --> 01:10:03,560
謝謝

2279
01:10:09,560 --> 01:10:11,560
主持人回來了

2280
01:10:11,560 --> 01:10:13,560
就是我們這個演講

2281
01:10:13,560 --> 01:10:15,560
基本上是一個小時

2282
01:10:15,560 --> 01:10:17,560
之前跟靜嘉講是一個小時

2283
01:10:17,560 --> 01:10:19,560
大概50分鐘的演講然後大概10到

2284
01:10:19,560 --> 01:10:21,560
20分鐘的Q&A討論

2285
01:10:21,560 --> 01:10:23,560
但是我們今天的形式所以變得比較像是

2286
01:10:23,560 --> 01:10:25,560
就是在演講過程中

2287
01:10:25,560 --> 01:10:27,560
就參雜了Q&A

2288
01:10:27,560 --> 01:10:29,560
所以我基本上就是當作我們已經

2289
01:10:29,560 --> 01:10:31,560
把Q&A結束了所以現在就是看說

2290
01:10:31,560 --> 01:10:33,560
靜嘉願不願意再多

2291
01:10:33,560 --> 01:10:35,560
留一點時間然後

2292
01:10:35,560 --> 01:10:37,560
可以再繼續做討論那我不知道你

2293
01:10:37,560 --> 01:10:39,560
大概願意再多大概10分鐘嗎

2294
01:10:39,560 --> 01:10:41,560
再繼續簡單討論

2295
01:10:43,560 --> 01:10:45,560
可以3點半嗎

2296
01:10:45,560 --> 01:10:47,560
3點半因為我剛好跟嘉佑約了

2297
01:10:47,560 --> 01:10:49,560
3點半沒關係

2298
01:10:49,560 --> 01:10:51,560
ok所以就是7分鐘後

2299
01:10:51,560 --> 01:10:53,560
對但大家也可以

2300
01:10:53,560 --> 01:10:55,560
就是如果還有什麼其他問題的話

2301
01:10:55,560 --> 01:10:57,560
可以就是email我

2302
01:10:57,560 --> 01:10:59,560
然後我可以在這邊先answer一些

2303
01:10:59,560 --> 01:11:01,560
就是可能比較大方向的question

2304
01:11:01,560 --> 01:11:03,560
好那我先做

2305
01:11:03,560 --> 01:11:05,560
另外一件事情就是我可不可以

2306
01:11:05,560 --> 01:11:07,560
我就先把錄影先關掉了

2307
01:11:07,560 --> 01:11:09,560
然後就是大家就

2308
01:11:09,560 --> 01:11:11,560
對好那我就先關錄影

