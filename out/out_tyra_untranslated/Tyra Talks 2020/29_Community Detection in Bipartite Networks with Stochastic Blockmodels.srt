1
00:00:00,000 --> 00:00:05,000
整理&字幕志願者 翻譯&字幕小組

2
00:00:30,000 --> 00:00:43,000
然後等一下大家在他的演講裡面都會聽到更多這方面的細節,然後他的專長是做這個Community Detection相關的題目,大家等一下就會聽到。

3
00:00:43,000 --> 00:01:09,000
我先稍微講一下紫棋的背景。紫棋在大學的時候念的是生物化學、生化研的東西,這讓我想到我一些做網絡的朋友。好像做網絡的朋友,大家都會斜槓去做很多其他的事情,結果被化學耽誤的網絡學家,後來還是得做網絡學家的那種感覺。

4
00:01:10,000 --> 00:01:32,000
在念完大學之後,紫棋做了很多相關的研究助理或者是程式設計師的工作,後來就慢慢地轉往資訊方面,從資訊科學的角度來切入網絡的科學。

5
00:01:32,000 --> 00:02:01,000
所有的資訊,紫棋對主持人來說非常有幫助,他有一個非常資訊完整的個人網頁。如果各位有興趣的話,都可以在網頁上找到,包括最近的活動,因為他有做很多網絡科學的推廣活動,最近的活動的schedule,大家可以去follow一下。

6
00:02:02,000 --> 00:02:14,000
在網頁上的文章等等,當然你都是有的,大家都可以去看。廢話不多說,讓我來代表大家熱烈地鼓掌,歡迎我們今天的講者紫棋。

7
00:02:15,000 --> 00:02:30,000
謝謝大家來聽我演講。我今天準備了大概五十張投影片,目標是在一個小時內講完。

8
00:02:31,000 --> 00:02:50,000
這個演講的題目是要在一種網絡叫做Bipartite Network上面做社群分析。一個Bipartite網絡就是現在畫面上看到的這種有藍色、紅色構成的網絡。

9
00:02:51,000 --> 00:03:03,000
Bipartite就規定好所有的藍色只能連到紅色,所有的紅色只能連到藍色,所以有這樣的結構的網絡。在這樣的網絡上面,其實還是可以有一些社群在裡面。

10
00:03:04,000 --> 00:03:23,000
我們跟我其中一個老闆叫做Daniel Laramore開發了一個方法,可以用一種統計模型叫做Stochastic Block Model來去fit這個網絡,可以找到這個網絡裡面的社群。

11
00:03:24,000 --> 00:03:48,000
其實最重要的工作就是在一個還不知道網絡結構的網絡裡面,就是這個圖的左邊這裡,你只知道它是Bipartite,可是你想要知道說它到底每一個Bipartite的類型還可不可以再繼續細分成一些比較小的網絡。

12
00:03:49,000 --> 00:03:58,000
如果可以的話,我們還想要知道說它到底有多大的統計顯著型,這樣我才知道說我應該要怎麼樣分群。

13
00:03:59,000 --> 00:04:03,000
所以大家可以看到有PDF slides跟相關的tweet。

14
00:04:05,000 --> 00:04:17,000
今天的目標就是要講一講為什麼我們要看網絡裡面比較大scale的結構,然後跟我們怎麼樣用統計方法找這些結構,以及相關的reference。

15
00:04:17,000 --> 00:04:46,000
在1718年的時候,我還在當工程師的時候,成大統計系的一個李正德老師找我去跟大一的統計系學生分享說,平常在做軟體工程的工作的時候會有怎麼樣的事情。

16
00:04:47,500 --> 00:04:56,000
我設計了一個課程,讓他們可以用Python來分析他們自己班上的網絡。

17
00:04:57,000 --> 00:05:12,000
那時候我有六個小時的時間講這個事情,然後我就設計了一個問卷。這個問卷就希望他們填名字、學號,填他們覺得班上有哪些人是他們的朋友。

18
00:05:12,000 --> 00:05:26,000
然後關於他自己,他還可以有他自己的一些attributes,性別、年齡系或是其他的東西。

19
00:05:43,000 --> 00:05:50,000
您有在播放嗎?

20
00:05:50,000 --> 00:05:54,000
我在我的keynote上面按播放。

21
00:06:21,000 --> 00:06:33,000
不好意思,我看一下。但我有一個問題,就是我按播放之後我就進入了全螢幕,然後我就再也沒有辦法碰到BlueJeans了。

22
00:06:33,000 --> 00:06:38,000
所以主持人要幫你看有沒有人跟你講話。

23
00:07:03,000 --> 00:07:08,000
所以這是我的第一頁,然後我剛才到了第二頁。

24
00:07:34,000 --> 00:07:38,000
他覺得誰跟他是朋友,然後包含他自己的一些資料。

25
00:07:42,000 --> 00:07:53,000
因為我要教他們用Python做一些網絡分析,所以我讓他們填問卷之後,進來以後就有Google Form,然後我就把它整理成一個CSV的表。

26
00:07:54,000 --> 00:08:10,000
然後呢,在這裡面呢,其實比如說ID就是填的每個人,然後ID of acquaintances就是說他到底說誰是他的朋友。

27
00:08:11,000 --> 00:08:27,000
所以這時候比如說第一行1跟58,你就可以連出一條線,從節點1連到節點58,然後還有後面一些關於那個節點的metadata,然後我就可以把它畫成一個班上的網路。

28
00:08:27,000 --> 00:08:45,000
很好玩的事情是,這個班上是一個統計系的班,所以你看大部分是統計系的,然後有一些外系的學生來修。外系的學生好像自己會跟外系的學生交朋友,就算統計系的學生也有好多統計系學生的小圈圈。

29
00:08:46,000 --> 00:08:53,000
更有趣的事情就是說,你如果說一個人是朋友,他不一定會說你是朋友,所以這個方向性還蠻殘酷的。

30
00:08:54,000 --> 00:09:10,000
那這裡面我們看到的是一個簡單的網路的例子,它是一個叫做簡單的,就是兩兩個點之間的連線它只有一條線,而且它是有方向性的網路。

31
00:09:11,000 --> 00:09:19,000
而且這個網路也是有metadata的,就是節點或是那個邊都可以有一些其他的數據附加在上面。

32
00:09:20,000 --> 00:09:36,000
那或是我們可以從美國國會,然後我們可以看看,就是美國國會議員們會發表演說,然後他會有那個議員跟他發表演說的文本。

33
00:09:37,000 --> 00:09:56,000
他可以看看說,隨著這個議員新上任以後,隨著每年時間演進,這個議員在用詞遣字上面有沒有什麼樣的變化,或是說大家有沒有慢慢形成一個共識。

34
00:09:57,000 --> 00:10:12,000
所以這是一個例子,它是一個bipartite的例子,就是有議員然後有他的演說文稿,只能連演說文稿跟議員,而且它是動態的,因為這個網路它會變。

35
00:10:13,000 --> 00:10:36,000
然後我們也可以從腦科學看到例子,比如說這是一個研究human connectome的工作,它刺激腦的不同地方,對腦做刺激,然後它可以看腦的不同地方的反應。

36
00:10:37,000 --> 00:10:58,000
比如說在左上角這個圖,它先把腦中每個地方都標記起來之後,看看說每個地方跟每個地方在某些task上面會不會有共同的反應,如果有的話就比較亮,如果沒有的話就比較暗。

37
00:10:59,000 --> 00:11:18,000
這麼做之後,它可以把那些看起來好像有點結構的東西,對它做community detection,我們可以知道說有哪些不同的腦區可能會有相似的反應。

38
00:11:19,000 --> 00:11:42,000
在這個工作裡面,我們看到的是兩個腦區的反應,可能是0到1之間的值,它是一個weighted network,而且每個腦區它是有空間的分佈的,所以它也是一個spatial network,或是可以說它是一個coordination matrix。

39
00:11:43,000 --> 00:11:59,000
在這裡面,我們想要了解它的結構,這時候我們用的工具有沒有統計性質就特別重要,因為其實我們不曉得它背後到底它的ground truth的結構是怎麼樣。

40
00:12:00,000 --> 00:12:24,000
好,所以我要定義說什麼是networks,然後我們在上面會問一些什麼問題,所以networks就是有很多節點構成的entity,然後這些節點我必須要選擇兩兩個節點連出一條線,把它叫做edge,這樣的一個collection我就叫做一個網路。

41
00:12:25,000 --> 00:12:49,000
我們在上面會問一些問題,比如說有很多電腦科學問的一些問題,比如說要怎麼樣找到最小的節點的集合,使得每一個不是那個集合的節點都至少有一條邊連到那個你選出來的集合等等。

42
00:12:50,000 --> 00:12:57,000
很多這種問題都是NP-hard,因為網路這個組合有很多種的關係,所以就變得很難。

43
00:12:58,000 --> 00:13:14,000
或是你也可以在上面,比如說你想要提出統計上的一個新的分布,這個分布可能跟一些你觀測到的某些系統的性質有關,所以你就可以說這個分布可以很好的去生成這些性質,這也是一個統計上的工作。

44
00:13:15,000 --> 00:13:29,000
或者是,在網路上面可能有一些現象,比如說我們現在有傳染病,所以我們人跟人之間會有一個傳染病在網路上面傳播,但是人跟人之間的接觸也可能是有變化的。

45
00:13:30,000 --> 00:13:36,000
所以我說的是dynamics on或是off networks,然後這上面就有很多有趣的問題。

46
00:13:37,000 --> 00:14:01,000
或是我們可以看它的結構,比如說我們現在看的是,比如說這個網路如果會變化的話,那我們可能可以預測說,兩兩個節點是不是在某些時間容易有生成新的邊,叫做link prediction,跟machine learning的工作有關。

47
00:14:01,000 --> 00:14:29,000
我現在的這個工作是在講靜態的large scale的community structure,我的large scale是說,比如說我的網路有很大,比如說有100個點,然後我large scale就是至少是絕對是比log100還要再大很多的,可能是它的10%,這種尺度叫大尺度的網路。

48
00:14:32,000 --> 00:14:51,000
還有我剛才說要連線的時候,我只能夠兩兩個點連起來,但是你要是有一些系統,它的結構是需要三個以上的東西才有interaction,那我可以連一條線,它是連三個點的,它們叫做hyper edge,我就可以有higher order的structure在裡面。

49
00:14:52,000 --> 00:14:59,000
那就有很多相關的工作,然後我推薦這些參考資料。

50
00:15:00,000 --> 00:15:22,000
這些研究,很久以前在數學的graph theory裡面有很多工作,但是近二十年來,因為大家用網路,又累積了更多數據,所以很多從數據出發問問題的方式來開發新方法。

51
00:15:23,000 --> 00:15:35,000
也許在這種large scale的structure裡面,蠻早的一篇文章可能是這篇工作,大概二十年前的工作,現在已經有很多citation。

52
00:15:36,000 --> 00:16:00,000
好,然後什麼樣是large scale structure呢?比如說,如果你有一百個點構成的網路的話,那這個網路可能要分成很多群,可能要分成五群、六群、十群,比如說分成五群的話,就是一百除以五,就是二十,所以它是一個二十跟一百是comparable的數字,這是一個large scale的尺度。

53
00:16:01,000 --> 00:16:18,000
在裡面就可以有很多結構,比如說左上角的那個叫associative,是說這比較像我們熟知的朋友圈,就是朋友們聚在一起,然後一群朋友,所以那一群之間的連線就比較密,比起群跟群之間還要密。

54
00:16:19,000 --> 00:16:36,000
但是也可以比如說,被你定義成一群的其實不太密,但是被你定義成一群的跟其他群是很密,就是像是我們說的bipartite network,它是一種associative structure,或是其他的。

55
00:16:37,000 --> 00:16:46,000
我們要怎麼樣量化這些東西呢?是今天這個talk的一個central theme。

56
00:16:47,000 --> 00:17:13,000
好,所以在剛才那個2002年的工作開始之後,以往要看一個靜態網路上面的結構,大家是用一個叫做quality function,如果是物理的話就是observable,叫做modularity來看它。

57
00:17:14,000 --> 00:17:30,000
這個modularity是右上角那個Q of B and A,這個B是說每一個點它在哪一個社群,比如說我現在有這個網路,把它分成了紅色跟藍色的社群,也就是0跟1。

58
00:17:30,000 --> 00:17:40,000
假如說這個網路有n個點,那我的B就是一個n位的向量,然後每一個向量的值要嘛是0要嘛是1。

59
00:17:41,000 --> 00:17:48,000
所以這是一個表示現在這個系統每一個節點到底是屬於哪一個群的向量,叫做B。

60
00:17:49,000 --> 00:18:11,000
然後那個A叫做相鄰矩陣,意思是說如果你有n個點,那這個A就是一個n乘以1的矩陣,然後每一個點它會有它的vertex ID,然後兩個兩個點如果有連線的話,你就把它寫成是1,如果沒有連線就是0。

61
00:18:11,000 --> 00:18:26,000
所以這個A就是一個0跟1的矩陣,然後如果我的系統確定的話,那這個A就是一個靜態的東西,所以這個modularity它是剛才說的A跟B的函數。

62
00:18:26,000 --> 00:18:48,000
這樣,然後A如果靜態的話,我的工作就是要,我要努力地改變每一個節點的0跟1,然後改變了以後我要看看說我的Q有沒有變大,這樣。然後我希望我可以變很多次,然後找到那個最大的Q,然後找到了以後呢,我就會說我找到了一個社群。

63
00:18:48,000 --> 00:19:17,500
因為Q它其實已經說好了就是,Q它就是已經設計好就是,如果很多同群的人被分在一起,然後他們有很多群之間的連線的話,那那個Q就會很大,然後群跟群之間的連線如果少一點的話,那個Q就會很大,會因為群之內很多很密,或是群之間很稀疏而很大,所以這樣就可以找到一個好的一個社群的結果。

64
00:19:19,000 --> 00:19:39,000
好,但是這個方法後來被發現有一些問題,就是如果你餵給這個方法一個隨機的網路,就是你知道你是隨機生成的,那你其實還是可以找到一個蠻好的modularity,可是那個結構是假的。所以這時候我要怎麼樣看我的結構,而且有統計性的看是一個很重要的問題。

65
00:19:40,000 --> 00:20:04,500
除此之外呢,也有很多大概是5到15年前這段期間,有很多相關的heuristic methods都有共同問題。比如說,它會有一個東西叫resolution limit,就是如果你有一個很大的網路,那你fit它以後,你可能沒有辦法找到一些這個網路裡面的小結構,它會被洗掉。

66
00:20:05,500 --> 00:20:23,500
然後一個比較有趣的問題是,如果我給定一個網路,然後我用剛才那個modularity去fit它,然後我想要找到這個網路裡面好的partition,好的分區,那我會發現其實有超級多好的分區,這是一個有趣的工作。

67
00:20:23,500 --> 00:20:42,500
比如說,它說好的分區其實有很多,而且它有點degenerate,每一個好的好的差不多。所以你每一次對這個系統跑modularity,其實你就不曉得看到的結構到底是什麼。

68
00:20:42,500 --> 00:21:07,500
所以通常大家就是跑好多次,然後看看有沒有consensus,跑那麼多次以後,可能大部分都是找到怎樣的社群,然後我就拿這個當我的社群。這也是一個問題。而且剛才說的是,同一個方法你用在這個網路,你要是用同一個網路的話,你可能不同的方法看到的社群也不太一樣。

69
00:21:08,500 --> 00:21:29,500
而且呢,如果這個系統它有一些,比如說我知道說,剛才說的那個班上,那個班上每一個同學他有系所別,兩不同的系所。那我想要知道說,這個網路的結構是不是跟系所有correlation,然後我就fit它,然後我看看說,到底那個結果跟那個系所有多大的correlation。

70
00:21:29,500 --> 00:21:55,500
可是在很多可能真的有這種correlation的系統裡面,其實弄一弄以後也發現說,它沒有辦法跟它的節點的那些annotation有correlation。所以我們不曉得到底是怎樣的true model。所以在這個工作裡面,我會說要用statistic來做正確的方法,而且可能沒有true model。

71
00:21:55,500 --> 00:22:24,500
我們這個工作的內容會集中在一種機率生成模型,叫做隨機區塊模型。因為這個隨機區塊模型的參數有機率的性質,所以我可以在裡面做一些統計上的乘數,結果可以有statistical significance。

72
00:22:26,500 --> 00:22:39,500
最重要的是,我想要拿它來當作no model,來看看我這個系統有沒有結構。所以重複講一次,我的工作就是希望左邊這邊有一個bipartite network,我希望可以知道它到底應該分成幾群。

73
00:22:40,500 --> 00:22:54,500
我們現在已經引入SBM,所以在剛才說的各種不同的網絡結構,其實在SBM裡面可以有相對應的。

74
00:22:54,500 --> 00:23:11,500
比如說,在SBM裡面有一個參數,就是你要怎麼樣連接群跟群之間,或是群跟群自己的機率。

75
00:23:11,500 --> 00:23:29,500
這樣子的話就是一個,比如說現在最左邊這個圖,如果你分成五群的話,它就是一個五乘五的matrix。五乘五的matrix,它的diagonal part就是說,我第一群跟第一群它要用多少的機率,0跟1之間去連。

76
00:23:29,500 --> 00:23:45,500
比如說我現在要assaultative,所以我可能就要要求它比較接近1,比如說0.9,然後去連。而off diagonal它可能數值就要小一點,所以我這樣子就可以連出一個像這樣子的assaultative的網路。

77
00:23:45,500 --> 00:23:58,500
所以剛才說的那種群跟群之間怎麼樣連的這種matrix,它formalize了剛才我們說的這種large-scale structure的觀念。

78
00:23:59,500 --> 00:24:19,500
然後呢,對這個工作呢,現在在Physical Review 1,是一個物理的期刊。然後呢,我覺得我們比較重要的貢獻是我們有一個,我現在highlight這些東西,然後這也是今天我會強調的重點。

79
00:24:20,500 --> 00:24:39,500
然後呢,第一個是我們有了剛才說的SBM之後,我們想要還有一個Bayesian的formulation。然後,所以我會特別講到說怎麼樣去估計所謂的SBM ensemble。

80
00:24:39,500 --> 00:24:58,500
然後呢,我們還是會去fit這個模型,但是我們fit的方法,我們會用MCMC,然後還有Dynamic Programming。最後呢,我們會想要看看我們提出來這個模型到底在統計上有沒有真的比較好。

81
00:24:59,500 --> 00:25:10,500
所以我會說一些例子,就是in terms of它是比較sensitive,就是說它比較有機會偵測到比較小的網路結構。

82
00:25:11,500 --> 00:25:33,500
如果它跟在我這個工作之前的比較general的方法比的時候呢,它通常會有比較高的posterior probability,後驗機率。所以它其實是一個比較精確的model。

83
00:25:34,500 --> 00:25:54,500
好,然後呢,這個工作做了四年,大概從2016年開始做,然後那時候我還在北京,然後我參與了一個專案叫做aminer.org,它是一個有點像是給researcher用的facebook。

84
00:25:55,500 --> 00:26:19,500
然後呢,我被交代的任務是我要畫出一個網路,用下面這篇paper的方法。然後呢,這個網路是說哪一個研究者他常常去哪些會議,所以我現在有一個研究者會議的metrics,然後我想要說我怎麼樣分群。

85
00:26:19,500 --> 00:26:42,500
比如說我現在選一個會議,我可以知道說常來這個會議的那些研究者,哪些人可能做的東西比較相近。然後遇到很多問題,比如說它裡面需要metrics factorization,然後這個data常常很sparse,所以每次factorize好像就遇到一些divided by zero這種問題,然後就不能做,或者是說我也不曉得我要分成幾群。

86
00:26:43,500 --> 00:26:55,500
然後後來呢,就看到了一個跟SBM有關的工作,然後有一個python的library叫做graphtool,它直接implement這個SBM,然後感覺很好用。

87
00:26:56,500 --> 00:27:17,500
而且感覺右邊這個圖在2014年的時候的工作,它是用bipartite SBM去提出了一種SBM叫做bipartite SBM。

88
00:27:17,500 --> 00:27:30,500
然後它說這種bipartite SBM可以嚴格地規定你不能夠找到一個網路的解,而這個解它是不bipartite,因為它就會違反一開始你對數據的假設。

89
00:27:30,500 --> 00:27:50,500
所以比如說,如果我要我這個系統,現在看到這個網路,如果我要我這個系統去fit一個k等於5的數目,community的數目的話,那如果是原版的SBM,它可能會找到一個結果。

90
00:27:51,500 --> 00:28:04,500
現在看到的這樣子,可是呢,它卻把不同type延在一起,它就不是一個bipartite的系統。

91
00:28:05,500 --> 00:28:14,500
如果我們有一個可以真的區分不同type的話,那可以找到一個解,它是bipartite,而且它其實也有更好的likelihood。

92
00:28:15,500 --> 00:28:26,500
這是那篇文章的工作。我那時候就想,這篇文章好像也沒有告訴我怎麼樣選k,到底要分成幾區。

93
00:28:27,500 --> 00:28:42,500
而且那時候我要申請博士班,所以我就寫了一個email給這篇文章的第一作者,就是Laramore,現在也變成我其中一個老闆,說我想要跟他一起做這個工作。

94
00:28:42,500 --> 00:29:01,500
然後我想問看看說,到底這個工作好像沒有告訴我們要怎麼選k,那如果說我們可不可以用那時候已經有的一些Bayesian的方法來選k,這樣的話我們就等於是延伸了他過去做過的工作。

95
00:29:02,500 --> 00:29:20,500
後來他就答應我了,我們就連上線開始一連串的工作。這篇工作雖然發在物理的期刊,但是它有一些特性跟統計跟兒訓認領有相關。

96
00:29:20,500 --> 00:29:25,500
我就在Twitter上面看到這個有趣的圖,然後我就想說來比一下。

97
00:29:26,500 --> 00:29:37,500
By the way,我剛才跟彥平聊到說純數跟硬數的差別,然後也有一個有趣的Twitter可以連接到。

98
00:29:38,500 --> 00:29:55,500
我的工作呢,這個參數推論是有的,預測的話是可以預測的,但是預測會遇到一個問題,就是我的這個模型到底跟我的這個數據到底有沒有consistent。

99
00:29:56,500 --> 00:30:10,500
因為我永遠不知道我的數據是除非特定的系統,但是我可能很難知道我這個數據真正是怎麼樣生成的,所以這個模型到底多適合這個數據是不一定的,然後那時候可能prediction就會不太好。

100
00:30:11,500 --> 00:30:28,500
然後我們也沒有做什麼decision making,然後模型確實可以解釋,因為是一個統計模型,然後decision好像也不是很確定是怎麼樣的decision,但是我們確實有model的regularization。

101
00:30:28,500 --> 00:30:38,500
regularization就是要規範model,我不想要我的model太複雜,所以我想要知道說我要選多大的K,所以我要把它束縛住。

102
00:30:40,500 --> 00:30:58,500
Theory是有的,然後Causality好像沒有,然後我們確實提供了代碼,然後Scalability好像也普通,因為沒有特別去看一個比較大的系統,然後也不太確定這樣子要怎麼樣變成一個online的algorithm。

103
00:30:59,500 --> 00:31:23,500
那總之就是我們的工作這樣,然後呢,我的outline呢,我會介紹什麼是SBN,然後在上面做Bayesian inference,然後貢獻呢,就是我們有一個model,這個model考慮了跟剛才說的原版的model不一樣的地方,就是我們有一個對付Bayesian的這個prior,它是bipartite的。

104
00:31:23,500 --> 00:31:49,500
然後,然後有一個search method,它是跟Dynamic Programming跟MCMC Monte Carlo Algorithm有關,然後以前的工作也沒有說用Dynamic Programming去fit這個模型的,然後呢,會講一些它在統計上的結果。

105
00:31:50,500 --> 00:31:53,500
然後最後就是一些outlook。

106
00:31:55,500 --> 00:31:57,500
目前為止,大家有問題嗎?

107
00:31:57,500 --> 00:32:22,500
好,對不起。就是呢,我看在對話視窗那邊沒有問題,但是有一個comment,它是說,SBN看起來跟指數隨機模型ERGM類似。你要不要comment一下?

108
00:32:23,500 --> 00:32:32,500
我一直沒有熟悉ERGM那一塊,我覺得是類似的。

109
00:32:33,500 --> 00:32:58,500
我覺得它在於,現在我這張slide會講說,我的BN是怎麼樣生成的,但是我覺得它,如果我的印象沒有錯的話,我覺得它是對於BN要怎麼樣生成的統計假設不太一樣,而且好像沒有社群的概念在裡面。

110
00:32:59,500 --> 00:33:07,500
但就,我沒有研究過那方面的事。

111
00:33:07,500 --> 00:33:09,500
ERGM。

112
00:33:38,500 --> 00:33:47,500
By the way,在問這個問題之前,大家如果有問題其實可以開麥克風自己講,當然打了我也可以念啦。

113
00:33:48,500 --> 00:33:56,500
Peter問的問題是,請問假如沒有ground truth,那要怎麼樣validate生成的模型?

114
00:33:56,500 --> 00:34:09,500
如果要回答Peter的問題的話,我會說,我們必須要在比較模型的時候,那些模型有相同的假設。

115
00:34:10,500 --> 00:34:20,500
這時候呢,如果是Face and Foremark的話,我其實可以直接的看一個東西叫做posterior probability,後驗機率。

116
00:34:21,500 --> 00:34:29,500
然後,如果它高的話,我就會說,這個模型比較像是用來生成這個數據的模型。

117
00:34:34,500 --> 00:34:40,500
所以是一個從模型為中心觀出發,來理解數據的工作。

118
00:34:40,500 --> 00:34:44,500
OK,他說,好,謝謝。

119
00:34:45,500 --> 00:34:46,500
OK,那大家還有問題嗎?

120
00:34:47,500 --> 00:35:02,500
我有一個問題就是,我還是不太懂為什麼一定要跟bipartite有關,因為看起來是在分類,類似就是分community,那為什麼一定是要bipartite的network?

121
00:35:02,500 --> 00:35:27,500
脈絡是這樣,就是說,在我們工作之前的方法,就是原版的SBM,其實也可以拿它來fitbipartite的網路。

122
00:35:28,500 --> 00:35:40,500
可是原版的SBM裡面,它也是base end,但是因為裡面的prior的假設,它允許了不要bipartite這個事情發生。

123
00:35:41,500 --> 00:35:53,500
所以呢,它可以預測,但是因為系統就是bipartite,它不會出現那種你覺得它要有prior的那些空間。

124
00:35:54,500 --> 00:36:05,500
所以這時候它就會有一些浪費,比如說它可能就沒有辦法有比較好的resolution,然後主要是這個部分。

125
00:36:06,500 --> 00:36:07,500
OK,謝謝。

126
00:36:08,500 --> 00:36:28,500
所以接下來我會講說,我們怎麼樣修改了原版的SBM的工作,然後說哪些地方其實是我們這種prior它是比較節約的,或是這種prior是比較sensitive的。

127
00:36:32,500 --> 00:36:33,500
謝謝。

128
00:36:38,500 --> 00:36:40,500
好,那我們就繼續哦,謝謝。

129
00:36:41,500 --> 00:36:49,500
繼續哦,然後,好,所以我現在要講的是某一個版本的SBM。

130
00:36:50,500 --> 00:37:05,500
然後,首先SBM就是一個生成模型,就是你跟它講一些參數之後,你可以說我要像支骰子一樣支出一個,骰子支出點數,但是SBM是生出一個網路。

131
00:37:06,500 --> 00:37:16,500
這個網路會有你剛才說的那些你要的機率,然後,所以它有哪些參數在裡面呢?

132
00:37:16,500 --> 00:37:37,500
它有右上角的那個那些參數,右上角它是一個P,是說呢,你given N,跟given K,given N,或是given Omega之後,你要生成一個特定的A,特定的網路的機率是多少這樣。

133
00:37:38,500 --> 00:37:52,500
好,那是什麼意思呢?是說呢,比如說,現在我畫出來一個這個圖,假設這個是我要剛才我說好的這些參數以後生出來的,然後呢,我第一步就是我要先選,我要有幾個點。

134
00:37:53,500 --> 00:37:59,500
選完之後呢,比如說我這裡有25個,我選25個點,然後我說我故意要分成兩個社群,K等於2。

135
00:38:00,500 --> 00:38:15,500
好,接下來呢,我要告訴我的系統說,K等於2之後,那25個點裡面有百分之多少的機率在1,百分之多少的機率在另外一個在2,兩個加起來都是1,所以我就可以知道說誰在哪一群。

136
00:38:16,500 --> 00:38:32,500
第三呢,就是我剛才說的那個矩陣,因為現在我只有選兩群,所以它是一個二乘二的矩陣,然後這個矩陣裡面的數值也是0到1,然後每一次生成的時候我還是直骰,看看兩兩個點要不要生成一個邊。

137
00:38:33,500 --> 00:38:43,500
然後所以,如果我選這種類型的Omega的話,因為Diagonal它比較大,所以它可能1跟1相連就會比較密,2跟2相連會比較密。

138
00:38:44,500 --> 00:39:01,500
好,所以我就可以選完之後呢,我就可以生成一個網路,就決定了N,然後我沒有做很多影像處理的工作,所以現在已經有一個edge被我決定了,但是你可以試很多次,那每一次的edge就會不太一樣。

139
00:39:01,500 --> 00:39:27,500
好,然後以往的工作呢,是你還是要知道說,我這個系統有多少K,然後我知道了以後呢,我去用一個方法叫做最大maximum likelihood,然後去知道說,given我這個數據,還有我選定了K之後。

140
00:39:32,500 --> 00:39:54,500
我要infer這邊的這個N跟這個Omega,然後因為我希望知道它的結構,就是連接的結構這樣,然後以往的工作呢,是用這種方法來理解那個數據的large-scale structure,就是那個Omega,但因為這裡有一個問題就是我不知道怎麼選K這樣。

141
00:39:55,500 --> 00:40:19,500
然後有一個點,現在不會用到,但是等一下會用到,就是我每一次生成這個網路之後,這個網路其實都有一些,是一個確定的網路,它就在那邊被你生成了,然後那個確定的網路上面有一些參數是我可以來描述它的,等一下我會解釋叫做B叫做E叫做K。

142
00:40:19,500 --> 00:40:44,500
它有點像是現在右上角看到的那種generative model的對應,然後等一下我會講得更清楚,然後我們會把它叫做是一個micro-canonical model,因為現在是叫canonical,它是右上角那些參數它要合理的話,是因為你要生成很多個,你要生成一個ensemble之後,它會合理這樣。

143
00:40:45,500 --> 00:40:49,500
但是在micro-canonical裡面你就不用,你就說好以後你就生成那些。

144
00:40:50,500 --> 00:41:15,500
好,那為什麼不知道怎麼樣選有幾群是一個問題呢?是因為你會overfitting,現在這邊的這個P of G,G就是剛才那個A,就是那個graph,然後B、E、K就是剛才我說的那些參數,except那個K,但基本上是差不多的東西。

145
00:41:16,500 --> 00:41:39,500
然後我們現在看看呢,我們要找到,我們決定了K之後,我們要找到那個相應的E跟K的那個值,然後我們要,因為我們要去理解那個網路的結構嘛,所以我決定K,然後我看看這個網路的結構是怎麼樣。

146
00:41:39,500 --> 00:41:55,500
然後好不好呢,我們就看看說我這樣子的maximum likelihood fitting的那個P有多大這樣,然後P如果越大的話,我現在取了log再加複數,它就會越小,有個參數叫做S。

147
00:41:56,500 --> 00:42:12,500
總之呢,我的目標就是我要fit它,然後看看P最大的或是S最小的地方,然後我們可以拿一個網路去fit它,然後我說今天我覺得這個網路是B等於2,然後我就fit,之後我得到一個likelihood,然後它有一個相對應的S,就這麼多。

148
00:42:13,500 --> 00:42:31,500
然後那我看看fitB等於3怎麼樣,然後我就再fit一次,然後因為B等於3,它有更多群在裡面了嘛,所以這時候剛才說的那種omega那種參數就會變得比較大,變成是從2乘以2變成3乘以3,所以你可以調的那個num就比較多。

149
00:42:32,500 --> 00:42:57,500
那結果你就會發現你fit的比較好,所以P就變大了,然後S變得就比較小,然後你就這麼做下去,你發現你把B越來越大的時候,它的S就越來越小,它的likelihood就越來越高,直到那個你要它分群的那個數目等於每一個,等於這個系統所有節點的數目的時候,

150
00:42:58,500 --> 00:43:12,500
你就拿到一個omega的矩陣,這個矩陣是n乘以n,而且那個n乘以n就跟你的adjacency matrix一樣,要嘛就是0,要嘛就是1,你預測的超級好,可是你就overfitting。

151
00:43:13,500 --> 00:43:23,500
所以這是一個如果你沒有regularize你的模型的參數會遇到的問題。

152
00:43:24,500 --> 00:43:45,500
好,所以這時候我們就要引入Bayesian的方法,剛才我們看到的是一個P of G,它是一個likelihood,然後我們要用Bayesian的方法來regularize這個likelihood裡面的一些參數。

153
00:43:46,500 --> 00:44:11,500
比如說呢,這個事情在平常做histogram的統計就會用到,比如說你有好多點,然後你說我今天要選幾個bin去分,然後你就選這麼多bin,然後你選多一點你好像看到更多結構這樣,可是你就不太知道說到底你要選幾個bin才是真的看到你要的結構。

154
00:44:12,500 --> 00:44:32,500
所以選bin這個事情也是一個問題,然後換成Bayesian語言來講的話,就是我想要知道我看到的數據有怎樣的機率會出現,然後我就用一個模型去看它,就是說我給定這個機率之後,這個模型它fit多好,我們把它叫posterior。

155
00:44:33,500 --> 00:44:54,500
然後這時候有一個Bayesian theorem,它就可以把posterior展開變成是下面這一項,然後我們就有一個叫做join likelihood,還有下面那個P of X叫做data出現的機率,通常叫做evidence,可是因為你通常拿來做問題的時候通常是一個確定的data,所以它通常就會消失掉。

156
00:44:55,500 --> 00:45:20,500
這個join likelihood它可以展開變兩項,一項是剛才我們說的likelihood那項,那個演進就是我們剛才看到很多參數的那個事情,然後因為參數在動嘛,我們不確定說我們會不會動到一些其實根本就不需要的參數,所以我想要把參數動的空間也考慮進來,這時候就有那個prior,就是我要知道我用這個演進看這個系統的時候,

157
00:45:21,500 --> 00:45:47,500
這個系統會被我演進看到的機率是怎麼樣,同樣的事情在網路上也是一樣,因為我的X就變成我的A,adjacency matrix,然後選不同的group就有不同的結構在裡面這樣,然後SBM就是有一些參數在裡面,然後我們要把剛才說的那種likelihood,就是這個綠色的,下面這個綠色的東西,

158
00:45:47,500 --> 00:46:16,500
我把它再規範一下,加上那個一個prior,然後最後我要看的,最大化我要看的東西是前面的這個,前面最前面這個P,然後就是那個posterior,然後我的目標就變成是從剛才的我想要去調整我的演進,看看我的likelihood的最大值,變成是不只調演進,還要調我的prior,

159
00:46:16,500 --> 00:46:45,500
然後看看最後的posterior的最大值,然後這時候通常就會有比較好的統計的結果這樣,好,然後,好,所以呢,剛才說了有一個我還沒有解釋的P of K1跟B這樣,然後這是一個跟這三個變量有關的一個函數,

160
00:46:45,500 --> 00:47:11,500
然後,嗯,我們不太確定要怎麼樣去去去算它的值,然後,嗯,但是呢,嗯,右下角這個,嗯,排球頭的文章呢,就是說,嗯,你可以用這種factorization的方法把它,嗯,factorize,然後你可以分別求三個不同的項,然後這時候呢,嗯,三個不同的項就有一些可以被你算出來,比如說呢,嗯,第一個項就是B,

161
00:47:12,500 --> 00:47:38,500
就是你在你的系統裡面觀察到它是特定的partition,就是剛才說的那種長度為n,然後如果是有兩個群的話,就是0跟1之間的相量這種,這種東西,嗯,你想要知道你的特定的partition它出現的機率是怎麼樣,node partition,然後呢,嗯,你知道node partition之後,因為每一個點它已經確定它在哪一個group的嘛,然後你又要連線,然後,嗯,連完之後呢,就會有一個matrix叫做E,

162
00:47:38,500 --> 00:48:07,500
這個E它是一個matrix,它,它,你有幾群,它就幾乘幾,它有點像是剛才說的那個omega的,嗯,micro-canonical的延伸,然後這時候呢,你就說,嗯,E裡面,嗯,哪一群跟哪一群,它到底連了幾條線,而不是機率,是連了幾條線,然後這時候呢,你就說它是它的edge count matrix,然後你也可以去說,欸,我,我規定說我要有,嗯,兩群,然後我有多少,

163
00:48:08,500 --> 00:48:37,500
edge,那這樣子的話,我這種二乘二的矩陣,它,嗯,它的分佈是怎麼樣,這是可以算出來的,因為這些東西都是discrete的東西,然後呢,有了剛才這兩個之後呢,我可以再去,嗯,摸,嗯,再去對它的,嗯,degree distribution做建模,我還沒有解釋什麼是degree,degree呢,是說一個點它到底連了幾條邊出去,這個數字就是它的degree,然後,嗯,嗯,

164
00:48:38,500 --> 00:49:05,500
然後有一些脈絡我也沒有講,就是,嗯,嗯,degree通常是大家會放進這個模型的東西,因為degree常常跟很多在網路上面發生的動態現象有關聯,這樣,所以他們也會希望可以把degree也capture進來這樣,所以,嗯,決定了剛才說的B跟E之後,其實我還可以把degree的distribution給寫出來,最後呢,就是這個network的,的那個likelihood,

165
00:49:05,500 --> 00:49:32,500
就是我剛才解釋的colonical version的SVM的likelihood,決定那三個東西怎麼生成A,這樣,然後所以我們就有一個model summary,先寫下來,但是看起來很可怕,它呢,每一下都可以互相對應的,比如說這個是likelihood,比如說這個是,嗯,生成那個degree distribution的prior,然後這個是生成那個,嗯,生成那個edge count的matrix的那個prior,

166
00:49:33,500 --> 00:49:58,500
然後這個是partition的prior,然後雖然這個東西看起來這麼複雜,然後我們有K啊,有E跟B,A是我原本就知道,因為它是我的input data,嗯,可是有一個有趣的點就是,好,我要講的是右上角那個畫,就是有趣的點就是,嗯,K跟E其實是一個,其實是B的函數,

167
00:49:58,500 --> 00:50:25,500
因為你想一下,如果你改變了一下B,那相應的那個剛才說的那個群跟群之間有連多少線的那個matrix E,就也會變啊,然後那個K也會一起變,所以K跟E其實是可以寫成是B的函數,所以,嗯,雖然我這個式子寫得這麼長,但是整串東西其實還是跟剛才在算modularity一樣,只要改變那個B就好了,就是改變每一個點它的社群,這樣就好了。

168
00:50:26,500 --> 00:50:52,500
然後,嗯,這篇的工作的其中一個貢獻呢,就是我們把,嗯,我現在寫出來這些priors,把不是bipartite的貢獻扣掉,這樣,所以,嗯,因為我扣掉了以後,嗯,模型就比較節約,所以這時候我的priors就會比較,數值就會比較大,就會比較sensitive,大概是這樣。

169
00:50:53,500 --> 00:51:05,500
好,然後,嗯,就是大家可以看一下顏色,就是相對應的partition,然後counts,degrees,然後最後跟,就是network likelihood,這樣。

170
00:51:05,500 --> 00:51:33,500
然後這裡面有很多工作是因為,嗯,是我需要知道我給定了剛才說的那些,嗯,discrete的參數,比如說B,比如說E,比如說K,然後我要怎麼樣知道,給定了這些之後,它到底,嗯,同樣都有這些參數的那麼多的網路到底有多少個,這樣,一個一個,就是可以知道有多少個,然後,嗯,這個算出它有多少個的這個方法,是,嗯,這個Pashoto2012年的這個文章發展的,這樣。

171
00:51:35,500 --> 00:52:00,500
好,嗯,為什麼算出多少個是重要的呢?因為我們希望我們可以,嗯,就用,就being Bayesian,就是我們可以沒有,嗯,對於特定的狀態我們沒有特別的偏好,我們希望它每一個狀態出現的機率都是一樣的,這樣,所以任何的網路,就是我剛才算出的多少個分之一,這樣。

172
00:52:00,500 --> 00:52:12,500
如果我最後算出的那個多少個其實很小的話,那代表說我其實找到一個不錯的模型,因為我從那個ensemble裡面隨便剪一下,都很像我要,嗯,觀測的那個網路,這樣。

173
00:52:12,500 --> 00:52:41,500
好,目前為止有問題嗎?好,然後,嗯,就,就,我覺得這是一個很好用的那個library,它就真的把SBM給,給放進去了,然後底層是C++這樣,就是剛才說的那些,嗯,ensemble counting的work的那個作者Pashoto,嗯,他的工作。

174
00:52:43,500 --> 00:53:08,500
好,然後我們,我們現在要recap一下我們到底做了什麼事情,就是呢,嗯,我們,我們的工作是要,我們要maximize一個posterior,就是左上角這個,嗯,藍色的東西這樣,然後我們可以,嗯,把它,嗯,用那個,嗯,Bayesian,嗯,Bayesian把它寫成這樣子,然後我們下面有一個,嗯,固定的像,因為我的數據是固定的這樣。

175
00:53:09,500 --> 00:53:29,500
然後所以其實重要的東西就是跟那個,嗯,所謂的total likelihood,把B跟A給放進來這種機率有關,然後,嗯,它到底是什麼東西呢?它是把,嗯,在以往Bayesian的framework之下呢,它是把你,嗯,會有那些B跟A的那些模型裡的參數的空間給積分掉。

176
00:53:29,500 --> 00:53:58,500
然後,嗯,通常你要去估這種posterior likelihood,你就必須要算這樣的積分這樣,然後,嗯,這件事情在SBN上面是很難做的,原因是因為,嗯,不太容易可以找到,嗯,以往做這個積分通常會在likelihood裡面去找到一些數學的像,然後去,去,然後去用相應的,嗯,conjugate prior,然後你這樣子積分才可以積出一個,哇,一個,一個好算的東西這樣。

177
00:53:59,500 --> 00:54:27,500
但是,嗯,在SBN上面這件事情很難,因為,嗯,就算有那個prior可能跟數據的分佈也離得很遠這樣。然後,嗯,這是我剛才說過的,就是,嗯,所以說呢,嗯,我們就想要說我們可不可以因為它是discrete,那我就,嗯,好好的把所有的state的數目都算出來,然後我就minimize那個數目就好了。

178
00:54:27,500 --> 00:54:46,500
然後,這時候呢,其實也是跟,嗯,跟整個Bayesian的framework是一致的這樣。然後,嗯,Petroto就說可以,因為,嗯,你可以把,這是一個recap,就是你可以把剛才說的那個,嗯,join likelihood把它分成兩項,然後這兩項你都可以discrete去算它。

179
00:54:47,500 --> 00:55:12,500
所以這時候呢,我就不用做積分了,我就,但是我還是可以,我還是需要對它的prior有些假設。好,然後我們這時候就開始算likelihood,嗯,我不會go through detail,但是我會講概念,因為時間的關係。然後呢,嗯,likelihood就是,我要知道說,我要固定了這些參數之後,遇到一個網路,我這個參數有多大的機會生成這個網路這樣。

180
00:55:12,500 --> 00:55:23,500
所以它其實是一個,嗯,這些參數所張開的那個空間的的的分之一,因為我希望它張開那個空間裡面的每一個instance都是同樣的機率出現的這樣。

181
00:55:23,500 --> 00:55:41,500
所以它其實是一個,嗯,一個,嗯,ensemble叫做,嗯,omega,big omega分之一這樣。然後它完全可以從,嗯,那些degree distribution啊,或是那個剛才說的那個edge count matrix跟adjacency matrix去算。

182
00:55:41,500 --> 00:56:02,500
然後呢,嗯,你在算的時候呢,嗯,你還是希望可以知道說,這個omega它,嗯,怎麼樣才會最小這樣。可是呢,通常因為omega它會跟你的,嗯,你改變每一個節點它在哪一群有關嗎?所以你就會遇到你有很大的空間要要search這樣。

183
00:56:03,500 --> 00:56:23,500
然後所以,嗯,通常大家就會用MCMC去sample這個space這樣。然後呢,嗯,這邊我就要說一些介紹一個新的名詞的,就是,嗯,他們會把,嗯,我們會把這個ensemble size取一個log叫做entropy這樣。

184
00:56:23,500 --> 00:56:52,500
所以說呢,嗯,雖然我們在這裡看到的只是likelihood,但是同樣的事情也會在prior那邊,嗯,發生一次。也就是說呢,嗯,我會說呢,嗯,我最後要最大化的那個positive probability,其實有對應到某一區的參數空間。然後呢,當我要最大化那個positive probability的這件事情,其實相當於我要最小化我張開的那個參數空間。

185
00:56:52,500 --> 00:57:07,500
然後我張開的參數空間的數目取一個log,我就把它叫做是entropy這樣。或是有的人把它叫做描述長度,description length。好,所以剛才這兩個東西是,嗯,一體兩面這樣。

186
00:57:07,500 --> 00:57:34,000
所以,嗯,嗯,最大化face and posterior的時候,就好像是在一個,嗯,所謂的最小,最小描述長度原理下工作,找到一個最節約的模型。好,然後,嗯,我們有了剛才算那個likelihood,就是決定,決定了參數之後,它張開的那個空間有多大。

187
00:57:34,000 --> 00:57:50,000
但是我們也可以來看看這個prior有,有,嗯,我們也可以看看這個prior怎麼算。然後這prior呢,就是說我有多少合理的參數,參數可以變,然後這時候張開的那個參數空間有多大這樣。

188
00:57:50,000 --> 00:58:03,000
好,比如說呢,嗯,我們可以有一個prior for node partition,就是,嗯,比如說現在是一個,嗯,有一個四個群的網路這樣。

189
00:58:04,000 --> 00:58:24,000
它還沒有過什麼網路,但是一個四個群的一個東西,然後我要分配1234四個數字在n個點上面,然後我的問題就是說,嗯,假設,假設,嗯,1234這四個數字分到n個點上面,我隨便分,然後每一次分完以後我都會得到一個partition這樣。

190
00:58:24,000 --> 00:58:44,000
然後這時候我要怎麼樣描述這個所有的,當我決定n多少,當我決定b等於4之後,這個partition有多大這樣。嗯,這件事情就是我在規範我,嗯,允許參數可以,嗯,變化的空間。

191
00:58:44,000 --> 00:59:02,000
然後所以,嗯,嗯,上面這個general prior呢,就是,嗯,以往的工作,就是,嗯,比如說呢,我們,我們可以把決定b之後決定大b之後,好,然後我,我有一些那個。

192
00:59:03,000 --> 00:59:28,000
嗯,就是投影片沒有做好的地方,我現在解釋,就是大家看到b跟看到k,他們是一樣的東西,然後,嗯,b就是群,然後k也是群,但是通常,嗯,在我一開始做這個工作的時候,我會把,我會用k當作b這樣,然後k就會分成k或kb,分別代表是它在,嗯,不同type,在bipartite不同type的的數目這樣。

193
00:59:29,000 --> 00:59:40,000
好,然後,嗯,在general prior裡面,我可以把,嗯,這個partition就這樣子把它分開來這樣,然後這時候呢,我就有一個,比如說我現在看到一個p of。

194
00:59:41,000 --> 01:00:00,000
沒有滑鼠,有點不方便,就是p of b given n這種東西這樣,它就是,嗯,這個n呢,就是這個n不是大寫n,是小寫n,小寫n的意思就是說,嗯,在每個群裡面有多少點這樣,不是總共有多少點,是在每個群裡面有多少點。

195
01:00:00,000 --> 01:00:19,000
也就是說呢,嗯,p of b given n,意思是說呢,嗯,當我確定了我在每個群之,嗯,在我每,嗯,當我確定了我每個群要分多少點之後,嗯,我有多少個,嗯,b的可能性這樣。

196
01:00:19,000 --> 01:00:43,000
所以它其實是,嗯,n的階層,然後除以每一個群裡面自己可以travel,它是equivalent,所以是大n的階層除以一個,嗯,product of很多n的階層的乘起來,這整個東西的,這整個東西是一個size,是一個比較大的數目,然後我把它倒數以後,就變成幾率這樣。

197
01:00:43,000 --> 01:01:03,000
所以,嗯,這個幾率是這樣算的,然後,然後比如說第二個,p of n given b,大寫b,就是群的數量,就是,嗯,我有了群的數量之後,我有多少的可能性可以允許我每個,有多少點放在每個群裡面這樣。

198
01:01:03,000 --> 01:01:21,000
然後這裡之所以有一些什麼簡易,它的意思是說,嗯,它的意思是說呢,我不能夠允許有那個空的群這樣,所以我至少先把一個點放在一個群裡面,然後by the way,那邊的k也是b這樣。

199
01:01:21,000 --> 01:01:50,980
所以當我至少把一個點放在一個群裡面的時候,嗯,有多少個點放在每個群的這件事情就是一個多少取幾的問題,比如說就是n減1,因為我放一個在裡面,n減1取k減1的這麼多的數目,然後因為我要取幾率,所以我把它倒數,然後p of b的話就是說我,嗯,同樣的那個b也是那個k,就是,嗯,我假設我每一個b出現的幾率都一樣,

200
01:01:50,980 --> 01:02:20,960
所以就是b分之一這樣,所以這時候我就有一個general prior,但,嗯,這裡面其實,嗯,這裡面呢,其實就少算一些,嗯,這裡面其實就有一些東西對於,嗯,bipartite網路來講是浪費的,比如說呢,嗯,bipartite網路裡面它就,嗯,bipartite網路,因為不同網路它要resize在它們自己的type裡面,所以,嗯,我在算,嗯,

201
01:02:21,560 --> 01:02:49,920
不能有空群的這件事情的時候,我就要算兩次,就是我不能有空群在type a,也不能有空群在type b,所以,嗯,嗯,中間有一個東西我就可以,嗯,剛才講的第二個那個,那個prior就可以,嗯,寫得更精確一點,這樣,然後包含,嗯,最後的p of b,現在變成p of兩個,兩個b,b就是k這樣,然後也不太一樣這樣,好,

202
01:02:49,920 --> 01:03:19,900
但是我們,重點就是我們把這個事情考慮得更精確以後,其實最後我拿到的posterior它就會比general還要再大一點,好,然後呢,嗯,這一頁我打算跳過去,但是呢,嗯,它大致上是講說呢,在general prior裡面,因為我要考慮的是那個p of e,就是那個群跟群之間怎麼連線的事情這樣,但是在general prior裡面它有考慮,嗯,同樣type裡面的群的連線,它把它也算在,嗯,

203
01:03:19,920 --> 01:03:49,340
剛才說的那個ensemble裡面,但是它其實是浪費的,這樣,然後我們就要把它浪費的部分給扣掉,這樣,所以我們才有一個比較節約的模型,所以當我們這麼做以後,我還是可以得到一個,嗯,比較大一點的prior,然後,嗯,degree也是一樣,但degree還蠻involved,我就跳過去,然後,但是degree基本上,嗯,跟群比較沒有關係,所以我們就用以前的方法這樣,對,最後呢,嗯,

204
01:03:49,340 --> 01:04:18,160
我們的prior就是,嗯,我們又貢獻,嗯,工作的其中一個貢獻就是,這個prior它有比較大的值,在各個不同的參數裡面,這樣,好,然後呢,嗯,我們現在呢,已經講了上面的那個紅色的部分,就是lightning跟右邊黃色部分prior,所以藏在一起,雖然參數很多,但其實都只跟,嗯,所謂的b,就是每一個點是在哪一個群有關,

205
01:04:18,160 --> 01:04:47,200
然後我們,我們想要知道,嗯,改變b,怎麼樣找到那個最大化的那個p,這樣,然後,嗯,嗯,我在做那個工作的時候呢,嗯,大概是三兩三年前,嗯,那時候呢,嗯,的,我沒有做太多對mcmc方法的的創新,然後那時候的mcmc呢,是你必須要決定了你有分級群之後,然後,嗯,每一個節點再去改變它的群,

206
01:04:48,160 --> 01:05:18,060
然後再看看你要最大化p是怎麼樣最大化,然後,嗯,就今年開始有,嗯,你可以有mcmc,它是可以把群給拆開來,或是把群給merge起來,然後這時候它的那個,嗯,嗯,mcmc就會比較有效率一點這樣,然後,嗯,我這邊特別列出來是說,呃,一個search algorithm,就是我要,呃,我已經有mcmc,但是mcmc是定時在

207
01:05:18,160 --> 01:05:48,140
特定的b了,然後我要怎麼樣在那個b的空間中移動,那個部分我們把它叫search algorithm,好,然後,我還沒有講mcmc,嗯,到底是怎麼樣的東西,然後我這裡有一個我覺得有趣的,就是我們要怎麼mcmc,然後為什麼mcmc在svm inference它有用,但是確實也蠻難的,然後,嗯,通常呢,嗯,在做mcmc的時候,是我要

208
01:05:48,660 --> 01:06:09,460
嗯,求一個有好多個參數的函數的集值,然後這個這個函數呢,它是一個我知道那些參數以後,我就知道怎麼算的的函數,可是我可能不知道怎麼樣把它解析化的寫下來,所以我不能用微積分的方法去去找到集值,所以這時候我就只能夠

209
01:06:10,460 --> 01:06:39,160
嗯,但是這是一個很有用的方法,就是我只能夠隨機的改變這個函數的那些參數,然後每一次改變以後,比如說我要最大化那個參數的值,然後每一次改變以後,這個值可能會變大或變小,那,嗯,我就說啊,如果,嗯,如果我改變以後這個值變大,我就接受這個改變,而如果我的改變,我的隨機的改變,如果讓我這個值變小的話,

210
01:06:39,160 --> 01:07:07,460
我就有條件的接受那個改變,這樣,然後那個所謂的有條件就是一個叫做metropolis-hastings algorithm,嗯,很有用的是當你選的那種特定的拒絕了某種某種proposal的條件之後,你最後生出來的這個你想要求,但是你不知道怎麼求的那個網路,呃,那個那個function的分佈就會很接近於真實的分佈,然後就可以在上面做,嗯,集值的運算這樣,

211
01:07:07,460 --> 01:07:36,860
所以MCMC在我這裡很有用,因為我,我就只要改變每一個group它,呃,每一個節點它的group,然後改變以後我看看,呃,我的likelihood,其實是posterior likelihood有怎樣的改變,我來決定我要不要接受它,這樣,但但有很多問題,比如說,比如說呢,嗯,我不太確定一開始,比如說B等於3,我不太確定一開始我到底要在怎樣的partition,然後,嗯,這邊其實是一個例子,就是你從,嗯,number of sweeps,就是你每一次的proposal,

212
01:07:37,460 --> 01:08:07,360
每一個sweep就是你每一次,有n個點就從0開始變一次變一次變一次變n次以後,這樣算是少一次,叫做一個sweep,這樣,那就number of sweeps就是你少好多次,這樣,你就預期說,因為我要最大化機率就是我要最小化entropy,然後我就預期說我entropy就降下來,很快地從隨機的,嗯,隨機的那個系統降下來,然後呢,可是呢,通常呢,有一些系統它可能會處在這種,嗯,就是卡在某個metastable states,

213
01:08:07,360 --> 01:08:36,460
然後你就,你要看entropy,你也不確定到底,嗯,這個mark of chain有沒有converge,這樣,然後,你不確定你要等多久,然後,所以這是unknown mixing time,然後,而且呢,嗯,最後,嗯,最後如果你真的覺得你等了夠久了之後,你要,嗯,求極致,因為我最後還是想要知道我的p最大是怎麼樣,然後,嗯,這時候呢,你就要規範那個chain,讓它不要動得那麼快,這樣,所以有一些alien scheme,

214
01:08:36,460 --> 01:09:06,360
然後我也不確定要怎麼樣,嗯,有alien scheme,這樣,然後,alien scheme下去以後有when to stop的問題,然後,總之呢,嗯,這裡有很多式的空間,然後,嗯,嗯,就是右下角這個paper也有解決了很多事情,這樣,好,然後呢,嗯,好,目前為止呢,我就講到的是,嗯,怎麼樣,嗯,確定了b之後,確定了quantum,

215
01:09:06,460 --> 01:09:35,960
確定了group之後,我要怎麼樣改變那些每一個節點的那個,然後看看p的值,但是我還沒有講說我要怎麼樣在group的空間做移動,這樣,然後,嗯,我們就提出一個search algorithm,這個search algorithm有一個概念叫做dynamic programming,然後,那時候的idea是這樣子,就是我在想啊,就是那時候我在發展這個演算法的時候,我就想說,嗯,嗯,我們如果從隨機出發,就是好像要把一個很亂的房間收整齊嗎?

216
01:09:35,960 --> 01:10:05,960
然後,嗯,嗯,收整齊的時候,我可以,我可以有個方法是,嗯,我就好好的拿一張,拿一個簿子記下來說,嗯,什麼東西要放在哪裡,什麼東西要放在哪裡,然後寫完,然後我就說,我寫了一百個東西,因為房間裡面沒有一百個東西,然後我就說,啊,房間看起來很整齊了,對,可是,嗯,可是你就會發現說,當你這麼做以後發現,哎,好像有一些,有一些東西,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,

217
01:10:05,960 --> 01:10:31,000
嗯,對,我,有一點想法,反而還是有一些東西,說書跟筆可能要放得靠近一點,所以你就想說,我可能不用這麼詳細的記錄書跟筆的,的,的位置,這樣我就可以,我我可以,我可以偷懶一下,嗯,我就說,嗯,書跟筆的位置,如果一開始做了很複雜的搜尋之後,嗯,下一次我想要做沒有那麼複雜的事情,

218
01:10:31,000 --> 01:11:00,680
我要分成一百群,但是第二次我可能那個九十九群,我就可以不要說我要分九十九群,而是從那個一百群的出發,然後說誰跟誰比較靠近,我就把他分在一起,這樣,然後我想要看看這麼做是不是,嗯,會讓我整個influence效率差很多,然後,嗯,結論就是呢,他,呃,會差很多,但是,嗯,你可以規範他,你說,假如說你偷懶,我不要認真的用剛才說的那個很複雜的mcmc去分

219
01:11:00,680 --> 01:11:00,840
但是我就偷懶的說,啊,什麼跟什麼靠得很近,我就把它寫在一起,這個事情呢,對不起,我要跳一下投影片,就是右下角的這個matrix的merge,就是這個是一個我的偷懶的步驟,不做mcmc,但是我就說,我merge看看會不會讓我的效果差很多,然後,嗯,嗯,如果這麼做可以的話,那我就可以把那個整個algorithm變快一點這樣,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯

220
01:11:30,680 --> 01:12:00,680
然後,嗯,答案就是這麼做可以,然後,嗯,我們有這麼做,而且我們這麼做以後,嗯,還用了一種,嗯,嗯,設計演算的方法,演算法的方法叫做dynamic programming,就是我想要在那個,嗯,這種,嗯,兩個k或是兩個b之間的空間中移動,但是我不想要把所有的值都算完,現在這個是我故意算的所有的值,我想要在裡面有效率的移動,然後呢,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯,嗯

221
01:12:00,680 --> 01:12:30,680
因為我知道我常常會重複的算到一些東西,然後所以這是我右上角的第二點就是overlapping subproblems,然後跟,嗯,跟一件事情就是,嗯,我知道這個系統有一個叫做optimal substructure,就是我要找剛才看到這個很複雜的landscape的最小值,然後,嗯,我不能夠全部找,因為很貴,所以我就每一次只能找這個粉紅色的這個小方塊,然後,嗯,

222
01:12:30,720 --> 01:12:59,760
如果這個系統有optimal substructure,就是說呢,假如說我找了好多次小方塊,然後,嗯,嗯,這個小方塊,很多次小方塊的最小值,嗯,真的就可以代表,嗯,如果我好好的認真找的最小值,這樣,就是你的substructure,就是有optimal的性質,你就可以拿來用,這樣,所以通常如果有這兩種性質的問題,我們就可以設計一個dynamic programming去解它這樣,

223
01:12:59,760 --> 01:13:25,440
然後我們就,嗯,把mcmc跟zackb的空間搜尋這兩個decouple,然後我們提出一個算法這樣,然後呢,嗯,我的時間已經不夠了,但是,嗯,我們比較的時候呢,我們比較了,嗯,另外一種在SVM上面的模型,

224
01:13:25,600 --> 01:13:53,880
它針對於剛才說的那個edge count跟,嗯,partition這兩個prior,嗯,做延伸,然後,嗯,把它變成一個階層狀的模型,然後這個階層狀的模型不像是剛才我說的,我假設每一個state都,都是有一樣的機率的,而是它假設每個state有一個特定的機率,而這個特定的機率是有稍微上層的一個網路,

225
01:13:53,880 --> 01:14:23,840
然後也是用SVM生出來的,然後這個網路,它上面還有一個更上層的網路生出來,然後直到最上面那個階層,它就是一個,嗯,沒有我剛才說的那種,呃,故意假設,呃,誰是誰生出來,就是一個uniform的一個,嗯,參數分佈這樣,然後,嗯,以往就有工作說,啊,如果你這樣子用一個階層化的網路去fit它的話,你就會得到,嗯,更好的products,

226
01:14:23,880 --> 01:14:53,840
然後我們有跟它做比較這樣子,然後有一個interlude,就是剛才說的這個hierarchical SVM,其實也可以用在topic modeling,然後就是它就比LDA在在數學上更好,就蠻有趣的,然後,嗯,我們有對我們的系統做resolution limit,就是我們故意用很多這種bipartite clique,它每一個小網路是bipartite,可是它的數量很多,然後我們想要看看,呃,比如說如果數量少的話,

227
01:14:53,880 --> 01:15:23,680
它應該可以分到我要的數字,可是數量越來越多的話,它是不是會多到某個程度,我就看不到這麼小的bipartite clique,然後我們就有把它跟,嗯,general model做比較,然後,嗯,知道說它大概界線在哪裡這樣,然後我們也,嗯,去看看它,因為我們的,嗯,貢獻有那個prior跟那個search,然後我們就比較了一些模型,然後變一下參數,然後我們,嗯,

228
01:15:23,680 --> 01:15:53,640
生成左邊的這個easy test跟右邊那個hard test,嗯,作為,嗯,模型的網路,然後我們看看它的,嗯,最後,嗯,找到的那個社群的結果,因為,因為那個easy test跟hard test的那個網路是我,是我plant上去的,是我,是我人造建出來,然後就知道說我的方法到底好不好,然後我們又做相關的比較,然後在真實網路上面呢,我們就有跟剛才說的那些方法比較,然後我們就有得到一些結果,

229
01:15:53,680 --> 01:16:23,640
就是,嗯,如果網路比較小的話,bipartite SBM通常會有比較好的那個posterior,這樣,嗯,這邊要比較的是數值越小越好,因為,嗯,這時候的那個sigma叫做,就是那個entropy,然後如果是比較大的網路,而,嗯,bipartite SBM比較好的話,那這個網路通常比較技術,我們得到這樣的結果,然後就可以看paper這樣,然後,嗯,這裡面我。

230
01:16:23,680 --> 01:16:53,080
做了很少,嗯,嗯,所謂的預測節點是什麼樣的的的事情,是,然後我們有一個,嗯,配合一個view,就是,嗯,一個David Walpert,嗯,證明了一個定理叫做沒有白吃午餐定理,他是說呢,嗯,嗯,他做了一個蠻強的陳述,然後呢,是說呢,嗯,嗯,我們有各種不同的。

231
01:16:54,360 --> 01:17:00,880
機器學習的模型,然後,嗯,所有的機器學習的模型,如果你把它的。

232
01:17:02,280 --> 01:17:13,360
嗯,他的performance對他的input data做平均的話,那麼所有的,嗯,機器學習的模型,他們的。

233
01:17:14,160 --> 01:17:30,360
嗯,performance都一樣,平均來講,對於所有的平均來講都一樣,所以,嗯,換句話說,嗯,我們做這個問題的觀點是比較模型本身,而,呃,沒有特別去看說是不是在哪些data,他要表現的比較好。

234
01:17:31,120 --> 01:17:31,920
嗯。

235
01:17:32,520 --> 01:17:42,720
所以,嗯,比如說有一個機器學習模型,他可能對某些類型的數據,他預測的很好,但是他可能對別的類型數據就預測的不好。

236
01:17:44,400 --> 01:17:47,480
所以就每個人賣的東西,就說他自己賣的東西比較好。

237
01:17:49,040 --> 01:17:58,440
好,然後差不多結束了,嗯,我們學到什麼東西呢?就是,嗯,如果我們有好的constraint的話,那我們就有比較節約的模型。

238
01:17:59,040 --> 01:18:15,000
然後,嗯,我們設計的,我們設計的方法呢,是,嗯,就沒有真的要預測什麼東西,是一個比較軟的模型,然後他可以,可以,呃,看分佈,可以看,可以看,嗯,參數的出現的機率這樣。

239
01:18:16,080 --> 01:18:20,280
然後,而且呢,嗯,嗯,就有很多新方法的可能性。

240
01:18:20,400 --> 01:18:20,880
對。

241
01:18:22,240 --> 01:18:33,920
然後,我們可以怎麼做呢?我們可以有什麼corporate free structure,是某種類型的structure,然後,嗯,我們也可以看一開始SBM的假設,然後有一些工作。

242
01:18:35,120 --> 01:18:41,760
然後,嗯,剛才說的那個B跟B的那個landscape,嗯,我們也可以看他的landscape。

243
01:18:41,760 --> 01:19:11,720
然後,有一些近期的工作,然後,嗯,最後呢,嗯,就是,嗯,最近有,有那個mcmc algorithm,他是可以直接把群分開或把群merge,然後通常效率就很高,但是很有趣的事情是,嗯,雖然說他,嗯,可以保證,就是數學上保證他有比較好的mixing time,就是他mcmc收斂的比較快,在特定的一些網路裡面,

244
01:19:11,760 --> 01:19:41,400
他,嗯,嗯,收,收斂了以後,可是呢,嗯,收斂的那個結果就,嗯,就是他,他,我們有,我們有,嗯,就是這個工作有說呢,就是,嗯,某些類型的數據,雖然你有比較好的mcmc algorithm可以去fit他,可是呢,嗯。

245
01:19:41,920 --> 01:19:58,000
常常還需要很長的收斂時間,所以換句話說呢,就是,嗯,可能有些數據是不適合用sbm來fit的,雖然sbm的假設看起來很,很,很,很簡單,這樣,所以就有那個model consistency的問題。

246
01:19:58,000 --> 01:20:25,360
好,然後呢,嗯,嗯,你也可以去問說,網路如果有變化的話,那麼,呃,在變化的時候,那個節點的分佈是用什麼樣的,呃,可能會不會有什麼physical load在高本,這樣,所以這是如果你getting網路的兩個snapshot,然後中間會變化,你就可以說,你要interpolation,這樣找出一個模型去fit中間的變化。

247
01:20:26,320 --> 01:20:35,160
好,然後,嗯,因為我們的工作很跟不同數據有關,所以我覺得一個takeaway就是要be curious about new experiments and applications。

248
01:20:35,160 --> 01:20:50,640
好,然後我最後有一個slide,就是我要感謝,嗯,這些人是我,嗯,嗯,從還沒有申請博士班到後來進博士班認識的人,然後很多人都跟這個工作有關係這樣。

249
01:20:51,640 --> 01:21:01,400
然後,嗯,Dan是我的指導老師,然後右下角的那個Joshua是我另外一個指導老師,然後,嗯,右邊的是那個派修圖。

250
01:21:03,120 --> 01:21:18,200
好,然後,嗯,最後我說我想要給一些就是,嗯,這個就就給大家看看這個社群這樣,然後,嗯,我自己有,嗯,參與了一個project叫做臺灣網路科學教育。

251
01:21:19,200 --> 01:21:37,200
目前就是一個網站,但是我希望裡面可以放很多跟臺灣有關的網路數據,因為,嗯,發展一個模型,你需要跟數據一起看這樣,然後,嗯,希望大家可以給我建議關於這個網站可能可以怎麼走,會影響比較多人這樣。

252
01:21:37,720 --> 01:21:54,920
然後我們翻譯了兩本小冊子,叫做《複雜系統,沒說你不知道》跟《網路素養》。我特別把網路跟網絡分開,因為我覺得網路是internet,可是網絡是,嗯,complex networks,雖然比較不是臺灣常用的詞,這樣。

253
01:21:54,920 --> 01:22:15,680
然後這個NetShred是,嗯,是一個收集各種網路數據的一個網站,這樣,然後,嗯,也有跟Taiwan很像的,嗯,seminar,就是Women in Network Science Seminar,然後,嗯,通常是每週演講,大家可以去去去訂閱他們的訊息這樣。

254
01:22:16,440 --> 01:22:43,480
然後,嗯,右下角這個NetShred是一個conference,它,嗯,很特別的,跟很多CS conference很不一樣的地方是,嗯,很多,嗯,去那邊投稿的人,嗯,他,嗯,那個工作可能不是一個完成的一個工作,然後他,嗯,社群還蠻活絡的,大家比較願意投一個,嗯,還沒有完全做完的工作,可是去那邊跟大家交換意見。

255
01:22:43,920 --> 01:23:07,060
然後那個NetShred這個conference,它有一些symposium,然後我覺得跟,嗯,promoting network science有關的是這兩個,這樣,比如說,嗯,這個network science in education,這個,嗯,可能就會講到很多,嗯,在教網路科學的實作面,甚至有,嗯,高中這個階段的教學,然後還有diversify NetShred,這樣。

256
01:23:07,380 --> 01:23:34,700
然後至於右上角呢,這個是一個那個,嗯,society of young network scientist,他,嗯,只要自認為,就是你覺得是,你是一樣,你就可以,你就可以加入這樣,然後,嗯,現在大部分是early career的professor,然後postdoc跟PhD學生這樣,然後我們有一個slack,如果你們有人想要加入的話,可以email給我,然後我的報告就到這邊結束,不好意思,超過了一個小時。

257
01:23:34,700 --> 01:24:04,660
好,沒關係,那個,那在開放問問題之前呢,讓我先代表大家用很熱烈的掌聲,感謝一下紫棋今天非常內容豐富的演講,OK,好,那謝謝大家,請問大家有沒有什麼問題,我覺得我覺得內容非常非常的豐富,而且你等一下讓大家想一下有什麼問題,然後讓我先問一個,這個是跟內容比較,請問一下你這麼多東西,你是寫在一篇裡面哦?

258
01:24:04,700 --> 01:24:18,700
嗯,我不知道是不是我講的關係,讓他變得很多,但是他是在一篇裡面,然後那篇有17頁,包含reference。

259
01:24:34,700 --> 01:24:46,700
那都是福利的貢獻,但是他們是互相協助,沒有錯啦,anyway,反正我只是覺得非常厲害,嗯,好,那大家有問題嗎?

260
01:24:47,380 --> 01:25:12,700
我想要問問題,嗯,你說直接說就好,聽得到嗎?有有有,好,非常非常感謝紫棋的分享,然後我覺得很受用,而且這個教育的意義很大,就是你不是只是從你這篇文章在做什麼,就是你從一開始這個脈絡整個很完整的敘述下來,我覺得非常棒,那有兩個問題想要請教,

261
01:25:12,700 --> 01:25:40,700
第一個是,嗯,因為我沒有看過這篇文章,就是我想要請問一下你們有沒有在測試有synthetic或者是simulated network,就是你們自己造一些,嗯,你們已經控制好他的這個參數的這個network,就像你們不是從實證上面去搜資料來的,你們是synthetic來的,然後你們去測哪些狀況之下你們的演算法表現比較好,哪些狀況下你們表現比較沒那麼好,

262
01:25:40,700 --> 01:26:09,700
因為你說我get得到的一個point是,你們在maximize posterior或是minimize那個description length,那你們是不是只用實證資料還是你們有synthetic,這是第一個,嗯,那第二個是,我在想這個未來的可以延伸的方向,那因為你是重視在bipartite networks嘛,那如果bipartite networks有weighted,就是說他有次數的,一個人去提了議案幾次,

263
01:26:09,700 --> 01:26:33,700
或者是看了幾次電影,或者是給他幾顆評價,那或者是signed,就是說他有正負號,或是甚至是temporal,那這種模型可不可以延伸,因為我這邊有一筆國會的資料,他就是說提案的立委跟議案,然後他就是一個weighted,然後他還有signed,因為他有分屬不同的政黨,

264
01:26:33,700 --> 01:26:41,700
那我不知道說直接套用你這個研究的框架或是package能不能使用,好,這兩個問題,謝謝。

265
01:26:42,700 --> 01:27:04,700
好,第一個問題比較容易回答,是在我這個工作裡面有用synthetic network,然後我最直接想到的是,我這邊講的比較快的一個投影片,就是我希望知道他到底可以支撐到什麼程度,

266
01:27:04,700 --> 01:27:30,700
所以呢,這個橫軸就是我的synthetic network的noise,然後那個adjacency matrix就是他的結構長那樣子,可是你看了很多雜訊的點,就是我就故意讓那個雜訊的點從小到多,然後我想要看他到什麼程度會不能,對,這是我想到的,有用到synthetic network的地方。

267
01:27:30,700 --> 01:27:58,700
然後,第二個問題的話,據我所知,現在還沒有同時有bipartite,就是現在我說的這種,我這篇paper的這些prior,再加上weighted的edge或是signed edge的SBM,但是現在已經有很好的weighted SBM。

268
01:28:00,700 --> 01:28:29,700
而且有不是太差的prior,來,馬上可以用,就是在那個graph tool裡面,然後也是那個,然後但是那裡面沒有,不對,就是那裡面呢,他對於那個edge,他是weighted edge,然後他可以是weighted,也可以是categorical,所以framework是一樣的。

269
01:28:30,700 --> 01:28:59,700
因為他是weighted或是categorical,他就一樣有相應的prior在裡面,然後就直接可以用,所以我不知道我覺得,我不知道如果故意把那種已經有的weighted SBM裡面bipartite的地方再調整一下,調到說我真的要用到一個weighted bipartite structure會有多大的改進。

270
01:28:59,700 --> 01:29:09,700
不然的話,我覺得這是一個工作沒有錯,但是我覺得直接off the shell在那個graph tool裡面就有可以用的,而且我覺得也應該會很不錯。

271
01:29:10,700 --> 01:29:20,700
所以再請問一下,你說的那個是off the shell是他已經有weighted,可是沒有bipartite,還是有weighted,然後還有bipartite?

272
01:29:50,700 --> 01:29:55,700
沒有被大家了解的例子之外,我覺得hierarchical會比較好。

273
01:29:56,700 --> 01:30:11,700
對,了解,那我最後一句話是,我們這邊有實證資料,是臺灣國會的立委的提案連署資料,如果你想要extend到weighted bipartite的話,歡迎你來做,然後我們資料可以讓你做實證研究。

274
01:30:11,700 --> 01:30:22,700
因為我想說你應該是差幾步就可以推廣到weighted,或者甚至是sign,那如果你有興趣延伸的話,我們可以在這方面合作,謝謝。

275
01:30:41,700 --> 01:30:44,700
可以啊。

276
01:31:12,700 --> 01:31:14,700
謝謝。

277
01:31:14,700 --> 01:31:16,700
謝謝。

